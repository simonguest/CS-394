[
  {
    "objectID": "src/01/notebooks/word2vec.html",
    "href": "src/01/notebooks/word2vec.html",
    "title": "Word2Vec",
    "section": "",
    "text": "This notebook explores Word2Vec embeddings to understand how they capture semantic relationships.\nUses pre-trained embeddings from Google News (trained on ~100 billion words).\n# Install required packages\n!uv pip install gensim numpy matplotlib scikit-learn -q",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#load-pretrained-word2vec-model",
    "href": "src/01/notebooks/word2vec.html#load-pretrained-word2vec-model",
    "title": "Word2Vec",
    "section": "Load Pretrained Word2Vec Model",
    "text": "Load Pretrained Word2Vec Model\n\nimport gensim.downloader as api\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load pre-trained Word2Vec model (Google News, 300-dimensional vectors)\nprint(\"Loading Word2Vec model...\")\nmodel = api.load('word2vec-google-news-300')\nprint(f\"Model loaded. Vocabulary size: {len(model)} words\")\nprint(f\"Vector dimension: {model.vector_size}\") # type: ignore\n\nLoading Word2Vec model...\nModel loaded. Vocabulary size: 3000000 words\nVector dimension: 300\n\n\n\nword = \"cat\"\nvector = model[word] # type: ignore\nvector[:10]\n\narray([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656],\n      dtype=float32)\n\n\n\nword = \"dog\"\nvector = model[word] # type: ignore\nvector[:10]\n\narray([ 0.05126953, -0.02233887, -0.17285156,  0.16113281, -0.08447266,\n        0.05737305,  0.05859375, -0.08251953, -0.01538086, -0.06347656],\n      dtype=float32)\n\n\n\nword = \"pizza\"\nvector = model[word] # type: ignore\nvector[:10]\n\narray([-1.2597656e-01,  2.5390625e-02,  1.6699219e-01,  5.5078125e-01,\n       -7.6660156e-02,  1.2890625e-01,  1.0253906e-01, -3.9482117e-04,\n        1.2158203e-01,  4.3212891e-02], dtype=float32)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#find-similar-words",
    "href": "src/01/notebooks/word2vec.html#find-similar-words",
    "title": "Word2Vec",
    "section": "Find Similar Words",
    "text": "Find Similar Words\nWords with similar meanings have similar vectors.\n\ndef find_similar_words(word, top_n=10):\n    \"\"\"Find the most similar words to a given word.\"\"\"\n    try:\n        similar = model.most_similar(word, topn=top_n) # type: ignore\n        print(f\"\\nWords most similar to '{word}':\")\n        print(\"-\" * 40)\n        for similar_word, similarity in similar:\n            print(f\"{similar_word:20s} | similarity: {similarity:.4f}\")\n    except KeyError:\n        print(f\"Word '{word}' not in vocabulary\")\n\n\nfind_similar_words(\"cat\")\nfind_similar_words(\"dog\")\nfind_similar_words(\"pizza\")\n\n\nWords most similar to 'cat':\n----------------------------------------\ncats                 | similarity: 0.8099\ndog                  | similarity: 0.7609\nkitten               | similarity: 0.7465\nfeline               | similarity: 0.7326\nbeagle               | similarity: 0.7151\npuppy                | similarity: 0.7075\npup                  | similarity: 0.6934\npet                  | similarity: 0.6892\nfelines              | similarity: 0.6756\nchihuahua            | similarity: 0.6710\n\nWords most similar to 'dog':\n----------------------------------------\ndogs                 | similarity: 0.8680\npuppy                | similarity: 0.8106\npit_bull             | similarity: 0.7804\npooch                | similarity: 0.7627\ncat                  | similarity: 0.7609\ngolden_retriever     | similarity: 0.7501\nGerman_shepherd      | similarity: 0.7465\nRottweiler           | similarity: 0.7438\nbeagle               | similarity: 0.7419\npup                  | similarity: 0.7407\n\nWords most similar to 'pizza':\n----------------------------------------\npizzas               | similarity: 0.7863\nDomino_pizza         | similarity: 0.7343\nPizza                | similarity: 0.6988\npepperoni_pizza      | similarity: 0.6903\nsandwich             | similarity: 0.6840\nburger               | similarity: 0.6570\nsandwiches           | similarity: 0.6495\ntakeout_pizza        | similarity: 0.6492\ngourmet_pizza        | similarity: 0.6401\nmeatball_sandwich    | similarity: 0.6377",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#compute-similarity",
    "href": "src/01/notebooks/word2vec.html#compute-similarity",
    "title": "Word2Vec",
    "section": "Compute Similarity",
    "text": "Compute Similarity\n\ndef compute_similarity(word1, word2):\n    \"\"\"Compute cosine similarity between two words.\"\"\"\n    try:\n        similarity = model.similarity(word1, word2) # type: ignore\n        print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n    except KeyError as e:\n        print(f\"Word not in vocabulary: {e}\")\n\n\ncompute_similarity('cat', 'dog')\ncompute_similarity('cat', 'kitten')\ncompute_similarity('cat', 'car')\ncompute_similarity('doctor', 'hospital')\ncompute_similarity('king', 'queen')\n\nSimilarity between 'cat' and 'dog': 0.7609\nSimilarity between 'cat' and 'kitten': 0.7465\nSimilarity between 'cat' and 'car': 0.2153\nSimilarity between 'doctor' and 'hospital': 0.5143\nSimilarity between 'king' and 'queen': 0.6511",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#vector-arithmetic",
    "href": "src/01/notebooks/word2vec.html#vector-arithmetic",
    "title": "Word2Vec",
    "section": "Vector Arithmetic",
    "text": "Vector Arithmetic\n\ndef vector_arithmetic(positive, negative, top_n=5):\n    \"\"\"Perform vector arithmetic: positive words - negative words.\"\"\"\n    try:\n        result = model.most_similar(positive=positive, negative=negative, topn=top_n) # type: ignore\n        print(f\"\\n{' + '.join(positive)} - {' - '.join(negative)}:\")\n        print(\"-\" * 50)\n        for word, similarity in result:\n            print(f\"{word:20s} | similarity: {similarity:.4f}\")\n    except KeyError as e:\n        print(f\"Word not in vocabulary: {e}\")\n\n\nvector_arithmetic(['king', 'woman'], ['man'])\nvector_arithmetic(['Paris', 'Italy'], ['France'])\nvector_arithmetic(['walking', 'swim'], ['walk'])\n\n\nking + woman - man:\n--------------------------------------------------\nqueen                | similarity: 0.7118\nmonarch              | similarity: 0.6190\nprincess             | similarity: 0.5902\ncrown_prince         | similarity: 0.5499\nprince               | similarity: 0.5377\n\nParis + Italy - France:\n--------------------------------------------------\nMilan                | similarity: 0.7222\nRome                 | similarity: 0.7028\nPalermo_Sicily       | similarity: 0.5968\nItalian              | similarity: 0.5911\nTuscany              | similarity: 0.5633\n\nwalking + swim - walk:\n--------------------------------------------------\nswimming             | similarity: 0.8246\nswam                 | similarity: 0.6807\nswims                | similarity: 0.6538\nswimmers             | similarity: 0.6495\npaddling             | similarity: 0.6424",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#d-visualization",
    "href": "src/01/notebooks/word2vec.html#d-visualization",
    "title": "Word2Vec",
    "section": "2D Visualization",
    "text": "2D Visualization\n\ndef visualize_words(words, method='tsne'):\n    \"\"\"Visualize word embeddings in 2D.\"\"\"\n    # Get vectors for words that exist in vocabulary\n    valid_words = [w for w in words if w in model]\n    if len(valid_words) &lt; 2:\n        print(\"Need at least 2 valid words to visualize\")\n        return\n    \n    vectors = np.array([model[w] for w in valid_words])\n    \n    # Reduce to 2D\n    if method == 'tsne':\n        reducer = TSNE(n_components=2, random_state=42, perplexity=min(5, len(valid_words)-1))\n    else:\n        reducer = PCA(n_components=2, random_state=42)\n    \n    vectors_2d = reducer.fit_transform(vectors)\n    \n    # Plot\n    plt.figure(figsize=(12, 8))\n    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=200, alpha=0.6)\n    \n    for i, word in enumerate(valid_words):\n        plt.annotate(word, \n                    xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n                    xytext=(5, 5),\n                    textcoords='offset points',\n                    fontsize=12,\n                    fontweight='bold')\n    \n    plt.title(f'Word Embeddings Visualization ({method.upper()})', fontsize=16)\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\nwords = ['cat', 'dog', 'kitten', 'puppy', 'lion', 'tiger', 'elephant', 'mouse', 'chicken', 'rat']\nvisualize_words(words)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/sentence-transformer.html",
    "href": "src/01/notebooks/sentence-transformer.html",
    "title": "Sentence Transformer",
    "section": "",
    "text": "In this notebook, we use the SentenceTransformer model to encode various sentences and test the cosine similarity between them.\n     \n\n# Install required packages\n!uv pip install sentence-transformers -q\n\n\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n\nembeddings = model.encode(\"The cat sat on the mat\")\nembeddings[:10]\n\narray([ 0.13040183, -0.01187013, -0.02811704,  0.05123864, -0.05597447,\n        0.03019154,  0.0301613 ,  0.02469837, -0.01837057,  0.05876679],\n      dtype=float32)\n\n\n\nembeddings = model.encode(\"The dog rested on the rug\")\nembeddings[:10]\n\narray([ 0.05627272,  0.02632686,  0.05896206,  0.12019245, -0.00399702,\n        0.08970873, -0.02332847, -0.01548103,  0.00939427,  0.01598458],\n      dtype=float32)\n\n\n\nembeddings = model.encode(\"I love pizza!\")\nembeddings[:10]\n\narray([-0.09438416,  0.02385838,  0.00920313,  0.04992779, -0.09533099,\n        0.0061356 ,  0.03513189,  0.00850056,  0.0105693 , -0.0578883 ],\n      dtype=float32)\n\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsentences = ['The cat sat on the mat', \n             'The dog rested on the rug',\n             'I love pizza']\nembeddings = model.encode(sentences)\n\nprint(cosine_similarity(embeddings))\n\n[[ 1.0000002   0.47530937  0.00155361]\n [ 0.47530937  1.         -0.04451237]\n [ 0.00155361 -0.04451237  1.        ]]",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "sentence-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/GPT-2.html",
    "href": "src/01/notebooks/GPT-2.html",
    "title": "Pre-trained GPT-2 Notebook",
    "section": "",
    "text": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load pre-trained GPT-2 model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n\nimport torch\n\ndef autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n    # Encode the prompt\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n\n    # Generate continuation\n    with torch.no_grad():\n        output = model.generate(\n            input_ids,\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n\n\nprompts = [\n    \"Mary had a little lamb\",\n    \"The future of artificial intelligence\",\n    \"In a galaxy far, far away\",\n    \"DigiPen is a place where\",\n    \"def calculate_fibonacci(n):\"\n]\n\nfor prompt in prompts:\n    print(f\"\\nPrompt: {prompt}\")\n    print(\"-\" * 50)\n    completion = autocomplete(prompt, max_length=80)\n    print(f\"Output: {completion}\\n\")\n\n\nPrompt: Mary had a little lamb\n--------------------------------------------------\nOutput: Mary had a little lamb on her lap and a little black bean, but I think she was just as happy to eat it as I was to drink it. She was quite surprised when I told her I had eaten it.\n\n\"I've been saying that for a while now, but I've been talking to you all the time.\"\n\n\"I didn't have any tea,\" said\n\n\nPrompt: The future of artificial intelligence\n--------------------------------------------------\nOutput: The future of artificial intelligence is not so much a matter of what will happen to humans but rather what we do with them.\n\nIn short, what's in your fridge?\n\nWhat's in your fridge?\n\nWhat's in your fridge?\n\nWhat's in your fridge?\n\nWhat's in your fridge?\n\nWhat's in your fridge?\n\nWhat's\n\n\nPrompt: In a galaxy far, far away\n--------------------------------------------------\nOutput: In a galaxy far, far away, where the universe is a vast expanse of nothingness, the only place where life exists is at the bottom of the ocean.\n\nThe only place where life exists is at the bottom of the ocean.\n\nThe only place where life exists is at the bottom of the ocean.\n\nThe only place where life exists is at the bottom of the\n\n\nPrompt: DigiPen is a place where\n--------------------------------------------------\nOutput: DigiPen is a place where you can get the most out of your pen. It's not just about writing with pen tools or with pens. The main thing you will notice is that the pen is not just a pen, it's a pen. It's a pen.\n\nThe pen is a pen. It's a pen.\n\nThe pen is a pen. It's a\n\n\nPrompt: def calculate_fibonacci(n):\n--------------------------------------------------\nOutput: def calculate_fibonacci(n): # calculate the Fibonacci number 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "GPT-2.ipynb"
    ]
  },
  {
    "objectID": "src/01/resources.html",
    "href": "src/01/resources.html",
    "title": "Week 1 Resources",
    "section": "",
    "text": "A Visual Introduction to Vector Embeddings, Pamela Fox\nA Visual Exploration of Vectors, Pamela Fox\n\n\n\n\n\nGoogle Colab Sign-up Page\nProject Jupyter Page\n\n\n\n\n\nLearn Python with Jupyter, Serena Bonaretti\n\n\n\n\n\nThe Illustrated Transformer",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#vector-embeddings",
    "href": "src/01/resources.html#vector-embeddings",
    "title": "Week 1 Resources",
    "section": "",
    "text": "A Visual Introduction to Vector Embeddings, Pamela Fox\nA Visual Exploration of Vectors, Pamela Fox",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#introduction-to-notebooks",
    "href": "src/01/resources.html#introduction-to-notebooks",
    "title": "Week 1 Resources",
    "section": "",
    "text": "Google Colab Sign-up Page\nProject Jupyter Page",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#learning-python",
    "href": "src/01/resources.html#learning-python",
    "title": "Week 1 Resources",
    "section": "",
    "text": "Learn Python with Jupyter, Serena Bonaretti",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#transformers",
    "href": "src/01/resources.html#transformers",
    "title": "Week 1 Resources",
    "section": "",
    "text": "The Illustrated Transformer",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/slides.html#course-description",
    "href": "src/00/slides.html#course-description",
    "title": "Welcome to CS-394/594!",
    "section": "Course Description",
    "text": "Course Description\n\nHow Generative AI Works focuses on the practical implementation of generative AI within custom software applications and games.\nThe course covers neural network architectures, including the impact of the Transformer model, customization of large language models across multiple vendors using APIs, and experimentation with multimodal models for image and audio recognition and generation.",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#course-description-1",
    "href": "src/00/slides.html#course-description-1",
    "title": "Welcome to CS-394/594!",
    "section": "Course Description",
    "text": "Course Description\n\nHands-on experience includes working with both hosted and locally run models, integrating AI with game engines such as Unity and Unreal, and developing AI agents that extend beyond simple chat-based interactions.\nEthical considerations and model evaluation are integrated throughout, emphasizing awareness of broader societal implications.\nThrough lectures, programming assignments, and a final project, the course provides the expertise needed to apply generative AI in creating innovative and interactive experiences.",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#learning-outcomes",
    "href": "src/00/slides.html#learning-outcomes",
    "title": "Welcome to CS-394/594!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the basic working principles and history of current LLMs (Large Language Models)\nUnderstand ethical and safety aspects of using generative models\nEvaluate and test generative models using industry benchmarks\nRun generative models on local, laptop-based hardware (using CPU, GPU, NPUs)",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#learning-outcomes-1",
    "href": "src/00/slides.html#learning-outcomes-1",
    "title": "Welcome to CS-394/594!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCreate AI-based agents and tools based on the MCP (Model Context Protocol) specification\nAvoid hallucinations by increasing the accuracy of models through RAG (Retrieval Augmented Generation) and fine-tuning\nExplore and use multimodal models for image and audio recognition and generation\nCreate and deploy API-based clients, accessing LLMs hosted by different vendors (OpenAI, Meta, Google)",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#overall",
    "href": "src/00/slides.html#overall",
    "title": "Welcome to CS-394/594!",
    "section": "Overall",
    "text": "Overall\n\nProvide a level of understanding beyond where most professional software developers are today\nFocus on augmentation vs. automation\nBuild a cool final project that you can add to your portfolio!",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#syllabus-overview-1",
    "href": "src/00/slides.html#syllabus-overview-1",
    "title": "Welcome to CS-394/594!",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\nTBD",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#schedule",
    "href": "src/00/slides.html#schedule",
    "title": "Welcome to CS-394/594!",
    "section": "Schedule",
    "text": "Schedule\n\nEvery Friday (Curie); 2pm - 4.50pm\n~1.5 hours of lecture, together with hands-on exercises\n~1.5 hours for in-class lab time, working on assignments\nExpectation of after-class work, especially for final project",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#during-class",
    "href": "src/00/slides.html#during-class",
    "title": "Welcome to CS-394/594!",
    "section": "During Class",
    "text": "During Class\n\nStrive for conversation and interactivity\n\nPlease ask questions, even mid-slide!\nI enjoy going off on tangents / on the whiteboard\nUse lab time to seek input / troubleshoot code",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#handing-in-work",
    "href": "src/00/slides.html#handing-in-work",
    "title": "Welcome to CS-394/594!",
    "section": "Handing in Work",
    "text": "Handing in Work\n\nEverything submitted via GitHub\n\nRecommend creating a repo for in-class assignments\nAnd another repo for your final project\nDon’t forget to give me permissions! @simonguest\n\nGraded Course\n\nGeneral rubric for the in-class assignments\nComplete rubric for the final project",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#handing-in-work-1",
    "href": "src/00/slides.html#handing-in-work-1",
    "title": "Welcome to CS-394/594!",
    "section": "Handing in Work",
    "text": "Handing in Work\n\nDeadlines (In-class Assignments)\n\nAssignments are due by the following week’s lesson\ni.e., you get a week for each assignment\nIf you need more time/exception, please reach out via Teams\n\nDeadlines (Final Project)\n\nUp until Week 15 presentations\n(We’ll cover in detail later in the semester)",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#ai-policy",
    "href": "src/00/slides.html#ai-policy",
    "title": "Welcome to CS-394/594!",
    "section": "AI Policy",
    "text": "AI Policy\n\nTBD",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#tools",
    "href": "src/00/slides.html#tools",
    "title": "Welcome to CS-394/594!",
    "section": "Tools",
    "text": "Tools\n\nWe will be introducing many tools\n\nColab Pro, OpenRouter, Hugging Face, etc.\nMost will be free\nExpect to need about $10-25 in OpenRouter API credits",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#languages",
    "href": "src/00/slides.html#languages",
    "title": "Welcome to CS-394/594!",
    "section": "Languages",
    "text": "Languages\n\nWe will be using (and learning) a lot of Python!\n\nMost of the in-class assignments will be in Python\nDon’t worry if you are new to Python as we’ll introduce concepts gradually\n\nAlthough recommend investing extra time (see resources in Week 1)\n\n\nFinal Project\n\nCan be any language\nProbably depending on what you choose to create",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#hardware",
    "href": "src/00/slides.html#hardware",
    "title": "Welcome to CS-394/594!",
    "section": "Hardware",
    "text": "Hardware\n\nWill will be training small models (SLMs) later in the semester\nThis training will require a decent GPU and VRAM\n\nColab Pro (CUDA)\nYour own NVIDIA-based laptop (CUDA)\nPotential of using MLX for any Mac users",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#need-help-1",
    "href": "src/00/slides.html#need-help-1",
    "title": "Welcome to CS-394/594!",
    "section": "Need Help?",
    "text": "Need Help?\n\nEverything we cover will be on my GitHub repo\n\nhttps://simonguest.github.io/CS-394\nSlides (current and prior lectures), Demo code, Resources\n\nOffice Hours\n\nThursdays 1pm - 3pm\nEither on campus or virtually via Teams\n\nTeams\n\nPrimary mechanism for updates, ask questions, request office hours, etc.",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#steep-learning-curve",
    "href": "src/00/slides.html#steep-learning-curve",
    "title": "Welcome to CS-394/594!",
    "section": "Steep Learning Curve",
    "text": "Steep Learning Curve\n\nWe will be using the latest tools and AI models\nLots of new tools, acronyms, frameworks, etc.\nMuch of the curriculum builds upon itself\n\nPlease try not to miss lectures\nAsk for help if you need to catch up",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#new-course-at-digipen",
    "href": "src/00/slides.html#new-course-at-digipen",
    "title": "Welcome to CS-394/594!",
    "section": "New Course at DigiPen!",
    "text": "New Course at DigiPen!\n\nThere may be some minor curriculum tweaks mid-flight\n\nEspecially for topics that need less/more time",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#fast-moving-space",
    "href": "src/00/slides.html#fast-moving-space",
    "title": "Welcome to CS-394/594!",
    "section": "Fast Moving Space",
    "text": "Fast Moving Space\n\nThere will be areas/questions that I don’t have experience of\n\ne.g., multiple new models are launched every week\n\nBut that’s what makes it exciting!",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS-394/594: How Generative AI Works",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CS-394/594 course.",
    "crumbs": [
      "CS-394/594"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "CS-394/594: How Generative AI Works",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CS-394/594 course.",
    "crumbs": [
      "CS-394/594"
    ]
  },
  {
    "objectID": "src/01/assignment.html",
    "href": "src/01/assignment.html",
    "title": "Week 1 Assignment - Build Your Own Text Continuation Style",
    "section": "",
    "text": "Objective: Create a Colab notebook that uses GPT-2 to generate creative text continuations with different “personalities” or styles.\nRequirements:\n\nLoad a pre-trained GPT-2 model (using HuggingFace transformers - same to what was demonstrated in GPT-2.ipynb)\nCreate 3 different prompt templates that guide the model toward different styles (e.g., “Write like a pirate:”, “Explain like I’m 5:”, “As a news reporter:”)\nFor each template, generate text using two different sampling strategies (e.g., greedy vs. temperature sampling)\nCompare and document the outputs (using Markdown in the notebook) - what differences do you notice?\nCreative component: Design your own custom prompt that produces interesting or useful outputs\n\nDeliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation\nOutputs of generated text from GPT-2\nMarkdown cells explaining what each sampling strategy does and any observations\n\nHint:\nFor best results, use a larger GPT-2 model on Colab.\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n\nRemember GPT-2 is a completion model (not instruction tuned), so you’ll need to create a template that provides examples for the model:\n\nprompt_template = \"\"\"Rewrite sentences as a pirate would say them:\n\nNormal: I'm going to the market.\nPirate: Arrr, I be sailin' to the marketplace, matey!\n\nNormal: That's a beautiful ship.\nPirate: Shiver me timbers, that be a fine vessel!\n\nNormal: {user_input}\nPirate:\"\"\"\n\n# Example usage:\nuser_input = \"Can you help me find my keys?\"\nprompt = prompt_template.format(user_input=user_input)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Assignment"
    ]
  },
  {
    "objectID": "src/01/slides.html#lesson-objectives",
    "href": "src/01/slides.html#lesson-objectives",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nExplore the history of vector embeddings and tokenization\nUnderstand the transformer architecture and how it works at a high level\nUse a simple sentence transformer to create vector embeddings and test for similarity\nCover a brief history of early generative transformers\nSetup and use Colab Pro, and start becoming familiar with the basics of notebooks and Python (if you haven’t used them already)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#rewind-to-2013",
    "href": "src/01/slides.html#rewind-to-2013",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Rewind To 2013",
    "text": "Rewind To 2013\n\nNLP (Natural Language Processing) was the thing!\n\nSentiment analysis, named entity recognition, parsing, etc.\n\nBut, you had limited options…\n\nOne-hot encoding\nHand crafted features\nNeural language models",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#word2vec-released",
    "href": "src/01/slides.html#word2vec-released",
    "title": "Week 1: Foundations of Generative AI",
    "section": "2013: Word2Vec Released",
    "text": "2013: Word2Vec Released\n\nWord2Vec introduced by Mikolov and colleagues at Google Research in two papers\n\nSkip-gram and Continuous Bag-of-Words (CBOW) (Mikolov, Chen, et al. 2013)\nNegative sampling and subsampling techniques (Mikolov, Sutskever, et al. 2013)\n\nParadigm shift from count-based methods\n\nUsed Neural Networks (NNs) to predict words vs. large matrices\n\nFoundation for modern NLP tasks",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work",
    "href": "src/01/slides.html#how-does-word2vec-work",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\nWord Embeddings are meaningful numerical representations of words\n\nRepresentations where words (or sentences) are encoded into multi-dimensional space\nLarge number of dimensions (200-500 is typical)\nSimilar words have similar numbers",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-1",
    "href": "src/01/slides.html#how-does-word2vec-work-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"cat\"\nvector = model[word] # type: ignore\nvector[:10]\n\narray([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656],\n      dtype=float32)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-2",
    "href": "src/01/slides.html#how-does-word2vec-work-2",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"dog\"\nvector = model[word] # type: ignore\nvector[:10]\n\narray([ 0.05126953, -0.02233887, -0.17285156,  0.16113281, -0.08447266,\n        0.05737305,  0.05859375, -0.08251953, -0.01538086, -0.06347656],\n      dtype=float32)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-3",
    "href": "src/01/slides.html#how-does-word2vec-work-3",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"pizza\"\nvector = model[word] # type: ignore\nvector[:10]\n\narray([-1.2597656e-01,  2.5390625e-02,  1.6699219e-01,  5.5078125e-01,\n       -7.6660156e-02,  1.2890625e-01,  1.0253906e-01, -3.9482117e-04,\n        1.2158203e-01,  4.3212891e-02], dtype=float32)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#why-do-this",
    "href": "src/01/slides.html#why-do-this",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Why Do This?",
    "text": "Why Do This?\n\nMapping words to multi-dimensional vectors enables\n\nTest for similarity\nCompute similarity\nPerform vector arithmetic\nExplore sets of words through visualizations",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-4",
    "href": "src/01/slides.html#how-does-word2vec-work-4",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nfind_similar_words(\"cat\")\nfind_similar_words(\"dog\")\nfind_similar_words(\"pizza\")\n\n\nWords most similar to 'cat':\n----------------------------------------\ncats                 | similarity: 0.8099\ndog                  | similarity: 0.7609\nkitten               | similarity: 0.7465\nfeline               | similarity: 0.7326\nbeagle               | similarity: 0.7151\npuppy                | similarity: 0.7075\npup                  | similarity: 0.6934\npet                  | similarity: 0.6892\nfelines              | similarity: 0.6756\nchihuahua            | similarity: 0.6710\n\nWords most similar to 'dog':\n----------------------------------------\ndogs                 | similarity: 0.8680\npuppy                | similarity: 0.8106\npit_bull             | similarity: 0.7804\npooch                | similarity: 0.7627\ncat                  | similarity: 0.7609\ngolden_retriever     | similarity: 0.7501\nGerman_shepherd      | similarity: 0.7465\nRottweiler           | similarity: 0.7438\nbeagle               | similarity: 0.7419\npup                  | similarity: 0.7407\n\nWords most similar to 'pizza':\n----------------------------------------\npizzas               | similarity: 0.7863\nDomino_pizza         | similarity: 0.7343\nPizza                | similarity: 0.6988\npepperoni_pizza      | similarity: 0.6903\nsandwich             | similarity: 0.6840\nburger               | similarity: 0.6570\nsandwiches           | similarity: 0.6495\ntakeout_pizza        | similarity: 0.6492\ngourmet_pizza        | similarity: 0.6401\nmeatball_sandwich    | similarity: 0.6377",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-5",
    "href": "src/01/slides.html#how-does-word2vec-work-5",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\ncompute_similarity('cat', 'dog')\ncompute_similarity('cat', 'kitten')\ncompute_similarity('cat', 'car')\ncompute_similarity('doctor', 'hospital')\ncompute_similarity('king', 'queen')\n\nSimilarity between 'cat' and 'dog': 0.7609\nSimilarity between 'cat' and 'kitten': 0.7465\nSimilarity between 'cat' and 'car': 0.2153\nSimilarity between 'doctor' and 'hospital': 0.5143\nSimilarity between 'king' and 'queen': 0.6511",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-6",
    "href": "src/01/slides.html#how-does-word2vec-work-6",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nvector_arithmetic(['king', 'woman'], ['man'])\nvector_arithmetic(['Paris', 'Italy'], ['France'])\nvector_arithmetic(['walking', 'swim'], ['walk'])\n\n\nking + woman - man:\n--------------------------------------------------\nqueen                | similarity: 0.7118\nmonarch              | similarity: 0.6190\nprincess             | similarity: 0.5902\ncrown_prince         | similarity: 0.5499\nprince               | similarity: 0.5377\n\nParis + Italy - France:\n--------------------------------------------------\nMilan                | similarity: 0.7222\nRome                 | similarity: 0.7028\nPalermo_Sicily       | similarity: 0.5968\nItalian              | similarity: 0.5911\nTuscany              | similarity: 0.5633\n\nwalking + swim - walk:\n--------------------------------------------------\nswimming             | similarity: 0.8246\nswam                 | similarity: 0.6807\nswims                | similarity: 0.6538\nswimmers             | similarity: 0.6495\npaddling             | similarity: 0.6424",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-7",
    "href": "src/01/slides.html#how-does-word2vec-work-7",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#challenges-with-word-embeddings",
    "href": "src/01/slides.html#challenges-with-word-embeddings",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Challenges with Word Embeddings",
    "text": "Challenges with Word Embeddings\n\nLarge vocabularies\n\n100K+ words\nAnd not particularly friendly to non-English vocabularies\n\nLittle representation between certain words\n\n“Run” and “Running” should be related\n\nLack of context\n\nEmbedding for the word “bank” is the same, regardless of context\nRiver bank != Savings bank",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#challenges-with-word-embeddings-1",
    "href": "src/01/slides.html#challenges-with-word-embeddings-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Challenges with Word Embeddings",
    "text": "Challenges with Word Embeddings\n\nSome researchers tried character-level models\n\nSmall vocabulary (26 letters + puntuation for English)\nBut very long sequences\nAnd hard to extra meaning",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#byte-pair-encoding-bpe",
    "href": "src/01/slides.html#byte-pair-encoding-bpe",
    "title": "Week 1: Foundations of Generative AI",
    "section": "2016: Byte Pair Encoding (BPE)",
    "text": "2016: Byte Pair Encoding (BPE)\n\nOriginally developed in 1994 as a simple compression algorithm (Gage 1994)\n\nFrequent pairs of adjacent bytes represented as a single byte\n\nIn 2016, adapted to neural machine translation (Sennrich, Haddow, and Birch 2016)\n\nApplied BPE to break words into subword units for better handling of rare words",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#byte-pair-encoding-bpe-1",
    "href": "src/01/slides.html#byte-pair-encoding-bpe-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "2016: Byte Pair Encoding (BPE)",
    "text": "2016: Byte Pair Encoding (BPE)\n\nBreaks words into frequent subword units (a.k.a. tokens)\n\n“unbelievable” → [“un”, “believ”, “able”]\n\nBalance between word level (large vocab) and character level (long sequences)\n\nSupports related words: [“Run”] and [“Run”, “ning”]\nSupports unknown words\n30-50K tokens vs. 100K\nAlso works well for non-English languages",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#search-for-context",
    "href": "src/01/slides.html#search-for-context",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Search for Context",
    "text": "Search for Context\n\nBPE provided efficiency and representation between words\nBut still didn’t solve context\n\ne.g., the River bank != Savings bank problem\n\nResearchers working on “attention tasks” using Recurrent Neural Networks (RNNs)\n\nBahdanau et al. introduce attention for translation (Bahdanau, Cho, and Bengio 2015)\nShowed that focusing on relevant parts of input improved translation quality",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#attention-is-all-you-need",
    "href": "src/01/slides.html#attention-is-all-you-need",
    "title": "Week 1: Foundations of Generative AI",
    "section": "2017: “Attention is all you need”",
    "text": "2017: “Attention is all you need”\n\nGoogle researchers publish “Attention is all you need” (Vaswani et al. 2017)\n\nIntroduced the Transformer a novel Neural Network (NN) architecture, eliminating the need for RNNs for sequence-to-sequence models\nUsed BPE tokenization, and creates contextual embeddings during training process\nAttention mechanism allows the model to weigh the importance of words in a sequence\nAchieved State Of The Art (SOTA) performance on language translation, while also being faster to train",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-1",
    "href": "src/01/slides.html#introducing-the-transformer-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    Transformer[Transformer]\n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Transformer --&gt; Decode --&gt; Output",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-2",
    "href": "src/01/slides.html#introducing-the-transformer-2",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        Encoder[Encoder]\n        Decoder[Decoder]\n        Encoder --&gt; Decoder\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Encoder\n    Decoder --&gt; Decode --&gt; Output",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-3",
    "href": "src/01/slides.html#introducing-the-transformer-3",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        direction LR\n        subgraph \"Encoder Stack (N layers)\"\n            E[Encoder&lt;br/&gt;Layers&lt;br/&gt;1...N]\n        end\n        \n        subgraph \"Decoder Stack (N layers)\"\n            D[Decoder&lt;br/&gt;Layers&lt;br/&gt;1...N]\n        end\n        \n        E -.-&gt;|Context| D\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; E\n    D --&gt; Decode --&gt; Output",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example",
    "href": "src/01/slides.html#example",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\n# @title Load Model\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"Helsinki-NLP/opus-mt-fr-en\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-1",
    "href": "src/01/slides.html#example-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\n# @title Tokenize\n\nfrench_text = \"Bonjour, comment allez-vous?\"\ninput_ids = tokenizer.encode(french_text, return_tensors=\"pt\")\nprint(input_ids[0])\nprint(\"Tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n\ntensor([8703,    2, 1027, 5682,   21,  682,   54,    0])\nTokens: ['▁Bonjour', ',', '▁comment', '▁allez', '-', 'vous', '?', '&lt;/s&gt;']",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-2",
    "href": "src/01/slides.html#example-2",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\n# @title Run through transformer\n\noutput_ids = model.generate(input_ids)\nprint(output_ids)\n\ntensor([[59513, 10537,     2,   541,    52,    55,    54,     0]])",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-the-encoderdecoder-work",
    "href": "src/01/slides.html#how-does-the-encoderdecoder-work",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How Does the Encoder/Decoder Work?",
    "text": "How Does the Encoder/Decoder Work?\noutput_ids = model.generate(input_ids)\n\nTakes input ids, runs through encoder\n\nGenerates contextual vectors using self attention across input tokens\n\nRuns the decoder iteratively to generate one token at a time\n\nUses self attention on previously generated tokens\nUses cross-attention to attend to encoder output\n\nContinues until it generates an end-of-sequence token or hits max length",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-4",
    "href": "src/01/slides.html#introducing-the-transformer-4",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        direction TB\n        \n        subgraph \"Encoder Layer\"\n            direction TB\n            E_SelfAttn[Multi-Head&lt;br/&gt;Self-Attention]\n            E_AddNorm1[Add & Norm]\n            E_FFN[Feed-Forward&lt;br/&gt;Network]\n            E_AddNorm2[Add & Norm]\n            \n            E_SelfAttn --&gt; E_AddNorm1 --&gt; E_FFN --&gt; E_AddNorm2\n        end\n        \n        subgraph \"Decoder Layer\"\n            direction TB\n            D_SelfAttn[Masked Multi-Head&lt;br/&gt;Self-Attention]\n            D_AddNorm1[Add & Norm]\n            D_CrossAttn[Multi-Head&lt;br/&gt;Cross-Attention]\n            D_AddNorm2[Add & Norm]\n            D_FFN[Feed-Forward&lt;br/&gt;Network]\n            D_AddNorm3[Add & Norm]\n            \n            D_SelfAttn --&gt; D_AddNorm1 --&gt; D_CrossAttn --&gt; D_AddNorm2 --&gt; D_FFN --&gt; D_AddNorm3\n        end\n        \n        E_AddNorm2 -.-&gt;|Encoder&lt;br/&gt;Output| D_CrossAttn\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; E_SelfAttn\n    D_AddNorm3 --&gt; Output",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-3",
    "href": "src/01/slides.html#example-3",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\n# @title Decode back to tokens to complete the translation\n\nenglish_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(\"Translation:\", english_text)\n\nTranslation: Hello, how are you?",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#origin-of-gpt-1",
    "href": "src/01/slides.html#origin-of-gpt-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "2018: Origin of “GPT”",
    "text": "2018: Origin of “GPT”\n\nGenerative Pre-trained Transformer\nName coined by OpenAI researchers (Alec Radford et. al) in “Improving Language Understanding by Generative Pre-Training” (Radford et al. 2018)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-gpt",
    "href": "src/01/slides.html#what-is-a-gpt",
    "title": "Week 1: Foundations of Generative AI",
    "section": "What is a GPT?",
    "text": "What is a GPT?\n\n“Decoder-only” architecture\n\nSelf attention is causal/masked - tokens can only attend to previous tokens, not future ones\n\nPre-training objective: Next token prediction\n\nTrained on a massive text corpora\nLearns grammar, facts, reasoning patterns just from this objective",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-gpt-1",
    "href": "src/01/slides.html#what-is-a-gpt-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "What is a GPT?",
    "text": "What is a GPT?\n\nAutoregressive generation\n\nGenerates one token at a time, feeding back each output as input\nTemperature and sampling strategies\nSame prompt can produce different outputs\n\nContext window\n\nFixed maximum length (2048 for GPT-2)\nEverything must fit within this window during generation\nIntroduced the concept of “context” vs. “knowledge” (prompt vs. training)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#gpt-2",
    "href": "src/01/slides.html#gpt-2",
    "title": "Week 1: Foundations of Generative AI",
    "section": "GPT-2",
    "text": "GPT-2\n\nReleased in 2019 by OpenAI\n\nInitially, only 117M param model released in Feb 2019 due to safety concerns\nStaged releases throughout the year, 1.5B in Nov 2019\n\nTrained on WebText, 8 million web pages/40GB of text\nZero-shot task performance\n\nDid well on translation, summarization, and question answering without task-specific training",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-4",
    "href": "src/01/slides.html#example-4",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load pre-trained GPT-2 model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-5",
    "href": "src/01/slides.html#example-5",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nimport torch\n\ndef autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n    # Encode the prompt\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n\n    # Generate continuation\n    with torch.no_grad():\n        output = model.generate(\n            input_ids,\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-6",
    "href": "src/01/slides.html#example-6",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nprompts = [\n    \"Mary had a little lamb\",\n    \"The future of artificial intelligence\",\n    \"In a galaxy far, far away\",\n    \"DigiPen is a place where\",\n    \"def calculate_fibonacci(n):\"\n]\n\nfor prompt in prompts:\n    print(f\"\\nPrompt: {prompt}\")\n    print(\"-\" * 50)\n    completion = autocomplete(prompt, max_length=80)\n    print(f\"Output: {completion}\\n\")\n\n\nPrompt: Mary had a little lamb\n--------------------------------------------------\nOutput: Mary had a little lamb on her lap and a little black bean, but I think she was just as happy to eat it as I was to drink it. She was quite surprised when I told her I had eaten it.\n\n\"I've been saying that for a while now, but I've been talking to you all the time.\"\n\n\"I didn't have any tea,\" said\n\n\nPrompt: The future of artificial intelligence\n--------------------------------------------------\nOutput: The future of artificial intelligence is not so much a matter of what will happen to humans but rather what we do with them.\n\nIn short, what's in your fridge?\n\nWhat's in your fridge?\n\nWhat's in your fridge?\n\nWhat's in your fridge?\n\nWhat's in your fridge?\n\nWhat's in your fridge?\n\nWhat's\n\n\nPrompt: In a galaxy far, far away\n--------------------------------------------------\nOutput: In a galaxy far, far away, where the universe is a vast expanse of nothingness, the only place where life exists is at the bottom of the ocean.\n\nThe only place where life exists is at the bottom of the ocean.\n\nThe only place where life exists is at the bottom of the ocean.\n\nThe only place where life exists is at the bottom of the\n\n\nPrompt: DigiPen is a place where\n--------------------------------------------------\nOutput: DigiPen is a place where you can get the most out of your pen. It's not just about writing with pen tools or with pens. The main thing you will notice is that the pen is not just a pen, it's a pen. It's a pen.\n\nThe pen is a pen. It's a pen.\n\nThe pen is a pen. It's a\n\n\nPrompt: def calculate_fibonacci(n):\n--------------------------------------------------\nOutput: def calculate_fibonacci(n): # calculate the Fibonacci number 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-notebook",
    "href": "src/01/slides.html#what-is-a-notebook",
    "title": "Week 1: Foundations of Generative AI",
    "section": "What is a Notebook?",
    "text": "What is a Notebook?\n\nAn interactive document that combines:\n\nLive code that can be executed\nRich text explanations (markdown)\nVisualizations and outputs\n\nThink of it as a computational narrative\n\nTell a story with code, data, and explanations\n\nOriginally designed for data science and research\nAlso used for learning, experimenting, and sharing results",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#a-brief-history-of-notebooks",
    "href": "src/01/slides.html#a-brief-history-of-notebooks",
    "title": "Week 1: Foundations of Generative AI",
    "section": "A Brief History of Notebooks",
    "text": "A Brief History of Notebooks\n\n2011: IPython Notebook project begins\n\nInteractive Python shell → web-based notebook\n\n2014: Renamed to Jupyter (Julia, Python, R)\n\nNow supports 40+ programming languages\n\n2017: Google launches Colab\n\nFree cloud-based Jupyter notebooks\nFree access to GPUs and TPUs\n\nToday: Industry standard for ML/AI development",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#anatomy-of-a-notebook",
    "href": "src/01/slides.html#anatomy-of-a-notebook",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Anatomy of a Notebook",
    "text": "Anatomy of a Notebook\n\nFormat: Extension is .ipynb\n\nJSON format, using Jupyter Document Schema\n\nCells: Building blocks of notebooks\n\nCode cells: Executable Python code\nMarkdown cells: Text, headings, images, equations\n\nKernel: The computational engine running your code\n\nMaintains state between cell executions\n\nOutputs: Results appear directly below code cells\n\nText, tables, plots, interactive widgets",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-to-run-notebooks",
    "href": "src/01/slides.html#how-to-run-notebooks",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How to Run Notebooks",
    "text": "How to Run Notebooks\n\nJupyter Notebook Server (Classic approach)\n\nWeb interface on localhost\n\nVS Code (Local development)\n\nJupyter extension for VS Code\nRun on your own machine\n\nGoogle Colab (Recommended)\n\nBrowser-based, no installation needed\nFree(-ish) GPU access\nCan also access local GPU",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#advantages-of-google-colab",
    "href": "src/01/slides.html#advantages-of-google-colab",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Advantages of Google Colab",
    "text": "Advantages of Google Colab\n\nAccess to GPUs and TPUs for AI-based tasks\n\ne.g., A100 and H100 with 40Gb/80Gb VRAM\n\nModel downloaded between cloud vendors\n\nvs. downloading large models via the DigiPen network\n\nMany libraries pre-installed\nEasy to share notebooks with others\nGenerous (free) GPU limits for students!",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#references-1",
    "href": "src/01/slides.html#references-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "References",
    "text": "References\n\n\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. “Neural Machine Translation by Jointly Learning to Align and Translate.” In International Conference on Learning Representations. https://arxiv.org/abs/1409.0473.\n\n\nGage, Philip. 1994. “A New Algorithm for Data Compression.” The C Users Journal 12 (2): 23–38.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” In International Conference on Learning Representations. https://arxiv.org/abs/1301.3781.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” In Advances in Neural Information Processing Systems, 3111–19. https://arxiv.org/abs/1310.4546.\n\n\nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.” https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf.\n\n\nSennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1715–25. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1162.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems. Vol. 30.",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html",
    "href": "src/01/notebooks/hello-world.html",
    "title": "Hello World Notebook!",
    "section": "",
    "text": "This is an example of the Jupyter .ipynb document format\n# This is an executable cell\nprint(\"Hello World!\")\n\nHello World!\n# Setting variables in Python\nx = 42\nx\n\n42\n# Variables persist after being set in previously executed cells\nx\n\n42",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html#markdown-cells-support-rich-formatting",
    "href": "src/01/notebooks/hello-world.html#markdown-cells-support-rich-formatting",
    "title": "Hello World Notebook!",
    "section": "Markdown Cells Support Rich Formatting",
    "text": "Markdown Cells Support Rich Formatting\nYou can use: - Bold and italic text - Lists (like this one!) - Links - inline code - And even LaTeX math: \\(E = mc^2\\)\nThis makes notebooks great for explaining your code!\n\n# You can perform calculations across cells\ny = 10\nz = x + y\nprint(f\"x ({x}) + y ({y}) = {z}\")\n\n\n# Notebooks make it easy to import and use libraries\nimport math\nimport random\n\n# Generate a random number and calculate its square root\nnum = random.randint(1, 100)\nsqrt_num = math.sqrt(num)\nprint(f\"The square root of {num} is {sqrt_num:.2f}\")\n\nThe square root of 29 is 5.39\n\n\n\n# Visualizations appear inline!\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx_values = np.linspace(0, 10, 100)\ny_values = np.sin(x_values)\n\nplt.figure(figsize=(8, 4))\nplt.plot(x_values, y_values)\nplt.title('Sine Wave')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html#what-happens-when-theres-an-error",
    "href": "src/01/notebooks/hello-world.html#what-happens-when-theres-an-error",
    "title": "Hello World Notebook!",
    "section": "What Happens When There’s an Error?",
    "text": "What Happens When There’s an Error?\nRun the cell below to see how notebooks handle errors.\nThe error appears in the output, but other cells continue to work.\n\n# This will cause an error\nresult = 10 / 0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[9], line 2\n      1 # This will cause an error\n----&gt; 2 result = 10 / 0\n\nZeroDivisionError: division by zero",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html",
    "href": "src/01/notebooks/translation-transformer.html",
    "title": "Translation Transformer",
    "section": "",
    "text": "In this notebook, we use a small transformer (Helsinki-NLP/opus-mt-fr-en) to translate from French to English.\n     \n\n# @title Load Model\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"Helsinki-NLP/opus-mt-fr-en\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n\n\n\n\n\n\n\n\n\n/Users/simon/Dev/CS-394/.venv/lib/python3.13/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n\n\n\n\n\n\n\n\n\n# @title Tokenize\n\nfrench_text = \"Bonjour, comment allez-vous?\"\ninput_ids = tokenizer.encode(french_text, return_tensors=\"pt\")\nprint(input_ids[0])\nprint(\"Tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n\ntensor([8703,    2, 1027, 5682,   21,  682,   54,    0])\nTokens: ['▁Bonjour', ',', '▁comment', '▁allez', '-', 'vous', '?', '&lt;/s&gt;']\n\n\n\n# @title Demonstrate contextual vectors using the encoder\n\n# French: \"Bonjour , comment allez  - vous  ?\"\n#          ↓       ↓    ↓      ↓    ↓  ↓    ↓\n# Encoder: [v1]   [v2] [v3]  [v4] [v5][v6][v7]  ← 7 vectors, each 512-dim\n#          └─────────────────────────────────┘\n\nencoder = model.get_encoder()\nencoder_output = encoder(input_ids)\nprint(\"Encoder output shape:\", encoder_output.last_hidden_state.shape)\nprint(\"Encoder output:\", encoder_output)\n\nEncoder output shape: torch.Size([1, 8, 512])\nEncoder output: BaseModelOutput(last_hidden_state=tensor([[[-0.3943,  0.4660,  0.0190,  ..., -0.5069,  0.2120, -0.3190],\n         [ 0.0957,  0.0780,  0.1918,  ..., -0.0854,  0.2138,  0.1528],\n         [-0.6160,  0.0295,  0.1918,  ..., -0.3886,  0.0770,  0.2311],\n         ...,\n         [-0.1839, -0.3798,  0.1832,  ..., -0.0041, -0.3633, -0.5455],\n         [ 0.0153,  0.0264,  0.1122,  ...,  0.1966, -0.3027, -0.3659],\n         [-0.0484,  0.0147,  0.0078,  ..., -0.1359, -0.0295, -0.0799]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), hidden_states=None, attentions=None)\n\n\n\n# @title Run through transformer\n\noutput_ids = model.generate(input_ids)\nprint(output_ids)\n\ntensor([[59513, 10537,     2,   541,    52,    55,    54,     0]])\n\n\n\n# @title Decode back to tokens to complete the translation\n\nenglish_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(\"Translation:\", english_text)\n\nTranslation: Hello, how are you?",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  }
]