{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRd5FboLjx1b"
      },
      "source": [
        "# Pre-trained GPT-2 Notebook\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/simonguest/CS-394/blob/main/src/01/notebooks/GPT-2.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a target=\"_blank\" href=\"https://github.com/simonguest/CS-394/raw/refs/heads/main/src/01/notebooks/GPT-2.ipynb\">\n",
        "  <img src=\"https://img.shields.io/badge/Download_.ipynb-blue\" alt=\"Download .ipynb\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "load-model",
      "metadata": {
        "id": "Z_RRNTep1FDj"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set pad token\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "autocomplete",
      "metadata": {
        "id": "UPVXBYRZ0Lsm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n",
        "    # Encode the prompt with attention mask\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    # Generate continuation\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "prompts",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiM3AC_x1dJB",
        "outputId": "7decbea9-1b4b-4208-a441-a65f9be042d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Mary had a little lamb\n",
            "--------------------------------------------------\n",
            "Output: Mary had a little lamb in her mouth, and when she looked at it, she saw that it was a little lamb, and she said, \"What is this?\" And she said, \"I have seen some of the things in the temple, and it is good for you to see them.\" So she said, \"Why, then, do you think I have seen some of the things in\n",
            "\n",
            "\n",
            "Prompt: The future of artificial intelligence\n",
            "--------------------------------------------------\n",
            "Output: The future of artificial intelligence in general is uncertain. The most likely scenarios are that humans will eventually develop the ability to perform complex tasks, like reading the news, and that the technologies will allow us to recognize and even recognize people as we see them. However, there is a large body of evidence that artificial intelligence may not be able to accomplish these tasks.\n",
            "\n",
            "The most important question is whether human\n",
            "\n",
            "\n",
            "Prompt: In a galaxy far, far away\n",
            "--------------------------------------------------\n",
            "Output: In a galaxy far, far away, an enormous asteroid is found.\n",
            "\n",
            "The asteroid is almost 20 times larger than the Earth's, and it's so big that it's the size of a football field.\n",
            "\n",
            "The asteroid's mass is about 1,500 times larger than the Earth's.\n",
            "\n",
            "\"It's a giant asteroid, but it's a very, very big one,\"\n",
            "\n",
            "\n",
            "Prompt: DigiPen is a place where\n",
            "--------------------------------------------------\n",
            "Output: DigiPen is a place where you can learn to create and share ideas with other people in the community. We are a group of people who love to create, and share ideas. This is the place where we'll see you at the party.\n",
            "\n",
            "In the meantime, check out our other projects on our official Facebook page!\n",
            "\n",
            "\n",
            "Prompt: def calculate_fibonacci(n):\n",
            "--------------------------------------------------\n",
            "Output: def calculate_fibonacci(n): return n + 1\n",
            "\n",
            "Now we're going to be using the same function in the second function, so we'll be using the same number of times:\n",
            "\n",
            "from mongo.db import MongoDB db.db.cursor = cursor.new_cursor()\n",
            "\n",
            "Now we'll use the same number of times as the previous\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "    \"Mary had a little lamb\",\n",
        "    \"The future of artificial intelligence\",\n",
        "    \"In a galaxy far, far away\",\n",
        "    \"DigiPen is a place where\",\n",
        "    \"def calculate_fibonacci(n):\"\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(\"-\" * 50)\n",
        "    completion = autocomplete(prompt, max_length=80)\n",
        "    print(f\"Output: {completion}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
