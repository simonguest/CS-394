[
  {
    "objectID": "src/01/hello-world.html",
    "href": "src/01/hello-world.html",
    "title": "Hello World Notebook!",
    "section": "",
    "text": "This is an example of the Jupyter .ipynb document format\n# This is an executable cell\nprint(\"Hello World!\")\n\nHello World!\n# Setting variables in Python\nx = 42\nx\n\n42\n# Variables persist after being set in previously executed cells\nx\n\n42",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Hands-on Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/hello-world.html#markdown-cells-support-rich-formatting",
    "href": "src/01/hello-world.html#markdown-cells-support-rich-formatting",
    "title": "Hello World Notebook!",
    "section": "Markdown Cells Support Rich Formatting",
    "text": "Markdown Cells Support Rich Formatting\nYou can use: - Bold and italic text - Lists (like this one!) - Links - inline code - And even LaTeX math: \\(E = mc^2\\)\nThis makes notebooks great for explaining your code!\n\n# You can perform calculations across cells\ny = 10\nz = x + y\nprint(f\"x ({x}) + y ({y}) = {z}\")\n\n\n# Notebooks make it easy to import and use libraries\nimport math\nimport random\n\n# Generate a random number and calculate its square root\nnum = random.randint(1, 100)\nsqrt_num = math.sqrt(num)\nprint(f\"The square root of {num} is {sqrt_num:.2f}\")\n\nThe square root of 29 is 5.39\n\n\n\n# Visualizations appear inline!\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx_values = np.linspace(0, 10, 100)\ny_values = np.sin(x_values)\n\nplt.figure(figsize=(8, 4))\nplt.plot(x_values, y_values)\nplt.title('Sine Wave')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Hands-on Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/hello-world.html#what-happens-when-theres-an-error",
    "href": "src/01/hello-world.html#what-happens-when-theres-an-error",
    "title": "Hello World Notebook!",
    "section": "What Happens When There’s an Error?",
    "text": "What Happens When There’s an Error?\nRun the cell below to see how notebooks handle errors.\nThe error appears in the output, but other cells continue to work.\n\n# This will cause an error\nresult = 10 / 0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[9], line 2\n      1 # This will cause an error\n----&gt; 2 result = 10 / 0\n\nZeroDivisionError: division by zero",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Hands-on Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/GPT-2.html",
    "href": "src/01/GPT-2.html",
    "title": "Pre-trained GPT-2 Notebook",
    "section": "",
    "text": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch\n\n# Load pre-trained GPT-2 model and tokenizer\nmodel_name = \"gpt2\"  # Options: \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Set model to evaluation mode\nmodel.eval()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n\n\n\ndef autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n    \"\"\"\n    Generate autocomplete text continuation from a prompt.\n\n    Args:\n        prompt: Input text to continue\n        max_length: Maximum total tokens (prompt + generation)\n        temperature: Sampling temperature (higher = more random)\n        top_k: Number of highest probability tokens to keep\n        top_p: Nucleus sampling threshold\n    \"\"\"\n    # Encode the prompt\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n\n    # Generate continuation\n    with torch.no_grad():\n        output = model.generate(\n            input_ids,\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n\n\nprompts = [\n    \"Mary had a little lamb\",\n    \"The future of artificial intelligence\",\n    \"In a galaxy far, far away\",\n    \"DigiPen is a place where\",\n    \"def calculate_fibonacci(n):\"\n]\n\nfor prompt in prompts:\n    print(f\"\\nPrompt: {prompt}\")\n    print(\"-\" * 50)\n    completion = autocomplete(prompt, max_length=80)\n    print(f\"Output: {completion}\\n\")\n\n\nPrompt: Mary had a little lamb\n--------------------------------------------------\nOutput: Mary had a little lamb, which I bought from the shopkeeper for her, and I was going to bring it home. She came back with a bundle of hay, which she put in a large box, and a little box containing a small bag of hay.\n\n\"When I went home, I was taken to a large garden, and I saw a tree with a branch in it,\n\n\nPrompt: The future of artificial intelligence\n--------------------------------------------------\nOutput: The future of artificial intelligence is likely to be far from bright.\n\nThe next big news will come from the company's future of AI.\n\nThis article was originally published on The Conversation. Read the original article.\n\n\nPrompt: In a galaxy far, far away\n--------------------------------------------------\nOutput: In a galaxy far, far away, I have seen a huge galaxy, with a massive amount of galaxies, and a huge amount of planets. I have seen a lot of them. I have seen planets that are so big that they have to be made of matter, like the ones in the movies, but they are not. I have seen planets that are so big that they have to be made\n\n\nPrompt: DigiPen is a place where\n--------------------------------------------------\nOutput: DigiPen is a place where you can learn more about the world of penmanship, and more importantly about penmanship, and learn about how to be a good penman.\n\nI'm glad to announce the launch of the first ever penmanship course in the U.S. on the Penmanship International Course Series.\n\nI'm very excited to see Penmanship International.\n\n\n\n\nPrompt: def calculate_fibonacci(n):\n--------------------------------------------------\nOutput: def calculate_fibonacci(n): print(n, \"\n\n\" + len(n))\n\nreturn n\n\nclass EqEq ( Eq ):\n\ndef __init__ ( self , n ):\n\nself .n = n\n\ndef __eq__ ( self , other ):\n\nreturn (n &lt; other)\n\nclass Eq",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Hands-on Notebooks",
      "GPT-2.ipynb"
    ]
  },
  {
    "objectID": "src/01/resources.html",
    "href": "src/01/resources.html",
    "title": "Week 1 Resources",
    "section": "",
    "text": "A Visual Introduction to Vector Embeddings, Pamela Fox\nA Visual Exploration of Vectors, Pamela Fox\n\n\n\n\n\nGoogle Colab Sign-up Page\nProject Jupyter Page\n\n\n\n\n\nLearn Python with Jupyter, Serena Bonaretti",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#vector-embeddings",
    "href": "src/01/resources.html#vector-embeddings",
    "title": "Week 1 Resources",
    "section": "",
    "text": "A Visual Introduction to Vector Embeddings, Pamela Fox\nA Visual Exploration of Vectors, Pamela Fox",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#introduction-to-notebooks",
    "href": "src/01/resources.html#introduction-to-notebooks",
    "title": "Week 1 Resources",
    "section": "",
    "text": "Google Colab Sign-up Page\nProject Jupyter Page",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#learning-python",
    "href": "src/01/resources.html#learning-python",
    "title": "Week 1 Resources",
    "section": "",
    "text": "Learn Python with Jupyter, Serena Bonaretti",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/slides.html#course-description",
    "href": "src/00/slides.html#course-description",
    "title": "Welcome to CS-394/594!",
    "section": "Course Description",
    "text": "Course Description\n\nHow Generative AI Works focuses on the practical implementation of generative AI within custom software applications and games.\nThe course covers neural network architectures, including the impact of the Transformer model, customization of large language models across multiple vendors using APIs, and experimentation with multimodal models for image and audio recognition and generation.",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#course-description-1",
    "href": "src/00/slides.html#course-description-1",
    "title": "Welcome to CS-394/594!",
    "section": "Course Description",
    "text": "Course Description\n\nHands-on experience includes working with both hosted and locally run models, integrating AI with game engines such as Unity and Unreal, and developing AI agents that extend beyond simple chat-based interactions.\nEthical considerations and model evaluation are integrated throughout, emphasizing awareness of broader societal implications.\nThrough lectures, programming assignments, and a final project, the course provides the expertise needed to apply generative AI in creating innovative and interactive experiences.",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#learning-outcomes",
    "href": "src/00/slides.html#learning-outcomes",
    "title": "Welcome to CS-394/594!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the basic working principles and history of current LLMs (Large Language Models)\nUnderstand ethical and safety aspects of using generative models\nEvaluate and test generative models using industry benchmarks\nRun generative models on local, laptop-based hardware (using CPU, GPU, NPUs)",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#learning-outcomes-1",
    "href": "src/00/slides.html#learning-outcomes-1",
    "title": "Welcome to CS-394/594!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCreate AI-based agents and tools based on the MCP (Model Context Protocol) specification\nAvoid hallucinations by increasing the accuracy of models through RAG (Retrieval Augmented Generation) and fine-tuning\nExplore and use multimodal models for image and audio recognition and generation\nCreate and deploy API-based clients, accessing LLMs hosted by different vendors (OpenAI, Meta, Google)",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#overall",
    "href": "src/00/slides.html#overall",
    "title": "Welcome to CS-394/594!",
    "section": "Overall",
    "text": "Overall\n\nProvide a level of understanding beyond where most professional software developers are today\nFocus on augmentation vs. automation\nBuild a cool final project that you can add to your portfolio!",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#syllabus-overview-1",
    "href": "src/00/slides.html#syllabus-overview-1",
    "title": "Welcome to CS-394/594!",
    "section": "Syllabus Overview",
    "text": "Syllabus Overview\n\nTBD",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#schedule",
    "href": "src/00/slides.html#schedule",
    "title": "Welcome to CS-394/594!",
    "section": "Schedule",
    "text": "Schedule\n\nEvery Friday (Curie); 2pm - 4.50pm\n~1.5 hours of lecture, together with hands-on exercises\n~1.5 hours for in-class lab time, working on assignments\nExpectation of after-class work, especially for final project",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#during-class",
    "href": "src/00/slides.html#during-class",
    "title": "Welcome to CS-394/594!",
    "section": "During Class",
    "text": "During Class\n\nStrive for conversation and interactivity\n\nPlease ask questions, even mid-slide!\nI enjoy going off on tangents / on the whiteboard\nUse lab time to seek input / troubleshoot code",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#handing-in-work",
    "href": "src/00/slides.html#handing-in-work",
    "title": "Welcome to CS-394/594!",
    "section": "Handing in Work",
    "text": "Handing in Work\n\nEverything submitted via GitHub\n\nRecommend creating a repo for in-class assignments\nAnd another repo for your final project\nDon’t forget to give me permissions! @simonguest\n\nGraded Course\n\nGeneral rubric for the in-class assignments\nComplete rubric for the final project",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#handing-in-work-1",
    "href": "src/00/slides.html#handing-in-work-1",
    "title": "Welcome to CS-394/594!",
    "section": "Handing in Work",
    "text": "Handing in Work\n\nDeadlines (In-class Assignments)\n\nAssignments are due by the following week’s lesson\ni.e., you get a week for each assignment\nIf you need more time/exception, please reach out via Teams\n\nDeadlines (Final Project)\n\nUp until Week 15 presentations\n(We’ll cover in detail later in the semester)",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#tools",
    "href": "src/00/slides.html#tools",
    "title": "Welcome to CS-394/594!",
    "section": "Tools",
    "text": "Tools\n\nWe will be introducing many tools\n\nColab Pro, OpenRouter, Hugging Face, etc.\nMost will be free\nExpect to need about $10-25 in OpenRouter API credits",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#languages",
    "href": "src/00/slides.html#languages",
    "title": "Welcome to CS-394/594!",
    "section": "Languages",
    "text": "Languages\n\nWe will be using (and learning) a lot of Python!\n\nMost of the in-class assignments will be in Python\nDon’t worry if you are new to Python as we’ll introduce concepts gradually\n\nAlthough recommend investing extra time (see resources in Week 1)\n\n\nFinal Project\n\nCan be any language\nProbably depending on what you choose to create",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#hardware",
    "href": "src/00/slides.html#hardware",
    "title": "Welcome to CS-394/594!",
    "section": "Hardware",
    "text": "Hardware\n\nWill will be training small models (SLMs) later in the semester\nThis training will require a decent GPU and VRAM\n\nColab Pro (CUDA)\nYour own NVIDIA-based laptop (CUDA)\nPotential of using MLX for any Mac users",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#need-help-1",
    "href": "src/00/slides.html#need-help-1",
    "title": "Welcome to CS-394/594!",
    "section": "Need Help?",
    "text": "Need Help?\n\nEverything we cover will be on my GitHub repo\n\nhttps://simonguest.github.io/CS-394\nSlides (current and prior lectures), Demo code, Resources\n\nOffice Hours\n\nThursdays 1pm - 3pm\nEither on campus or virtually via Teams\n\nTeams\n\nPrimary mechanism for updates, ask questions, request office hours, etc.",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#steep-learning-curve",
    "href": "src/00/slides.html#steep-learning-curve",
    "title": "Welcome to CS-394/594!",
    "section": "Steep Learning Curve",
    "text": "Steep Learning Curve\n\nWe will be using the latest tools and AI models\nLots of new tools, acronyms, frameworks, etc.\nMuch of the curriculum builds upon itself\n\nPlease try not to miss lectures\nAsk for help if you need to catch up",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#new-course-at-digipen",
    "href": "src/00/slides.html#new-course-at-digipen",
    "title": "Welcome to CS-394/594!",
    "section": "New Course at DigiPen!",
    "text": "New Course at DigiPen!\n\nThere may be some minor curriculum tweaks mid-flight\n\nEspecially for topics that need less/more time",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#fast-moving-space",
    "href": "src/00/slides.html#fast-moving-space",
    "title": "Welcome to CS-394/594!",
    "section": "Fast Moving Space",
    "text": "Fast Moving Space\n\nThere will be areas/questions that I don’t have experience of\n\ne.g., multiple new models are launched every week\n\nBut that’s what makes it exciting!",
    "crumbs": [
      "Welcome!",
      "Slides"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS-394/594: How Generative AI Works",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CS-394/594 course.",
    "crumbs": [
      "CS-394/594"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "CS-394/594: How Generative AI Works",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CS-394/594 course.",
    "crumbs": [
      "CS-394/594"
    ]
  },
  {
    "objectID": "src/01/assignment.html",
    "href": "src/01/assignment.html",
    "title": "Week 1 Assignment",
    "section": "",
    "text": "Week 1 Assignment\n\nTBD",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Assignment"
    ]
  },
  {
    "objectID": "src/01/slides.html#lesson-objectives",
    "href": "src/01/slides.html#lesson-objectives",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand how generative AI fits with other AI techniques\nHow to create vector embeddings and test for similarity\nUnderstand the transformer architecture and how it works at a high level\nSetup and use Colab Pro for experimenting with vector embeddings and downloading/testing a GPT-2 model.\nStart becoming familiar with the basics of notebooks and Python (if you haven’t used it already)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#a-recap-of-mlneural-network-architectures-1",
    "href": "src/01/slides.html#a-recap-of-mlneural-network-architectures-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "A recap of ML/Neural Network architectures",
    "text": "A recap of ML/Neural Network architectures\n\nTBD",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-are-vector-embeddings",
    "href": "src/01/slides.html#what-are-vector-embeddings",
    "title": "Week 1: Foundations of Generative AI",
    "section": "What are Vector Embeddings?",
    "text": "What are Vector Embeddings?\n\nVector Embeddings are meaningful numerical representations of words\n\nRepresentations where strings of words (i.e., sentences) are encoded into multi-dimensional space\nLarge number of dimensions (we’ll use 384 in our examples)\nSimilar sentences have similar numbers",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example",
    "href": "src/01/slides.html#example",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nembeddings = model.encode(\"The cat sat on the mat\")\nembeddings[:10]\n\narray([ 0.13040183, -0.01187013, -0.02811704,  0.05123864, -0.05597447,\n        0.03019154,  0.0301613 ,  0.02469837, -0.01837057,  0.05876679],\n      dtype=float32)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-1",
    "href": "src/01/slides.html#example-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nembeddings = model.encode(\"The dog rested on the rug\")\nembeddings[:10]\n\narray([ 0.05627272,  0.02632686,  0.05896206,  0.12019245, -0.00399702,\n        0.08970873, -0.02332847, -0.01548103,  0.00939427,  0.01598458],\n      dtype=float32)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-2",
    "href": "src/01/slides.html#example-2",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nembeddings = model.encode(\"I love pizza!\")\nembeddings[:10]\n\narray([-0.09438416,  0.02385838,  0.00920313,  0.04992779, -0.09533099,\n        0.0061356 ,  0.03513189,  0.00850056,  0.0105693 , -0.0578883 ],\n      dtype=float32)",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#are-they-similar",
    "href": "src/01/slides.html#are-they-similar",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Are They Similar?",
    "text": "Are They Similar?\n\nWe can test with cosine similarity\nMeasures the angle between two vectors:\n\nSame direction = very similar (similarity close to 1)\nOpposite direction = very different (similarity of -1)\n\nCosine similarity focuses on the angle of the vector vs. length\n\nUseful for comparing texts of different sizes",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#are-they-similar-1",
    "href": "src/01/slides.html#are-they-similar-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Are They Similar?",
    "text": "Are They Similar?\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsentences = ['The cat sat on the mat', \n             'The dog rested on the rug',\n             'I love pizza']\nembeddings = model.encode(sentences)\n\nprint(cosine_similarity(embeddings))\n\n[[ 1.0000002   0.47530937  0.00155361]\n [ 0.47530937  1.         -0.04451237]\n [ 0.00155361 -0.04451237  1.        ]]",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#overview-of-embedding-models",
    "href": "src/01/slides.html#overview-of-embedding-models",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Overview of Embedding Models",
    "text": "Overview of Embedding Models\n\nTBD",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-embeddings-are-used-for",
    "href": "src/01/slides.html#what-embeddings-are-used-for",
    "title": "Week 1: Foundations of Generative AI",
    "section": "What Embeddings Are Used For",
    "text": "What Embeddings Are Used For\n\nTBD",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-notebook-1",
    "href": "src/01/slides.html#what-is-a-notebook-1",
    "title": "Week 1: Foundations of Generative AI",
    "section": "What is a Notebook?",
    "text": "What is a Notebook?\n\nAn interactive document that combines:\n\nLive code that can be executed\nRich text explanations (markdown)\nVisualizations and outputs\n\nThink of it as a computational narrative\n\nTell a story with code, data, and explanations\n\nOriginally designed for data science and research\nAlso used for learning, experimenting, and sharing results",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#a-brief-history-of-notebooks",
    "href": "src/01/slides.html#a-brief-history-of-notebooks",
    "title": "Week 1: Foundations of Generative AI",
    "section": "A Brief History of Notebooks",
    "text": "A Brief History of Notebooks\n\n2011: IPython Notebook project begins\n\nInteractive Python shell → web-based notebook\n\n2014: Renamed to Jupyter (Julia, Python, R)\n\nNow supports 40+ programming languages\n\n2017: Google launches Colab\n\nFree cloud-based Jupyter notebooks\nFree access to GPUs and TPUs\n\nToday: Industry standard for ML/AI development",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#anatomy-of-a-notebook",
    "href": "src/01/slides.html#anatomy-of-a-notebook",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Anatomy of a Notebook",
    "text": "Anatomy of a Notebook\n\nFormat: Extension is .ipynb\n\nJSON format, using Jupyter Document Schema\n\nCells: Building blocks of notebooks\n\nCode cells: Executable Python code\nMarkdown cells: Text, headings, images, equations\n\nKernel: The computational engine running your code\n\nMaintains state between cell executions\n\nOutputs: Results appear directly below code cells\n\nText, tables, plots, interactive widgets",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-to-run-notebooks",
    "href": "src/01/slides.html#how-to-run-notebooks",
    "title": "Week 1: Foundations of Generative AI",
    "section": "How to Run Notebooks",
    "text": "How to Run Notebooks\n\nJupyter Notebook Server (Classic approach)\n\nWeb interface on localhost\n\nVS Code (Local development)\n\nJupyter extension for VS Code\nRun on your own machine\n\nGoogle Colab (Recommended)\n\nBrowser-based, no installation needed\nFree GPU access",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#advantages-of-google-colab",
    "href": "src/01/slides.html#advantages-of-google-colab",
    "title": "Week 1: Foundations of Generative AI",
    "section": "Advantages of Google Colab",
    "text": "Advantages of Google Colab\n\nAccess to GPUs and TPUs for AI-based tasks\n\ne.g., A100 and H100 with 40Gb/80Gb VRAM\n\nModel downloaded between cloud vendors\n\nvs. downloading large models via the DigiPen network\n\nMany libraries pre-installed\nEasy to share notebooks with others\nGenerous (free) GPU limits for students!",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/embeddings.html",
    "href": "src/01/embeddings.html",
    "title": "Embeddings",
    "section": "",
    "text": "In this notebook, we use the SentenceTransformer model to encode various sentences and test the cosine similarity between them.\n     \n\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nembeddings = model.encode(\"The cat sat on the mat\")\nembeddings[:10]\n\narray([ 0.13040183, -0.01187013, -0.02811704,  0.05123864, -0.05597447,\n        0.03019154,  0.0301613 ,  0.02469837, -0.01837057,  0.05876679],\n      dtype=float32)\n\n\n\nembeddings = model.encode(\"The dog rested on the rug\")\nembeddings[:10]\n\narray([ 0.05627272,  0.02632686,  0.05896206,  0.12019245, -0.00399702,\n        0.08970873, -0.02332847, -0.01548103,  0.00939427,  0.01598458],\n      dtype=float32)\n\n\n\nembeddings = model.encode(\"I love pizza!\")\nembeddings[:10]\n\narray([-0.09438416,  0.02385838,  0.00920313,  0.04992779, -0.09533099,\n        0.0061356 ,  0.03513189,  0.00850056,  0.0105693 , -0.0578883 ],\n      dtype=float32)\n\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsentences = ['The cat sat on the mat', \n             'The dog rested on the rug',\n             'I love pizza']\nembeddings = model.encode(sentences)\n\nprint(cosine_similarity(embeddings))\n\n[[ 1.0000002   0.47530937  0.00155361]\n [ 0.47530937  1.         -0.04451237]\n [ 0.00155361 -0.04451237  1.        ]]",
    "crumbs": [
      "Week 1: Foundations of Generative AI",
      "Hands-on Notebooks",
      "embeddings.ipynb"
    ]
  }
]