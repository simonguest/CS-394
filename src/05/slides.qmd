---
title: "Module 5: Running Models on Local Hardware"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Understood the fundamentals and history of diffuser models
- Explored and used models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet
- Setup and used Replicate to create a custom pipeline of production-grade models
- Understood the fundamentals and history of Vision Encoders and VLMs
- Implemented/tested a local VLM model for on-device inference

## Lesson Objectives

- Understand the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile
- Understand hardware requirements and architectures for model inference - e.g., CUDA vs. ONNX vs. MLX vs. WebGPU
- Explore how quantization works and understand techniques and formats for quantizing existing models
- Use llama.cpp to quantize and run an SLM on local hardware/gaming PC
- Integrate a quantized model within Unity/Unreal/WebAssembly

# Why Local Models?

## Why Local Models?

- **Privacy**
  - Every call you make to OpenAI/Claude/OpenRouter may (or may not) get logged and/or be used for training purposes
  - Many organizations don't want their customer/financial data logged with an AI vendor
  - There may also be legal regulations/restrictions controlling this

## Why Local Models?

- **Offline**
  - Every call you make to OpenAI/Claude/OpenRouter needs an Internet connection
  - That's not always guaranteed!
  - Education is a good example - remote school in India and/or rural districts here in the US

## Why Local Models?

- **Latency**
  - Even with a network connection, calls can suffer from increased latency
  - Can be a challenge if your application needs frequent, quick responses
  - e.g., using a VLM to determine the contents of a video stream for a user with vision impairments

## Why Local Models?

- **Cost**
  - While per-API costs are fractions of a cent, these can grow out of control with exponential growth
  - More pronounced for long conversation threads (think call center)
  - Or agents with verbose tool call JSON requests/responses

# What's Your Hardware?

## What's Your Hardware?

- NVIDIA (CUDA)
- AMD (ROCm)
- Apple Silicon
- Various NPU (Neural Processing Unit) vendors

## NVIDIA CUDA

- CUDA (Compute Unified Device Architecture)
  - Launched in 2006 to introduce programming on GPUs (GPGPUs or General Purpose GPUs)
  - A C-like programming interface
  - Perfectly timed for the deep learning revolution of the 2010s
  - Additional libraries (e.g., cuBLAS, cuDNN) make CUDA the defacto standard today

## How CUDA Works

- Massive parallelism: CUDA exploits thousands of GPU cores simultaneously, making it ideal for matrix operations.
- Memory hierarchy: A tiered memory system with global, shared, and registers.
- Kernel execution: Programs can launch kernels - same function that can operate on different data.

## NVIDIA CUDA - Hardware Support

- Consumer: RTX 40- and 50- cards with various VRAM options (8Gb - 24Gb) for local inference and small fine-tuning tasks. RTX 30- series still popular for education.
- Laptop: RTX 40- and 50- series also available on laptops (although less performant than discrete cards)
- Workstation: DGX Spark launch in 2025, with GB10 and 128Gb of unified memory for medium fine-tuning tasks
- Datacenter GPUs: A/H series and GB-series for datacenters. NVLink for multi-GPU interconnectivity.

## Sidebar: TOPS

- TOPS (Terra Operations Per Second)
  - How many trillion operations a processor can perform per second
  - Often qualified with the data type
  - 64 INT8 TOPS == 64 trillion 8-bit operations per second
- TFLOPS (Terra Floating-Point Operations Per Second)
  - 1 TFLOPS == 1 FP32 (32-bit floating point) TOPS

## Sidebar: TOPS

- Rough Throughput Calculations
  - You have an NVIDIA 3090 (advertized at 35 TFLOPS)
  - Assume a 7B param model with FP32 weights
  - Each token generation requires ~2 FLOPs per parameter
  - Each token generation ~= 14B FLOPs (7B params × 2)
  - Theoretical max = 35T FLOPs/sec ÷ 14B FLOPs/token ≈ 2,500 tokens/sec
  - Reality: 10-100 tokens/sec typical due to memory bandwidth bottlenecks

## AMD ROCm

- ROCm (Radeon Open Compute)
  - Launched in 2016 as an open-source alternative to CUDA
  - Embraced open standards (e.g., OpenCL), positioning as avoiding vendor lock-in, although this fragmentation initially hurt adoption
  - Has evolved significantly since (e.g., rocBLAS) although ecosystem gaps compared to CUDA persist

## AMD ROCm - Hardware Support

- Consumer: RX7000 series offer sustantial VRAM (up to 24Gb) at competitive prices compared to NVIDIA RTX
- Laptop: Some laptop options for AMD-based machines
- Workstation: Strix Halo, competitor to DGX Spark, with RX8060S and 128Gb unified memory
- Software support: Linux only with no Windows support (some via WSL)

## Apple Silicon

- Apple Silicon
  - Metal, a low-level graphics and compute API, launched in 2014 and later expanded for general GPU compute tasks
  - MPS (Metal Performance Shaders) introduced in 2017 and optimized primitives for neural networks. PyTorch added MPS device support in 2022.
  - MLX released in 2023, providing NumPy-like API for Apple Silicon hardware

## Apple Silicon - Hardware Support

- Available on all M-series hardware
- Unified memory by default, upto 128Gb on laptops and 512Gb for the Mac Studio with M3 Ultra
- Non portable models. (MLX uses safetensors/npz format and MLX-specific code for Metal.)

## Sidebar: Unified Memory

- GPUs have historically had separate memory (VRAM)
- Unified memory is a process to share memory between CPU and GPU
  - For Apple/MLX, it's a true SoC (System on a Chip); NVIDIA DGX Spark, two physical components connected via NVLink-C2C
- Higher memory availability, but lower memory bandwidth
  - ~275Gb/s for Spark/MLX; ~1TB/s for 5080; ~1.7TB/s for 5090; ~3TBs for H100

## NPUs

- NPUs (Neural Processing Units) are specialized AI accelerators, designed for lower power consumption
- Optimized specifically for NN operations (e.g., matmul, convolutions, activations)
- Commonly found in edge devices (smartphones, IoT, embedded systems)
- 15-80 TOPS common for NPUs (~10x less that desktop PCI-based GPUs)

## NPU Vendors

- Intel: Acquired Movidius in 2016; released Myriad X in 2017 and a neural compute stick. Superceded by NPUs in Core Ultra processors.
- Qualcomm: Snapdragon 865 range in 2019; now Snapdragon X and 8 ranges. Used in Windows/ARM devices. Popular with Android, although Google recently moved to their own TPUs.
- AMD: XDNA formerly Xilinx; Windows Copilot PC range competing with SnapDragon and Intel Core Ultra.
- Apple: ANE (Apple Neural Engine) to support CoreML workloads on iPhone, iPad devices

# Overcoming Compute and Memory Bottlenecks

## Overcoming Compute and Memory Bottlenecks

- TBD



## "Out of VRAM"

One challenge of running models on your own hardware is VRAM

- Roughly speaking, the size of the model will determine how much VRAM you need
- Gemma 3 models
  - gemma-3-1b-it = 2Gb
  - gemma-3-4b-it = 8.6Gb
  - gemma-3-12b-it = 23.37Gb
  - Qwen3-VL-235B-A22B-Thinking = ~475Gb

## "Out of VRAM"

- Google Colab Tiers
  - Colab Free T4 = 16Gb VRAM (15Gb usable)
  - Colab Pro V100 = 16Gb VRAM
  - Colab Pro A100 = 40Gb VRAM 
- Your Gaming PC
  - Probably 8Gb VRAM
- Your Phone
  - V-what? :)

## Quantization

Process of reducing the precision of a model's weights and activations. For example, 16-bit numbers to 4-bit.

- Parameter count matters more than precision
  - A 70B parameter model at 4-bit often beats a 13B model at b16
  - The models knowledge remains largely intact
  - Often the extra precision doesn't meaningfully improve outputs

## Quantization Formats

The llama.cpp project (implementing LLMs in pure C/C++) has driven advancements in quantization

- GGUF (GPT-Generated Unified Format)
  - Single file architecture
  - Model format supporting multiple quantization levels (2-bit through 8-bit) with CPU and GPU handoff
- MLX (Apple's ML framework and format for Apple Silicon)
  - Debuted in late 2023
  - Supports 4 and 8 bit quantization schemes

## Running Quantized Models

- Tools built upon llama.cpp
  - Ollama, LM Studio, koboldcpp

## Demo: C# Client <-> Gemma 3 27B Local {.center}

demos/01/lmstudio-client/LMStudioClient.csproj

## Hosting Models in Unity

- Download the GGUF model locally to Assets/StreamingAssets folder
- Use llama.cpp bindings for C# to host
  - LLAMASharp: https://github.com/SciSharp/LLamaSharp
- Use OpenAI SDK (or similar) as client
- Unity Demo
  - https://github.com/eublefar/LLAMASharpUnityDemo

# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/05/assignment.html){.external target="_blank"}
- TBD

# References

## References
