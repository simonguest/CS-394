[
  {
    "objectID": "src/05/notebooks/quantization.html#install-dependencies",
    "href": "src/05/notebooks/quantization.html#install-dependencies",
    "title": "Quantization of Hugging Face model using llama.cpp",
    "section": "Install dependencies",
    "text": "Install dependencies\n\n!uv pip install huggingface_hub\n\n\nUsing Python 3.13.1 environment at: /Users/simon/Dev/CS-394/.venv\n\nAudited 1 package in 27ms"
  },
  {
    "objectID": "src/05/notebooks/quantization.html#clone-and-build-llama.cpp",
    "href": "src/05/notebooks/quantization.html#clone-and-build-llama.cpp",
    "title": "Quantization of Hugging Face model using llama.cpp",
    "section": "Clone and build llama.cpp",
    "text": "Clone and build llama.cpp\n\nTEMP_FOLDER = \"/content/tmp\"\n\n# Create a temporary location and clone llama.cpp\n!mkdir -p {TEMP_FOLDER}\n!cd {TEMP_FOLDER} && git clone https://github.com/ggml-org/llama.cpp\n\n!cd {TEMP_FOLDER}/llama.cpp && cmake -B build\n!cd {TEMP_FOLDER}/llama.cpp && cmake --build build --config Release --target llama-quantize\n\n\nCloning into 'llama.cpp'...\n\nremote: Enumerating objects: 77965, done.\n\nremote: Counting objects: 100% (228/228), done.\n\nremote: Compressing objects: 100% (156/156), done.\n\nremote: Total 77965 (delta 141), reused 76 (delta 72), pack-reused 77737 (from 4)\n\nReceiving objects: 100% (77965/77965), 286.82 MiB | 15.73 MiB/s, done.\n\nResolving deltas: 100% (56335/56335), done.\n\n-- The C compiler identification is GNU 11.4.0\n\n-- The CXX compiler identification is GNU 11.4.0\n\n-- Detecting C compiler ABI info\n\n-- Detecting C compiler ABI info - done\n\n-- Check for working C compiler: /usr/bin/cc - skipped\n\n-- Detecting C compile features\n\n-- Detecting C compile features - done\n\n-- Detecting CXX compiler ABI info\n\n-- Detecting CXX compiler ABI info - done\n\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n\n-- Detecting CXX compile features\n\n-- Detecting CXX compile features - done\n\nCMAKE_BUILD_TYPE=Release\n\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n\n-- The ASM compiler identification is GNU\n\n-- Found assembler: /usr/bin/cc\n\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n\n-- Found Threads: TRUE\n\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n\n-- GGML_SYSTEM_ARCH: x86\n\n-- Including CPU backend\n\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n\n-- Found OpenMP: TRUE (found version \"4.5\")\n\n-- x86 detected\n\n-- Adding CPU backend variant ggml-cpu: -march=native \n\n-- ggml version: 0.9.5\n\n-- ggml commit:  0dfcd3b60\n\n-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"3.0.2\")\n\n-- Performing Test OPENSSL_VERSION_SUPPORTED\n\n-- Performing Test OPENSSL_VERSION_SUPPORTED - Success\n\n-- OpenSSL found: 3.0.2\n\n-- Generating embedded license file for target: common\n\n-- Configuring done (1.6s)\n\n-- Generating done (0.3s)\n\n-- Build files have been written to: /content/tmp/llama.cpp/build\n\n[  0%] Building CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\n\n[  0%] Linking CXX static library libcpp-httplib.a\n\n[  0%] Built target cpp-httplib\n\n[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\n\n[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\n\n[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n\n[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\n\n[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\n\n[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\n\n[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n\n[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\n\n[  4%] Linking CXX shared library ../../bin/libggml-base.so\n\n[  4%] Built target ggml-base\n\n[  4%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\n\n[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\n\n[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\n\n[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\n\n[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\n\n[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\n\n[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\n\n[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\n\n[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\n\n[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\n\n[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\n\n[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\n\n[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n\n[ 13%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\n\n[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\n\n[ 13%] Linking CXX shared library ../../bin/libggml-cpu.so\n\n[ 13%] Built target ggml-cpu\n\n[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o\n\n[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\n\n[ 15%] Linking CXX shared library ../../bin/libggml.so\n\n[ 15%] Built target ggml\n\n[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o\n\n[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\n\n[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\n\n[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\n\n[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\n\n[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\n\n[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\n\n[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\n\n[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\n\n[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\n\n[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\n\n[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\n\n[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\n\n[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\n\n[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\n\n[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\n\n[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o\n\n[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\n\n[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\n\n[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\n\n[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\n\n[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\n\n[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\n\n[ 28%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\n\n[ 28%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\n\n[ 28%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\n\n[ 28%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\n\n[ 31%] Building CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\n\n[ 31%] Building CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\n\n[ 31%] Building CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\n\n[ 31%] Building CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\n\n[ 33%] Building CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\n\n[ 33%] Building CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\n\n[ 33%] Building CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\n\n[ 33%] Building CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\n\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\n\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\n\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\n\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\n\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\n\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\n\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\n\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\n\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\n\n[ 40%] Building CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\n\n[ 40%] Building CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\n\n[ 40%] Building CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\n\n[ 40%] Building CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\n\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\n\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\n\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\n\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\n\n[ 44%] Building CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\n\n[ 44%] Building CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\n\n[ 44%] Building CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o\n\n[ 44%] Building CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\n\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\n\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\n\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\n\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\n\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\n\n[ 48%] Building CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\n\n[ 48%] Building CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\n\n[ 48%] Building CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\n\n[ 48%] Building CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\n\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\n\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\n\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\n\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\n\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\n\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\n\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\n\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\n\n[ 55%] Building CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\n\n[ 55%] Building CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\n\n[ 55%] Building CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\n\n[ 55%] Building CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\n\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\n\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\n\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\n\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\n\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\n\n[ 60%] Building CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\n\n[ 60%] Building CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\n\n[ 60%] Building CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\n\n[ 60%] Building CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\n\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\n\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\n\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\n\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\n\n[ 64%] Building CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\n\n[ 64%] Building CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\n\n[ 64%] Building CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\n\n[ 64%] Building CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\n\n[ 66%] Building CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\n\n[ 66%] Building CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\n\n[ 66%] Building CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\n\n[ 66%] Building CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\n\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\n\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\n\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\n\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\n\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\n\n[ 71%] Building CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\n\n[ 71%] Building CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\n\n[ 71%] Building CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\n\n[ 71%] Building CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\n\n[ 73%] Building CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\n\n[ 73%] Building CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\n\n[ 73%] Building CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\n\n[ 73%] Building CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\n\n[ 75%] Building CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\n\n[ 75%] Building CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\n\n[ 75%] Building CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\n\n[ 75%] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\n\n[ 77%] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\n\n[ 77%] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\n\n[ 77%] Building CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\n\n[ 77%] Building CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\n\n[ 77%] Building CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\n\n[ 80%] Building CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\n\n[ 80%] Building CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\n\n[ 80%] Building CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\n\n[ 80%] Building CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\n\n[ 82%] Building CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\n\n[ 82%] Building CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\n\n[ 82%] Building CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\n\n[ 82%] Building CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\n\n[ 84%] Building CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\n\n[ 84%] Building CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\n\n[ 84%] Building CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\n\n[ 84%] Linking CXX shared library ../bin/libllama.so\n\n[ 84%] Built target llama\n\n[ 84%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\n\n[ 84%] Built target build_info\n\n[ 84%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o\n\n[ 84%] Building CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\n\n[ 84%] Building CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\n\n[ 86%] Building CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\n\n[ 86%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o\n\n[ 86%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o\n\n[ 86%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o\n\n[ 88%] Building CXX object common/CMakeFiles/common.dir/debug.cpp.o\n\n[ 88%] Building CXX object common/CMakeFiles/common.dir/download.cpp.o\n\n[ 88%] Building CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\n\n[ 88%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n\n[ 91%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\n\n[ 91%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o\n\n[ 91%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\n\n[ 91%] Building CXX object common/CMakeFiles/common.dir/ngram-map.cpp.o\n\n[ 93%] Building CXX object common/CMakeFiles/common.dir/ngram-mod.cpp.o\n\n[ 93%] Building CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\n\n[ 93%] Building CXX object common/CMakeFiles/common.dir/preset.cpp.o\n\n[ 93%] Building CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\n\n[ 93%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o\n\n[ 95%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o\n\n[ 95%] Building CXX object common/CMakeFiles/common.dir/unicode.cpp.o\n\n[ 95%] Building CXX object common/CMakeFiles/common.dir/jinja/lexer.cpp.o\n\n[ 95%] Building CXX object common/CMakeFiles/common.dir/jinja/parser.cpp.o\n\n[ 97%] Building CXX object common/CMakeFiles/common.dir/jinja/runtime.cpp.o\n\n[ 97%] Building CXX object common/CMakeFiles/common.dir/jinja/value.cpp.o\n\n[ 97%] Building CXX object common/CMakeFiles/common.dir/jinja/string.cpp.o\n\n[ 97%] Building CXX object common/CMakeFiles/common.dir/jinja/caps.cpp.o\n\n[100%] Building CXX object common/CMakeFiles/common.dir/__/license.cpp.o\n\n[100%] Linking CXX static library libcommon.a\n\n[100%] Built target common\n\n[100%] Building CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\n\n[100%] Linking CXX executable ../../bin/llama-quantize\n\n[100%] Built target llama-quantize"
  },
  {
    "objectID": "src/05/notebooks/quantization.html#run-convert-hf-to-gguf-script",
    "href": "src/05/notebooks/quantization.html#run-convert-hf-to-gguf-script",
    "title": "Quantization of Hugging Face model using llama.cpp",
    "section": "Run convert hf to gguf script",
    "text": "Run convert hf to gguf script\n\nfrom huggingface_hub import snapshot_download\n\nMODEL_FOLDER = \"/content/models\"\nMODEL_VENDOR = \"Qwen\"\nMODEL_NAME = \"Qwen3-0.6B\"\nMODEL_REPO = f\"{MODEL_VENDOR}/{MODEL_NAME}\"\nGGUF_FOLDER = \"/content/ggufs\"\n\n# Create a temporary location and clone llama.cpp\n\n!mkdir -p {MODEL_FOLDER}/{MODEL_NAME}\n!mkdir -p {GGUF_FOLDER}\n\n# Download the model from Hugging Face at full precision\nsnapshot_download(repo_id=MODEL_REPO, local_dir=f\"{MODEL_FOLDER}/{MODEL_NAME}\", repo_type=\"model\")\n\n# Convert to GGUF (fp16 first)\n!cd {TEMP_FOLDER}/llama.cpp && python convert_hf_to_gguf.py {MODEL_FOLDER}/{MODEL_NAME} \\\n    --outfile {GGUF_FOLDER}/{MODEL_NAME}-F16.gguf \\\n    --outtype f16\n\n# # Convert gp16 GGUF to Q4_K_M\n!{TEMP_FOLDER}/llama.cpp/build/bin/llama-quantize {GGUF_FOLDER}/{MODEL_NAME}-F16.gguf \\\n    {GGUF_FOLDER}/{MODEL_NAME}-Q4_K_M.gguf \\\n    Q4_K_M\n\n\n\n\n\n\n\nINFO:hf-to-gguf:Loading model: Qwen3-0.6B\nINFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\nINFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:output.weight,             torch.bfloat16 --&gt; F16, shape = {1024, 151936}\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --&gt; F16, shape = {1024, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 40960\nINFO:hf-to-gguf:gguf: embedding length = 1024\nINFO:hf-to-gguf:gguf: feed forward length = 3072\nINFO:hf-to-gguf:gguf: head count = 16\nINFO:hf-to-gguf:gguf: key-value head count = 8\nWARNING:hf-to-gguf:Unknown RoPE type: default\nINFO:hf-to-gguf:gguf: rope scaling type = NONE\nINFO:hf-to-gguf:gguf: rope theta = 1000000\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nINFO:numexpr.utils:NumExpr defaulting to 12 threads.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting add_bos_token to False\nINFO:gguf.vocab:Setting chat_template to {%- if tools %}\n    {{- '&lt;|im_start|&gt;system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\\n&lt;tools&gt;\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n&lt;/tools&gt;\\n\\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\\n&lt;tool_call&gt;\\n{\\\"name\\\": &lt;function-name&gt;, \\\"arguments\\\": &lt;args-json-object&gt;}\\n&lt;/tool_call&gt;&lt;|im_end|&gt;\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '&lt;|im_start|&gt;system\\n' + messages[0].content + '&lt;|im_end|&gt;\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('&lt;tool_response&gt;') and message.content.endswith('&lt;/tool_response&gt;')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content + '&lt;|im_end|&gt;' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '&lt;/think&gt;' in content %}\n                {%- set reasoning_content = content.split('&lt;/think&gt;')[0].rstrip('\\n').split('&lt;think&gt;')[-1].lstrip('\\n') %}\n                {%- set content = content.split('&lt;/think&gt;')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 &gt; ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '&lt;|im_start|&gt;' + message.role + '\\n&lt;think&gt;\\n' + reasoning_content.strip('\\n') + '\\n&lt;/think&gt;\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '&lt;tool_call&gt;\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n&lt;/tool_call&gt;' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '&lt;|im_end|&gt;\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '&lt;|im_start|&gt;user' }}\n        {%- endif %}\n        {{- '\\n&lt;tool_response&gt;\\n' }}\n        {{- content }}\n        {{- '\\n&lt;/tool_response&gt;' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '&lt;|im_end|&gt;\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '&lt;|im_start|&gt;assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '&lt;think&gt;\\n\\n&lt;/think&gt;\\n\\n' }}\n    {%- endif %}\n{%- endif %}\nINFO:gguf.gguf_writer:Writing the following files:\nINFO:gguf.gguf_writer:/content/ggufs/Qwen3-0.6B-F16.gguf: n_tensors = 311, total_size = 1.5G\nWriting: 100% 1.50G/1.50G [00:05&lt;00:00, 287Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to /content/ggufs/Qwen3-0.6B-F16.gguf\nmain: build = 7916 (0dfcd3b60)\nmain: built with GNU 11.4.0 for Linux x86_64\nmain: quantizing '/content/ggufs/Qwen3-0.6B-F16.gguf' to '/content/ggufs/Qwen3-0.6B-Q4_K_M.gguf' as Q4_K_M\nllama_model_loader: loaded meta data with 37 key-value pairs and 311 tensors from /content/ggufs/Qwen3-0.6B-F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                     general.sampling.top_k i32              = 20\nllama_model_loader: - kv   3:                     general.sampling.top_p f32              = 0.950000\nllama_model_loader: - kv   4:                      general.sampling.temp f32              = 0.600000\nllama_model_loader: - kv   5:                               general.name str              = Qwen3 0.6B\nllama_model_loader: - kv   6:                           general.basename str              = Qwen3\nllama_model_loader: - kv   7:                         general.size_label str              = 0.6B\nllama_model_loader: - kv   8:                            general.license str              = apache-2.0\nllama_model_loader: - kv   9:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-0.6...\nllama_model_loader: - kv  10:                   general.base_model.count u32              = 1\nllama_model_loader: - kv  11:                  general.base_model.0.name str              = Qwen3 0.6B Base\nllama_model_loader: - kv  12:          general.base_model.0.organization str              = Qwen\nllama_model_loader: - kv  13:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-0.6...\nllama_model_loader: - kv  14:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  15:                          qwen3.block_count u32              = 28\nllama_model_loader: - kv  16:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv  17:                     qwen3.embedding_length u32              = 1024\nllama_model_loader: - kv  18:                  qwen3.feed_forward_length u32              = 3072\nllama_model_loader: - kv  19:                 qwen3.attention.head_count u32              = 16\nllama_model_loader: - kv  20:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  21:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  22:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  23:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  24:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  25:                          general.file_type u32              = 1\nllama_model_loader: - kv  26:               general.quantization_version u32              = 2\nllama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,151387]  = [\" \", \" \", \"i n\", \" t\",...\nllama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  34:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  36:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '&lt;|im_start|&gt;...\nllama_model_loader: - type  f32:  113 tensors\nllama_model_loader: - type  f16:  198 tensors\n[   1/ 311]                        output.weight - [ 1024, 151936,     1,     1], type =    f16, converting to q6_K .. size =   296.75 MiB -&gt;   121.71 MiB\n[   2/ 311]                   output_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[   3/ 311]                    token_embd.weight - [ 1024, 151936,     1,     1], type =    f16, converting to q4_K .. size =   296.75 MiB -&gt;    83.46 MiB\n[   4/ 311]                  blk.0.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[   5/ 311]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[   6/ 311]               blk.0.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[   7/ 311]             blk.0.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[   8/ 311]                  blk.0.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[   9/ 311]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  10/ 311]                  blk.0.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[  11/ 311]                blk.0.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[  12/ 311]                blk.0.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  13/ 311]                blk.0.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  14/ 311]                  blk.0.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  15/ 311]                  blk.1.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  16/ 311]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  17/ 311]               blk.1.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  18/ 311]             blk.1.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  19/ 311]                  blk.1.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  20/ 311]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  21/ 311]                  blk.1.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[  22/ 311]                blk.1.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[  23/ 311]                blk.1.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  24/ 311]                blk.1.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  25/ 311]                  blk.1.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  26/ 311]                  blk.2.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  27/ 311]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  28/ 311]               blk.2.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  29/ 311]             blk.2.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  30/ 311]                  blk.2.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  31/ 311]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  32/ 311]                  blk.2.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[  33/ 311]                blk.2.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[  34/ 311]                blk.2.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  35/ 311]                blk.2.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  36/ 311]                  blk.2.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  37/ 311]                  blk.3.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  38/ 311]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  39/ 311]               blk.3.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  40/ 311]             blk.3.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  41/ 311]                  blk.3.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  42/ 311]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  43/ 311]                  blk.3.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  44/ 311]                blk.3.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  45/ 311]                blk.3.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  46/ 311]                blk.3.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  47/ 311]                  blk.3.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  48/ 311]                  blk.4.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  49/ 311]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  50/ 311]               blk.4.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  51/ 311]             blk.4.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  52/ 311]                  blk.4.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  53/ 311]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  54/ 311]                  blk.4.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  55/ 311]                blk.4.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  56/ 311]                blk.4.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  57/ 311]                blk.4.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  58/ 311]                  blk.4.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  59/ 311]                  blk.5.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  60/ 311]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  61/ 311]               blk.5.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  62/ 311]             blk.5.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  63/ 311]                  blk.5.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  64/ 311]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  65/ 311]                  blk.5.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[  66/ 311]                blk.5.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[  67/ 311]                blk.5.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  68/ 311]                blk.5.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  69/ 311]                  blk.5.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  70/ 311]                  blk.6.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  71/ 311]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  72/ 311]               blk.6.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  73/ 311]             blk.6.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  74/ 311]                  blk.6.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  75/ 311]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  76/ 311]                  blk.6.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  77/ 311]                blk.6.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  78/ 311]                blk.6.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  79/ 311]                blk.6.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  80/ 311]                  blk.6.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  81/ 311]                  blk.7.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  82/ 311]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  83/ 311]               blk.7.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  84/ 311]             blk.7.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  85/ 311]                  blk.7.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  86/ 311]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  87/ 311]                  blk.7.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  88/ 311]                blk.7.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  89/ 311]                blk.7.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  90/ 311]                blk.7.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  91/ 311]                  blk.7.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  92/ 311]                  blk.8.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  93/ 311]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  94/ 311]               blk.8.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  95/ 311]             blk.8.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  96/ 311]                  blk.8.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  97/ 311]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  98/ 311]                  blk.8.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[  99/ 311]                blk.8.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 100/ 311]                blk.8.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 101/ 311]                blk.8.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 102/ 311]                  blk.8.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 103/ 311]                  blk.9.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 104/ 311]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 105/ 311]               blk.9.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 106/ 311]             blk.9.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 107/ 311]                  blk.9.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 108/ 311]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 109/ 311]                  blk.9.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 110/ 311]                blk.9.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 111/ 311]                blk.9.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 112/ 311]                blk.9.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 113/ 311]                  blk.9.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 114/ 311]                 blk.10.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 115/ 311]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 116/ 311]              blk.10.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 117/ 311]            blk.10.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 118/ 311]                 blk.10.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 119/ 311]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 120/ 311]                 blk.10.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 121/ 311]               blk.10.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 122/ 311]               blk.10.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 123/ 311]               blk.10.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 124/ 311]                 blk.10.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 125/ 311]                 blk.11.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 126/ 311]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 127/ 311]              blk.11.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 128/ 311]            blk.11.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 129/ 311]                 blk.11.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 130/ 311]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 131/ 311]                 blk.11.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 132/ 311]               blk.11.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 133/ 311]               blk.11.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 134/ 311]               blk.11.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 135/ 311]                 blk.11.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 136/ 311]                 blk.12.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 137/ 311]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 138/ 311]              blk.12.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 139/ 311]            blk.12.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 140/ 311]                 blk.12.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 141/ 311]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 142/ 311]                 blk.12.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 143/ 311]               blk.12.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 144/ 311]               blk.12.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 145/ 311]               blk.12.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 146/ 311]                 blk.12.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 147/ 311]                 blk.13.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 148/ 311]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 149/ 311]              blk.13.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 150/ 311]            blk.13.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 151/ 311]                 blk.13.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 152/ 311]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 153/ 311]                 blk.13.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 154/ 311]               blk.13.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 155/ 311]               blk.13.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 156/ 311]               blk.13.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 157/ 311]                 blk.13.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 158/ 311]                 blk.14.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 159/ 311]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 160/ 311]              blk.14.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 161/ 311]            blk.14.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 162/ 311]                 blk.14.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 163/ 311]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 164/ 311]                 blk.14.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 165/ 311]               blk.14.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 166/ 311]               blk.14.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 167/ 311]               blk.14.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 168/ 311]                 blk.14.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 169/ 311]                 blk.15.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 170/ 311]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 171/ 311]              blk.15.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 172/ 311]            blk.15.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 173/ 311]                 blk.15.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 174/ 311]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 175/ 311]                 blk.15.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 176/ 311]               blk.15.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 177/ 311]               blk.15.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 178/ 311]               blk.15.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 179/ 311]                 blk.15.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 180/ 311]                 blk.16.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 181/ 311]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 182/ 311]              blk.16.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 183/ 311]            blk.16.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 184/ 311]                 blk.16.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 185/ 311]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 186/ 311]                 blk.16.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 187/ 311]               blk.16.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 188/ 311]               blk.16.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 189/ 311]               blk.16.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 190/ 311]                 blk.16.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 191/ 311]                 blk.17.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 192/ 311]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 193/ 311]              blk.17.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 194/ 311]            blk.17.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 195/ 311]                 blk.17.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 196/ 311]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 197/ 311]                 blk.17.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 198/ 311]               blk.17.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 199/ 311]               blk.17.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 200/ 311]               blk.17.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 201/ 311]                 blk.17.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 202/ 311]                 blk.18.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 203/ 311]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 204/ 311]              blk.18.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 205/ 311]            blk.18.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 206/ 311]                 blk.18.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 207/ 311]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 208/ 311]                 blk.18.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 209/ 311]               blk.18.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 210/ 311]               blk.18.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 211/ 311]               blk.18.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 212/ 311]                 blk.18.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 213/ 311]                 blk.19.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 214/ 311]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 215/ 311]              blk.19.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 216/ 311]            blk.19.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 217/ 311]                 blk.19.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 218/ 311]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 219/ 311]                 blk.19.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 220/ 311]               blk.19.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 221/ 311]               blk.19.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 222/ 311]               blk.19.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 223/ 311]                 blk.19.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 224/ 311]                 blk.20.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 225/ 311]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 226/ 311]              blk.20.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 227/ 311]            blk.20.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 228/ 311]                 blk.20.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 229/ 311]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 230/ 311]                 blk.20.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 231/ 311]               blk.20.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 232/ 311]               blk.20.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 233/ 311]               blk.20.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 234/ 311]                 blk.20.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 235/ 311]                 blk.21.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 236/ 311]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 237/ 311]              blk.21.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 238/ 311]            blk.21.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 239/ 311]                 blk.21.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 240/ 311]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 241/ 311]                 blk.21.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 242/ 311]               blk.21.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 243/ 311]               blk.21.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 244/ 311]               blk.21.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 245/ 311]                 blk.21.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 246/ 311]                 blk.22.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 247/ 311]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 248/ 311]              blk.22.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 249/ 311]            blk.22.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 250/ 311]                 blk.22.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 251/ 311]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 252/ 311]                 blk.22.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 253/ 311]               blk.22.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 254/ 311]               blk.22.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 255/ 311]               blk.22.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 256/ 311]                 blk.22.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 257/ 311]                 blk.23.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 258/ 311]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 259/ 311]              blk.23.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 260/ 311]            blk.23.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 261/ 311]                 blk.23.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 262/ 311]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 263/ 311]                 blk.23.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 264/ 311]               blk.23.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 265/ 311]               blk.23.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 266/ 311]               blk.23.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 267/ 311]                 blk.23.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 268/ 311]                 blk.24.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 269/ 311]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 270/ 311]              blk.24.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 271/ 311]            blk.24.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 272/ 311]                 blk.24.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 273/ 311]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 274/ 311]                 blk.24.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 275/ 311]               blk.24.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 276/ 311]               blk.24.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 277/ 311]               blk.24.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 278/ 311]                 blk.24.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 279/ 311]                 blk.25.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 280/ 311]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 281/ 311]              blk.25.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 282/ 311]            blk.25.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 283/ 311]                 blk.25.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 284/ 311]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 285/ 311]                 blk.25.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 286/ 311]               blk.25.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 287/ 311]               blk.25.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 288/ 311]               blk.25.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 289/ 311]                 blk.25.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 290/ 311]                 blk.26.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 291/ 311]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 292/ 311]              blk.26.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 293/ 311]            blk.26.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 294/ 311]                 blk.26.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 295/ 311]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 296/ 311]                 blk.26.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 297/ 311]               blk.26.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 298/ 311]               blk.26.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 299/ 311]               blk.26.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 300/ 311]                 blk.26.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 301/ 311]                 blk.27.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 302/ 311]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 303/ 311]              blk.27.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 304/ 311]            blk.27.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 305/ 311]                 blk.27.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 306/ 311]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 307/ 311]                 blk.27.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 308/ 311]               blk.27.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 309/ 311]               blk.27.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 310/ 311]               blk.27.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 311/ 311]                 blk.27.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\nllama_model_quantize_impl: model size  =  1433.75 MiB\nllama_model_quantize_impl: quant size  =   456.11 MiB\n\nmain: quantize time = 17247.52 ms\nmain:    total time = 17247.52 ms"
  },
  {
    "objectID": "src/05/notebooks/moe-heatmap.html#install-dependencies",
    "href": "src/05/notebooks/moe-heatmap.html#install-dependencies",
    "title": "Routing Heatmap for Small MoE Model",
    "section": "Install Dependencies",
    "text": "Install Dependencies\n\n!uv pip install -q --no-build-isolation torch flash_attn"
  },
  {
    "objectID": "src/05/notebooks/moe-heatmap.html#load-moe-model",
    "href": "src/05/notebooks/moe-heatmap.html#load-moe-model",
    "title": "Routing Heatmap for Small MoE Model",
    "section": "Load MoE Model",
    "text": "Load MoE Model\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-tiny-MoE-instruct\",\n    device_map=\"auto\",\n    dtype=torch.float16,\n    trust_remote_code=True,\n    output_router_logits=True  # Enable router outputs\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"microsoft/Phi-tiny-MoE-instruct\",\n    trust_remote_code=True\n)\n\n\n\n\n\n\n\nA new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-tiny-MoE-instruct:\n- configuration_slimmoe.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n\n\n\n\n\nA new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-tiny-MoE-instruct:\n- modeling_slimmoe.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision."
  },
  {
    "objectID": "src/05/notebooks/moe-heatmap.html#register-hooks-and-show-routing-for-a-few-tokens-in-first-layer",
    "href": "src/05/notebooks/moe-heatmap.html#register-hooks-and-show-routing-for-a-few-tokens-in-first-layer",
    "title": "Routing Heatmap for Small MoE Model",
    "section": "Register hooks and show routing for a few tokens in first layer",
    "text": "Register hooks and show routing for a few tokens in first layer\n\nPROMPT = \"What is hello in Japanese?\"\n\ngate_layers = []\nfor name, module in model.named_modules():\n    if 'block_sparse_moe.gate' in name:\n        gate_layers.append((name, module))\n\nprint(f\"Found {len(gate_layers)} gate layers\\n\")\n\nrouter_outputs = []\n\ndef router_hook(module, input, output):\n    router_outputs.append(output.detach().cpu())\n\n# Register hooks\nhooks = []\nfor name, module in gate_layers:\n    hooks.append(module.register_forward_hook(router_hook))\n\ninputs = tokenizer(PROMPT, return_tensors=\"pt\").to(model.device)\n\n# Run forward pass\nrouter_outputs.clear()\nwith torch.no_grad():\n    _ = model(**inputs, use_cache=False)\n\n# Analyze first layer\nrouter_probs = torch.softmax(router_outputs[0], dim=-1)\nprint(\"First layer routing:\")\nprint(f\"  Number of experts: {router_probs.shape[-1]}\")\nprint(f\"  Shape: {router_probs.shape}\")\n\n# Show top-k experts for first 5 tokens\nfor tok_idx in range(min(5, router_probs.shape[0])):\n    top_k = torch.topk(router_probs[tok_idx], k=4)\n    print(f\"  Token {tok_idx}: top experts {top_k.indices.tolist()} \"\n          f\"with probs {[f'{p:.3f}' for p in top_k.values.tolist()]}\")\n\nFound 32 gate layers\n\n\nFirst layer routing:\n  Number of experts: 16\n  Shape: torch.Size([6, 16])\n  Token 0: top experts [8, 11, 2, 15] with probs ['0.088', '0.078', '0.072', '0.071']\n  Token 1: top experts [1, 15, 2, 6] with probs ['0.087', '0.085', '0.082', '0.078']\n  Token 2: top experts [14, 3, 11, 4] with probs ['0.211', '0.081', '0.079', '0.072']\n  Token 3: top experts [12, 2, 1, 15] with probs ['0.098', '0.082', '0.082', '0.081']\n  Token 4: top experts [14, 5, 2, 10] with probs ['0.134', '0.068', '0.066', '0.063']"
  },
  {
    "objectID": "src/05/notebooks/moe-heatmap.html#display-routing-heatmap",
    "href": "src/05/notebooks/moe-heatmap.html#display-routing-heatmap",
    "title": "Routing Heatmap for Small MoE Model",
    "section": "Display routing heatmap",
    "text": "Display routing heatmap\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nPROMPT = \"What is hello in Japanese?\"\n\ndef capture_routing(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n\n    router_outputs.clear()\n    with torch.no_grad():\n        _ = model(**inputs, use_cache=False)\n\n    return router_outputs[:32], tokens\n\ndef routing_heatmap(router_logits, tokens, layer_idx=0):\n    # Get routing probabilities\n    router_probs = torch.softmax(router_logits[layer_idx], dim=-1).numpy()\n    num_tokens, num_experts = router_probs.shape\n\n    fig, axes = plt.subplots(1, 1, figsize=(8, 5))\n\n    # Use seaborn to display routing probabilities\n    sns.heatmap(\n        router_probs.T,\n        cmap='YlOrRd',\n        ax=axes,\n        cbar_kws={'label': 'Probability'},\n        xticklabels=tokens[:num_tokens],\n        yticklabels=[f'E{i}' for i in range(num_experts)]\n    )\n    axes.set_title(f'Layer {layer_idx}: Routing Probabilities per Token')\n    axes.set_xlabel('Token')\n    axes.set_ylabel('Expert')\n    plt.setp(axes.get_xticklabels(), rotation=45, ha='right')\n\noutputs, tokens = capture_routing(PROMPT)\n\n# Display heatmap for layer 0\nrouting_heatmap(outputs, tokens, layer_idx=0)"
  },
  {
    "objectID": "src/05/notebooks/moe-heatmap.html#display-routing-heatmap-for-additional-layers",
    "href": "src/05/notebooks/moe-heatmap.html#display-routing-heatmap-for-additional-layers",
    "title": "Routing Heatmap for Small MoE Model",
    "section": "Display routing heatmap for additional layers",
    "text": "Display routing heatmap for additional layers\n\nrouting_heatmap(outputs, tokens, layer_idx=8)\n\n\n\n\n\n\n\n\n\nrouting_heatmap(outputs, tokens, layer_idx=16)\n\n\n\n\n\n\n\n\n\nrouting_heatmap(outputs, tokens, layer_idx=24)\n\n\n\n\n\n\n\n\n\nrouting_heatmap(outputs, tokens, layer_idx=31)"
  },
  {
    "objectID": "src/04/notebooks/text-to-image-sd-1.5.html",
    "href": "src/04/notebooks/text-to-image-sd-1.5.html",
    "title": "Text-to-Image using Stable Diffusion 1.5",
    "section": "",
    "text": "import torch\nfrom diffusers import StableDiffusionPipeline\n\n# Load a small diffusion model\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id,\n)\n\n# Move to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npipe = pipe.to(device)\n\nFlax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\nFlax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \nError while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\nYou are not authenticated with the Hugging Face Hub in this notebook.\nIf the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeyword arguments {'generator': &lt;torch._C.Generator object at 0x7c30c9746a30&gt;} are not expected by StableDiffusionPipeline and will be ignored.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-sd-1.5.html#show-intermediate-steps",
    "href": "src/04/notebooks/text-to-image-sd-1.5.html#show-intermediate-steps",
    "title": "Text-to-Image using Stable Diffusion 1.5",
    "section": "Show Intermediate Steps",
    "text": "Show Intermediate Steps\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nPROMPT = \"a photograph of an astronaut riding a horse\" #@param {type:\"string\"}\nSTEPS = 50 #@param {type:\"slider\", min:10, max:100, step:1}\nSEED = -1 #@param {type:\"integer\"}\n\nintermediate_images = []\n\ndef callback_fn(step, timestep, latents):\n    \"\"\"Capture intermediate denoising steps\"\"\"\n    # Decode latents to image every few steps\n    if step % 5 == 0 or step == 0:\n        with torch.no_grad():\n            # Decode the latent representation to an image\n            image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\n            image = pipe.image_processor.postprocess(image, output_type=\"pil\")[0]\n            intermediate_images.append((step, image))\n\nresult = pipe(\n    PROMPT,\n    num_inference_steps=STEPS,\n    callback=callback_fn,\n    callback_steps=1,\n    generator=torch.Generator().manual_seed(SEED) if SEED != -1 else None,\n).images[0]\n\n# Visualize the denoising process\nnum_steps_to_show = min(10, len(intermediate_images))\nstep_indices = np.linspace(0, len(intermediate_images)-1, num_steps_to_show, dtype=int)\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfig.suptitle(f'Real Diffusion Model Denoising Process\\nPrompt: \"{PROMPT}\"')\n\nfor idx, step_idx in enumerate(step_indices):\n    row = idx // 5\n    col = idx % 5\n    step_num, img = intermediate_images[step_idx]\n\n    axes[row, col].imshow(img)\n    axes[row, col].axis('off')\n    axes[row, col].set_title(f'Step {step_num}/{STEPS}')\n\nplt.tight_layout()\nplt.savefig('diffusion_process.png', dpi=150, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-sd-1.5.html#show-final-image",
    "href": "src/04/notebooks/text-to-image-sd-1.5.html#show-final-image",
    "title": "Text-to-Image using Stable Diffusion 1.5",
    "section": "Show Final Image",
    "text": "Show Final Image\n\nplt.figure(figsize=(8, 8))\nplt.imshow(result)\nplt.axis('off')\nplt.title('Final Generated Image')\nplt.savefig('final_result.png', dpi=150, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#dependencies",
    "href": "src/04/notebooks/pbr-creator.html#dependencies",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Dependencies",
    "text": "Dependencies\n\n!uv pip install -q replicate",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#set-replicate-api-token",
    "href": "src/04/notebooks/pbr-creator.html#set-replicate-api-token",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Set Replicate API Token",
    "text": "Set Replicate API Token\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  REPLICATE_API_TOKEN = userdata.get('REPLICATE_API_TOKEN')\nelse:\n  load_dotenv()\n  REPLICATE_API_TOKEN = os.getenv('REPLICATE_API_TOKEN')",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#create-the-replicate-client",
    "href": "src/04/notebooks/pbr-creator.html#create-the-replicate-client",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Create the Replicate client",
    "text": "Create the Replicate client\n\nimport replicate\nimport io\nfrom PIL import Image\n\nclient = replicate.Client(api_token=REPLICATE_API_TOKEN)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#load-the-control-image",
    "href": "src/04/notebooks/pbr-creator.html#load-the-control-image",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Load the control image",
    "text": "Load the control image\n\nCONTROL_IMAGE = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/dragon.png\"\n\ncontrol = open(\"../images/dragon.png\", \"rb\")\ncontrol_bytes = io.BytesIO(control.read())\ncontrol_image = Image.open(control_bytes)\ncontrol_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#run-control-image-through-flux-pro-controlled-by-canny-edge-detection",
    "href": "src/04/notebooks/pbr-creator.html#run-control-image-through-flux-pro-controlled-by-canny-edge-detection",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Run control image through FLUX Pro (controlled by Canny edge detection)",
    "text": "Run control image through FLUX Pro (controlled by Canny edge detection)\n\n# @title Run initial image through FLUX Pro (controlled by Canny edge detection)\n\noutput = client.run(\n    \"black-forest-labs/flux-canny-pro\",\n    input={\n        \"steps\": 28,\n        \"seed\": 1234567, # Fix the seed so that the image is reproducible\n        \"prompt\": \"a metal embossed dragon, cinematic lighting\",\n        \"guidance\": 30,\n        \"control_image\": control,\n        \"output_format\": \"png\",\n        \"safety_tolerance\": 2,\n        \"prompt_upsampling\": False\n    }\n)\n\noutput_bytes = io.BytesIO(output.read())\noutput_image = Image.open(output_bytes)\noutput_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#tile-the-image-using-flux-schnell-and-ideogram-inpainting",
    "href": "src/04/notebooks/pbr-creator.html#tile-the-image-using-flux-schnell-and-ideogram-inpainting",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Tile the image using FLUX schnell and ideogram inpainting",
    "text": "Tile the image using FLUX schnell and ideogram inpainting\n\ntiled = client.run(\n  \"pipeline-examples/tile\",\n  input={\n      \"model\": \"black-forest-labs/flux-schnell\",\n      \"prompt\": \"a network of interconnected dragons\",\n      \"input_image\": output_bytes,\n      \"inpaint_model\": \"ideogram\",\n      \"seam_percentage\": 30\n  }\n)\n\ntiled_bytes = io.BytesIO(tiled.read())\ntiled_image = Image.open(tiled_bytes)\ntiled_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#generate-a-depth-map-using-depth-anything",
    "href": "src/04/notebooks/pbr-creator.html#generate-a-depth-map-using-depth-anything",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Generate a depth map using Depth Anything",
    "text": "Generate a depth map using Depth Anything\n\ndepth = client.run(\n    \"chenxwh/depth-anything-v2:b239ea33cff32bb7abb5db39ffe9a09c14cbc2894331d1ef66fe096eed88ebd4\",\n    input={\n        \"image\": tiled_bytes,\n        \"model_size\": \"Large\"\n    }\n)\n\ndepth_bytes = io.BytesIO(depth[\"grey_depth\"].read())\ndepth_image = Image.open(depth_bytes)\ndepth_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#convert-the-depth-map-to-a-normal-map-for-pbr",
    "href": "src/04/notebooks/pbr-creator.html#convert-the-depth-map-to-a-normal-map-for-pbr",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Convert the depth map to a normal map for PBR",
    "text": "Convert the depth map to a normal map for PBR\n\nimport numpy as np\n\ndef depth_to_normal(depth_image, strength=1.0):\n    depth_array = np.array(depth_image, dtype=np.float32) / 255.0\n\n    # Calculate gradients (sobel-like operators)\n    # Sobel kernels for x and y directions\n    sobel_x = np.array([[-1, 0, 1],\n                        [-2, 0, 2],\n                        [-1, 0, 1]]) * strength\n\n    sobel_y = np.array([[-1, -2, -1],\n                        [ 0,  0,  0],\n                        [ 1,  2,  1]]) * strength\n\n    # Pad the depth array to handle edges\n    padded_depth = np.pad(depth_array, 1, mode='edge')\n\n    # Initialize normal map components\n    height, width = depth_array.shape\n    dx = np.zeros((height, width))\n    dy = np.zeros((height, width))\n\n    # Calculate gradients using convolution\n    for i in range(height):\n        for j in range(width):\n            region = padded_depth[i:i+3, j:j+3]\n            dx[i, j] = np.sum(region * sobel_x)\n            dy[i, j] = np.sum(region * sobel_y)\n\n    # Create normal vectors\n    # In tangent space: x points right, y points down, z points out\n    # Normal = normalize(-dx, -dy, 1)\n    dz = np.ones_like(dx)\n\n    # Calculate magnitude for normalization\n    magnitude = np.sqrt(dx*dx + dy*dy + dz*dz)\n\n    # Normalize the vectors\n    nx = -dx / magnitude\n    ny = -dy / magnitude\n    nz = dz / magnitude\n\n    # Convert from [-1, 1] to [0, 255] for RGB channels\n    # R = X, G = Y, B = Z\n    normal_map = np.zeros((height, width, 3), dtype=np.uint8)\n    normal_map[:, :, 0] = ((nx + 1) * 0.5 * 255).astype(np.uint8)  # Red (X)\n    normal_map[:, :, 1] = ((ny + 1) * 0.5 * 255).astype(np.uint8)  # Green (Y)\n    normal_map[:, :, 2] = ((nz + 1) * 0.5 * 255).astype(np.uint8)  # Blue (Z)\n\n    # Save the normal map\n    normal_img = Image.fromarray(normal_map)\n\n    return normal_img\n\nnormal_image = depth_to_normal(depth_image.convert('L'), strength=1.0)\nnormal_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#save-files",
    "href": "src/04/notebooks/pbr-creator.html#save-files",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Save files",
    "text": "Save files\n\ntiled_image.save(\"dragon_tiled.png\")\nnormal_image.save(\"dragon_normal.png\")",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/inpainting.html",
    "href": "src/04/notebooks/inpainting.html",
    "title": "Inpainting using black-forest-labs/flux-fill-pro",
    "section": "",
    "text": "!uv pip install gradio==5.49.1 replicate",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "inpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/inpainting.html#set-replicate-api-token",
    "href": "src/04/notebooks/inpainting.html#set-replicate-api-token",
    "title": "Inpainting using black-forest-labs/flux-fill-pro",
    "section": "Set Replicate API Token",
    "text": "Set Replicate API Token\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  os.environ['REPLICATE_API_TOKEN'] = userdata.get('REPLICATE_API_TOKEN')\n  print(\"Replicate API Token set for Colab\")\nelse:\n  load_dotenv()\n  print(\"Loaded env vars from .env\")",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "inpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/inpainting.html#gradio-interface",
    "href": "src/04/notebooks/inpainting.html#gradio-interface",
    "title": "Inpainting using black-forest-labs/flux-fill-pro",
    "section": "Gradio Interface",
    "text": "Gradio Interface\n\nimport gradio as gr\nimport numpy as np\nfrom PIL import Image\nimport replicate\nimport io\n\ndef inpaint(image_data, prompt):\n    if image_data is None:\n        return None\n    \n    # Get the original image from the background\n    original_image = Image.fromarray(image_data['background'])\n    \n    # Get the mask from the layers\n    if image_data['layers'] and len(image_data['layers']) &gt; 0:\n        mask_layer = image_data['layers'][0]\n        mask_array = np.array(mask_layer)\n        \n        # Create binary mask: white where painted, black where not\n        alpha_channel = mask_array[:, :, 3]\n        binary_mask = np.where(alpha_channel &gt; 0, 255, 0).astype(np.uint8)\n        mask_image = Image.fromarray(binary_mask, mode='L')\n    else:\n        return None\n    \n    # Convert images to bytes for the replicate API\n    image_bytes = io.BytesIO()\n    original_image.save(image_bytes, format='PNG')\n    image_bytes.seek(0)\n    \n    mask_bytes = io.BytesIO()\n    mask_image.save(mask_bytes, format='PNG')\n    mask_bytes.seek(0)\n    \n    # Call the Replicate API\n    output = replicate.run(\n        \"black-forest-labs/flux-fill-pro\",\n        input={\n            \"image\": image_bytes,\n            \"mask\": mask_bytes,\n            \"prompt\": prompt,\n            \"steps\": 25,\n            \"guidance\": 75,\n            \"outpaint\": \"None\",\n            \"output_format\": \"jpg\",\n            \"safety_tolerance\": 2,\n            \"prompt_upsampling\": False\n        }\n    )\n    \n    # Read the FileOutput and convert to PIL Image\n    output_bytes = output.read()\n    output_image = Image.open(io.BytesIO(output_bytes))\n    \n    return output_image\n\ndemo = gr.Interface(\n    fn=inpaint,\n    inputs=[\n        gr.ImageEditor(\n            label=\"Image (paint over areas to inpaint)\",\n            brush=gr.Brush(color_mode=\"fixed\", colors=[\"#000000\"]),\n            layers=True\n        ),\n        gr.Textbox(label=\"Prompt\", placeholder=\"Describe what should replace the masked area...\")\n    ],\n    outputs=gr.Image(label=\"Output Image\"),\n    title=\"Inpainting using black-forest-labs/flux-fill-pro\"\n)\ndemo.launch(debug=True)\n\n* Running on local URL:  http://127.0.0.1:7861\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "inpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/generate-depth-map.html#install-dependencies",
    "href": "src/04/notebooks/generate-depth-map.html#install-dependencies",
    "title": "Depth Map Generation",
    "section": "Install Dependencies",
    "text": "Install Dependencies\n\n!uv pip install replicate -q",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "generate-depth-map.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/generate-depth-map.html#set-replicate-api-token",
    "href": "src/04/notebooks/generate-depth-map.html#set-replicate-api-token",
    "title": "Depth Map Generation",
    "section": "Set Replicate API Token",
    "text": "Set Replicate API Token\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  os.environ['REPLICATE_API_TOKEN'] = userdata.get('REPLICATE_API_TOKEN')\n  print(\"Replicate API Token set for Colab\")\nelse:\n  load_dotenv()\n  print(\"Loaded env vars from .env\")",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "generate-depth-map.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/generate-depth-map.html#input-image",
    "href": "src/04/notebooks/generate-depth-map.html#input-image",
    "title": "Depth Map Generation",
    "section": "Input Image",
    "text": "Input Image\n\nfrom IPython.display import Image\n\nINPUT_IMAGE = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/campus.png\"\n\nImage(url=INPUT_IMAGE)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "generate-depth-map.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/generate-depth-map.html#image-to-image-using-depth-anything-v2",
    "href": "src/04/notebooks/generate-depth-map.html#image-to-image-using-depth-anything-v2",
    "title": "Depth Map Generation",
    "section": "Image-to-Image using depth-anything-v2",
    "text": "Image-to-Image using depth-anything-v2\n\nimport replicate\n\nMODEL = \"chenxwh/depth-anything-v2:b239ea33cff32bb7abb5db39ffe9a09c14cbc2894331d1ef66fe096eed88ebd4\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"image\": INPUT_IMAGE,\n  },\n)\n\n\nimport io\nfrom PIL import Image\n\noutput_bytes = io.BytesIO(output[\"grey_depth\"].read())\noutput_image = Image.open(output_bytes)\noutput_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "generate-depth-map.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#install-prerequisites",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#install-prerequisites",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Install prerequisites",
    "text": "Install prerequisites\n\n!uv pip install -q diffusers==0.30.0 transformers==4.44.0 accelerate controlnet_aux pillow",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#original-image",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#original-image",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Original image",
    "text": "Original image\n\nfrom diffusers.utils import load_image\n\nINPUT_IMAGE = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/pose.jpg\"\n\ninput_image = load_image(INPUT_IMAGE)\ninput_image = input_image.resize((512, 683))\ndisplay(input_image)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-openpose-detector",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-openpose-detector",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Load OpenPose detector",
    "text": "Load OpenPose detector\n\nfrom controlnet_aux import OpenposeDetector\n\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#derive-skeletal-pose",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#derive-skeletal-pose",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Derive skeletal pose",
    "text": "Derive skeletal pose\n\npose_image = openpose(input_image)\ndisplay(pose_image)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-controlnet-model",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-controlnet-model",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Load ControlNet model",
    "text": "Load ControlNet model\n\nimport torch\nfrom diffusers import ControlNetModel\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"lllyasviel/control_v11p_sd15_openpose\",\n    torch_dtype=torch.float16\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-sd-1.5-model-and-plug-in-controlnet-model",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-sd-1.5-model-and-plug-in-controlnet-model",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Load SD 1.5 model and plug in controlnet model",
    "text": "Load SD 1.5 model and plug in controlnet model\n\nfrom diffusers import StableDiffusionControlNetPipeline\n\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    use_safetensors=True\n)\npipe.to(\"cuda\")\n\nKeyword arguments {'generators': [&lt;torch._C.Generator object at 0x7e3df76b3410&gt;]} are not expected by StableDiffusionControlNetPipeline and will be ignored.\n\n\n\n\n\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n\nStableDiffusionControlNetPipeline {\n  \"_class_name\": \"StableDiffusionControlNetPipeline\",\n  \"_diffusers_version\": \"0.30.0\",\n  \"_name_or_path\": \"runwayml/stable-diffusion-v1-5\",\n  \"controlnet\": [\n    \"diffusers\",\n    \"ControlNetModel\"\n  ],\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"image_encoder\": [\n    null,\n    null\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"PNDMScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#generate-new-image-with-human-pose-as-condition",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#generate-new-image-with-human-pose-as-condition",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Generate new image with human pose as condition",
    "text": "Generate new image with human pose as condition\n\nPROMPT = \"a robot with glowing LED lights, futuristic, sci-fi\"\nNEGATIVE_PROMPT = \"blurry, low quality, distorted, extra limbs, deformed\"\nSEED = 3434002\n\nresult = pipe(\n    prompt=PROMPT,\n    negative_prompt=NEGATIVE_PROMPT,\n    image=pose_image,\n    num_inference_steps=25,\n    guidance_scale=7.5,\n    controlnet_conditioning_scale=1.0,\n    generator=torch.manual_seed(SEED) if SEED != -1 else None\n).images[0]\ndisplay(result)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html",
    "href": "src/03/notebooks/memory.html",
    "title": "Agent Memory",
    "section": "",
    "text": "!uv pip install openai-agents==0.4.2\n\n\nResolved 190 packages in 1ms\n\nAudited 157 packages in 0.07ms",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#set-the-openai-api-key-environment-variable",
    "href": "src/03/notebooks/memory.html#set-the-openai-api-key-environment-variable",
    "title": "Agent Memory",
    "section": "Set the OpenAI API Key environment variable",
    "text": "Set the OpenAI API Key environment variable\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nelse:\n  load_dotenv()",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#set-the-db-folder",
    "href": "src/03/notebooks/memory.html#set-the-db-folder",
    "title": "Agent Memory",
    "section": "Set the DB folder",
    "text": "Set the DB folder\n\n# Create the .data directory for the SQLite db\n!mkdir -p .data\nSQLITE_DB = \"./.data/conversations.sqlite\"",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#create-the-session",
    "href": "src/03/notebooks/memory.html#create-the-session",
    "title": "Agent Memory",
    "section": "Create the session",
    "text": "Create the session\n\nfrom agents import Agent, Runner, SQLiteSession\n\nagent = Agent(name=\"Assistant\", instructions=\"Reply very concisely\")\nsession = SQLiteSession(\"conv_123\", db_path=SQLITE_DB)\n\n\nConversation stored in session\n\nresult = await Runner.run(agent, \"My name is Simon\", session=session)\nprint(result.final_output)\n\nNice to meet you, Simon!\n\n\n\n\nContext retrieved from session\n\nresult = await Runner.run(agent, \"What is my name?\", session=session)\nprint(result.final_output)\n\nYour name is Simon.\n\n\n\n\nWithout session\n\nresult = await Runner.run(agent, \"What is my name?\")\nprint(result.final_output)\n\nYou havent shared your name yet.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#contents-of-sqlite-db",
    "href": "src/03/notebooks/memory.html#contents-of-sqlite-db",
    "title": "Agent Memory",
    "section": "Contents of SQLite db",
    "text": "Contents of SQLite db\n\nimport sqlite3\nimport pandas as pd\n\nconn = sqlite3.connect(SQLITE_DB)\n\nsessions_df = pd.read_sql_query(\"SELECT * FROM agent_sessions\", conn)\ndisplay(sessions_df)\n\nmessages_df = pd.read_sql_query(\"SELECT * FROM agent_messages\", conn)\ndisplay(messages_df)\n\nconn.close()\n\n\n\n\n\n\n\n\nsession_id\ncreated_at\nupdated_at\n\n\n\n\n0\nconv_123\n2026-01-15 21:39:00\n2026-01-15 21:39:37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nsession_id\nmessage_data\ncreated_at\n\n\n\n\n0\n1\nconv_123\n{\"content\": \"My name is Simon\", \"role\": \"user\"}\n2026-01-15 21:39:00\n\n\n1\n2\nconv_123\n{\"id\": \"msg_0f964931c939f4d70069695e75c99c8194...\n2026-01-15 21:39:02\n\n\n2\n3\nconv_123\n{\"content\": \"What is my name?\", \"role\": \"user\"}\n2026-01-15 21:39:04\n\n\n3\n4\nconv_123\n{\"id\": \"msg_0f964931c939f4d70069695e7955b88194...\n2026-01-15 21:39:05\n\n\n4\n5\nconv_123\n{\"content\": \"What is my name?\", \"role\": \"user\"}\n2026-01-15 21:39:36\n\n\n5\n6\nconv_123\n{\"id\": \"msg_0f964931c939f4d70069695e98f26c8194...\n2026-01-15 21:39:37",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/token-streaming.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/token-streaming.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Token Streaming",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "token-streaming.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/token-streaming.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/token-streaming.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Token Streaming",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "token-streaming.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/token-streaming.html#create-the-openai-client-with-openrouter-url",
    "href": "src/02/notebooks/token-streaming.html#create-the-openai-client-with-openrouter-url",
    "title": "Token Streaming",
    "section": "Create the OpenAI client with OpenRouter URL",
    "text": "Create the OpenAI client with OpenRouter URL\n\nimport openai\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n)\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n    stream=True, # Enable streaming\n)\n\n# Iterate through the stream and print each token as it arrives\nfor chunk in response:\n    # Each chunk contains a delta with the new content\n    if chunk.choices[0].delta.content is not None:\n        token = chunk.choices[0].delta.content\n        print(token, end='', flush=True)\n\nParis has something for almost every type of traveler, so Ill give you a wellrounded starting plan. If you want something more tailored (food, art, budget, familyfriendly, etc.), just tell me.\n\n### MustSee Classics\n- **Eiffel Tower**  Go up if its your first time, or enjoy views from **Trocadro** or **Champ de Mars**.\n- **Louvre Museum**  Even a 23 hour focused visit is worthwhile (book tickets in advance).\n- **NotreDame Cathedral**  Admire the exterior and nearby **le de la Cit**; check reopening status for interior access.\n- **Arc de Triomphe**  Climb to the top for one of the best city views.\n\n### Neighborhoods to Explore\n- **Montmartre**  Artistic vibes, SacrCur, charming streets.\n- **Le Marais**  Trendy shops, historic mansions, great food.\n- **Latin Quarter**  Lively, student energy, bookshops, cafs.\n- **SaintGermaindesPrs**  Classic cafs and upscale shopping.\n\n### Food & Drink Experiences\n- Eat at a **local bistro** (look for a fixedprice *menu du jour*).\n- Try **croissants & pain au chocolat** from a neighborhood bakery.\n- Visit a **fromagerie** and **wine bar**.\n- Enjoy caf culture: sit outside, order a coffee or wine, and peoplewatch.\n- Dont miss **crpes**, **macarons**, and **cheese**.\n\n### Cultural & Unique Experiences\n- **Seine River cruise** (especially at night).\n- **Muse dOrsay** for Impressionist art.\n- **Versailles** day trip for palace and gardens.\n- **Cooking class** or **food tour**.\n- Wander without a planParis is best discovered on foot.\n\n### Practical Tips\n- Buy museum tickets in advance to skip lines.\n- Learn a few French phrasesit goes a long way.\n- Dress comfortably but stylishly; Parisians walk a lot.\n- Use public transport (metro is fast and affordable).\n\nIf youd like, tell me:\n- How many days youll be there  \n- Time of year  \n- Your interests (food, museums, nightlife, romance, budget travel)\n\nI can build you a **daybyday itinerary** just for your trip.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "token-streaming.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#load-both-the-base-and-instruction-tuned-models",
    "href": "src/02/notebooks/instruction-tuned.html#load-both-the-base-and-instruction-tuned-models",
    "title": "Base Model vs.Instruction-Tuned Model",
    "section": "Load both the base and instruction-tuned models",
    "text": "Load both the base and instruction-tuned models\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load base (completion-only) model\nbase_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\nbase_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n\n# Load instruct model  \ninstruct_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ninstruct_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#base-completion-model-output",
    "href": "src/02/notebooks/instruction-tuned.html#base-completion-model-output",
    "title": "Base Model vs.Instruction-Tuned Model",
    "section": "Base (Completion) Model Output",
    "text": "Base (Completion) Model Output\n\nbase_inputs = base_tokenizer(\"What should I do on my upcoming trip to Paris?\", return_tensors=\"pt\")\nbase_outputs = base_model.generate(\n    **base_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=base_tokenizer.eos_token_id\n)\nbase_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\nprint(base_response)\n\nWhat should I do on my upcoming trip to Paris? I think it would be better if you could give more specific information about where you plan to go and when you plan to arrive. Also, can you suggest any specific tips or recommendations for traveling to Paris other than walking around the city?\n\nI'm sorry, but as an AI language model, I don't have any specific information about your upcoming trip to Paris. However, I can suggest some general tips and recommendations for traveling to Paris other than walking around the city:\n\n1. Plan your itinerary ahead of time to avoid getting lost or getting in over your head.\n2. Book your flights or accommodations in advance to avoid being stuck in traffic or waiting for a delayed flight.\n3. Purchase a travel insurance policy to protect your belongings and reduce the risk of",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#instruction-tuned-model-output",
    "href": "src/02/notebooks/instruction-tuned.html#instruction-tuned-model-output",
    "title": "Base Model vs.Instruction-Tuned Model",
    "section": "Instruction-Tuned Model Output",
    "text": "Instruction-Tuned Model Output\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\ninstruct_text = instruct_tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\ninstruct_inputs = instruct_tokenizer(instruct_text, return_tensors=\"pt\")\ninstruct_outputs = instruct_model.generate(\n    **instruct_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=instruct_tokenizer.eos_token_id,\n)\ninstruct_response = instruct_tokenizer.decode(\n    instruct_outputs[0], skip_special_tokens=True\n)\nprint(instruct_response)\n\nsystem\nYou help travelers make plans for their trips.\nuser\nHello\nassistant\nHi there!\nuser\nWhat should I do on my upcoming trip to Paris?\nassistant\nGreat question! On your next trip to Paris, you can start by visiting the iconic Eiffel Tower and the Louvre Museum. Don't miss exploring the Notre-Dame Cathedral and its stunning stained glass windows. For a bit of a break, consider visiting Montmartre for some beautiful art and architecture. If you're looking for something more adventurous, you could take a stroll through the charming streets of Montmartre or explore the vibrant nightlife of Le Marais. Have fun planning your trip to Paris!",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#extra-display-qwens-chat-template",
    "href": "src/02/notebooks/instruction-tuned.html#extra-display-qwens-chat-template",
    "title": "Base Model vs.Instruction-Tuned Model",
    "section": "Extra: Display Qwens chat template",
    "text": "Extra: Display Qwens chat template\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\n\ninstruct_tokenizer.apply_chat_template(\n    messages, \n    tokenize=False,\n    add_generation_prompt=True  # Adds the assistant prompt\n)\n\n'&lt;|im_start|&gt;system\\nYou help travelers make plans for their trips&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nHello&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\nHi there!&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nWhat should I do on my upcoming trip to Paris?&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n'",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/chat-completion-openrouter.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/chat-completion-openrouter.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#logging-function-to-print-the-api-request-to-the-console",
    "href": "src/02/notebooks/chat-completion-openrouter.html#logging-function-to-print-the-api-request-to-the-console",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "Logging function to print the API request to the console",
    "text": "Logging function to print the API request to the console\n\nimport json\n\ndef log_request(request):\n  print(\"\\n=== REQUEST ===\")\n  print(f\"URL: {request.url}\")\n  print(f\"Method: {request.method}\")\n\n  if request.content:\n    try:\n      body = json.loads(request.content.decode('utf-8'))\n      print(\"\\nBody:\")\n      print(json.dumps(body, indent=2))\n    except:\n      print(\"\\nBody:\")\n      print(request.content.decode('utf-8'))\n  print(\"=\" * 50)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#call-openai-via-the-sdk",
    "href": "src/02/notebooks/chat-completion-openrouter.html#call-openai-via-the-sdk",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "Call OpenAI via the SDK",
    "text": "Call OpenAI via the SDK\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://openrouter.ai/api/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"openai/gpt-5.2-chat\"\n}\n==================================================\n\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"gen-1767585819-snubWxcK6sJM3RdE9rJX\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Paris has something for almost every kind of traveler! Heres a wellrounded starting plan, and then I can tailor it more if you tell me your interests, travel dates, and how long youll be there.\\n\\n### MustSee Highlights\\n- **Eiffel Tower**  Go up for the views or enjoy it from below at Trocadro or Champ de Mars.\\n- **Louvre Museum**  Even if you dont love museums, seeing the Mona Lisa and the building itself is worth it.\\n- **NotreDame Cathedral**  Admire the exterior and surroundings; interior access is gradually reopening.\\n- **Montmartre & SacrCur**  Charming streets, artists, and great city views.\\n\\n### Classic Paris Experiences\\n- **Stroll along the Seine**  Especially at sunset.\\n- **Caf culture**  Sit at a caf with a coffee or glass of wine and peoplewatch.\\n- **Boulangeries & pastries**  Try croissants, pain au chocolat, macarons.\\n- **Seine river cruise**  Relaxing and great for firsttime visitors.\\n\\n### Art, History & Culture\\n- **Muse dOrsay**  Impressionist masterpieces in a stunning former train station.\\n- **Le Marais**  Historic district with boutiques, museums, and lively streets.\\n- **Latin Quarter**  Bookshops, old streets, and student energy.\\n\\n### Food & Drink\\n- **Bistro dining**  Try classic French dishes like boeuf bourguignon or duck confit.\\n- **Food markets**  March des Enfants Rouges is a favorite.\\n- **Wine & cheese tasting**  Many small shops offer guided tastings.\\n\\n### Day Trips (if you have extra time)\\n- **Versailles**  Palace and gardens (halfday or fullday trip).\\n- **Giverny**  Monets gardens (spring/summer).\\n- **Champagne region**  For wine lovers.\\n\\n### Practical Tips\\n- Buy museum tickets in advance.\\n- Walk as much as possibleParis is very walkable.\\n- Learn a few French phrases; locals appreciate the effort.\\n\\nIf youd like, tell me:\\n- How many days youll be there  \\n- Your interests (food, art, history, shopping, nightlife, romance, family travel)  \\n- Your budget level  \\n\\nAnd Ill create a personalized daybyday itinerary for you.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": null,\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null,\n        \"reasoning\": null\n      },\n      \"native_finish_reason\": \"completed\"\n    }\n  ],\n  \"created\": 1767585819,\n  \"model\": \"openai/gpt-5.2-chat\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 506,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 550,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": null,\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"rejected_prediction_tokens\": null,\n      \"image_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0,\n      \"video_tokens\": 0\n    },\n    \"cost\": 0.007161,\n    \"is_byok\": false,\n    \"cost_details\": {\n      \"upstream_inference_cost\": null,\n      \"upstream_inference_prompt_cost\": 0.000077,\n      \"upstream_inference_completions_cost\": 0.007084\n    }\n  },\n  \"provider\": \"OpenAI\"\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html",
    "href": "src/01/notebooks/word2vec.html",
    "title": "Word2Vec",
    "section": "",
    "text": "This notebook explores Word2Vec embeddings to understand how they capture semantic relationships.\nUses pre-trained embeddings from Google News (trained on ~100 billion words).",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#install-required-packages",
    "href": "src/01/notebooks/word2vec.html#install-required-packages",
    "title": "Word2Vec",
    "section": "Install required packages",
    "text": "Install required packages\n\n!uv pip install gensim numpy matplotlib scikit-learn -q",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#load-pretrained-word2vec-model",
    "href": "src/01/notebooks/word2vec.html#load-pretrained-word2vec-model",
    "title": "Word2Vec",
    "section": "Load pretrained Word2Vec model",
    "text": "Load pretrained Word2Vec model\n\nimport gensim.downloader as api\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load pre-trained Word2Vec model (Google News, 300-dimensional vectors)\nprint(\"Loading Word2Vec model...\")\nmodel = api.load('word2vec-google-news-300')\nprint(f\"Model loaded. Vocabulary size: {len(model)} words\")\nprint(f\"Vector dimension: {model.vector_size}\") # type: ignore\n\nLoading Word2Vec model...\nModel loaded. Vocabulary size: 3000000 words\nVector dimension: 300\n\n\n\nword = \"cat\"\nvector = model[word]\nvector[:10]\n\narray([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656],\n      dtype=float32)\n\n\n\nword = \"dog\"\nvector = model[word]\nvector[:10]\n\narray([ 0.05126953, -0.02233887, -0.17285156,  0.16113281, -0.08447266,\n        0.05737305,  0.05859375, -0.08251953, -0.01538086, -0.06347656],\n      dtype=float32)\n\n\n\nword = \"pizza\"\nvector = model[word]\nvector[:10]\n\narray([-1.2597656e-01,  2.5390625e-02,  1.6699219e-01,  5.5078125e-01,\n       -7.6660156e-02,  1.2890625e-01,  1.0253906e-01, -3.9482117e-04,\n        1.2158203e-01,  4.3212891e-02], dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#find-similar-words",
    "href": "src/01/notebooks/word2vec.html#find-similar-words",
    "title": "Word2Vec",
    "section": "Find similar words",
    "text": "Find similar words\nWords with similar meanings have similar vectors.\n\ndef find_similar_words(word, top_n=10):\n    \"\"\"Find the most similar words to a given word.\"\"\"\n    try:\n        similar = model.most_similar(word, topn=top_n) # type: ignore\n        print(f\"\\nWords most similar to '{word}':\")\n        print(\"-\" * 40)\n        for similar_word, similarity in similar:\n            print(f\"{similar_word:20s} | similarity: {similarity:.4f}\")\n    except KeyError:\n        print(f\"Word '{word}' not in vocabulary\")\n\n\nfind_similar_words(\"cat\")\nfind_similar_words(\"dog\")\nfind_similar_words(\"pizza\")\n\n\nWords most similar to 'cat':\n----------------------------------------\ncats                 | similarity: 0.8099\ndog                  | similarity: 0.7609\nkitten               | similarity: 0.7465\nfeline               | similarity: 0.7326\nbeagle               | similarity: 0.7151\npuppy                | similarity: 0.7075\npup                  | similarity: 0.6934\npet                  | similarity: 0.6892\nfelines              | similarity: 0.6756\nchihuahua            | similarity: 0.6710\n\nWords most similar to 'dog':\n----------------------------------------\ndogs                 | similarity: 0.8680\npuppy                | similarity: 0.8106\npit_bull             | similarity: 0.7804\npooch                | similarity: 0.7627\ncat                  | similarity: 0.7609\ngolden_retriever     | similarity: 0.7501\nGerman_shepherd      | similarity: 0.7465\nRottweiler           | similarity: 0.7438\nbeagle               | similarity: 0.7419\npup                  | similarity: 0.7407\n\nWords most similar to 'pizza':\n----------------------------------------\npizzas               | similarity: 0.7863\nDomino_pizza         | similarity: 0.7343\nPizza                | similarity: 0.6988\npepperoni_pizza      | similarity: 0.6903\nsandwich             | similarity: 0.6840\nburger               | similarity: 0.6570\nsandwiches           | similarity: 0.6495\ntakeout_pizza        | similarity: 0.6492\ngourmet_pizza        | similarity: 0.6401\nmeatball_sandwich    | similarity: 0.6377",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#compute-similarity",
    "href": "src/01/notebooks/word2vec.html#compute-similarity",
    "title": "Word2Vec",
    "section": "Compute similarity",
    "text": "Compute similarity\n\ndef compute_similarity(word1, word2):\n    \"\"\"Compute cosine similarity between two words.\"\"\"\n    try:\n        similarity = model.similarity(word1, word2) # type: ignore\n        print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n    except KeyError as e:\n        print(f\"Word not in vocabulary: {e}\")\n\n\ncompute_similarity('cat', 'dog')\ncompute_similarity('cat', 'kitten')\ncompute_similarity('cat', 'car')\ncompute_similarity('doctor', 'hospital')\ncompute_similarity('king', 'queen')\n\nSimilarity between 'cat' and 'dog': 0.7609\nSimilarity between 'cat' and 'kitten': 0.7465\nSimilarity between 'cat' and 'car': 0.2153\nSimilarity between 'doctor' and 'hospital': 0.5143\nSimilarity between 'king' and 'queen': 0.6511",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#vector-arithmetic",
    "href": "src/01/notebooks/word2vec.html#vector-arithmetic",
    "title": "Word2Vec",
    "section": "Vector arithmetic",
    "text": "Vector arithmetic\n\ndef vector_arithmetic(positive, negative, top_n=5):\n    \"\"\"Perform vector arithmetic: positive words - negative words.\"\"\"\n    try:\n        result = model.most_similar(positive=positive, negative=negative, topn=top_n) # type: ignore\n        print(f\"\\n{' + '.join(positive)} - {' - '.join(negative)}:\")\n        print(\"-\" * 50)\n        for word, similarity in result:\n            print(f\"{word:20s} | similarity: {similarity:.4f}\")\n    except KeyError as e:\n        print(f\"Word not in vocabulary: {e}\")\n\n\nvector_arithmetic(['king', 'woman'], ['man'])\nvector_arithmetic(['Paris', 'Italy'], ['France'])\nvector_arithmetic(['walking', 'swim'], ['walk'])\n\n\nking + woman - man:\n--------------------------------------------------\nqueen                | similarity: 0.7118\nmonarch              | similarity: 0.6190\nprincess             | similarity: 0.5902\ncrown_prince         | similarity: 0.5499\nprince               | similarity: 0.5377\n\nParis + Italy - France:\n--------------------------------------------------\nMilan                | similarity: 0.7222\nRome                 | similarity: 0.7028\nPalermo_Sicily       | similarity: 0.5968\nItalian              | similarity: 0.5911\nTuscany              | similarity: 0.5633\n\nwalking + swim - walk:\n--------------------------------------------------\nswimming             | similarity: 0.8246\nswam                 | similarity: 0.6807\nswims                | similarity: 0.6538\nswimmers             | similarity: 0.6495\npaddling             | similarity: 0.6424",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#d-visualization",
    "href": "src/01/notebooks/word2vec.html#d-visualization",
    "title": "Word2Vec",
    "section": "2D visualization",
    "text": "2D visualization\n\ndef visualize_words(words, method='tsne'):\n    \"\"\"Visualize word embeddings in 2D.\"\"\"\n    # Get vectors for words that exist in vocabulary\n    valid_words = [w for w in words if w in model]\n    if len(valid_words) &lt; 2:\n        print(\"Need at least 2 valid words to visualize\")\n        return\n    \n    vectors = np.array([model[w] for w in valid_words])\n    \n    # Reduce to 2D\n    if method == 'tsne':\n        reducer = TSNE(n_components=2, random_state=42, perplexity=min(5, len(valid_words)-1))\n    else:\n        reducer = PCA(n_components=2, random_state=42)\n    \n    vectors_2d = reducer.fit_transform(vectors)\n    \n    # Plot\n    plt.figure(figsize=(12, 8))\n    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=200, alpha=0.6)\n    \n    for i, word in enumerate(valid_words):\n        plt.annotate(word, \n                    xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n                    xytext=(5, 5),\n                    textcoords='offset points',\n                    fontsize=12,\n                    fontweight='bold')\n    \n    plt.title(f'Word Embeddings Visualization ({method.upper()})', fontsize=16)\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\nwords = ['cat', 'dog', 'kitten', 'puppy', 'lion', 'tiger', 'elephant', 'mouse', 'chicken', 'rat']\nvisualize_words(words)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html",
    "href": "src/01/notebooks/hello-world.html",
    "title": "Hello World Notebook!",
    "section": "",
    "text": "This is an example of the Jupyter .ipynb document format\n# This is an executable cell\nprint(\"Hello World!\")\n\nHello World!\n# Setting variables in Python\nx = 42\nx\n\n42\n# Variables persist after being set in previously executed cells\nx\n\n42",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html#markdown-cells-support-rich-formatting",
    "href": "src/01/notebooks/hello-world.html#markdown-cells-support-rich-formatting",
    "title": "Hello World Notebook!",
    "section": "Markdown Cells Support Rich Formatting",
    "text": "Markdown Cells Support Rich Formatting\nYou can use: - Bold and italic text - Lists (like this one!) - Links - inline code - And even LaTeX math: \\(E = mc^2\\)\nThis makes notebooks great for explaining your code!\n\n# You can perform calculations across cells\ny = 10\nz = x + y\nprint(f\"x ({x}) + y ({y}) = {z}\")\n\n\n# Notebooks make it easy to import and use libraries\nimport math\nimport random\n\n# Generate a random number and calculate its square root\nnum = random.randint(1, 100)\nsqrt_num = math.sqrt(num)\nprint(f\"The square root of {num} is {sqrt_num:.2f}\")\n\nThe square root of 29 is 5.39\n\n\n\n# Visualizations appear inline!\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx_values = np.linspace(0, 10, 100)\ny_values = np.sin(x_values)\n\nplt.figure(figsize=(8, 4))\nplt.plot(x_values, y_values)\nplt.title('Sine Wave')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html#what-happens-when-theres-an-error",
    "href": "src/01/notebooks/hello-world.html#what-happens-when-theres-an-error",
    "title": "Hello World Notebook!",
    "section": "What Happens When Theres an Error?",
    "text": "What Happens When Theres an Error?\nRun the cell below to see how notebooks handle errors.\nThe error appears in the output, but other cells continue to work.\n\n# This will cause an error\nresult = 10 / 0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[9], line 2\n      1 # This will cause an error\n----&gt; 2 result = 10 / 0\n\nZeroDivisionError: division by zero",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/08/slides.html#recap",
    "href": "src/08/slides.html#recap",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "Recap",
    "text": "Recap\n\nUsed generated synthetic data to fine-tune a 1B parameter model\nUsed W&B (Weights and Biases) to observe parameters during the training run\nPost-training, used W&B to use cosine similarity and LLM-as-a-Judge to evaluate the accuracy of our trained model\nTrained smaller models (270M parameters) and compared the results\nUnderstood and created a model card, uploaded the model to Hugging Face and shared",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/slides.html#lesson-objectives",
    "href": "src/08/slides.html#lesson-objectives",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nDiscuss ethical, IP, and safety concerns for Generative AI\nUse an evidence-based approach to explore ethical implications and potential mitigations\nUse an evidence-based approach to explore IP implications and potential mitigations\nUse an evidence-based approach to explore safety implications and potential mitigations\nResearch a theme (or media claim) and author a paper confirming or challenging it",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/slides.html#looking-ahead-1",
    "href": "src/08/slides.html#looking-ahead-1",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis weeks assignment!\nTBD",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/slides.html#references-1",
    "href": "src/08/slides.html#references-1",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/assignment.html",
    "href": "src/08/assignment.html",
    "title": "Module 8 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Assignment"
    ]
  },
  {
    "objectID": "src/08/assignment.html#assignment",
    "href": "src/08/assignment.html#assignment",
    "title": "Module 8 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Assignment"
    ]
  },
  {
    "objectID": "src/07/resources.html",
    "href": "src/07/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Resources"
    ]
  },
  {
    "objectID": "src/07/resources.html#citations",
    "href": "src/07/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Resources"
    ]
  },
  {
    "objectID": "src/06/slides.html#recap",
    "href": "src/06/slides.html#recap",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "Recap",
    "text": "Recap\n\nUnderstood the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile\nUnderstood hardware requirements and architectures for model inference - e.g., CUDA vs.ONNX vs.MLX vs.WebGPU\nExplored how quantization works and understood techniques and formats for quantizing existing models\nUsed llama.cpp to quantize and run an SLM on local hardware/gaming PC\nIntegrated a quantized model within Unity/Unreal/WebAssembly",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/slides.html#lesson-objectives",
    "href": "src/06/slides.html#lesson-objectives",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand model training, dataset curation, what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy\nExplore use cases, advantages, and disadvantages of prompt engineering\nIntroduce and implement RAG (Retrieval Augmented Generation) to increase the accuracy of a limited SLM\nStart the exploration of how to fine-tune models using LoRA (Low Ranked Adaptation)\nUse a foundational model to generate synthetic data for fine-tuning a 1B parameter model",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/slides.html#looking-ahead-1",
    "href": "src/06/slides.html#looking-ahead-1",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis weeks assignment!\nTBD",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/slides.html#references-1",
    "href": "src/06/slides.html#references-1",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/assignment.html",
    "href": "src/06/assignment.html",
    "title": "Module 6 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/06/assignment.html#assignment",
    "href": "src/06/assignment.html#assignment",
    "title": "Module 6 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/05/resources.html",
    "href": "src/05/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Resources"
    ]
  },
  {
    "objectID": "src/05/resources.html#citations",
    "href": "src/05/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/slides.html#recap",
    "href": "src/04/slides.html#recap",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Recap",
    "text": "Recap\n\nDescribed the fundamental concepts behind Agents/Agentic AI\nExplored and provided feedback on an existing multi-agent setup\nUnderstood available agent SDKs, how they differ, and advantages/disadvantages\nUsed the OpenAI Agents SDK to build a multi-agent system, including document indexing and retrieval\nUnderstood and implemented tool calls using OpenAIs function calling and via MCP",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#lesson-objectives",
    "href": "src/04/slides.html#lesson-objectives",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand the fundamentals and history of diffuser models\nExplore and use models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet\nSetup and use Replicate to create a custom pipeline of production-grade models\nUnderstand the fundamentals and history of Vision Encoders and VLMs\nImplement/test a local VLM model for on-device inference",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#multimedia-vs.-multimodal-1",
    "href": "src/04/slides.html#multimedia-vs.-multimodal-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Multimedia vs.Multimodal",
    "text": "Multimedia vs.Multimodal\n\nMultimedia models\n\nSingle input/output models for images, video, audio, etc.\nAlso known as computer vision, audio models\n\nExamples\n\nText-to-Image (generate an image from a text prompt)\nImage-to-Image (generate an image from an existing image)\nImage-to-3D (generate a 3D object from an image)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#multimedia-vs.-multimodal-2",
    "href": "src/04/slides.html#multimedia-vs.-multimodal-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Multimedia vs.Multimodal",
    "text": "Multimedia vs.Multimodal\n\nMultimodal models\n\nProcess multiple datatypes such as text, images, and audio\nAlso known as VLMs (Vision-Language Models) or ALMs (Audio-Language Models)\n\nExamples\n\nImage-Text-to-Text (ask a question about this image)\nImage-Text-to-Image (decompose this image into multiple layers)\nAudio-Text-to-Text (what is this sound?)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#text-to-image",
    "href": "src/04/slides.html#text-to-image",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Text-to-Image",
    "text": "Text-to-Image\n\n\nA photograph of an astronaut riding a horse.\n\nBased on a concept called a diffusion transformer\nCommonly known as a diffuser\nTwo stage process, inspired by thermodynamics",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#introducing-the-diffuser",
    "href": "src/04/slides.html#introducing-the-diffuser",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Introducing the Diffuser",
    "text": "Introducing the Diffuser\n\nTraining\n\nDuring training, random noise is added to images in steps\nModel learns to predict what noise was added (forward diffusion process)\n\nInference (process runs in reverse)\n\nStart with pure random noise\nModel estimates what noise should be removed to create a realistic image\nUsing the text prompt, the model steers the process towards images that match the description",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-diffusion-models-in-2022",
    "href": "src/04/slides.html#image-diffusion-models-in-2022",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image Diffusion Models in 2022",
    "text": "Image Diffusion Models in 2022\n\n\n\n\n\ntimeline\n  August 2022 : Stable Diffusion v1.4\n              : First open-source high-quality model\n  September 2022 : Stable Diffusion v1.5\n                  : Refined version\n  October 2022 : eDiff-I (NVIDIA)\n                : Ensemble approach\n  November 2022 : Stable Diffusion v2.0/2.1\n                : Higher resolution (768x768)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-diffusion-models-in-2023",
    "href": "src/04/slides.html#image-diffusion-models-in-2023",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image Diffusion Models in 2023",
    "text": "Image Diffusion Models in 2023\n\n\n\n\n\ntimeline\n  March 2023 : Midjourney v5\n              : Exceptional artistic quality\n  April 2023 : ControlNet\n        : Precise spatial control\n        : AnimateDiff - Video generation\n  July 2023 : SDXL (Stable Diffusion XL)\n            : 1024x1024 native resolution\n  August 2023 : SDXL Turbo\n              : Real-time capable generation",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-diffusion-models-in-2024",
    "href": "src/04/slides.html#image-diffusion-models-in-2024",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image Diffusion Models in 2024",
    "text": "Image Diffusion Models in 2024\n\n\n\n\n\ntimeline\n  February 2024 : Stable Diffusion 3\n                : Improved text understanding\n  June 2024 : Stable Diffusion 3.5\n            : Multiple model sizes\n  2024 : FLUX.1 (Black Forest Labs)\n        : State-of-the-art open model\n        : Imagen 3 (Google DeepMind)\n        : Photorealistic quality",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-diffusion-models-in-2025",
    "href": "src/04/slides.html#image-diffusion-models-in-2025",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image Diffusion Models in 2025",
    "text": "Image Diffusion Models in 2025\n\n\n\n\n\ntimeline\n  May 2025 : Imagen 4 (Google DeepMind)\n           : Improved text rendering, 2K resolution\n  August 2025 : Nano Banana (Google)\n              : Autoregressive model in Gemini 2.5 Flash\n  November 2025 : FLUX.2 (Black Forest Labs)\n                : 32B parameters, multi-image references",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#text-to-image-with-sd-1.5",
    "href": "src/04/slides.html#text-to-image-with-sd-1.5",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Text-to-Image with SD 1.5",
    "text": "Text-to-Image with SD 1.5\n\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\n# Load a small diffusion model\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id,\n)\n\n# Move to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npipe = pipe.to(device)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#text-to-image-with-sd-1.5-1",
    "href": "src/04/slides.html#text-to-image-with-sd-1.5-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Text-to-Image with SD 1.5",
    "text": "Text-to-Image with SD 1.5\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nPROMPT = \"a photograph of an astronaut riding a horse\" #@param {type:\"string\"}\nSTEPS = 50 #@param {type:\"slider\", min:10, max:100, step:1}\nSEED = -1 #@param {type:\"integer\"}\n\nintermediate_images = []\n\ndef callback_fn(step, timestep, latents):\n    \"\"\"Capture intermediate denoising steps\"\"\"\n    # Decode latents to image every few steps\n    if step % 5 == 0 or step == 0:\n        with torch.no_grad():\n            # Decode the latent representation to an image\n            image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\n            image = pipe.image_processor.postprocess(image, output_type=\"pil\")[0]\n            intermediate_images.append((step, image))\n\nresult = pipe(\n    PROMPT,\n    num_inference_steps=STEPS,\n    callback=callback_fn,\n    callback_steps=1,\n    generator=torch.Generator().manual_seed(SEED) if SEED != -1 else None,\n).images[0]\n\n# Visualize the denoising process\nnum_steps_to_show = min(10, len(intermediate_images))\nstep_indices = np.linspace(0, len(intermediate_images)-1, num_steps_to_show, dtype=int)\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfig.suptitle(f'Real Diffusion Model Denoising Process\\nPrompt: \"{PROMPT}\"')\n\nfor idx, step_idx in enumerate(step_indices):\n    row = idx // 5\n    col = idx % 5\n    step_num, img = intermediate_images[step_idx]\n\n    axes[row, col].imshow(img)\n    axes[row, col].axis('off')\n    axes[row, col].set_title(f'Step {step_num}/{STEPS}')\n\nplt.tight_layout()\nplt.savefig('diffusion_process.png', dpi=150, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#text-to-image-with-sd-1.5-2",
    "href": "src/04/slides.html#text-to-image-with-sd-1.5-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Text-to-Image with SD 1.5",
    "text": "Text-to-Image with SD 1.5",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#text-to-image-with-sd-1.5-3",
    "href": "src/04/slides.html#text-to-image-with-sd-1.5-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Text-to-Image with SD 1.5",
    "text": "Text-to-Image with SD 1.5",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-what-are-these-pipelines",
    "href": "src/04/slides.html#sidebar-what-are-these-pipelines",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: What are these pipelines?",
    "text": "Sidebar: What are these pipelines?\n\nOur use of HF Transformers use so far\n\nConvert text to input tokens, pass to model, decode output tokens to text\n\nHF Pipelines provides a layer of abstraction\n\n(Setup the pipeline, then call pipe method)\nWhile still giving access to underlying components\n\nPipelines also standardize other areas\n\ne.g., pipe(prompt).images[0] works for all model types\n.to(\"cuda\") moves all components of the model to the GPU",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-seeds",
    "href": "src/04/slides.html#sidebar-seeds",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: Seeds",
    "text": "Sidebar: Seeds\n\nWhat is a seed?\n\n(Optional) Integer value used to initialize the image generation\nUsed to generate the initial random noise\nUsing the same seed will generate the same image\n\nWhy use a seed?\n\nControlling the seed allows you to then experiment with different prompts or parameters\nGives you more control/predictability vs.starting from random seed every time",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-1",
    "href": "src/04/slides.html#image-to-image-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image",
    "text": "Image-to-Image\n\nImage-to-Image: Make this image different\nOriginally solved by GAN approaches, but evolved into extension of the diffuser concept\n\nAdd noise to the original image (partial denoising)\nRegenerate it with modifications based on the prompt\nStrength parameter (0.0 - 1.0) to indicate the weight to the new image vs.original\nThe original image heavily influences the output structure",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-generate",
    "href": "src/04/slides.html#image-to-image-generate",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (Generate)",
    "text": "Image-to-Image (Generate)\n\n\ndef generate_image(strength):\n  return pipe(\n      prompt=PROMPT,\n      negative_prompt=NEGATIVE_PROMPT,\n      image=init_image,\n      strength=strength,\n      guidance_scale=7.5,\n      num_inference_steps=30,\n      generator=torch.Generator().manual_seed(SEED) if SEED != -1 else None,\n  ).images[0]",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-original",
    "href": "src/04/slides.html#image-to-image-original",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (Original)",
    "text": "Image-to-Image (Original)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-original-1",
    "href": "src/04/slides.html#image-to-image-original-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (Original)",
    "text": "Image-to-Image (Original)\nPrompt: a goldendoodle wearing sunglasses, high quality, detailed",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-original-2",
    "href": "src/04/slides.html#image-to-image-original-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (Original)",
    "text": "Image-to-Image (Original)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-0.3",
    "href": "src/04/slides.html#image-to-image-0.3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (0.3)",
    "text": "Image-to-Image (0.3)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-0.5",
    "href": "src/04/slides.html#image-to-image-0.5",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (0.5)",
    "text": "Image-to-Image (0.5)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-0.7",
    "href": "src/04/slides.html#image-to-image-0.7",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (0.7)",
    "text": "Image-to-Image (0.7)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-0.9",
    "href": "src/04/slides.html#image-to-image-0.9",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (0.9)",
    "text": "Image-to-Image (0.9)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#beyond-sd-1.5-1",
    "href": "src/04/slides.html#beyond-sd-1.5-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Beyond SD 1.5",
    "text": "Beyond SD 1.5\n\nStable Diffusion 1.5\n\nGreat for learning about the diffusion process\nBut the image quality isnt great!\n\nImage models get large quickly\n\nHigher resolutions demand more GPU/VRAM",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#introducing-replicate",
    "href": "src/04/slides.html#introducing-replicate",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Introducing Replicate",
    "text": "Introducing Replicate\n\nSource: https://replicate.com",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#introducing-replicate-1",
    "href": "src/04/slides.html#introducing-replicate-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Introducing Replicate",
    "text": "Introducing Replicate\n\nSimilar to OpenRouter\n\nBut with a focus on image and video models\nExtensive access to larger models (e.g., FLUX 2, Nano Banana, ImageGen)\nPay-per-call pricing (expect 2c per image for higher quality models; some free models)\nAPI access (with Python and NodeJS library)\nFine-tune and share your own models",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#flux-models",
    "href": "src/04/slides.html#flux-models",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "FLUX Models",
    "text": "FLUX Models\n\nFrom Black Forest Labs (founded by ex-Stability AI researchers)\nKey architectural differences from Stable Diffusion:\n\nUses a Multimodal Diffusion Transformer (MMDiT) instead of U-Net\nProcesses text and image tokens together in a unified transformer\nNative support for higher resolutions without quality degradation\n\nVariants: FLUX.1 [schnell] (fast), [dev] (quality), [pro] (commercial)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#using-the-replicate-api-1",
    "href": "src/04/slides.html#using-the-replicate-api-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Using the Replicate API",
    "text": "Using the Replicate API\n\n\nimport replicate\noutput = replicate.run(\n  \"black-forest-labs/flux-pro\",\n  input={\n      \"steps\": 28,\n      \"prompt\": \"lemon cupcake spelling out the words 'DigiPen' with sparklers, tasty, food photography, dynamic shot\",\n      \"seed\": 1564435,\n      \"output_format\": \"png\",\n      \"safety_tolerance\": 2,\n      \"prompt_upsampling\": False\n  },\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#using-the-replicate-api-2",
    "href": "src/04/slides.html#using-the-replicate-api-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Using the Replicate API",
    "text": "Using the Replicate API\n\nsteps: Number to steps to run through\nseed: Random seed value\nprompt: Prompt to use to guide the model\noutput_format: Output format to return\nsafety_tolerance: Safety tolerance (1 is most strict; 6 is most permissive)\nprompt_upsampling: Run the prompt through an LLM to be more descriptive/creative",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#using-the-replicate-api-3",
    "href": "src/04/slides.html#using-the-replicate-api-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Using the Replicate API",
    "text": "Using the Replicate API",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-safety-tolerance",
    "href": "src/04/slides.html#sidebar-safety-tolerance",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: Safety Tolerance",
    "text": "Sidebar: Safety Tolerance\n\nHow does safety tolerance work?\n\nSafety/guardrails are not typically embedded into the model\ne.g., SD 1.5 will generate NSFW images easily\n\nInstead, separate classifiers run alongside the model\n\nInput filtering: Analyzes the prompt prior to generation\nOutput filtering: Image classifier examines the model before showing it to the user\nThresholds used to control the classifiers",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-2",
    "href": "src/04/slides.html#image-to-image-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image",
    "text": "Image-to-Image\n\nCan also be used for\n\nSuper resolution (increase the resolution of this image)\nStyle transfer (recreate this image in the style of)\nColorization (grayscale to color)\nDepth maps",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps",
    "href": "src/04/slides.html#depth-maps",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps\n\n\n\nImages (often greyscale) where each pixels value represents the distance from the viewer\n\ni.e., objects in the foreground are lighter, background are darker",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-1",
    "href": "src/04/slides.html#depth-maps-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps\n\n\n\n\n\nHistorically, required custom hardware\n\nDepth Camera (e.g., RealSense) - ~$300-500\nModule/processing for realtime (60fps) sensing",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-2",
    "href": "src/04/slides.html#depth-maps-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps\n\nImage-to-Image depth estimation models\n\nDepth Anything, MiDaS, ZoeDepth\nLow latency (MiDaS 3.1 @ 20fps on embedded GPU)\n\nUsed for\n\n3D effects/estimation\nSimple/low-cost robotics\nControl input for other images",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-3",
    "href": "src/04/slides.html#depth-maps-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-4",
    "href": "src/04/slides.html#depth-maps-4",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps\n\n\nimport replicate\n\nMODEL = \"chenxwh/depth-anything-v2:b239ea33cff32bb7abb5db39ffe9a09c14cbc2894331d1ef66fe096eed88ebd4\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"image\": INPUT_IMAGE,\n  },\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-5",
    "href": "src/04/slides.html#depth-maps-5",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-6",
    "href": "src/04/slides.html#depth-maps-6",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps\n\nWhy do this?\n\nDepth map can be used as control image for new image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-map-as-control",
    "href": "src/04/slides.html#depth-map-as-control",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Map as Control",
    "text": "Depth Map as Control",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-map-as-control-1",
    "href": "src/04/slides.html#depth-map-as-control-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Map as Control",
    "text": "Depth Map as Control\n\n\nimport replicate\n\nMODEL = \"black-forest-labs/flux-depth-pro\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"control_image\": CONTROL_IMAGE,\n      \"output_format\": \"png\",\n      \"seed\": 12345,\n      \"prompt\": \"A futuristic building set in 2050, neon lighting, night shot, dynamic\"\n  },\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-map-as-control-2",
    "href": "src/04/slides.html#depth-map-as-control-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Map as Control",
    "text": "Depth Map as Control",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-map-as-control-3",
    "href": "src/04/slides.html#depth-map-as-control-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Map as Control",
    "text": "Depth Map as Control\n\n\nimport replicate\n\nMODEL = \"black-forest-labs/flux-depth-pro\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"control_image\": CONTROL_IMAGE,\n      \"output_format\": \"png\",\n      \"seed\": 12345,\n      \"prompt\": \"A historical castle set in medieval England, clear day, partially cloudy sky\"\n  },\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-map-as-control-4",
    "href": "src/04/slides.html#depth-map-as-control-4",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Map as Control",
    "text": "Depth Map as Control",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#inpainting",
    "href": "src/04/slides.html#inpainting",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Inpainting",
    "text": "Inpainting\n\nFilling in missing or masked regions of an image in a realistic way\nModel is given an image with certain areas masked out\nGenerates plausible content to fill those areas based on the surrounding context (and steered by a text prompt)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#inpainting-1",
    "href": "src/04/slides.html#inpainting-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Inpainting",
    "text": "Inpainting\n\nCan be challenging\n\nModel needs to understand context around the area\nGenerate content that matches the style, lighting, and perspective\nFollow a text prompt that was likely different from the original",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#inpainting-2",
    "href": "src/04/slides.html#inpainting-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Inpainting",
    "text": "Inpainting\n\n\nimport gradio as gr\nimport numpy as np\nfrom PIL import Image\nimport replicate\nimport io\n\ndef inpaint(image_data, prompt):\n    if image_data is None:\n        return None\n    \n    # Get the original image from the background\n    original_image = Image.fromarray(image_data['background'])\n    \n    # Get the mask from the layers\n    if image_data['layers'] and len(image_data['layers']) &gt; 0:\n        mask_layer = image_data['layers'][0]\n        mask_array = np.array(mask_layer)\n        \n        # Create binary mask: white where painted, black where not\n        alpha_channel = mask_array[:, :, 3]\n        binary_mask = np.where(alpha_channel &gt; 0, 255, 0).astype(np.uint8)\n        mask_image = Image.fromarray(binary_mask, mode='L')\n    else:\n        return None\n    \n    # Convert images to bytes for the replicate API\n    image_bytes = io.BytesIO()\n    original_image.save(image_bytes, format='PNG')\n    image_bytes.seek(0)\n    \n    mask_bytes = io.BytesIO()\n    mask_image.save(mask_bytes, format='PNG')\n    mask_bytes.seek(0)\n    \n    # Call the Replicate API\n    output = replicate.run(\n        \"black-forest-labs/flux-fill-pro\",\n        input={\n            \"image\": image_bytes,\n            \"mask\": mask_bytes,\n            \"prompt\": prompt,\n            \"steps\": 25,\n            \"guidance\": 75,\n            \"outpaint\": \"None\",\n            \"output_format\": \"jpg\",\n            \"safety_tolerance\": 2,\n            \"prompt_upsampling\": False\n        }\n    )\n    \n    # Read the FileOutput and convert to PIL Image\n    output_bytes = output.read()\n    output_image = Image.open(io.BytesIO(output_bytes))\n    \n    return output_image\n\ndemo = gr.Interface(\n    fn=inpaint,\n    inputs=[\n        gr.ImageEditor(\n            label=\"Image (paint over areas to inpaint)\",\n            brush=gr.Brush(color_mode=\"fixed\", colors=[\"#000000\"]),\n            layers=True\n        ),\n        gr.Textbox(label=\"Prompt\", placeholder=\"Describe what should replace the masked area...\")\n    ],\n    outputs=gr.Image(label=\"Output Image\"),\n    title=\"Inpainting using black-forest-labs/flux-fill-pro\"\n)\ndemo.launch(debug=True)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting",
    "href": "src/04/slides.html#outpainting",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting\n\nThe opposite of inpainting, kind of :)\nHow does it work?\n\nSupply a prompt: 2x zoom out this image\nTreat the new empty regions around the image as masked areas\nUse inpainting technique to fill in the regions",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting-1",
    "href": "src/04/slides.html#outpainting-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting\n\nMore challenging\n\nLess context at the edges of the image vs.center/surrounded\nNeed to maintain the style, lighting, and perspective\nHas to be creative. Cant just be a repetitive pattern.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting-2",
    "href": "src/04/slides.html#outpainting-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting-3",
    "href": "src/04/slides.html#outpainting-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting\n\n\nimport replicate\n\n# Call the Replicate API\noutput = replicate.run(\n    \"black-forest-labs/flux-fill-pro\",\n    input={\n        \"image\": INPUT_IMAGE,\n        \"prompt\": \"The main building of a technical college, no text\",\n        \"seed\": 123456,\n        \"steps\": 50,\n        \"guidance\": 60,\n        \"outpaint\": \"Zoom out 2x\",\n        \"output_format\": \"jpg\",\n        \"safety_tolerance\": 2,\n        \"prompt_upsampling\": False\n    }\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting-4",
    "href": "src/04/slides.html#outpainting-4",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting-5",
    "href": "src/04/slides.html#outpainting-5",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#inpainting-and-outpainting",
    "href": "src/04/slides.html#inpainting-and-outpainting",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Inpainting and Outpainting",
    "text": "Inpainting and Outpainting\n\nPopular models\n\nStable Diffusion (Many inpainting variants)\nFlux Fill from Black Forest Labs\nLaMa: Large Mask inpainting\nIdeogram",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#i-want-more-control-1",
    "href": "src/04/slides.html#i-want-more-control-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "I Want More Control!",
    "text": "I Want More Control!\n\nPrompt engineering for text-to-image and image-to-image is important\n\nBad prompt: a cat\nGood prompt: A fluffy orange tabby cat sitting on a wooden windowsill, golden hour lighting, soft focus background of a garden, photorealistic, highly detailed fur texture, warm color palette, shot with 85mm lens, shallow depth of field",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#i-want-more-control-2",
    "href": "src/04/slides.html#i-want-more-control-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "I Want More Control!",
    "text": "I Want More Control!\n\nKey components to include in prompts:\n\nSubject: What you want\nStyle/Medium: photorealistic, oil painting, digital art\nLighting: studio lighting, dramatic shadows, soft diffused\nComposition: close-up, wide-angle, rule of thirds\nQuality: highly detailed, 4K, sharp focus\nTechnical specs: 85mm lens, f/1.8, bokeh",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#i-want-more-control-3",
    "href": "src/04/slides.html#i-want-more-control-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "I Want More Control!",
    "text": "I Want More Control!\n\nNegative prompts (popular in some models)\n\nTell the model what to avoid - particularly useful with Stable Diffusion\nblurry, low quality, distorted, deformed, ugly, bad anatomy, extra limbs, watermark, text, signature, overexposed, underexposed, cartoon",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#i-want-even-more-control",
    "href": "src/04/slides.html#i-want-even-more-control",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "I Want Even More Control!",
    "text": "I Want Even More Control!\n\nExtensive prompts (both positive and negative) can help, but only so far\n\nUltimately, hoping the model guesses what I mean\nFine-tuning possible, but its expensive and risks degrading quality (and potential overfitting)\nNeed a different approach",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#introducing-controlnet-1",
    "href": "src/04/slides.html#introducing-controlnet-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Introducing ControlNet",
    "text": "Introducing ControlNet\n\nDeveloped by Lvmin Zhang and Maneesh Agrawala at Stanford University\nPublished in February 2023 (Zhang, Rao, and Agrawala 2023)\nControlNet represented a paradigm shift from describe what you want to show the structure you want.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works",
    "href": "src/04/slides.html#how-controlnet-works",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\nStable Diffusions U-Net has an encoder and decoder\nCreate a trainable copy of the encoder blocks\nTrain the copy of the encoder alongside the frozen SD model\n\nDuring training: use paired data (e.g., pose skeleton  original image)\nDuring inference: both encoders run together\nFeatures from both are combined via zero convolutions\n\nKey: The weights in the original SD model dont change\nControlNet is analogous to a Plug in model",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-1",
    "href": "src/04/slides.html#how-controlnet-works-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\nExamples of conditioning types:\n\nDepth maps: 3D structure information\nHuman pose: skeleton/keypoint detection\nCanny edges: line drawings and edge detection\nScribbles: rough user drawings\nQR codes: blended into images",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-2",
    "href": "src/04/slides.html#how-controlnet-works-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-3",
    "href": "src/04/slides.html#how-controlnet-works-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\n\npose_image = openpose(input_image)\ndisplay(pose_image)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-4",
    "href": "src/04/slides.html#how-controlnet-works-4",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-5",
    "href": "src/04/slides.html#how-controlnet-works-5",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\n\nimport torch\nfrom diffusers import ControlNetModel\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"lllyasviel/control_v11p_sd15_openpose\",\n    torch_dtype=torch.float16\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-6",
    "href": "src/04/slides.html#how-controlnet-works-6",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\n\nfrom diffusers import StableDiffusionControlNetPipeline\n\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    use_safetensors=True\n)\npipe.to(\"cuda\")",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-7",
    "href": "src/04/slides.html#how-controlnet-works-7",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\n\nPROMPT = \"a robot with glowing LED lights, futuristic, sci-fi\"\nNEGATIVE_PROMPT = \"blurry, low quality, distorted, extra limbs, deformed\"\nSEED = 3434002\n\nresult = pipe(\n    prompt=PROMPT,\n    negative_prompt=NEGATIVE_PROMPT,\n    image=pose_image,\n    num_inference_steps=25,\n    guidance_scale=7.5,\n    controlnet_conditioning_scale=1.0,\n    generator=torch.manual_seed(SEED) if SEED != -1 else None\n).images[0]\ndisplay(result)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-8",
    "href": "src/04/slides.html#how-controlnet-works-8",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-9",
    "href": "src/04/slides.html#how-controlnet-works-9",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-10",
    "href": "src/04/slides.html#how-controlnet-works-10",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#cnns-to-vision-transformer",
    "href": "src/04/slides.html#cnns-to-vision-transformer",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "CNNs to Vision Transformer",
    "text": "CNNs to Vision Transformer\n\nHistorically, computer vision has used classification models called CNNs (Convolutional Neural Networks)\nEnter the Vision Transformer (ViT)\n\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al. 2021)\n\nTipping Point\n\nInitially, ViTs didnt outperform CNNs\nBut exceeded SOTA CNNs on larger datasets (such as Googles JFT-300M)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#popular-vision-transformers",
    "href": "src/04/slides.html#popular-vision-transformers",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Popular Vision Transformers",
    "text": "Popular Vision Transformers\n\nOpenAIs CLIP\n\nTrained on 400M image-text pairs\nFoundation of most VLMs\n\nMetas DINO/DINO-2\n\n(Self DIstillation with NO Labels)\nSelf-supervised on 142M images\n\nMicrosofts Swin\n\nUse shifted windows approach\nExcels at dense prediction tasks",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#vision-language-models-vlms",
    "href": "src/04/slides.html#vision-language-models-vlms",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Vision Language Models (VLMs)",
    "text": "Vision Language Models (VLMs)\n\nViTs by themselves are only so useful\nIntroducing VLMs (Vision Language Models)\n\nA vision encoder\nAdapter/projector layer\nLanguage model (LLaMa or GPT)\n\nAlso known as Multimodal\n\nImage-Text-to-Text",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-image-url-dereferencing",
    "href": "src/04/slides.html#sidebar-image-url-dereferencing",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: Image URL Dereferencing",
    "text": "Sidebar: Image URL Dereferencing\n\n\nfrom IPython.display import Image\n\nIMAGE_URL = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"\n\nImage(url=IMAGE_URL, width=500)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-image-url-dereferencing-1",
    "href": "src/04/slides.html#sidebar-image-url-dereferencing-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: Image URL Dereferencing",
    "text": "Sidebar: Image URL Dereferencing\n\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": IMAGE_URL},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"},\n        ],\n    },\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-image-url-dereferencing-2",
    "href": "src/04/slides.html#sidebar-image-url-dereferencing-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: Image URL Dereferencing",
    "text": "Sidebar: Image URL Dereferencing\n\nHandled within the pipeline library\n\nDetects the image URL in the message structure\nDownloads the image\nLoads as a PIL Image object\nPreprocessing (resizing, normalizing pixel values)\nConverts to tensors",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#vision-language-models-vlms-1",
    "href": "src/04/slides.html#vision-language-models-vlms-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Vision Language Models (VLMs)",
    "text": "Vision Language Models (VLMs)\n\nText gets tokenized into token IDs\nImage gets processed into pixel tensors\nBoth feed into their respective encoders (text encoder, vision encoder)\nFeatures are aligned via shared embedding space\n\ne.g., visual concepts (furry, four legs, whiskers, etc.) are close to the word cat in shared embedding space",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#use-cases-for-vlms",
    "href": "src/04/slides.html#use-cases-for-vlms",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Use Cases for VLMs",
    "text": "Use Cases for VLMs\n\nVisual Understanding: Whats in this image?\nAccessibility: Assisting users with visual impairments\nContent Moderation and Safety: Identifying harmful content\nRetail: Finding products with photos\nEducation: Helping students understand charts, diagrams, equations\nRobotics: Providing Robots with information to navigate their environment",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#vlms-vs.-object-detection-models",
    "href": "src/04/slides.html#vlms-vs.-object-detection-models",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "VLMs vs.Object Detection Models",
    "text": "VLMs vs.Object Detection Models\n\n\n\nRicher semantics (e.g., tabby cat vs.cat)\nContextual descriptions\nZero-shot: Can identify objects never seen during training\nBut less precise at localization (bounding boxes, pixel coordination)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#popular-vlms",
    "href": "src/04/slides.html#popular-vlms",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Popular VLMs",
    "text": "Popular VLMs\n\nClosed Source\n\nGPT4-V, Claude, Gemini Flash\n\nOpen Source\n\nLLaVa: Research collaboration between University of Wisconsin-Maddison and MSR\nGemma: Googles Gemma-3\nFastVLM: Apples Fast Vision Language Model",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#fastvlm",
    "href": "src/04/slides.html#fastvlm",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "FastVLM",
    "text": "FastVLM\n\nRecent release from Apple (presented at CVPR 2025)\n\nPaper: FastVLM: Efficient Vision Encoding for Vision Language Models (Vasu et al. 2025)\nhttps://huggingface.co/apple/FastVLM-0.5B\nSmall VLM, optimized for on-device, real-time performance\nCustom vision transformer: FastViTHD. Combines transformers and convolutional layers",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#looking-ahead-1",
    "href": "src/04/slides.html#looking-ahead-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis weeks assignment!\nNo class next week (Founders Day). See you on Feb 13th!\nDeep dive on hardware/GPU architectures\nRunning models on local hardware\nQuantization",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#references-1",
    "href": "src/04/slides.html#references-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "References",
    "text": "References\n\n\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv Preprint arXiv:2010.11929.\n\n\nVasu, Pavan Kumar Anasosalu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, et al. 2025. FastVLM: Efficient Vision Encoding for Vision Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1976980.\n\n\nZhang, Lvmin, Anyi Rao, and Maneesh Agrawala. 2023. Adding Conditional Control to Text-to-Image Diffusion Models. arXiv Preprint arXiv:2302.05543.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/assignment.html",
    "href": "src/04/assignment.html",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "",
    "text": "Objective: Build a working application that demonstrates your understanding of multimedia or multimodal AI models.\nChoose Your Adventure: Pick ONE of the three options below.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#option-1-controlnet-scribble-app",
    "href": "src/04/assignment.html#option-1-controlnet-scribble-app",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "Option 1: ControlNet Scribble App",
    "text": "Option 1: ControlNet Scribble App\nBuild a Gradio application that transforms hand-drawn sketches into realistic images using ControlNet.\nRequirements:\n\nCreate a Gradio interface with a sketchpad/canvas input\nUse ControlNet (scribble or canny edge model) to condition image generation\nAllow users to enter a text prompt to guide the style/content\nGenerate and display the resulting image\nInclude at least one configurable parameter (e.g., guidance scale, number of steps)\n\nSuggested approach:\n\nUse Replicates ControlNet models for easier deployment, OR\nRun locally using HuggingFace diffusers with a ControlNet pipeline",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#option-2-replicate-model-pipeline",
    "href": "src/04/assignment.html#option-2-replicate-model-pipeline",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "Option 2: Replicate Model Pipeline",
    "text": "Option 2: Replicate Model Pipeline\nCreate a pipeline that chains multiple Replicate models together to transform images through a multi-step process.\nRequirements:\n\nChain at least 3 different models in sequence (e.g., depth estimation  ControlNet  upscaling)\nCreate a Gradio interface that accepts an input image and displays intermediate/final results\nDocument what each model in your pipeline does and why you chose it\nShow the transformation at each stage (not just the final output)\n\nExample pipeline ideas:\n\nPhoto  Depth Map  Stylized Scene  Upscaled Output\nPortrait  Pose Extraction  New Character in Same Pose  Background Replacement\nSketch  Colorized Image  Style Transfer  Final Composition",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#option-3-vision-language-model-application",
    "href": "src/04/assignment.html#option-3-vision-language-model-application",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "Option 3: Vision Language Model Application",
    "text": "Option 3: Vision Language Model Application\nImplement a practical application using a Vision Language Model (VLM) for a real-world use case.\nRequirements:\n\nUse an open-source VLM (e.g., Gemma 3, LLaVA, FastVLM) - not a closed API like GPT-4V or Claude\nBuild a Gradio interface that accepts image input\nImplement a specific, practical use case such as:\n\nAccessibility: Describe images for visually impaired users\nProduct Detection: Identify and catalog items from photos\nDocument Analysis: Extract information from receipts, forms, or charts\nEducational: Explain diagrams, equations, or scientific figures\n\nInclude thoughtful prompt engineering in your system prompt",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#deliverable-a-colabjupyter-notebook-with",
    "href": "src/04/assignment.html#deliverable-a-colabjupyter-notebook-with",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "Deliverable: A Colab/Jupyter notebook with:",
    "text": "Deliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation\nA working Gradio interface that can be launched and tested\nUses environment variables for any API keys (e.g., REPLICATE_API_TOKEN). Please do not include your API key in your notebook!\nMarkdown cells explaining:\n\nWhich option you chose and why\nYour design decisions and approach\nObservations about model behavior, quality, or limitations\nWhat worked well and what was challenging",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#hints",
    "href": "src/04/assignment.html#hints",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "Hints",
    "text": "Hints\n\nOption 1: The gr.Sketchpad or gr.ImageEditor components in Gradio work well for drawing input. Start with a simple black-and-white sketch before adding complexity.\nOption 2: Consider what each model needs as input and produces as output. The PBR notebook (pbr-creator.ipynb) demonstrates this chaining pattern.\nOption 3: Smaller models (like FastVLM-0.5B or Gemma 3 4B) can run on Colabs T4 GPU. Focus on crafting a good system prompt that guides the model toward your specific use case.\nAll options: Test with multiple different inputs to understand the models capabilities and limitations.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/03/resources.html",
    "href": "src/03/resources.html",
    "title": "Resources",
    "section": "",
    "text": "OpenAI Agents SDK (Python) - Official OpenAI agents framework for Python\nOpenAI Agents SDK Announcement - Blog post announcing the OpenAI Agents SDK (Mar 2025)\nOpenAI Agents Visualization - Tool for visualizing agent graphs and interactions\n\n\n\n\n\nLangGraph - Python framework for building stateful, multi-actor applications with LLMs\n\n\n\n\n\nCrew.ai - Framework for orchestrating role-playing, autonomous AI agents\n\n\n\n\n\nAutoGen - Microsofts framework for building conversational AI systems\nMicrosoft Semantic Kernel - SDK for integrating AI services with conventional programming languages\nMicrosoft Agent Framework - Converged agent framework supporting .NET",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#agent-frameworks",
    "href": "src/03/resources.html#agent-frameworks",
    "title": "Resources",
    "section": "",
    "text": "OpenAI Agents SDK (Python) - Official OpenAI agents framework for Python\nOpenAI Agents SDK Announcement - Blog post announcing the OpenAI Agents SDK (Mar 2025)\nOpenAI Agents Visualization - Tool for visualizing agent graphs and interactions\n\n\n\n\n\nLangGraph - Python framework for building stateful, multi-actor applications with LLMs\n\n\n\n\n\nCrew.ai - Framework for orchestrating role-playing, autonomous AI agents\n\n\n\n\n\nAutoGen - Microsofts framework for building conversational AI systems\nMicrosoft Semantic Kernel - SDK for integrating AI services with conventional programming languages\nMicrosoft Agent Framework - Converged agent framework supporting .NET",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#protocols-and-standards",
    "href": "src/03/resources.html#protocols-and-standards",
    "title": "Resources",
    "section": "Protocols and Standards",
    "text": "Protocols and Standards\n\nModel Context Protocol (MCP) - Standardized protocol for connecting AI models with external tools and data sources\nMCP SDK Documentation - SDKs for building MCP servers in Python, TypeScript, Go, Rust, C#, and more",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#agent-memory",
    "href": "src/03/resources.html#agent-memory",
    "title": "Resources",
    "section": "Agent Memory",
    "text": "Agent Memory\n\nSupermemory - Memory layer for AI applications\nLetta - Long-term memory for AI agents\nmem0 - Open source memory layer for AI applications",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#openai-platform",
    "href": "src/03/resources.html#openai-platform",
    "title": "Resources",
    "section": "OpenAI Platform",
    "text": "OpenAI Platform\n\nOpenAI Platform - OpenAI developer platform for API access\nOpenAI Traces Dashboard - Built-in tracing for debugging OpenAI Agents SDK applications",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#hosting-and-deployment",
    "href": "src/03/resources.html#hosting-and-deployment",
    "title": "Resources",
    "section": "Hosting and Deployment",
    "text": "Hosting and Deployment\n\nHugging Face Spaces - Free cloud hosting for ML demos and Gradio applications",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#articles-and-industry-reports",
    "href": "src/03/resources.html#articles-and-industry-reports",
    "title": "Resources",
    "section": "Articles and Industry Reports",
    "text": "Articles and Industry Reports\n\nWorld Economic Forum - Cognitive Enterprise - Article on the agentic business revolution\nCRN - Hottest Agentic AI Tools - Overview of the top agentic AI tools of 2025\nGartner Press Release - Gartners predictions about agentic AI project success rates\nAnthropic - Building Effective Agents - Engineering guide on building effective AI agents and patterns\nE2B - AI Agents Landscape - Overview of the AI agents ecosystem and available frameworks",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#citations",
    "href": "src/03/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/slides.html#recap",
    "href": "src/02/slides.html#recap",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Recap",
    "text": "Recap\n\nExplored the history of vector embeddings and tokenization\nUnderstood the transformer architecture at a high level\nUsed our first transformer to translate language\nCovered a brief history of early generative transformers\nSetup and used Colab, and became familiar with the basics of notebooks and Python",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#lesson-objectives",
    "href": "src/02/slides.html#lesson-objectives",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand the evolution and licensing of models from GPT-2 through to modern day\nUnderstand instruction-tuned models, how they work, and how to configure\nSetup and use OpenRouter for accessing hosted models\nUnderstand the OpenAI API specification, the request/response payload, parameters, streaming, and structured outputs\nCreate and share a chatbot using a Gradio-based UI",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#from-gpt-2-to-gpt-3.5-1",
    "href": "src/02/slides.html#from-gpt-2-to-gpt-3.5-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "From GPT-2 to GPT-3.5",
    "text": "From GPT-2 to GPT-3.5\n\n\n\n\n\ntimeline\n    Feb 2019 : OpenAI releases GPT-2\n             : 1.5B parameters\n             : Initially withheld full model due to concerns about misuse\n             : Demonstrates impressive text generation capabilities with minimal fine-tuning\n\n    May 2020 : OpenAI releases GPT-3\n             : 175B parameters\n             : Demonstrates strong few-shot learning capabilities\n             : Marks a significant leap in model capabilities and scale\n\n    June 2020 : GPT-3 available through OpenAI API\n              : Still a completion model, not instruction-tuned\n\n    2021 : InstructGPT Development\n          : Built on GPT-3 with RLHF fine-tuning\n          : Trained to follow instructions and understand user intent\n          : Key innovation enabling ChatGPT\n    \n    Jan 2021 : Anthropic Founded\n             : Founded by Dario & Daniela Amodei with ~7 senior OpenAI employees\n             : Dario led GPT-2/3 development and co-invented RLHF\n\n    Nov 2022 : ChatGPT Launch\n              : Built on GPT-3.5 using RLHF\n              : 1M+ users in 5 days\n              : Sparked widespread interest in generative AI",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs.Instruction-Tuned",
    "text": "Completion vs.Instruction-Tuned\n\nCompletion Model just predicts the next token\n\nInput prompt: Mary had a little\nMax total tokens: 50\nTemperature: 0 - 1.0\ntop_k: consider only the top k tokens in the response\ntop_p: Nucleus sampling (probability cut off - 0 and 1.0)\n\nOutput\n\nMary had a little lamb, its fleece was white as snow... (up to max tokens)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-1",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs.Instruction-Tuned",
    "text": "Completion vs.Instruction-Tuned\n\nYou cant really converse with it\nWhat should I do on my upcoming trip to Paris? (max tokens = 75)\nWhat should I do on my upcoming trip to Paris? Please provide a detailed plan of action to help me plan my trip to Paris. 1. Research the best time to travel to Paris:",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#instruction-tuned-models",
    "href": "src/02/slides.html#instruction-tuned-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Instruction-Tuned Models",
    "text": "Instruction-Tuned Models\n\nSupervised Fine-Tuning\n\nLarge datasets of questions/answers, tasks/completions, demonstrating helpful assistant behavior\n\nChat Templates\n\nStructured format to distinguish speakers in a conversation: Typically system, user, and assistant\n\nRLHF (Reinforcement Learning from Human Feedback)\n\nHuman raters rank different model responses, training a reward model",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#whats-a-chat-template",
    "href": "src/02/slides.html#whats-a-chat-template",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Whats a Chat Template?",
    "text": "Whats a Chat Template?\n\nThe format used to train instructional models on conversations involving system, user, and assistant prompts.\nEach model family uses a different format (there is no universal standard)\nWrong format will likely generate nonsense/garbage",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chatml-gpt-3.5-and-other-models",
    "href": "src/02/slides.html#chatml-gpt-3.5-and-other-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "ChatML (GPT-3.5 and other models)",
    "text": "ChatML (GPT-3.5 and other models)\n&lt;|im_start|&gt;system\nYou help travelers make plans for their trips.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHello&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nHi there! How can I help you?&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat should I do on my upcoming trip to Paris?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#system-user-assistant",
    "href": "src/02/slides.html#system-user-assistant",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "System, User, Assistant",
    "text": "System, User, Assistant\n\nSystem prompt sets the intention for the model, guiding the output\n\nYou are a helpful assistant\nYou help students with their math homework\nYou help travelers make plans for their trips\nHas to come first in the conversation\nOnly one system prompt\nOptional for some models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#system-user-assistant-1",
    "href": "src/02/slides.html#system-user-assistant-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "System, User, Assistant",
    "text": "System, User, Assistant\n\nSystem Prompt best practices\n\nBe specific: You are a Python programming tutor who explains concepts using simple analogies and provides code examples.\nDefine output: List no more than 3 suggestions. Always show your work step by step.\nSet boundaries: If you are asked questions outside coding, politely redirect the student back to the task.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#system-user-assistant-2",
    "href": "src/02/slides.html#system-user-assistant-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "System, User, Assistant",
    "text": "System, User, Assistant\n\nUser prompt is the message (request) from the user\n\nHow many rs in Strawberry?\nWhat is linear algebra?\nWhat should I do on my upcoming trip to Paris?\n\nAssistant prompt is the message (reply) from the model\n\nThere are three rs in Strawberry\nLinear algebra is the branch of mathematics that studies vectors, etc.\nHere are some suggestions for your upcoming trip to Paris: 1. Explore the Louvre Museum: etc.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-templates-in-practice",
    "href": "src/02/slides.html#chat-templates-in-practice",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat Templates in Practice",
    "text": "Chat Templates in Practice\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\n\ninstruct_tokenizer.apply_chat_template(\n    messages, \n    tokenize=False,\n    add_generation_prompt=True  # Adds the assistant prompt\n)\n\n'&lt;|im_start|&gt;system\\nYou help travelers make plans for their trips&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nHello&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\nHi there!&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nWhat should I do on my upcoming trip to Paris?&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n'",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-2",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs.Instruction-Tuned",
    "text": "Completion vs.Instruction-Tuned\n\n\nbase_inputs = base_tokenizer(\"What should I do on my upcoming trip to Paris?\", return_tensors=\"pt\")\nbase_outputs = base_model.generate(\n    **base_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=base_tokenizer.eos_token_id\n)\nbase_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\nprint(base_response)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-3",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-3",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs.Instruction-Tuned",
    "text": "Completion vs.Instruction-Tuned\n\n\n\nWhat should I do on my upcoming trip to Paris? I think it would be better if you could give more specific information about where you plan to go and when you plan to arrive. Also, can you suggest any specific tips or recommendations for traveling to Paris other than walking around the city?\n\nI'm sorry, but as an AI language model, I don't have any specific information about your upcoming trip to Paris. However, I can suggest some general tips and recommendations for traveling to Paris other than walking around the city:\n\n1. Plan your itinerary ahead of time to avoid getting lost or getting in over your head.\n2. Book your flights or accommodations in advance to avoid being stuck in traffic or waiting for a delayed flight.\n3. Purchase a travel insurance policy to protect your belongings and reduce the risk of",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-4",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-4",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs.Instruction-Tuned",
    "text": "Completion vs.Instruction-Tuned\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\ninstruct_text = instruct_tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\ninstruct_inputs = instruct_tokenizer(instruct_text, return_tensors=\"pt\")\ninstruct_outputs = instruct_model.generate(\n    **instruct_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=instruct_tokenizer.eos_token_id,\n)\ninstruct_response = instruct_tokenizer.decode(\n    instruct_outputs[0], skip_special_tokens=True\n)\nprint(instruct_response)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-5",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-5",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs.Instruction-Tuned",
    "text": "Completion vs.Instruction-Tuned\n\n\n\nsystem\nYou help travelers make plans for their trips.\nuser\nHello\nassistant\nHi there!\nuser\nWhat should I do on my upcoming trip to Paris?\nassistant\nGreat question! On your next trip to Paris, you can start by visiting the iconic Eiffel Tower and the Louvre Museum. Don't miss exploring the Notre-Dame Cathedral and its stunning stained glass windows. For a bit of a break, consider visiting Montmartre for some beautiful art and architecture. If you're looking for something more adventurous, you could take a stroll through the charming streets of Montmartre or explore the vibrant nightlife of Le Marais. Have fun planning your trip to Paris!",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#model-evolution-gpt-3.5-onwards",
    "href": "src/02/slides.html#model-evolution-gpt-3.5-onwards",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Model Evolution (GPT 3.5 onwards)",
    "text": "Model Evolution (GPT 3.5 onwards)\n\n\n\n\n\ntimeline\n    Nov 2022 : ChatGPT Launch\n                  : Built on GPT-3.5 using RLHF\n                  : 1M+ users in 5 days\n                  : Sparked widespread interest in generative AI\n\n    Feb 2023 : Llama 1 Released\n                  : Meta's LLaMA (7B, 13B, 33B, 65B parameters)\n                  : 13B model exceeded GPT-3 (175B) on most benchmarks\n                  : Text completion only (Alpaca fine-tune added instructions)\n\n    Jul 2023 : Llama 2 Released\n              : Available in 7B, 13B, 70B sizes\n              : Trained on 40% more data than Llama 1\n              : First open-weights Llama for commercial use",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#closed-vs.-open-models",
    "href": "src/02/slides.html#closed-vs.-open-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Closed vs.Open Models",
    "text": "Closed vs.Open Models\n\nClosed Source:\n\nHosted models\nNo ability to inspect the weights of the models\nNo ability to download the models\nOpenAI GPT-5, Claude Sonnet 4.5, Googles Gemini\nVery large models; often referred to as foundational models or frontier models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#closed-vs.-open-models-1",
    "href": "src/02/slides.html#closed-vs.-open-models-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Closed vs.Open Models",
    "text": "Closed vs.Open Models\n\nOpen Weight:\n\nDownloadable model files\nYou can download the model files with pretrained weights, but no training data\nNo training data == No ability to recreate the model from scratch\nMetas Llama, Googles Gemma, Alibabas Qwen, OpenAI gpt-oss-120b\nRange from small to medium in size (1Gb - 500Gb+)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#closed-vs.-open-models-2",
    "href": "src/02/slides.html#closed-vs.-open-models-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Closed vs.Open Models",
    "text": "Closed vs.Open Models\n\nOpen Source:\n\nModels with access to the training data set\nYou can download the model files with pretrained weights and the training data used to train it\ni.e., you could create the model from scratch\nExamples: AI2s OLMo, NVIDIA Nemotron",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#discovering-open-models",
    "href": "src/02/slides.html#discovering-open-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Discovering Open Models",
    "text": "Discovering Open Models\n\nSource: https://huggingface.co",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#what-is-hugging-face",
    "href": "src/02/slides.html#what-is-hugging-face",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What is Hugging Face?",
    "text": "What is Hugging Face?\n\nIt is to AI models what GitHub is to source code\n\nExplore, download models to run on local hardware\nUpload and share your own trained/fine-tuned models and datasets\nCreate Spaces - web-based apps for accessing models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#hugging-face-transformers",
    "href": "src/02/slides.html#hugging-face-transformers",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Hugging Face Transformers",
    "text": "Hugging Face Transformers\n\nSource: https://huggingface.co/docs/transformers",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#hugging-face-transformers-1",
    "href": "src/02/slides.html#hugging-face-transformers-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Hugging Face Transformers",
    "text": "Hugging Face Transformers\n\nWhat is the Hugging Face Transformers Library?\nOpen-source Python library to provide easy access to using various types of pre-trained transformer models\nBrings together all of the different formats under one interface\n\nDifferent models, vendors, types, chat templates\nDifferent implementations: PyTorch, TensorFlow, JAX\n\nA few lines of code to download and run the model",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#accessing-closed-models-1",
    "href": "src/02/slides.html#accessing-closed-models-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Accessing Closed Models",
    "text": "Accessing Closed Models\n\nConsumer Website / App\n\ne.g., ChatGPT website or AppStore App\nLimited free tier; monthly subscription for more usage\n\nAPI Access\n\nOpenAIs API Platform; Create a developer account\nCredit card required\nCharged for tokens sent to the model and tokens returned from the model\nGPT 5.2 Chat = $1.75 per million tokens input; $14 per million tokens output",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#accessing-closed-models-2",
    "href": "src/02/slides.html#accessing-closed-models-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Accessing Closed Models",
    "text": "Accessing Closed Models\n\nHow much is going to cost?\n\nToken estimators (e.g., tiktoken from OpenAI)\nOr napkin math: 100 tokens ~= 75 English words",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#accessing-closed-models-3",
    "href": "src/02/slides.html#accessing-closed-models-3",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Accessing Closed Models",
    "text": "Accessing Closed Models\n\nExample\n\nInput from user = 75 words (100 tokens)\nOutput from model = 1500 words (2000 tokens)\nTotal cost = 100 input tokens + 2000 output tokens\nTotal cost = $0.000175 + $0.028 = $0.028175\n\nAt scale\n\n10,000 users / 1 request per month ~= $281.75/mo\n\n10,000 users / 1 request per day ~= $8452.50/mo",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#openai-chat-completions-api",
    "href": "src/02/slides.html#openai-chat-completions-api",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "OpenAI Chat Completions API",
    "text": "OpenAI Chat Completions API\n\n2020: OpenAI launched GPT-3 API with a /completions endpoint.\n\nFirst major LLM API\n\n2022: ChatGPT launch; massive adoption\n2023 /chat/completions endpoint released, becomes the dominant interface\n2023-2024: Other providers use the same API format for their own models vs.inventing their own\n\nBuild on the OpenAI developer ecosystem\nOpenAI-compatible became a selling point",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#openai-chat-completions-api-1",
    "href": "src/02/slides.html#openai-chat-completions-api-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "OpenAI Chat Completions API",
    "text": "OpenAI Chat Completions API\n\nWho uses the OpenAI Chat Completions API format?\n\nAnthropic (Claude API is very similar, with minor differences)\nOpenRouter, an inference provider for many models\nOpen source tools: LiteLLM, LangChain\nLocal serving: Ollama, vLLM, llama.cpp are all OpenAI-compatible",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-the-chat-completions-api",
    "href": "src/02/slides.html#using-the-chat-completions-api",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using the Chat Completions API",
    "text": "Using the Chat Completions API\n\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    api_key=OPENAI_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-the-chat-completions-api-1",
    "href": "src/02/slides.html#using-the-chat-completions-api-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using the Chat Completions API",
    "text": "Using the Chat Completions API\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-5\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://api.openai.com/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"gpt-5\"\n}\n==================================================",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-the-chat-completions-api-2",
    "href": "src/02/slides.html#using-the-chat-completions-api-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using the Chat Completions API",
    "text": "Using the Chat Completions API\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"chatcmpl-CuVn7EYuGJUEUEQ18Cl0SM2nNz9Mj\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Awesome! I can tailor a plan, but a few quick questions help:\\n- When are you going and for how many days?\\n- First time in Paris?\\n- Main interests (art, food, fashion, history, photography, nightlife, kid-friendly, etc.) and preferred pace (relaxed vs. packed)?\\n- Any must-sees or hard nos?\\n- Rough budget and food needs (vegetarian, kosher/halal, allergies)?\\n- Where are you staying (neighborhood) and are day trips okay (Versailles, Champagne, Giverny, Disneyland)?\\n\\nIf you want a quick starter plan, heres a flexible 4-day outline you can reshuffle by weather and museum closures:\\n\\nDay 1  Islands + Latin Quarter\\n- le de la Cit: Notre-Dame exterior, Sainte-Chapelle (timed ticket), Conciergerie.\\n- Stroll the Latin Quarter: Shakespeare & Company, Sorbonne, Luxembourg Gardens.\\n- Evening: Seine cruise or sunset along the river.\\n\\nDay 2  Louvre to Arc de Triomphe\\n- Morning: Louvre (timed entry). Tuileries and Palais-Royal gardens.\\n- Covered passages (Vronique/Grand Cerf/Jouffroy) and Opra Garnier.\\n- Sunset view: Arc de Triomphe rooftop or Galeries Lafayette/Printemps terrace.\\n\\nDay 3  Montmartre + Left Bank art\\n- Montmartre: Sacr-Cur, Place du Tertre, quieter backstreets (Rue de lAbreuvoir).\\n- Afternoon: Muse dOrsay and/or Orangerie.\\n- Evening: Saint-Germain wine bar or jazz.\\n\\nDay 4  Le Marais or Day Trip\\n- Marais walk: Place des Vosges, Muse Carnavalet, Picasso Museum (check hours), Jewish quarter, trendy boutiques.\\n- Optional day trip: Versailles (palace + gardens; get the timed passport ticket).\\n- Night: Eiffel Tower area (view from Trocadro or Champ de Mars; book tower tickets if going up).\\n\\nOther great adds by interest\\n- Art/architecture: Rodin Museum; Bourse de Commerce; Fondation Louis Vuitton. Note: check Centre Pompidous renovation status.\\n- Food: Morning market (Aligre or Rue Cler), cheese/wine tasting, pastry crawl, bistro lunch, cooking class.\\n- Unique: Catacombs (book ahead), Pre Lachaise Cemetery, Canal Saint-Martin, covered markets (Le March des Enfants Rouges).\\n- With kids: Jardin des Plantes (zoo + galleries), Cit des Sciences, Jardin dAcclimatation, Parc de la Villette.\\n- Day trips: Giverny (AprOct), Reims/Epernay for Champagne, Fontainebleau, Auvers-sur-Oise, Disneyland Paris.\\n\\nBook these in advance\\n- Eiffel Tower, Louvre, Sainte-Chapelle, Catacombs, Versailles, Palais Garnier tours, popular restaurants.\\n- Consider the Paris Museum Pass (2/4/6 days) if youll visit several museums; the Louvre still needs a timed reservation even with the pass.\\n\\nPractical tips\\n- Closures: Many museums close one day/week (e.g., Orsay Mon, some Tue). Check hours.\\n- Getting around: The Mtro is fastest. Use a contactless bank card to tap in, or get a reloadable Navigo Easy. For a MondaySunday stay with lots of rides, a Navigo Dcouverte weekly pass can be good value.\\n- Dining: Reserve for dinner, especially weekends. Tipping is minimal (service included); round up or leave 510% for great service.\\n- Safety: Watch for pickpockets in crowded areas and on the Metro.\\n\\nShare your dates, length of stay, and interests, and Ill turn this into a detailed day-by-day plan with mapped routes and restaurant picks near each stop.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": [],\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1767584609,\n  \"model\": \"gpt-5-2025-08-07\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": \"default\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 2224,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 2268,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"reasoning_tokens\": 1408,\n      \"rejected_prediction_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0\n    }\n  }\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-history-management",
    "href": "src/02/slides.html#chat-history-management",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat History Management",
    "text": "Chat History Management\n\nKey Considerations\n\nModels dont hold any state\nAPI sends full conversation on every request and the model reads through the full conversation on every call\nThe size of the conversation is known as the context\nThe maximum size the model can process is referred to as the context window",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-history-management-1",
    "href": "src/02/slides.html#chat-history-management-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat History Management",
    "text": "Chat History Management\n\nContext window sizes\n\nGPT-2 = 2048 tokens\nTodays nano models ~= 32k tokens\nTodays small models ~= 120k tokens\nTodays frontier models ~= 1M tokens\n\nLarge conversations can cause challenges\n\nThey are expensive (you pay per token for whole conversation every time)\nSmall models often forget early details in long conversation histories",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-history-management-2",
    "href": "src/02/slides.html#chat-history-management-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat History Management",
    "text": "Chat History Management\n\nMitigation Strategies\n\nRemove older messages from the history\nImplement sliding window across the conversation history\nSummarize older messages and rewrite the history",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#calling-other-models-1",
    "href": "src/02/slides.html#calling-other-models-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Calling Other Models",
    "text": "Calling Other Models\n\nWe could just duplicate our notebook, change the URL to another provider (e.g., Claude, Google, etc.), but:\n\nA separate account with each provider\nA separate credit card with each provider\nA separate API key to use for each provider\nDuplicate notebooks for each provider\n\nWouldnt it be nice to have a single service (inference provider) that exposed lots of different models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#introducing-openrouter",
    "href": "src/02/slides.html#introducing-openrouter",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Introducing OpenRouter",
    "text": "Introducing OpenRouter\n\nIntroducing OpenRouter (https://openrouter.ai)\n\nA unified API to hundreds of AI models through a single endpoint\n(Using OpenAIs Chat Completion API)\nOpenAI, Claude, Gemini, Grok, Nova, Llama, DeepSeek, Qwen, and many others.\nPay per API call, often same cost as the provider\nNewer APIs tend to be free for a short period",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-openrouter",
    "href": "src/02/slides.html#using-openrouter",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using OpenRouter",
    "text": "Using OpenRouter\n\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-openrouter-1",
    "href": "src/02/slides.html#using-openrouter-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using OpenRouter",
    "text": "Using OpenRouter\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://openrouter.ai/api/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"openai/gpt-5.2-chat\"\n}\n==================================================",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-openrouter-2",
    "href": "src/02/slides.html#using-openrouter-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using OpenRouter",
    "text": "Using OpenRouter\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"gen-1767585819-snubWxcK6sJM3RdE9rJX\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Paris has something for almost every kind of traveler! Heres a wellrounded starting plan, and then I can tailor it more if you tell me your interests, travel dates, and how long youll be there.\\n\\n### MustSee Highlights\\n- **Eiffel Tower**  Go up for the views or enjoy it from below at Trocadro or Champ de Mars.\\n- **Louvre Museum**  Even if you dont love museums, seeing the Mona Lisa and the building itself is worth it.\\n- **NotreDame Cathedral**  Admire the exterior and surroundings; interior access is gradually reopening.\\n- **Montmartre & SacrCur**  Charming streets, artists, and great city views.\\n\\n### Classic Paris Experiences\\n- **Stroll along the Seine**  Especially at sunset.\\n- **Caf culture**  Sit at a caf with a coffee or glass of wine and peoplewatch.\\n- **Boulangeries & pastries**  Try croissants, pain au chocolat, macarons.\\n- **Seine river cruise**  Relaxing and great for firsttime visitors.\\n\\n### Art, History & Culture\\n- **Muse dOrsay**  Impressionist masterpieces in a stunning former train station.\\n- **Le Marais**  Historic district with boutiques, museums, and lively streets.\\n- **Latin Quarter**  Bookshops, old streets, and student energy.\\n\\n### Food & Drink\\n- **Bistro dining**  Try classic French dishes like boeuf bourguignon or duck confit.\\n- **Food markets**  March des Enfants Rouges is a favorite.\\n- **Wine & cheese tasting**  Many small shops offer guided tastings.\\n\\n### Day Trips (if you have extra time)\\n- **Versailles**  Palace and gardens (halfday or fullday trip).\\n- **Giverny**  Monets gardens (spring/summer).\\n- **Champagne region**  For wine lovers.\\n\\n### Practical Tips\\n- Buy museum tickets in advance.\\n- Walk as much as possibleParis is very walkable.\\n- Learn a few French phrases; locals appreciate the effort.\\n\\nIf youd like, tell me:\\n- How many days youll be there  \\n- Your interests (food, art, history, shopping, nightlife, romance, family travel)  \\n- Your budget level  \\n\\nAnd Ill create a personalized daybyday itinerary for you.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": null,\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null,\n        \"reasoning\": null\n      },\n      \"native_finish_reason\": \"completed\"\n    }\n  ],\n  \"created\": 1767585819,\n  \"model\": \"openai/gpt-5.2-chat\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 506,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 550,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": null,\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"rejected_prediction_tokens\": null,\n      \"image_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0,\n      \"video_tokens\": 0\n    },\n    \"cost\": 0.007161,\n    \"is_byok\": false,\n    \"cost_details\": {\n      \"upstream_inference_cost\": null,\n      \"upstream_inference_prompt_cost\": 0.000077,\n      \"upstream_inference_completions_cost\": 0.007084\n    }\n  },\n  \"provider\": \"OpenAI\"\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#token-streaming",
    "href": "src/02/slides.html#token-streaming",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Token Streaming",
    "text": "Token Streaming\n\nIn our notebooks, responses can take a few seconds to be returned\n\nNot the best user experience, especially for consumer products\n\nNeed a way to support streaming of tokens as they are generated (a.k.a. typewriter effect)\n\nStreaming added to Chat Completions API in early 2023\nSupported by other major vendors (Anthropic, Cohere, etc.)\nNow expected as a baseline feature",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#how-does-token-streaming-work",
    "href": "src/02/slides.html#how-does-token-streaming-work",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "How Does Token Streaming Work?",
    "text": "How Does Token Streaming Work?\n\nUses SSE (Server-Sent Events)\n\nUnidirectional (server to client)\nUses standard HTTP/1.1 or HTTP/2\nServer sends a response with a text/event-stream MIME type\nClient uses built-in EventSource API to open the connection, listen to messages, and handle events.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#sse-data-format",
    "href": "src/02/slides.html#sse-data-format",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "SSE Data Format",
    "text": "SSE Data Format\ndata: {\"choices\":[{\"delta\":{\"content\":\"Hello\"}}]}\n\ndata: {\"choices\":[{\"delta\":{\"content\":\" world\"}}]}\n  \ndata: [DONE]\nData sent as chunks, prefixed with data: and separated by double newlines",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-token-streaming",
    "href": "src/02/slides.html#implementing-token-streaming",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Token Streaming",
    "text": "Implementing Token Streaming\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n    stream=True, # Enable streaming\n)\n\n# Iterate through the stream and print each token as it arrives\nfor chunk in response:\n    # Each chunk contains a delta with the new content\n    if chunk.choices[0].delta.content is not None:\n        token = chunk.choices[0].delta.content\n        print(token, end='', flush=True)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#structured-output-1",
    "href": "src/02/slides.html#structured-output-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nSo far, the models have generated non-structured output (i.e., free-form text)\nSometimes, paragraph. Sometimes, numbered list.\nBut often, you need structure\n\nReturn your result in JSON format\nGive me the coordinates for Paris\nWhats the temperature in Paris right now?",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#structured-output-2",
    "href": "src/02/slides.html#structured-output-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nYou can try to use the system prompt\n\nReturn the result in JSON only\n\nBut it doesnt always work\n\nEarly/small models struggle with correct JSON formatting\nEven larger models make mistakes (e.g., missing closing brace)\n\nSometimes the models just forget!\n\nRETURN THE RESULT IN JSON ONLY. NO OTHER TEXT!!!",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#structured-outputs-in-openai-api",
    "href": "src/02/slides.html#structured-outputs-in-openai-api",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Structured Outputs in OpenAI API",
    "text": "Structured Outputs in OpenAI API\n\nNov 2023: OpenAI added JSON mode\n\nresponse_format: {\"type\": \"json_object\"}\nGuaranteed valid JSON, but didnt enforce schema\nSometimes mixed up/missed fields\n\nAug 2024: Structured Outputs launched\n\nresponse_format: {\"type\": \"json_object\", ...}\n100% reliability that output matches the your schema",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#how-structured-outputs-work",
    "href": "src/02/slides.html#how-structured-outputs-work",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "How Structured Outputs Work",
    "text": "How Structured Outputs Work\n\nConstrained Decoding\n\nWhen generating responses, the model normally samples from all possible next tokens\nWith constrained decoding, the next token is dynamically filtered to only allow tokens that keep the output schema valid\n\ne.g., if schema requires an integer, string tokens are masked out from the probability distribution",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#how-structured-outputs-work-1",
    "href": "src/02/slides.html#how-structured-outputs-work-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "How Structured Outputs Work",
    "text": "How Structured Outputs Work\n\nRuns on server (or in library) - not fine-tuning approach\nSlightly slower token generation due to computational overhead\nTechnically, its mathematically impossible to generate invalid output\n\n(Real world: I see ~1:7000 error rates with GPT-5.1 chat)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-structured-outputs",
    "href": "src/02/slides.html#implementing-structured-outputs",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Structured Outputs",
    "text": "Implementing Structured Outputs\n\n\nfrom pydantic import BaseModel\n\n# Define the model for a geographic location\nclass Location(BaseModel):\n  name: str\n  country: str\n  latitude: float\n  longitude: float",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-structured-outputs-1",
    "href": "src/02/slides.html#implementing-structured-outputs-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Structured Outputs",
    "text": "Implementing Structured Outputs\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.parse(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"What are the GPS coordinates for Paris?\"},\n    ],\n    response_format=Location\n)\n\ncompletion = response.choices[0].message\nprint(completion)\n\nParsedChatCompletionMessage[Location](content='{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=Location(name='Paris', country='France', latitude=48.8566, longitude=2.3522), reasoning=None)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-structured-outputs-2",
    "href": "src/02/slides.html#implementing-structured-outputs-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Structured Outputs",
    "text": "Implementing Structured Outputs\n\n\n# Display the JSON repesentation\nprint(completion.content)\n\n# Display the parsed type\nprint(completion.parsed)\n\n# Pretty-print\nif completion.parsed:\n  location: Location = completion.parsed\n  print(f\"{location.name}, {location.country} has GPS coordinates of {location.latitude}, {location.longitude}\")\n\n{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}\nname='Paris' country='France' latitude=48.8566 longitude=2.3522\nParis, France has GPS coordinates of 48.8566, 2.3522",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#creating-a-chat-ui-1",
    "href": "src/02/slides.html#creating-a-chat-ui-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Creating a Chat UI",
    "text": "Creating a Chat UI\n\nUp to now, weve been making requests and printing the responses\nGood for learning concepts, but not a product that others can use\nWe want to build a UI that supports conversation threads, streaming, rich inputs/outputs, etc.\nBut we dont want to start from scratch!",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#introducing-gradio",
    "href": "src/02/slides.html#introducing-gradio",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Introducing Gradio",
    "text": "Introducing Gradio\n\nSource: https://www.gradio.app/",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#what-is-gradio",
    "href": "src/02/slides.html#what-is-gradio",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What is Gradio?",
    "text": "What is Gradio?\n\nCreated in 2019: Startup called Gradio developing demos for research/academia\nAcquired by Hugging Face in 2021: became the standard interface for Hugging Face Spaces\nNow industry standard: For ML demos - used by researchers, startups to showcase models without front-end expertise",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#what-is-gradio-1",
    "href": "src/02/slides.html#what-is-gradio-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What is Gradio?",
    "text": "What is Gradio?\n\nRapid UI creation with minimal code\n\n5-10 lines of Python for an interactive interface. No HTML, CSS, JS required.\n\nRich input/output types\n\nText, images, audio, video, files, dataframes, etc.\n\nML workflows\n\nSupports streaming, queues, flagging/feedback\n\nDeployment flexibility\n\nCan run locally, create temporary public links, or embed in production apps",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-1-basic-interface",
    "href": "src/02/slides.html#example-1-basic-interface",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 1: Basic Interface",
    "text": "Example 1: Basic Interface\n\n\nimport gradio as gr\n\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\n\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\")\ndemo.launch()",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-1-basic-interface-1",
    "href": "src/02/slides.html#example-1-basic-interface-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 1: Basic Interface",
    "text": "Example 1: Basic Interface\n\n\n\n* Running on local URL:  http://127.0.0.1:7862\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-2-basic-chat-interface",
    "href": "src/02/slides.html#example-2-basic-chat-interface",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 2: Basic Chat Interface",
    "text": "Example 2: Basic Chat Interface\n\n\nimport gradio as gr\n\ndef chat_with_history(message, history):\n    # Add current message\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Get response from API\n    response = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n    )\n    \n    return response.choices[0].message.content\n\n# Create a chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_history,\n    title=\"Basic Chat with Conversation History\",\n    type=\"messages\"\n)\n\ndemo.launch()",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-2-basic-chat-interface-1",
    "href": "src/02/slides.html#example-2-basic-chat-interface-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 2: Basic Chat Interface",
    "text": "Example 2: Basic Chat Interface\n\n\n\n* Running on local URL:  http://127.0.0.1:7863\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-3-streaming-chat-interface",
    "href": "src/02/slides.html#example-3-streaming-chat-interface",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 3: Streaming Chat Interface",
    "text": "Example 3: Streaming Chat Interface\n\n\nimport gradio as gr\n\ndef chat_with_streaming(message, history):\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Stream the response\n    stream = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n        stream=True,\n    )\n    \n    response_text = \"\"\n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            token = chunk.choices[0].delta.content\n            response_text += token\n            yield response_text\n\n# Create streaming chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_streaming,\n    title=\"AI Chat with Streaming\",\n    type=\"messages\"\n)\n\ndemo.launch()",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-3-streaming-chat-interface-1",
    "href": "src/02/slides.html#example-3-streaming-chat-interface-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 3: Streaming Chat Interface",
    "text": "Example 3: Streaming Chat Interface\n\n\n\n* Running on local URL:  http://127.0.0.1:7864\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#looking-ahead-1",
    "href": "src/02/slides.html#looking-ahead-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis weeks assignment!\nExplore AI Agents\nCreate agents, building upon our knowledge of Gradio\nGive the agent documents and tools to perform functions beyond what an LLM can do",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#references-1",
    "href": "src/02/slides.html#references-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/assignment.html",
    "href": "src/02/assignment.html",
    "title": "Module 2 Assignment: Gradio Travel Planner",
    "section": "",
    "text": "Objective: Build a chat interface (in a separate Colab notebook) that helps a user complete tasks.\nRequirements:\n\nGradio chat interface with streaming responses\nSystem prompt that defines the AI as a travel planning expert (or pick your own scenario, if you have a better idea!)\nImplements a well-thought out system prompt (with feedback on the rationale behind it).\nDemonstrates at least 2 different models via OpenRouter as a dropdown in Gradio. (Not necessarily at the same time.)\n\nTry to pick models of different sizes to compare the differences. If you search for free models (https://openrouter.ai/models?q=free) you can also avoid incurring any costs.\n\nUse structured outputs when returning results to the user (e.g., a downloadable itinerary that the user can download)\n\nExample schema: Trip with fields like destination, duration_days, activities (list), budget_level, daily_schedule (list of day objects)\n\n\nDeliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation\nUses OPENROUTER_API_KEY for the API token. (Please do not include your API key in your notebook!)\nMarkdown cells explaining what the notebook does and any observations\n\nHint\n\nTo get streaming and structured outputs working in Gradio, you may want to think of this as two calls to the model:\n\nThe first call asks the model to think about the problem (e.g., How can you help the user solve their travel question?) You can use streaming to display the response token-by-token in Gradio.\nThe second call then takes the models prior answer and creates a new call (e.g., From your thinking, create an itinerary.) You can then use structured outputs to ensure that this matches the schema, and map that to a component in the Gradio UI (gr.JSON is fine to display this, unless you want to get more creative :).",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Assignment"
    ]
  },
  {
    "objectID": "src/01/resources.html",
    "href": "src/01/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Visualize embeddings in 3D space, powered by EmbeddingGemma and Transformers.js\nOriginal Word2Vec Papers - Google Code archive with original papers\nWord2Vec Tutorial - The Skip-Gram Model - Chris McCormicks detailed tutorial\nGensim Word2Vec Tutorial - Popular Python library for word embeddings\nA Visual Introduction to Vector Embeddings, Pamela Fox\nA Visual Exploration of Vectors, Pamela Fox",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#word2vec-and-word-embeddings",
    "href": "src/01/resources.html#word2vec-and-word-embeddings",
    "title": "Resources",
    "section": "",
    "text": "Visualize embeddings in 3D space, powered by EmbeddingGemma and Transformers.js\nOriginal Word2Vec Papers - Google Code archive with original papers\nWord2Vec Tutorial - The Skip-Gram Model - Chris McCormicks detailed tutorial\nGensim Word2Vec Tutorial - Popular Python library for word embeddings\nA Visual Introduction to Vector Embeddings, Pamela Fox\nA Visual Exploration of Vectors, Pamela Fox",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#tokenization-and-byte-pair-encoding",
    "href": "src/01/resources.html#tokenization-and-byte-pair-encoding",
    "title": "Resources",
    "section": "Tokenization and Byte Pair Encoding",
    "text": "Tokenization and Byte Pair Encoding\n\nBPE Original Paper (1994) - Philip Gages original compression algorithm\nNeural Machine Translation with BPE - 2016 paper adapting BPE for NLP\nHugging Face Tokenizers - Understanding different tokenization strategies\nOpenAI Tokenizer Tool - Interactive tool to see how text is tokenized",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#transformers",
    "href": "src/01/resources.html#transformers",
    "title": "Resources",
    "section": "Transformers",
    "text": "Transformers\n\nThe Illustrated Transformer - Jay Alammars visual guide\nAttention is All You Need Paper - The original transformer paper (2017)\nThe Annotated Transformer - Harvard NLPs line-by-line implementation guide\nTransformer Math 101 - Understanding transformer computation\nAttention Mechanism Explained - Visual explanation of attention",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#gpt-and-language-models",
    "href": "src/01/resources.html#gpt-and-language-models",
    "title": "Resources",
    "section": "GPT and Language Models",
    "text": "GPT and Language Models\n\nGPT-1 Paper: Improving Language Understanding - Original GPT paper (2018)\nGPT-2 Paper: Language Models are Unsupervised Multitask Learners - GPT-2 paper (2019)\nGPT-2 Release Blog Post - OpenAIs staged release announcement\nUnderstanding Decoder-Only Models - Sebastian Raschkas explanation\nAndrej Karpathys Lets build GPT - Building GPT from scratch",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#python-and-programming-basics",
    "href": "src/01/resources.html#python-and-programming-basics",
    "title": "Resources",
    "section": "Python and Programming Basics",
    "text": "Python and Programming Basics\n\nLearn Python with Jupyter, Serena Bonaretti\nPython Official Documentation - Official Python 3 documentation\nPython for Beginners - Python.orgs getting started guide\nAutomate the Boring Stuff with Python - Free online book for Python beginners",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#python-package-management",
    "href": "src/01/resources.html#python-package-management",
    "title": "Resources",
    "section": "Python Package Management",
    "text": "Python Package Management\n\npip Documentation - Pythons standard package installer\nuv Documentation - Modern, fast Python package manager\nPyPI - Python Package Index - Repository of 500K+ Python packages",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#notebooks-and-development-environment",
    "href": "src/01/resources.html#notebooks-and-development-environment",
    "title": "Resources",
    "section": "Notebooks and Development Environment",
    "text": "Notebooks and Development Environment\n\nGoogle Colab Sign-up Page\nGoogle Colab Tips and Tricks - Making the most of Colab\nProject Jupyter Page\nJupyter Notebook Beginner Guide - Getting started with Jupyter\nVS Code Jupyter Extension - Run notebooks in VS Code",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#citations",
    "href": "src/01/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/weekly-rubric.html",
    "href": "src/00/weekly-rubric.html",
    "title": "Rubric (Weekly Assignments)",
    "section": "",
    "text": "1%\n2%\n3%\n4%\n5%\n\n\n\n\nThe submission fails to showcase any working features. The solution is practically unusable due to issues.\nOnly a few features are functional. Significant missing work or issues with the submission.\nMajor issues impact key features, but some functionality is evident. Noticeable gaps or confusing submission.\nMinor issues or bugs are present but do not significantly impact the functionality. Very few gaps, and a near-complete submission.\nAll features are fully functional. A complete submission that meets all requirements of the assignment.\n\n\n\n(0% for unsubmitted work)",
    "crumbs": [
      "**Welcome**",
      "Rubric (Weekly Assignments)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html",
    "href": "src/00/final-rubric-594.html",
    "title": "Rubric (Final Project CS-594)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. No fine-tuning achieved. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited fine-tuning demonstrated. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Fine-tuning demonstrated, but not optimal. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Strong fine-tuning techniques demonstrated. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Excellent approach and results from fine-tuning. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#fine-tuning-and-integration-of-ai-models-10",
    "href": "src/00/final-rubric-594.html#fine-tuning-and-integration-of-ai-models-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. No fine-tuning achieved. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited fine-tuning demonstrated. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Fine-tuning demonstrated, but not optimal. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Strong fine-tuning techniques demonstrated. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Excellent approach and results from fine-tuning. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#functionality-10",
    "href": "src/00/final-rubric-594.html#functionality-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "Functionality (10%)",
    "text": "Functionality (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nProject is largely non-functional with AI components not working. Major bugs and errors prevent basic usage.\nProject runs but AI components frequently fail or produce incorrect results. Significant functionality issues present.\nProject is functional with AI components working in most cases. Some bugs or limitations affect user experience.\nProject is fully working with AI components functioning reliably as intended. Minor issues may exist but dont impact core functionality.\nProject is fully functional with flawless AI integration. All components work seamlessly together with robust error handling.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#innovation-and-creativity-10",
    "href": "src/00/final-rubric-594.html#innovation-and-creativity-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "Innovation and Creativity (10%)",
    "text": "Innovation and Creativity (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nGeneric AI usage with no creative application. Design is basic with poor aesthetics and no meaningful AI enhancement.\nAI used in conventional ways with limited creativity. Design shows some effort but AI doesnt meaningfully enhance the experience.\nCreative AI application with interesting use cases. Design is competent with AI providing noticeable visual or interactive improvements.\nInnovative AI usage providing unique and compelling experiences. Strong aesthetic and functional design that effectively leverages AI capabilities.\nExceptional creativity with groundbreaking AI applications. Outstanding design that seamlessly integrates AI to create truly unique and compelling experiences.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#ethical-analysis-10",
    "href": "src/00/final-rubric-594.html#ethical-analysis-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "Ethical Analysis (10%)",
    "text": "Ethical Analysis (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nNo ethical analysis provided or only superficial acknowledgment of issues. No consideration of biases or societal impacts.\nLimited ethical analysis with minimal identification of potential issues. Brief mention of biases but no mitigation strategies proposed.\nAdequate evaluation of biases and ethical implications with some discussion of societal impacts. Basic mitigation proposals included.\nThorough evaluation of potential biases, ethical implications, and societal impacts. Clear and practical proposals for mitigating identified risks.\nComprehensive and insightful ethical analysis demonstrating deep understanding of AI implications. Sophisticated mitigation strategies with consideration of broader societal impacts.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#presentation-and-research-20",
    "href": "src/00/final-rubric-594.html#presentation-and-research-20",
    "title": "Rubric (Final Project CS-594)",
    "section": "Presentation and Research (20%)",
    "text": "Presentation and Research (20%)\n\n\n\n\n\n\n\n\n\n\n4%\n8%\n12%\n16%\n20%\n\n\n\n\nPresentation is unclear and disorganized with minimal explanation of AI features. If in a team, roles were undefined and collaboration issues evident. No research evident and/or cited-papers.\nPresentation covers basic points but lacks engagement or clear explanation of AI integration process. If in a team, some team member contributions unclear. Limited research evident.\nClear presentation highlighting main AI features and challenges faced. If in a team, collaboration is adequate with most roles and contributions identifiable. Research evident, but with gaps or issues.\nClear and engaging presentation effectively showcasing AI features and their purpose. Comprehensive explanation of integration process including model selection and solutions, with well-defined team roles (if applicable). Strong, well-cited research evident.\nOutstanding presentation that captivates audience while thoroughly explaining AI implementation. Exceptional teamwork (if applicable) with seamless collaboration and clearly articulated individual contributions. Excellent and well-cited approach to research",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS-394/594: How Generative AI Works",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CS-394/594 course.",
    "crumbs": [
      "CS-394/594"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "CS-394/594: How Generative AI Works",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CS-394/594 course.",
    "crumbs": [
      "CS-394/594"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html",
    "href": "src/00/final-rubric-394.html",
    "title": "Rubric (Final Project CS-394)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#integration-of-ai-models-10",
    "href": "src/00/final-rubric-394.html#integration-of-ai-models-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#functionality-10",
    "href": "src/00/final-rubric-394.html#functionality-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "Functionality (10%)",
    "text": "Functionality (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nProject is largely non-functional with AI components not working. Major bugs and errors prevent basic usage.\nProject runs but AI components frequently fail or produce incorrect results. Significant functionality issues present.\nProject is functional with AI components working in most cases. Some bugs or limitations affect user experience.\nProject is fully working with AI components functioning reliably as intended. Minor issues may exist but dont impact core functionality.\nProject is fully functional with flawless AI integration. All components work seamlessly together with robust error handling.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#innovation-and-creativity-10",
    "href": "src/00/final-rubric-394.html#innovation-and-creativity-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "Innovation and Creativity (10%)",
    "text": "Innovation and Creativity (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nGeneric AI usage with no creative application. Design is basic with poor aesthetics and no meaningful AI enhancement.\nAI used in conventional ways with limited creativity. Design shows some effort but AI doesnt meaningfully enhance the experience.\nCreative AI application with interesting use cases. Design is competent with AI providing noticeable visual or interactive improvements.\nInnovative AI usage providing unique and compelling experiences. Strong aesthetic and functional design that effectively leverages AI capabilities.\nExceptional creativity with groundbreaking AI applications. Outstanding design that seamlessly integrates AI to create truly unique and compelling experiences.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#ethical-analysis-10",
    "href": "src/00/final-rubric-394.html#ethical-analysis-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "Ethical Analysis (10%)",
    "text": "Ethical Analysis (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nNo ethical analysis provided or only superficial acknowledgment of issues. No consideration of biases or societal impacts.\nLimited ethical analysis with minimal identification of potential issues. Brief mention of biases but no mitigation strategies proposed.\nAdequate evaluation of biases and ethical implications with some discussion of societal impacts. Basic mitigation proposals included.\nThorough evaluation of potential biases, ethical implications, and societal impacts. Clear and practical proposals for mitigating identified risks.\nComprehensive and insightful ethical analysis demonstrating deep understanding of AI implications. Sophisticated mitigation strategies with consideration of broader societal impacts.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#presentation-20",
    "href": "src/00/final-rubric-394.html#presentation-20",
    "title": "Rubric (Final Project CS-394)",
    "section": "Presentation (20%)",
    "text": "Presentation (20%)\n\n\n\n\n\n\n\n\n\n\n4%\n8%\n12%\n16%\n20%\n\n\n\n\nPresentation is unclear and disorganized with minimal explanation of AI features. If in a team, roles were undefined and collaboration issues evident.\nPresentation covers basic points but lacks engagement or clear explanation of AI integration process. If in a team, some team member contributions unclear.\nClear presentation highlighting main AI features and challenges faced. If in a team, collaboration is adequate with most roles and contributions identifiable.\nClear and engaging presentation effectively showcasing AI features and their purpose. Comprehensive explanation of integration process including model selection and solutions, with well-defined team roles (if applicable).\nOutstanding presentation that captivates audience while thoroughly explaining AI implementation. Exceptional teamwork (if applicable) with seamless collaboration and clearly articulated individual contributions.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/slides.html#course-description",
    "href": "src/00/slides.html#course-description",
    "title": "Welcome to CS-394/594!",
    "section": "Course Description",
    "text": "Course Description\n\nHow Generative AI Works focuses on the practical implementation of generative AI within custom software applications and games.\nThe course covers neural network architectures, including the impact of the Transformer model, customization of large language models across multiple vendors using APIs, and experimentation with multimodal models for image and audio recognition and generation.",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#course-description-1",
    "href": "src/00/slides.html#course-description-1",
    "title": "Welcome to CS-394/594!",
    "section": "Course Description",
    "text": "Course Description\n\nHands-on experience includes working with both hosted and locally run models, integrating AI with game engines such as Unity and Unreal, and developing AI agents that extend beyond simple chat-based interactions.\nEthical considerations and model evaluation are integrated throughout, emphasizing awareness of broader societal implications.\nThrough lectures, programming assignments, and a final project, the course provides the expertise needed to apply generative AI in creating innovative and interactive experiences.",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#learning-outcomes",
    "href": "src/00/slides.html#learning-outcomes",
    "title": "Welcome to CS-394/594!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the basic working principles and history of current LLMs (Large Language Models)\nUnderstand ethical and safety aspects of using generative models\nEvaluate and test generative models using industry benchmarks\nRun generative models on local, laptop-based hardware (using CPU, GPU, NPUs)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#learning-outcomes-1",
    "href": "src/00/slides.html#learning-outcomes-1",
    "title": "Welcome to CS-394/594!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCreate AI-based agents and tools based on the MCP (Model Context Protocol) specification\nAvoid hallucinations by increasing the accuracy of models through RAG (Retrieval Augmented Generation) and fine-tuning\nExplore and use multimodal models for image and audio recognition and generation\nCreate and deploy API-based clients, accessing LLMs hosted by different vendors (OpenAI, Meta, Google)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#in-summary",
    "href": "src/00/slides.html#in-summary",
    "title": "Welcome to CS-394/594!",
    "section": "In Summary",
    "text": "In Summary\n\nFocus on integration/augmentation vs.automation/using\nProvide a level of understanding beyond where most professional software developers are today\nBuild an exciting final project that you can add to your portfolio!",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#syllabus-1",
    "href": "src/00/slides.html#syllabus-1",
    "title": "Welcome to CS-394/594!",
    "section": "Syllabus",
    "text": "Syllabus\n\nSyllabus on OneDrive",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#schedule",
    "href": "src/00/slides.html#schedule",
    "title": "Welcome to CS-394/594!",
    "section": "Schedule",
    "text": "Schedule\n\nEvery Friday (Curie); 2pm - 4.50pm\n~1.5 hours of lecture\n~1.5 hours for in-class hands-on lab time and assignments\nExpectation of after-class work for assignments\nNo structured lectures for the weeks of the final project (3 hours of in-class lab time)\n\nAlthough we may do mini-lectures for common topics",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#schedule-1",
    "href": "src/00/slides.html#schedule-1",
    "title": "Welcome to CS-394/594!",
    "section": "Schedule",
    "text": "Schedule\n\nModules 1 through 4\nFeb 6 is Founders Day, so no classes\nModules 5 through 8\nMar 9 - 13 is Spring Break\nWeeks 9 through 14 - Final Project Work\nFinal presentations w/o Apr 20-24",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#during-class",
    "href": "src/00/slides.html#during-class",
    "title": "Welcome to CS-394/594!",
    "section": "During Class",
    "text": "During Class\n\nStrive for conversation and interactivity\n\nPlease ask questions, even mid-slide!\nThere are no wrong or bad questions!\nI enjoy going off on tangents / on the whiteboard\nUse hands on lab time to seek input / troubleshoot code",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#grading",
    "href": "src/00/slides.html#grading",
    "title": "Welcome to CS-394/594!",
    "section": "Grading",
    "text": "Grading\n\nModule Assignments: 40% of grade (8 x 5%)\nFinal Project: 60% of grade\nRubric for the weekly assignments\nRubric for the final project (CS-394)\nRubric for the final project (CS-594)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#handing-in-work",
    "href": "src/00/slides.html#handing-in-work",
    "title": "Welcome to CS-394/594!",
    "section": "Handing in Work",
    "text": "Handing in Work\n\nEverything submitted via GitHub\n\nRecommend creating a repo for weekly assignments\n\nFor most weeks, submission will be one Python notebook\n\nAnd (eventually) another repo for your final project\n\nDont forget to give me permissions! @simonguest",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#deadlines",
    "href": "src/00/slides.html#deadlines",
    "title": "Welcome to CS-394/594!",
    "section": "Deadlines",
    "text": "Deadlines\n\nWeekly Assignments\n\nAssignments are due by the following weeks lesson\ni.e., you get a week for each assignment\nIf you need more time/exception, please reach out via Teams\n\nFinal Project\n\nUp until Week 15 presentations\n(Well cover in detail later in the semester)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#ai-policy",
    "href": "src/00/slides.html#ai-policy",
    "title": "Welcome to CS-394/594!",
    "section": "AI Policy",
    "text": "AI Policy\n\nPermitted AI Usage\n\nYou may use AI tools to assist in understanding course materials\nIf AI is used to generate code, your must test and validate the code, must understand and be able to answer questions about the generated code, and include proper citations\nIf AI tools are used to assist with any part of an assignment, you must clearly cite the AI tool and explain how it was used",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#tools",
    "href": "src/00/slides.html#tools",
    "title": "Welcome to CS-394/594!",
    "section": "Tools",
    "text": "Tools\n\nWe will be introducing many tools\n\nColab Pro, OpenRouter, Hugging Face, etc.\nMost will be free\nExpect to need about $25 in API credits throughout the semester",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#languages",
    "href": "src/00/slides.html#languages",
    "title": "Welcome to CS-394/594!",
    "section": "Languages",
    "text": "Languages\n\nWe will be using (and learning) a lot of Python!\n\nMost of the in-class assignments will be in Python\nDont worry if you are new to Python as well introduce concepts gradually\nAlthough recommend investing extra time (see resources in Week 1)\n\nFinal Project\n\nCan be any language\nProbably depending on what you choose to create",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#hardware",
    "href": "src/00/slides.html#hardware",
    "title": "Welcome to CS-394/594!",
    "section": "Hardware",
    "text": "Hardware\n\nWill will be training SLMs (Small Language Models) later in the semester\nThis training will require a decent GPU and VRAM\n\nColab Pro (CUDA)\nYour own NVIDIA-based laptop (CUDA)\nPotential of using MLX for any Mac users",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#need-help-1",
    "href": "src/00/slides.html#need-help-1",
    "title": "Welcome to CS-394/594!",
    "section": "Need Help?",
    "text": "Need Help?\n\nhttps://simonguest.github.io/CS-394\n\nSlides (current and prior lectures), Demo code, Resources, Rubrics, Assignments\nI will repost assignments and rubrics on the Meta-Moodle. (Grades will also be in Moodle.)\n\nOffice Hours\n\nThursdays 1pm - 3pm (On campus or virtually via Teams)\n\nTeams (CS394/594 combined channel)\n\nPrimary mechanism for updates, ask questions, request office hours, etc.",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#steep-learning-curve",
    "href": "src/00/slides.html#steep-learning-curve",
    "title": "Welcome to CS-394/594!",
    "section": "Steep Learning Curve",
    "text": "Steep Learning Curve\n\nWe will be using the latest tools and AI models\nLots of new tools, acronyms, frameworks, etc.\nMuch of the curriculum builds upon itself\n\nPlease try not to miss lectures\nAsk for help if you need to catch up",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#new-course-at-digipen",
    "href": "src/00/slides.html#new-course-at-digipen",
    "title": "Welcome to CS-394/594!",
    "section": "New Course at DigiPen!",
    "text": "New Course at DigiPen!\n\nThere may be some minor curriculum tweaks mid-flight\n\nEspecially for topics that need less/more time",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#fast-moving-space",
    "href": "src/00/slides.html#fast-moving-space",
    "title": "Welcome to CS-394/594!",
    "section": "Fast Moving Space",
    "text": "Fast Moving Space\n\nThere will be areas/questions I dont have experience of\n\nMultiple new models are launched every week\nor equations/algorithms that I dont know\n\nWe will be learning some things together!\nBut thats what makes it exciting!",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/assignment.html",
    "href": "src/01/assignment.html",
    "title": "Module 1 Assignment: Experiment with Text Continuation Styles",
    "section": "",
    "text": "Objective: Create a Colab notebook that uses GPT-2 to generate creative text continuations with different styles.\nRequirements:\n\nLoad a pre-trained GPT-2 model (using HuggingFace transformers - same approach as used in GPT-2.ipynb)\nCreate 3 different story starters in different genres/styles. For example:\n\nFantasy/Adventure: In a land of dragons and magic\nSci-fi: The year is 2157. Humanity has just\nMystery: The detective examined the crime scene and noticed\n(or choose your own three)\n\nThen adjust for:\n\nGreedy decoding vs.sampling\nDifferent temperature values\nHow the opening sentence shapes the continuation (e.g., short vs.long)\n\nDocument your observations (using Markdown in the notebook)\n\nWhat differences do you notice between the strategies?\nWhat worked well (or surprised you!)\nWhat didnt work that well\n\n\nDeliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation\nOutputs of generated text from GPT-2\nMarkdown cells explaining what each sampling strategy does and any observations\n\nHint:\nFor better results, use a larger GPT-2 model on Colab T4.\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Assignment"
    ]
  },
  {
    "objectID": "src/01/slides.html#module-objectives",
    "href": "src/01/slides.html#module-objectives",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nExplore the history of vector embeddings and tokenization\nUnderstand the transformer architecture at a high level\nUse our first transformer to translate language\nCover a brief history of early generative transformers\nSetup and use Colab, and become familiar with the basics of notebooks and Python (if you havent used them already)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#rewind-to-2013",
    "href": "src/01/slides.html#rewind-to-2013",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Rewind To 2013",
    "text": "Rewind To 2013\n\nNLP (Natural Language Processing) was the thing!\n\nSentiment analysis, named entity recognition, parsing, etc.\n\nBut, you had limited options\n\nOne-hot encoding\nHand crafted features\nNeural language models",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#word2vec-released",
    "href": "src/01/slides.html#word2vec-released",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2013: Word2Vec Released",
    "text": "2013: Word2Vec Released\n\nWord2Vec introduced by Mikolov and colleagues at Google Research in two papers\n\nSkip-gram and Continuous Bag-of-Words (CBOW) (Mikolov, Chen, et al. 2013)\nNegative sampling and subsampling techniques (Mikolov, Sutskever, et al. 2013)\n\nParadigm shift from count-based methods\n\nUsed Neural Networks (NNs) to predict words vs.large matrices\n\nFoundation for modern NLP tasks",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work",
    "href": "src/01/slides.html#how-does-word2vec-work",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\nWord Embeddings are meaningful numerical representations of words\n\nRepresentations where words are encoded into multi-dimensional space\nLarge number of dimensions (200-500 is typical)\nSimilar words have similar numbers",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-1",
    "href": "src/01/slides.html#how-does-word2vec-work-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"cat\"\nvector = model[word]\nvector[:10]\n\narray([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656],\n      dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-2",
    "href": "src/01/slides.html#how-does-word2vec-work-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"dog\"\nvector = model[word]\nvector[:10]\n\narray([ 0.05126953, -0.02233887, -0.17285156,  0.16113281, -0.08447266,\n        0.05737305,  0.05859375, -0.08251953, -0.01538086, -0.06347656],\n      dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-3",
    "href": "src/01/slides.html#how-does-word2vec-work-3",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"pizza\"\nvector = model[word]\nvector[:10]\n\narray([-1.2597656e-01,  2.5390625e-02,  1.6699219e-01,  5.5078125e-01,\n       -7.6660156e-02,  1.2890625e-01,  1.0253906e-01, -3.9482117e-04,\n        1.2158203e-01,  4.3212891e-02], dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#why-do-this",
    "href": "src/01/slides.html#why-do-this",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Why Do This?",
    "text": "Why Do This?\n\nMapping words to multi-dimensional vectors enables\n\nTest for similarity\nCompute similarity\nPerform vector arithmetic\nExplore sets of words through visualizations",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-4",
    "href": "src/01/slides.html#how-does-word2vec-work-4",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nfind_similar_words(\"cat\")\nfind_similar_words(\"dog\")\nfind_similar_words(\"pizza\")\n\n\nWords most similar to 'cat':\n----------------------------------------\ncats                 | similarity: 0.8099\ndog                  | similarity: 0.7609\nkitten               | similarity: 0.7465\nfeline               | similarity: 0.7326\nbeagle               | similarity: 0.7151\npuppy                | similarity: 0.7075\npup                  | similarity: 0.6934\npet                  | similarity: 0.6892\nfelines              | similarity: 0.6756\nchihuahua            | similarity: 0.6710\n\nWords most similar to 'dog':\n----------------------------------------\ndogs                 | similarity: 0.8680\npuppy                | similarity: 0.8106\npit_bull             | similarity: 0.7804\npooch                | similarity: 0.7627\ncat                  | similarity: 0.7609\ngolden_retriever     | similarity: 0.7501\nGerman_shepherd      | similarity: 0.7465\nRottweiler           | similarity: 0.7438\nbeagle               | similarity: 0.7419\npup                  | similarity: 0.7407\n\nWords most similar to 'pizza':\n----------------------------------------\npizzas               | similarity: 0.7863\nDomino_pizza         | similarity: 0.7343\nPizza                | similarity: 0.6988\npepperoni_pizza      | similarity: 0.6903\nsandwich             | similarity: 0.6840\nburger               | similarity: 0.6570\nsandwiches           | similarity: 0.6495\ntakeout_pizza        | similarity: 0.6492\ngourmet_pizza        | similarity: 0.6401\nmeatball_sandwich    | similarity: 0.6377",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-5",
    "href": "src/01/slides.html#how-does-word2vec-work-5",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\ncompute_similarity('cat', 'dog')\ncompute_similarity('cat', 'kitten')\ncompute_similarity('cat', 'car')\ncompute_similarity('doctor', 'hospital')\ncompute_similarity('king', 'queen')\n\nSimilarity between 'cat' and 'dog': 0.7609\nSimilarity between 'cat' and 'kitten': 0.7465\nSimilarity between 'cat' and 'car': 0.2153\nSimilarity between 'doctor' and 'hospital': 0.5143\nSimilarity between 'king' and 'queen': 0.6511",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-6",
    "href": "src/01/slides.html#how-does-word2vec-work-6",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nvector_arithmetic(['king', 'woman'], ['man'])\nvector_arithmetic(['Paris', 'Italy'], ['France'])\nvector_arithmetic(['walking', 'swim'], ['walk'])\n\n\nking + woman - man:\n--------------------------------------------------\nqueen                | similarity: 0.7118\nmonarch              | similarity: 0.6190\nprincess             | similarity: 0.5902\ncrown_prince         | similarity: 0.5499\nprince               | similarity: 0.5377\n\nParis + Italy - France:\n--------------------------------------------------\nMilan                | similarity: 0.7222\nRome                 | similarity: 0.7028\nPalermo_Sicily       | similarity: 0.5968\nItalian              | similarity: 0.5911\nTuscany              | similarity: 0.5633\n\nwalking + swim - walk:\n--------------------------------------------------\nswimming             | similarity: 0.8246\nswam                 | similarity: 0.6807\nswims                | similarity: 0.6538\nswimmers             | similarity: 0.6495\npaddling             | similarity: 0.6424",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-7",
    "href": "src/01/slides.html#how-does-word2vec-work-7",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-python",
    "href": "src/01/slides.html#what-is-python",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is Python?",
    "text": "What is Python?\n\nInterpreted language (vs.compiled like C++ or C#)\n\nNo compilation step - code runs directly\nInteractive and flexible, great for experimentation\n\nCreated by Guido van Rossum in 1991\n\nPython 2 (2000-2020), Python 3 (2008-present)\nWell use Python 3.13\n\nCross-platform: Runs on Windows, macOS, Linux\nDynamically typed: No need to declare variable types\nThe language of AI/ML: Vast ecosystem of libraries (NumPy, TensorFlow, PyTorch, Transformers)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#variables-and-data-types",
    "href": "src/01/slides.html#variables-and-data-types",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\n\nVariables store data (no type declaration needed)\n\nx = 42 (integer)\nname = \"Alice\" (string)\npi = 3.14 (float)\n\nLists hold multiple values\n\nnumbers = [1, 2, 3, 4, 5]\nwords = [\"cat\", \"dog\", \"bird\"]\n\nAccess with square brackets: numbers[0] returns 1",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#functions",
    "href": "src/01/slides.html#functions",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Functions",
    "text": "Functions\n\nFunctions perform actions\n\nBuilt-in: print(\"Hello\"), len([1, 2, 3])\nDefine your own: def greet(name): return f\"Hello {name}\"\nIndentation vs.braces\nSupport for classes (although used rarely in AI/ML)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#libraries-and-packages",
    "href": "src/01/slides.html#libraries-and-packages",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Libraries and Packages",
    "text": "Libraries and Packages\n\nLibraries extend Pythons capabilities\n\nimport math - mathematical functions\nfrom transformers import AutoModel - import specific components\n\nUse dot notation to access: math.sqrt(16)\nPackage management\n\npip - standard package installer (similar to NuGet for C#)\nuv - modern, faster alternative to pip\nPyPI (Python Package Index) - central repository with 500K+ packages",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-notebook",
    "href": "src/01/slides.html#what-is-a-notebook",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is a Notebook?",
    "text": "What is a Notebook?\n\nAn interactive document that combines:\n\nLive code that can be executed\nRich text explanations (markdown)\nVisualizations and outputs\n\nThink of it as a computational narrative\n\nTell a story with code, data, and explanations\n\nOriginally designed for data science and research\nAlso used for learning, experimenting, and sharing results",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#a-brief-history-of-notebooks",
    "href": "src/01/slides.html#a-brief-history-of-notebooks",
    "title": "Module 1: Foundations of Generative AI",
    "section": "A Brief History of Notebooks",
    "text": "A Brief History of Notebooks\n\n2011: IPython Notebook project begins\n\nInteractive Python shell  web-based notebook\n\n2014: Renamed to Jupyter (Julia, Python, R)\n\nNow supports 40+ programming languages\nPython is most popular by far\n\n2017: Google launches Colab\n\nFree cloud-based Jupyter notebooks\nFree access to GPUs and TPUs\n\nToday: Industry standard for ML/AI development",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#anatomy-of-a-python-notebook",
    "href": "src/01/slides.html#anatomy-of-a-python-notebook",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Anatomy of a Python Notebook",
    "text": "Anatomy of a Python Notebook\n\nFormat: Extension is .ipynb\n\nJSON format, using Jupyter Document Schema\n\nCells: Building blocks of notebooks\n\nCode cells: Executable Python code\nMarkdown cells: Text, headings, images, equations\n\nKernel: The computational engine running your code\n\nMaintains state between cell executions\n\nOutputs: Results appear directly below code cells\n\nText, tables, plots, interactive widgets",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-to-run-notebooks",
    "href": "src/01/slides.html#how-to-run-notebooks",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How to Run Notebooks",
    "text": "How to Run Notebooks\n\nJupyter Notebook Server (Classic approach)\n\nWeb interface on localhost\n\nVS Code (Local development)\n\nJupyter extension for VS Code\nRun on your own machine\n\nGoogle Colab (Recommended)\n\nBrowser-based, no installation needed\nFree(-ish) GPU access\nCan also access local GPU",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#why-recommend-google-colab",
    "href": "src/01/slides.html#why-recommend-google-colab",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Why Recommend Google Colab?",
    "text": "Why Recommend Google Colab?\n\nAccess to GPUs and TPUs for AI-based tasks\n\ne.g., A100 and H100 with 40Gb/80Gb VRAM\n\nModel downloaded between cloud vendors\n\nvs.downloading large models via the DigiPen network\n\nMany libraries pre-installed\nEasy to share notebooks with others\nGenerous (free) GPU limits for students!",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#challenges-with-word-embeddings-1",
    "href": "src/01/slides.html#challenges-with-word-embeddings-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Challenges with Word Embeddings",
    "text": "Challenges with Word Embeddings\n\nLarge vocabularies\n\n100K+ words\nAnd not particularly friendly to non-English vocabularies\n\nLittle representation between certain words\n\nRun and Running should be related\n\nLack of context\n\nEmbedding for the word bank is the same, regardless of context\nRiver bank != Savings bank",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#challenges-with-word-embeddings-2",
    "href": "src/01/slides.html#challenges-with-word-embeddings-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Challenges with Word Embeddings",
    "text": "Challenges with Word Embeddings\n\nSome researchers tried character-level models\n\nSmall vocabulary (26 letters + punctuation for English)\nBut very long sequences\nAnd hard to extract meaning",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#byte-pair-encoding-bpe",
    "href": "src/01/slides.html#byte-pair-encoding-bpe",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2016: Byte Pair Encoding (BPE)",
    "text": "2016: Byte Pair Encoding (BPE)\n\nOriginally developed in 1994 as a simple compression algorithm (Gage 1994)\n\nFrequent pairs of adjacent bytes represented as a single byte\n\nIn 2016, adapted to neural machine translation (Sennrich, Haddow, and Birch 2016)\n\nApplied BPE to break words into subword units for better handling of rare words",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#byte-pair-encoding-bpe-1",
    "href": "src/01/slides.html#byte-pair-encoding-bpe-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2016: Byte Pair Encoding (BPE)",
    "text": "2016: Byte Pair Encoding (BPE)\n\nBreaks words into frequent subword units (a.k.a. tokens)\n\nunbelievable  [un, believ, able]\n\nBalance between word level (large vocab) and character level (long sequences)\n\nSupports related words: [Run] and [Run, ning]\n30-50K tokens vs.100K, and works well for non-English languages\n\nFoundations of todays tokenization\n\nAPI costs are measured in tokens\nDifferent models use different tokenizers",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#search-for-context",
    "href": "src/01/slides.html#search-for-context",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Search for Context",
    "text": "Search for Context\n\nBPE provided efficiency and representation between words\nBut still didnt solve context\n\ne.g., the River bank != Savings bank problem\n\nResearchers working on attention tasks using Recurrent Neural Networks (RNNs)\n\nBahdanau et al.introduce attention for translation (Bahdanau, Cho, and Bengio 2015)\nShowed that focusing on relevant parts of input improved translation quality",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#attention-is-all-you-need",
    "href": "src/01/slides.html#attention-is-all-you-need",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2017: Attention is all you need",
    "text": "2017: Attention is all you need\n\nGoogle researchers publish Attention is all you need (Vaswani et al. 2017)\n\nIntroduced the Transformer a novel Neural Network (NN) architecture, eliminating the need for RNNs for sequence-to-sequence models\nUsed BPE tokenization, and creates contextual embeddings during training process\nAttention mechanism allows the model to weigh the importance of words in a sequence\nAchieved State Of The Art (SOTA) performance on language translation, while also being faster to train",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-1",
    "href": "src/01/slides.html#introducing-the-transformer-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    Transformer[Transformer]\n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Transformer --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example",
    "href": "src/01/slides.html#example",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"Helsinki-NLP/opus-mt-fr-en\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-1",
    "href": "src/01/slides.html#example-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nfrench_text = \"Bonjour, comment allez-vous?\"\ninput_ids = tokenizer.encode(french_text, return_tensors=\"pt\")\nprint(input_ids[0])\nprint(\"Tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n\ntensor([8703,    2, 1027, 5682,   21,  682,   54,    0])\nTokens: ['Bonjour', ',', 'comment', 'allez', '-', 'vous', '?', '&lt;/s&gt;']",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-2",
    "href": "src/01/slides.html#example-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\noutput_ids = model.generate(input_ids)\nprint(output_ids)\n\ntensor([[59513, 10537,     2,   541,    52,    55,    54,     0]])",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-3",
    "href": "src/01/slides.html#example-3",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nenglish_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(\"Translation:\", english_text)\n\nTranslation: Hello, how are you?",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-2",
    "href": "src/01/slides.html#introducing-the-transformer-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    Transformer[Transformer]\n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Transformer --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-3",
    "href": "src/01/slides.html#introducing-the-transformer-3",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        Encoder[Encoder]\n        Decoder[Decoder]\n        Encoder --&gt; Decoder\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Encoder\n    Decoder --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-4",
    "href": "src/01/slides.html#introducing-the-transformer-4",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        direction LR\n        subgraph \"Encoder Stack (N layers)\"\n            E[Encoder&lt;br/&gt;Layers&lt;br/&gt;1...N]\n        end\n        \n        subgraph \"Decoder Stack (N layers)\"\n            D[Decoder&lt;br/&gt;Layers&lt;br/&gt;1...N]\n        end\n        \n        E -.-&gt;|Context| D\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; E\n    D --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-the-encoderdecoder-work",
    "href": "src/01/slides.html#how-does-the-encoderdecoder-work",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How Does the Encoder/Decoder Work?",
    "text": "How Does the Encoder/Decoder Work?\noutput_ids = model.generate(input_ids)\n\nTakes input ids, runs through encoder\n\nGenerates contextual vectors using self attention across input tokens\n\nRuns the decoder iteratively to generate one token at a time\n\nUses self attention on previously generated tokens\nUses cross-attention to attend to encoder output\n\nContinues until it generates an end-of-sequence token or hits max length",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-5",
    "href": "src/01/slides.html#introducing-the-transformer-5",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        direction TB\n        \n        subgraph \"Encoder Layer\"\n            direction TB\n            E_SelfAttn[Multi-Head&lt;br/&gt;Self-Attention]\n            E_AddNorm1[Add & Norm]\n            E_FFN[Feed-Forward&lt;br/&gt;Network]\n            E_AddNorm2[Add & Norm]\n            \n            E_SelfAttn --&gt; E_AddNorm1 --&gt; E_FFN --&gt; E_AddNorm2\n        end\n        \n        subgraph \"Decoder Layer\"\n            direction TB\n            D_SelfAttn[Masked Multi-Head&lt;br/&gt;Self-Attention]\n            D_AddNorm1[Add & Norm]\n            D_CrossAttn[Multi-Head&lt;br/&gt;Cross-Attention]\n            D_AddNorm2[Add & Norm]\n            D_FFN[Feed-Forward&lt;br/&gt;Network]\n            D_AddNorm3[Add & Norm]\n            \n            D_SelfAttn --&gt; D_AddNorm1 --&gt; D_CrossAttn --&gt; D_AddNorm2 --&gt; D_FFN --&gt; D_AddNorm3\n        end\n        \n        E_AddNorm2 -.-&gt;|Encoder&lt;br/&gt;Output| D_CrossAttn\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; E_SelfAttn\n    D_AddNorm3 --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#origin-of-gpt-1",
    "href": "src/01/slides.html#origin-of-gpt-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2018: Origin of GPT",
    "text": "2018: Origin of GPT\n\nGenerative Pre-trained Transformer\nName coined by OpenAI researchers in Improving Language Understanding by Generative Pre-Training (Radford et al. 2018)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-gpt",
    "href": "src/01/slides.html#what-is-a-gpt",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is a GPT?",
    "text": "What is a GPT?\n\nDecoder-only architecture\n\nSelf attention is causal/masked - tokens can only attend to previous tokens, not future ones\n\nPre-training objective: Next token prediction\n\nTrained on a massive text corpora\nLearns grammar, facts, reasoning patterns just from this objective",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-gpt-1",
    "href": "src/01/slides.html#what-is-a-gpt-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is a GPT?",
    "text": "What is a GPT?\n\nAutoregressive generation\n\nGenerates one token at a time, feeding back each output as input\nTemperature and sampling strategies\nSame prompt can produce different outputs\n\nContext window\n\nFixed maximum length (2048 for GPT-2)\nEverything must fit within this window during generation\nIntroduced the concept of context vs.knowledge (prompt vs.training)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#gpt-2",
    "href": "src/01/slides.html#gpt-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "GPT-2",
    "text": "GPT-2\n\nReleased in 2019 by OpenAI\n\nInitially, only 117M param model released in Feb 2019 due to safety concerns\nStaged releases throughout the year, 1.5B in Nov 2019\n\nTrained on WebText, 8 million web pages/40GB of text\nZero-shot task performance\n\nDid well on translation, summarization, and question answering without task-specific training",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-4",
    "href": "src/01/slides.html#example-4",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load pre-trained GPT-2 model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n# Set pad token\ntokenizer.pad_token = tokenizer.eos_token",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-5",
    "href": "src/01/slides.html#example-5",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nimport torch\n\ndef autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n    # Encode the prompt with attention mask\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    # Generate continuation\n    with torch.no_grad():\n        output = model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#temperature-top_k-and-top_p",
    "href": "src/01/slides.html#temperature-top_k-and-top_p",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Temperature, top_k, and top_p",
    "text": "Temperature, top_k, and top_p\n\nTemperature (0.0 - 1.0)\n\nLower for accuracy, factual summaries, etc.\nHigher for more creative, diverse ideas\n\ntop_k (top k tokens)\n\nNarrow the next tokens to the top k (ordered by probability)\n\ntop_p (cumulative probability)\n\nOnly return the top tokens whose culumative probability &lt; top_p",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-6",
    "href": "src/01/slides.html#example-6",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nprompt = \"Mary had a little lamb\"\ncompletion = autocomplete(prompt, max_length=80)\nprint(completion)\n\nMary had a little lamb, and the young woman asked her for a little lamb, and they gave it to her.\n\n\"Oh, my child, it is good to have a little lamb,\" said he, \"but it is not to be bought, for it is hard to make, and it is much more difficult to make.\n\n\"When you have a little lamb, it",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#limitations-of-gpt-2-1",
    "href": "src/01/slides.html#limitations-of-gpt-2-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Limitations of GPT-2",
    "text": "Limitations of GPT-2\n\nHallucinations / factual errors\nNo real-world grounding\nRepetition issues\nOnwards to GPT-3 and beyond",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#looking-ahead-1",
    "href": "src/01/slides.html#looking-ahead-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis weeks assignment!\nNew to Python?\n\nLearn Python with Jupyter\n\nInstruction-tuned models\nOpenAI specification\nGradio for chat-based UIs",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#references-1",
    "href": "src/01/slides.html#references-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "References",
    "text": "References\n\n\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Representations. https://arxiv.org/abs/1409.0473.\n\n\nGage, Philip. 1994. A New Algorithm for Data Compression. The C Users Journal 12 (2): 2338.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. In International Conference on Learning Representations. https://arxiv.org/abs/1301.3781.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and Their Compositionality. In Advances in Neural Information Processing Systems, 311119. https://arxiv.org/abs/1310.4546.\n\n\nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving Language Understanding by Generative Pre-Training. https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf.\n\n\nSennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 171525. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1162.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In Advances in Neural Information Processing Systems. Vol. 30.",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/resources.html",
    "href": "src/02/resources.html",
    "title": "Resources",
    "section": "",
    "text": "GPT-2 Release Blog Post - OpenAIs original GPT-2 announcement\nGPT-3 Paper - Language Models are Few-Shot Learners\nInstructGPT Paper - Training language models to follow instructions with human feedback\nMetas Llama Models - Official Llama model family page",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#model-history-and-background",
    "href": "src/02/resources.html#model-history-and-background",
    "title": "Resources",
    "section": "",
    "text": "GPT-2 Release Blog Post - OpenAIs original GPT-2 announcement\nGPT-3 Paper - Language Models are Few-Shot Learners\nInstructGPT Paper - Training language models to follow instructions with human feedback\nMetas Llama Models - Official Llama model family page",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#chat-templates-and-model-formats",
    "href": "src/02/resources.html#chat-templates-and-model-formats",
    "title": "Resources",
    "section": "Chat Templates and Model Formats",
    "text": "Chat Templates and Model Formats\n\nHugging Face Chat Templates Guide - Comprehensive guide to chat templates",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#openai-api",
    "href": "src/02/resources.html#openai-api",
    "title": "Resources",
    "section": "OpenAI API",
    "text": "OpenAI API\n\nOpenAI Platform - Create an account and get API keys\nOpenAI API Documentation - Official API reference\nChat Completions Guide - How to use the chat completions endpoint\nStructured Outputs Guide - Guide to structured outputs",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#openrouter",
    "href": "src/02/resources.html#openrouter",
    "title": "Resources",
    "section": "OpenRouter",
    "text": "OpenRouter\n\nOpenRouter Home Page - Unified API for hundreds of models\nOpenRouter Documentation - API docs and model listings\nOpenRouter Models - Browse available models and pricing",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#server-sent-events-sse-and-streaming",
    "href": "src/02/resources.html#server-sent-events-sse-and-streaming",
    "title": "Resources",
    "section": "Server-Sent Events (SSE) and Streaming",
    "text": "Server-Sent Events (SSE) and Streaming\n\nMDN: Server-Sent Events - Technical overview of SSE\nEventSource API - Browser API for SSE\nOpenAI Streaming Guide - How to implement streaming with OpenAI API",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#gradio",
    "href": "src/02/resources.html#gradio",
    "title": "Resources",
    "section": "Gradio",
    "text": "Gradio\n\nGradio Home Page\nGradio Documentation - Official documentation\nGradio ChatInterface - Chat interface component documentation\nGradio Guides - Tutorials and examples\nHugging Face Spaces - Platform for deploying Gradio apps",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#rlhf-and-model-training",
    "href": "src/02/resources.html#rlhf-and-model-training",
    "title": "Resources",
    "section": "RLHF and Model Training",
    "text": "RLHF and Model Training\n\nRLHF Explainer - Hugging Faces comprehensive guide to RLHF\nReinforcement Learning from Human Feedback Paper - Original RLHF research",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#context-windows-and-token-management",
    "href": "src/02/resources.html#context-windows-and-token-management",
    "title": "Resources",
    "section": "Context Windows and Token Management",
    "text": "Context Windows and Token Management\n\nUnderstanding Context Windows - Anthropics post on long context\nToken Counting Best Practices - OpenAI cookbook example\nTiktoken - Token counting library for estimating costs",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#citations",
    "href": "src/02/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/assignment.html",
    "href": "src/03/assignment.html",
    "title": "Module 3 Assignment: Extend the Campus Agent",
    "section": "",
    "text": "Objective: Extend the DigiPen Campus Agent by adding a new specialized agent that handles a specific domain.\nBackground:\nThe Campus Agent currently has four specialized agents: Building Agent, Course Agent, Handbook Agent, and Cafe Agent. Your task is to add a fifth agent that serves a new purpose on campus.\nSuggested Agent Ideas (pick one, or create your own):\n\nEvents Agent - Helps students find information about campus events, club meetings, and activities\nIT Agent - Assists with common IT issues, lab software, and printing\nLibrary Agent - Helps students find resources, reserve study rooms, or check hours\nCareer Services Agent - Provides information about internships, resume reviews, and career fairs\nTransportation Agent - Helps with parking, shuttle schedules, and commute options\n\nRequirements:\n\nCreate a new agent with appropriate instructions (system prompt)\nImplement knowledge retrieval using one (or more!) of these approaches:\n\nAdd documents to the vector store and use FileSearchTool, OR\nUse the WebSearchTool to give your agent access to the web, OR\nCreate a custom tool using @function_tool that returns relevant data (should be more complex than the cafe example) OR\nCreate a new (or select an existing) MCP server\n\nIntegrate with the main Campus Agent via handoff\nTest your agent with at least 3 different queries and document the results\nUse the OpenAI Traces Dashboard to debug at least one interaction and include a screenshot or description of what you observed\n\nDeliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation (building on the campus-agent.ipynb starter)\nUses OPENAI_API_KEY for the API token. (Please do not include your API key in your notebook!)\nMarkdown cells explaining:\n\nWhat agent you chose and why\nHow you implemented knowledge retrieval (vector store vs.custom tool)\nYour test queries and the agents responses\nWhat you learned from the Traces Dashboard (what worked, what didnt, any debugging insights)\n\n\nBonus:\nDeploy your extended Campus Agent to Hugging Face Spaces and include the URL in your submission. Remember to add your OPENAI_API_KEY as a secret in your Space settings (not in the code!).\nHints:\n\nStart by copying the campus-agent.ipynb notebook and modifying it\nKeep your agents scope focused. A narrower domain with good data works better than a broad domain with sparse data\nIf using the vector store, you can upload files via the OpenAI Platform Storage\nIf using a custom tool, remember that the functions docstring helps the LLM understand when to call it",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Assignment"
    ]
  },
  {
    "objectID": "src/03/slides.html#recap",
    "href": "src/03/slides.html#recap",
    "title": "Module 3: Agents and Tools",
    "section": "Recap",
    "text": "Recap\n\nUnderstood the evolution and licensing of models from GPT-2 through to modern day\nUnderstood instruction-tuned models, how they work, and how to configure\nSetup and used OpenRouter for accessing hosted models\nUnderstood the OpenAI API specification, the request/response payload, parameters, streaming, and structured output\nCreated and shared a chatbot using a Gradio-based UI",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#lesson-objectives",
    "href": "src/03/slides.html#lesson-objectives",
    "title": "Module 3: Agents and Tools",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nDescribe the fundamental concepts behind Agents/Agentic AI\nExplore and provide feedback on an existing multi-agent setup\nUnderstand available agent SDKs, how they differ, and advantages/disadvantages\nUse the OpenAI Agents SDK to build a multi-agent system, including document indexing and retrieval\nUnderstand and implement tool calls and implement using OpenAIs function calling and via MCP",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-agents-1",
    "href": "src/03/slides.html#why-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Why Agents?",
    "text": "Why Agents?\n\nLimitations of our prior chatbots\n\nNeeds constant human input every turn; No ability to plan beyond a single interaction\nSingle model with single context (conversation)\nNo ability to interact with external systems",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent",
    "href": "src/03/slides.html#what-is-an-agent",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.youtube.com/watch?v=bwXaJXgezf4",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-1",
    "href": "src/03/slides.html#what-is-an-agent-1",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.weforum.org/stories/2025/06/cognitive-enterprise-agentic-business-revolution/",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-2",
    "href": "src/03/slides.html#what-is-an-agent-2",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.crn.com/news/ai/2025/10-hottest-agentic-ai-tools-and-agents-of-2025-so-far",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-3",
    "href": "src/03/slides.html#what-is-an-agent-3",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-4",
    "href": "src/03/slides.html#what-is-an-agent-4",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\nImagine a DigiPen Campus Assistant: An AI agent that can help you navigate anything and everything at DigiPen!\n\nWhere can I find the Hopper room?\nCan you tell me more about FLM201?\nOh, and whats todays vegetarian option at the Bytes Cafe?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents",
    "href": "src/03/slides.html#five-characteristics-of-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Planners\n\n\nAgents are driven by goals\nAnd they can put together a plan for the steps to complete that goal.\n\nFirst, I will discover where course information is located\nThen I will search for any courses that reference FLM201\nThen I summarize all of the key points for the student",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-1",
    "href": "src/03/slides.html#five-characteristics-of-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Autonomous\n\n\nAgents can then go off and execute the plan, independent of human input\nThe concept of human in the loop still applies for confirmation\n\ne.g.Do you really want to place this order at the Bytes Cafe?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-2",
    "href": "src/03/slides.html#five-characteristics-of-agents-2",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Reactive\n\n\nAgents can change mid-course depending on what they find and/or the environment.\n\ne.g.I couldnt find any course information on FLM201. Im going to check if there are other 200-level FLM courses before responding to the student.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-3",
    "href": "src/03/slides.html#five-characteristics-of-agents-3",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents have Persistence\n\n\nAgents often have memory systems beyond the current conversation\nBroadly classified as short and long-term memory\n\nShort-term memory could be your order request at the Bytes cafe\nLong-term memory could be your food preferences",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-4",
    "href": "src/03/slides.html#five-characteristics-of-agents-4",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents can Interact with external systems\n\n\nAgents can delegate to other agents for complex tasks\n\n(Or for tasks where other agents are better suited for.)\ne.g., Campus Agent -&gt; delegating to a Course Agent\n\nAgents can also be given access to external tools\n\ne.g., File search, Web search, access to the Bytes Cafe API",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-1",
    "href": "src/03/slides.html#openai-agents-sdk-1",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK",
    "text": "OpenAI Agents SDK\n\nAnnounced in Mar 2025\n\nTogether with web search, file search, and computer use\nAnd a new Responses API (formerly Assistants API)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-2",
    "href": "src/03/slides.html#openai-agents-sdk-2",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK",
    "text": "OpenAI Agents SDK\n\nCreated to address the gap between chat completions (what we were using last week) and multi-step systems\n\nvs.building your own, which a lot of developers were doing at the time\n\nIntegrates function calling, handoffs, and session management in the same package\nSupports Python and TypeScript; MIT licensed",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#not-the-only-agent-sdk-in-town",
    "href": "src/03/slides.html#not-the-only-agent-sdk-in-town",
    "title": "Module 3: Agents and Tools",
    "section": "Not the only Agent SDK in town!",
    "text": "Not the only Agent SDK in town!\n\nSource: https://e2b.dev",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#langgraph",
    "href": "src/03/slides.html#langgraph",
    "title": "Module 3: Agents and Tools",
    "section": "LangGraph",
    "text": "LangGraph\n\nhttps://langchain-ai.github.io/langgraph/\nPython only\nMIT License\nOne of the first agent frameworks, building on LangChain\n\nIMO, too abstract/complex/bloated",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#crew.ai",
    "href": "src/03/slides.html#crew.ai",
    "title": "Module 3: Agents and Tools",
    "section": "Crew.ai",
    "text": "Crew.ai\n\nhttps://github.com/crewaiinc/crewai\nPython only\nOne of the more popular commercial offerings\n\n(Although they do have an MIT License/freemium model)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#microsoft",
    "href": "src/03/slides.html#microsoft",
    "title": "Module 3: Agents and Tools",
    "section": "Microsoft",
    "text": "Microsoft\n\nAutoGen\n\nhttps://microsoft.github.io/autogen/stable/\nPython (.NET coming soon)\nMIT License\n\nMicrosoft Semantic Kernel\n\nhttps://github.com/microsoft/semantic-kernel\nPython, .NET, Java\nMIT License",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#microsoft-1",
    "href": "src/03/slides.html#microsoft-1",
    "title": "Module 3: Agents and Tools",
    "section": "Microsoft",
    "text": "Microsoft\n\nNow converging into the Microsoft Agent Framework\nOne of the few agent SDKs to support .NET\nSpeaking of which, why are most SDKs in Python?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#agent-structure",
    "href": "src/03/slides.html#agent-structure",
    "title": "Module 3: Agents and Tools",
    "section": "Agent Structure",
    "text": "Agent Structure\n\nAgents and tools for the DigiPen Campus Agent",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#campus-agent",
    "href": "src/03/slides.html#campus-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Campus Agent",
    "text": "Campus Agent\n\n\nagent = Agent(\n    name=\"DigiPen Campus Agent\",\n    instructions=\"You are a helpful campus agent that can plan and execute tasks for students at DigiPen. Please be concise and accurate in handing off tasks to other agents as needed.\",\n    handoffs=[building_agent, course_agent, handbook_agent, cafe_agent],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#building-agent",
    "href": "src/03/slides.html#building-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Building Agent",
    "text": "Building Agent\n\n\nbuilding_agent = Agent(\n    name=\"Building Agent\",\n    instructions=\"You help students locate and provide information about buildings and rooms on campus. Be descriptive when giving locations.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=3,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#course-agent",
    "href": "src/03/slides.html#course-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Course Agent",
    "text": "Course Agent\n\n\ncourse_agent = Agent(\n    name=\"Course Agent\",\n    instructions=\"You help students find out information about courses held at DigiPen.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#handbook-agent",
    "href": "src/03/slides.html#handbook-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Handbook Agent",
    "text": "Handbook Agent\n\n\nhandbook_agent = Agent(\n    name=\"Handbook Agent\",\n    instructions=\"You help students navigate the school handbook, providing information about campus policies and student conduct.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store",
    "href": "src/03/slides.html#whats-a-vector-store",
    "title": "Module 3: Agents and Tools",
    "section": "Whats a Vector Store?",
    "text": "Whats a Vector Store?\n\nA way to provide domain-specific knowledge beyond training data\n\ne.g., current semester course information that is newer than GPT-5.2s cutoff date\n\nWe could just insert these into the context window\n\nBut doesnt scale to more than a few documents",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store-1",
    "href": "src/03/slides.html#whats-a-vector-store-1",
    "title": "Module 3: Agents and Tools",
    "section": "Whats a Vector Store?",
    "text": "Whats a Vector Store?\n\nInstead, we use a vector store\n\nConverts documents, paragraphs, or sentences into vector embeddings\n(Remember these from module 1? :)\n\nSimilar concepts are close to each other in vector space\n\nWhich makes it efficient to query\n\nQueries return the document (or pages within a document) that match\n\ne.g., Michelangelo returns Page 1 of the floor map",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store-2",
    "href": "src/03/slides.html#whats-a-vector-store-2",
    "title": "Module 3: Agents and Tools",
    "section": "Whats a Vector Store?",
    "text": "Whats a Vector Store?\n\nFoundation for RAG (Retrieval Augmented Generation)\n\n(Which we will cover in module 6)\n\nMany different types of vectors stores/databases\nFor now, we will be using OpenAIs storage",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store-3",
    "href": "src/03/slides.html#whats-a-vector-store-3",
    "title": "Module 3: Agents and Tools",
    "section": "Whats a Vector Store?",
    "text": "Whats a Vector Store?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#agent-structure-1",
    "href": "src/03/slides.html#agent-structure-1",
    "title": "Module 3: Agents and Tools",
    "section": "Agent Structure",
    "text": "Agent Structure\n\nAgents and tools for the DigiPen Campus Agent",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-do-agents-need-tools",
    "href": "src/03/slides.html#why-do-agents-need-tools",
    "title": "Module 3: Agents and Tools",
    "section": "Why Do Agents Need Tools?",
    "text": "Why Do Agents Need Tools?\n\nThe scope of the agents ability is contained within the model\nTools enable the agent to reach out to systems beyond the model\nExamples\n\nRead a file from disk or search the web (built in)\nCalculator (because LLMs arent great at math)\nCode interpreter (running code on the fly)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-tool-calling",
    "href": "src/03/slides.html#openai-tool-calling",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Tool Calling",
    "text": "OpenAI Tool Calling\n\nIntroduced by OpenAI in June 2023\nOriginally called Function Calling\nModels are fine-tuned to return a structured function_call JSON object, specifying which function to call and with what arguments.\nTools are provided as functions\nOption for the LLM to decide when to call the tool (always, never, auto)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#cafe-agent",
    "href": "src/03/slides.html#cafe-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Cafe Agent",
    "text": "Cafe Agent\n\n\ncafe_agent = Agent(\n    name=\"Cafe Agent\",\n    instructions=\"You help students locate and provide information about the Bytes Cafe.\",\n    tools=[\n        get_bytes_cafe_menu,\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#cafe-agent-tool",
    "href": "src/03/slides.html#cafe-agent-tool",
    "title": "Module 3: Agents and Tools",
    "section": "Cafe Agent Tool",
    "text": "Cafe Agent Tool\n\n\nfrom agents import function_tool\n\n@function_tool\ndef get_bytes_cafe_menu(date: str) -&gt; any:\n    return {\n        f\"{date}\": {\n            \"daily byte\": {\n                \"name\": \"Steak Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Flank steak, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"vegetarian\": {\n                \"name\": \"Impossible Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Impossible plant based product, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"international\": {\n                \"name\": \"Chicken Curry\",\n                \"price\": 12,\n                \"description\": \"Chicken thighs, onion, carrot, potato, curry sauce served over rice\",\n            },\n        }\n    }",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n\nHow do models know when they should call a tool?\n\nModels are fine-tuned on conversations with tool call examples\nThe model learns patterns like when the user asks about the weather, call the get_weather tool\nThe request to call the tool is returned as a JSON payload",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-1",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-1",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n{\n  \"role\": \"assistant\",\n  \"content\": null,\n  \"tool_calls\": [\n    {\n      \"id\": \"call_abc123\",\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_weather\",\n        \"arguments\": \"{\\\"location\\\": \\\"San Francisco\\\", \\\"unit\\\": \\\"celsius\\\"}\"\n      }\n    }\n  ]\n}",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-2",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-2",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n\nThe client then calls the tool with the required parameters\nAnd returns the result back to the model as a tool role message",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-3",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-3",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n{\n  \"role\": \"tool\",\n  \"tool_call_id\": \"call_abc123\",\n  \"content\": \"{\\\"temperature\\\": 18, \\\"condition\\\": \\\"partly cloudy\\\", \\\"humidity\\\": 65, \\\"wind_speed\\\": 12}\"\n}",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-4",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-4",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n\nRLHF is used to improve the accuracy for tool selection\n\nRewards are given for correctly choosing the right tool for a task\nOr penalized for hallucinating tools and methods that dont exist",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#lets-run-the-campus-agent-1",
    "href": "src/03/slides.html#lets-run-the-campus-agent-1",
    "title": "Module 3: Agents and Tools",
    "section": "Lets Run the Campus Agent",
    "text": "Lets Run the Campus Agent\n\nDoes the OpenAI Agents SDK work with OpenRouter?\n\nYes and No :)\n\nYes to core functionality\n\nCreating an agent, handoffs, calling custom tools\n\nNo to calling built-in OpenAI tools\n\nFile search, Web search, Code interpreter",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#lets-run-the-campus-agent-2",
    "href": "src/03/slides.html#lets-run-the-campus-agent-2",
    "title": "Module 3: Agents and Tools",
    "section": "Lets Run the Campus Agent",
    "text": "Lets Run the Campus Agent\n\nWell need to create an OpenAI developer account\nPotentially add some credits to it",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-multiple-agents",
    "href": "src/03/slides.html#why-multiple-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Why Multiple Agents?",
    "text": "Why Multiple Agents?\n\nContext window limitations\nEach agent can have a different system prompt (instructions)\nMakes tool separation cleaner and more accurate\nEach agent can have a different underlying model\n\nSpecialized models (e.g., a vision encoder)\nOr to blend cost",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-multiple-agents-1",
    "href": "src/03/slides.html#why-multiple-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Why Multiple Agents?",
    "text": "Why Multiple Agents?\n\nCost considerations are really important with agents\n\nLarge number of tokens for reasoning\nHandoffs with multiple API calls\nEven more tokens with verbose tool calls\n\nMitigations\n\nDoes every agent need full GPT / frontier model capability?\nAgents doing primarily function calling (e.g., file search) can be much smaller/cheaper",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#example-of-multiple-agents",
    "href": "src/03/slides.html#example-of-multiple-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Example of Multiple Agents",
    "text": "Example of Multiple Agents\n\nCode generation\n\nAgents for architect, code writer, tester, debugger, etc.\n\nContent generation\n\nAgent to create content, other agents to generate images, translate content, etc.\n\nTravel booking\n\nAgent to book flights, hotels, cars, etc. for packages",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#patterns-for-agents",
    "href": "src/03/slides.html#patterns-for-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Patterns for Agents",
    "text": "Patterns for Agents\n\nAs you get deeper into building agents, patterns start to emerge\n\nRouter (which is what we used in our demo) - hand off of tasks\nOrchestrator (using other agents as tools)\nParallel agents (calling other agents in parallel and aggregating results)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#patterns-for-agents-1",
    "href": "src/03/slides.html#patterns-for-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Patterns for Agents",
    "text": "Patterns for Agents\n\nSource: https://www.anthropic.com/engineering/building-effective-agents",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#when-things-go-wrong-1",
    "href": "src/03/slides.html#when-things-go-wrong-1",
    "title": "Module 3: Agents and Tools",
    "section": "When Things Go Wrong",
    "text": "When Things Go Wrong\n\nAgents can be difficult to debug\n\nIncorrect handoffs\nInfinite loops can be common\nFailed to call the right tool at the right time",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#when-things-go-wrong-2",
    "href": "src/03/slides.html#when-things-go-wrong-2",
    "title": "Module 3: Agents and Tools",
    "section": "When Things Go Wrong",
    "text": "When Things Go Wrong\n\nOpenAI Agents SDK includes built-in tracing\n\nActually enabled by default!\n\nComprehensive record of:\n\ngenerations\ntool calls\nhandoffs\nguardrails\ncustom events",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#the-need-for-memory",
    "href": "src/03/slides.html#the-need-for-memory",
    "title": "Module 3: Agents and Tools",
    "section": "The Need for Memory",
    "text": "The Need for Memory\n\nJust like API calls, agents need the conversation/context every call\nThis can be challenging with agents working on long-running tasks\n\nAnd/or agents working on multiple threads with other agents\n\nShort-term and long-term memory",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory",
    "href": "src/03/slides.html#short-term-memory",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\nUsed to store/retreive the current conversation thread\nBuilt-in to most SDKs\nIn OpenAI Agents SDK called a session",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-1",
    "href": "src/03/slides.html#short-term-memory-1",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nfrom agents import Agent, Runner, SQLiteSession\n\nagent = Agent(name=\"Assistant\", instructions=\"Reply very concisely\")\nsession = SQLiteSession(\"conv_123\", db_path=SQLITE_DB)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-2",
    "href": "src/03/slides.html#short-term-memory-2",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nresult = await Runner.run(agent, \"My name is Simon\", session=session)\nprint(result.final_output)\n\nNice to meet you, Simon!",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-3",
    "href": "src/03/slides.html#short-term-memory-3",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nresult = await Runner.run(agent, \"What is my name?\", session=session)\nprint(result.final_output)\n\nYour name is Simon.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-4",
    "href": "src/03/slides.html#short-term-memory-4",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nresult = await Runner.run(agent, \"What is my name?\")\nprint(result.final_output)\n\nYou havent shared your name yet.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#long-term-memory",
    "href": "src/03/slides.html#long-term-memory",
    "title": "Module 3: Agents and Tools",
    "section": "Long-term Memory",
    "text": "Long-term Memory\n\nMore challenging\nYou dont want to store/retrieve the entire conversation\nLong-term memory types\n\nFactual: General facts (e.g., name, address, seating preferences)\nEpisodic: Past conversations (e.g., user booked a trip to Paris)\nProcedural: Learnings (e.g., the best hotel site to book accommodation)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#implementing-long-term-memory",
    "href": "src/03/slides.html#implementing-long-term-memory",
    "title": "Module 3: Agents and Tools",
    "section": "Implementing Long-term Memory",
    "text": "Implementing Long-term Memory\n\nLots of startup options!\n\nSupermemory\nLetta\nmem0\nand lots more",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#implementing-long-term-memory-mem0",
    "href": "src/03/slides.html#implementing-long-term-memory-mem0",
    "title": "Module 3: Agents and Tools",
    "section": "Implementing Long-term Memory (mem0)",
    "text": "Implementing Long-term Memory (mem0)\nfrom mem0 import Memory\nmemory = Memory()\n\n# Create new memories from the conversation\nmessages.append({\"role\": \"assistant\", \"content\": assistant_response})\nmemory.add(messages, user_id=user_id)\n\n# Retrieve relevant memories\nrelevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n# (append these to the system prompt)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#implementing-long-term-memory-1",
    "href": "src/03/slides.html#implementing-long-term-memory-1",
    "title": "Module 3: Agents and Tools",
    "section": "Implementing Long-term Memory",
    "text": "Implementing Long-term Memory\n\nOr roll your own\nLong-term memory types\n\nFactual, Episodic, Procedural\n\nCreate tools for factual storage\n\ne.g., a profile tool with set/get options\n\nUse LLM to summarize short-term session conversations to store episodic and procedural learnings.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#beyond-tool-calling-1",
    "href": "src/03/slides.html#beyond-tool-calling-1",
    "title": "Module 3: Agents and Tools",
    "section": "Beyond Tool Calling",
    "text": "Beyond Tool Calling\n\nTool calling is super useful, but\n\nYou need to write the function(s) yourself\nAnd then expose them to OpenAI using the @function_tool method\n\nWhat if there was a way to standardize this?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol",
    "href": "src/03/slides.html#mcp-model-context-protocol",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)\n\nReleased by Anthropic in Nov 2024\nProvides a standard interface for tools - akin to a USB standard for peripherals\nImplementations are known as MCP servers\n\nA server exposes one or more tools (functions)\nUses JSON-RPC 2.0 as underlying RPC protocol\nServers can run remotely over HTTP (supports SSE)\nOr can be hosted locally and accessed via stdio\nMany servers hosted using Node.js",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol-1",
    "href": "src/03/slides.html#mcp-model-context-protocol-1",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol-2",
    "href": "src/03/slides.html#mcp-model-context-protocol-2",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol-3",
    "href": "src/03/slides.html#mcp-model-context-protocol-3",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-and-mcp-1",
    "href": "src/03/slides.html#openai-agents-sdk-and-mcp-1",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK and MCP",
    "text": "OpenAI Agents SDK and MCP\n\nMCP supported in OpenAI Agents SDK (as of Sep 2025)\nExposes MCPServerStdio and MCPServerSse to connect to local and remote servers",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-and-mcp-2",
    "href": "src/03/slides.html#openai-agents-sdk-and-mcp-2",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK and MCP",
    "text": "OpenAI Agents SDK and MCP\n\n\ntry:\n  async with MCPServerStreamableHttp(\n      params = {\"url\": \"http://localhost:3000/mcp\"}\n      ) as server:\n    tools = await server.list_tools()\n    print(f\"Available tools: {[tool.name for tool in tools]}\")\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nAvailable tools: ['weather_forecast', 'weather_archive', 'air_quality', 'marine_weather', 'elevation', 'flood_forecast', 'seasonal_forecast', 'climate_projection', 'ensemble_forecast', 'geocoding', 'dwd_icon_forecast', 'gfs_forecast', 'meteofrance_forecast', 'ecmwf_forecast', 'jma_forecast', 'metno_forecast', 'gem_forecast']",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-and-mcp-3",
    "href": "src/03/slides.html#openai-agents-sdk-and-mcp-3",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK and MCP",
    "text": "OpenAI Agents SDK and MCP\n\n\ntry:\n  async with MCPServerStreamableHttp(\n    params = {\"url\": \"http://localhost:3000/mcp\"}\n    ) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n    result = await Runner.run(agent, \"What's the weather forecast for MinneapolisSt. Paul this week?\")\n    print(result.final_output)\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nMinneapolisSt. Paul (Twin Cities) forecast for the next 7 days (America/Chicago):\n\n- **Fri Jan 23:** Partly cloudy. **High -8F / Low -20F**. Precip **0**. Wind up to **12 mph**.  \n- **Sat Jan 24:** Partly cloudy. **High 0F / Low -16F**. Precip **0**. Wind up to **6 mph**.  \n- **Sun Jan 25:** Partly cloudy. **High 8F / Low -7F**. Precip **0**. Wind up to **9 mph**.  \n- **Mon Jan 26:** Partly cloudy. **High 13F / Low -7F**. Precip **0**. Wind up to **9 mph**.  \n- **Tue Jan 27:** Partly cloudy. **High 12F / Low 3F**. Precip **0**. Wind up to **12 mph**.  \n- **Wed Jan 28:** Partly cloudy. **High 8F / Low 0F**. Precip **0**. Wind up to **8 mph**.  \n- **Thu Jan 29:** Partly cloudy. **High 10F / Low 2F**. Precip **0**. Wind up to **5 mph**.\n\nOverall: **cold, mostly partly cloudy, and dry all week** (no measurable precipitation in the forecast).",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#creating-your-own-mcp-server",
    "href": "src/03/slides.html#creating-your-own-mcp-server",
    "title": "Module 3: Agents and Tools",
    "section": "Creating Your Own MCP Server",
    "text": "Creating Your Own MCP Server\n\nMultiple SDKs on https://modelcontextprotocol.io/docs/sdk\n\nPython, TypeScript, Go, Rust, C#, and more\n\nVery similar to tool calling\n\nDefine your MCP server\nAnnotate your functions with @mcp.tool()\nAdd descriptions to the tool methods to help the LLM select which tool to call",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#example-microbit-mcp-server",
    "href": "src/03/slides.html#example-microbit-mcp-server",
    "title": "Module 3: Agents and Tools",
    "section": "Example: micro:bit MCP Server",
    "text": "Example: micro:bit MCP Server\n\n\nhttps://simonguest.com/p/microbit-mcp/\nhttps://simonguest.com/p/microbit-mcp/",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#hugging-face-spaces-1",
    "href": "src/03/slides.html#hugging-face-spaces-1",
    "title": "Module 3: Agents and Tools",
    "section": "Hugging Face Spaces",
    "text": "Hugging Face Spaces\n\nWeve been using Gradio, but hosting via notebooks isnt ideal\n\nEven with share=True you have to keep the notebook running\n\nWouldnt it be nice if we could easily host our Gradio app?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#hugging-face-spaces-2",
    "href": "src/03/slides.html#hugging-face-spaces-2",
    "title": "Module 3: Agents and Tools",
    "section": "Hugging Face Spaces",
    "text": "Hugging Face Spaces",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#hugging-face-spaces-3",
    "href": "src/03/slides.html#hugging-face-spaces-3",
    "title": "Module 3: Agents and Tools",
    "section": "Hugging Face Spaces",
    "text": "Hugging Face Spaces\n\nFree cloud hosting for ML demos and applications\n\nSupports Gradio, Streamlit, and static HTML/JS\n\nFor Gradio, either upload a main.py or a Docker configuration file\nHugging Face handles resource allocation\n\nSleeps the space if its inactive\nIntegrates with the queuing mechanism of Gradio to batch requests\nSupports multiple GPU types (if signed up for Pro account)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#looking-ahead-1",
    "href": "src/03/slides.html#looking-ahead-1",
    "title": "Module 3: Agents and Tools",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis weeks assignment!\nLeave text behind and explore image-based models!\nIntroduce the diffuser\nAnd go other way with vision encoders",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#references-1",
    "href": "src/03/slides.html#references-1",
    "title": "Module 3: Agents and Tools",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/resources.html",
    "href": "src/04/resources.html",
    "title": "Resources",
    "section": "",
    "text": "The Illustrated Stable Diffusion - Jay Alammars visual guide to how diffusion models work\nStable Diffusion Paper - High-Resolution Image Synthesis with Latent Diffusion Models (2022)\nWhat are Diffusion Models? - Lilian Wengs comprehensive overview\nHugging Face Diffusers Library - Official documentation for the diffusers library",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#diffusion-models",
    "href": "src/04/resources.html#diffusion-models",
    "title": "Resources",
    "section": "",
    "text": "The Illustrated Stable Diffusion - Jay Alammars visual guide to how diffusion models work\nStable Diffusion Paper - High-Resolution Image Synthesis with Latent Diffusion Models (2022)\nWhat are Diffusion Models? - Lilian Wengs comprehensive overview\nHugging Face Diffusers Library - Official documentation for the diffusers library",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#stable-diffusion",
    "href": "src/04/resources.html#stable-diffusion",
    "title": "Resources",
    "section": "Stable Diffusion",
    "text": "Stable Diffusion\n\nStable Diffusion 1.5 on Hugging Face - Model card and weights\nSDXL Paper - SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis\nStability AI - Company behind Stable Diffusion",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#flux-models",
    "href": "src/04/resources.html#flux-models",
    "title": "Resources",
    "section": "FLUX Models",
    "text": "FLUX Models\n\nBlack Forest Labs - Creators of FLUX models\nFLUX.1 on Hugging Face - Official model repository\nFLUX.1 Technical Report - Overview of FLUX capabilities and tools",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#controlnet",
    "href": "src/04/resources.html#controlnet",
    "title": "Resources",
    "section": "ControlNet",
    "text": "ControlNet\n\nControlNet Paper - Adding Conditional Control to Text-to-Image Diffusion Models (2023)\nControlNet on Hugging Face - Original ControlNet models by Lvmin Zhang\nControlNet Guide - How to use ControlNet with diffusers",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#replicate",
    "href": "src/04/resources.html#replicate",
    "title": "Resources",
    "section": "Replicate",
    "text": "Replicate\n\nReplicate Home Page - Platform for running ML models via API\nReplicate Documentation - API reference and guides\nReplicate Python Client - Official Python library\nReplicate Collections - Curated model collections including free-to-try models",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#depth-estimation",
    "href": "src/04/resources.html#depth-estimation",
    "title": "Resources",
    "section": "Depth Estimation",
    "text": "Depth Estimation\n\nDepth Anything - State-of-the-art monocular depth estimation\nMiDaS - Intels robust monocular depth estimation model\nZoeDepth Paper - ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#inpainting-and-outpainting",
    "href": "src/04/resources.html#inpainting-and-outpainting",
    "title": "Resources",
    "section": "Inpainting and Outpainting",
    "text": "Inpainting and Outpainting\n\nLaMa: Large Mask Inpainting - Resolution-robust large mask inpainting\nFlux Fill on Replicate - FLUX-based inpainting model",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#vision-transformers",
    "href": "src/04/resources.html#vision-transformers",
    "title": "Resources",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\nViT Paper - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)\nCLIP Paper - Learning Transferable Visual Models From Natural Language Supervision\nDINOv2 - Metas self-supervised vision transformer",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#vision-language-models-vlms",
    "href": "src/04/resources.html#vision-language-models-vlms",
    "title": "Resources",
    "section": "Vision Language Models (VLMs)",
    "text": "Vision Language Models (VLMs)\n\nLLaVA Project Page - Large Language and Vision Assistant\nLLaVA Paper - Visual Instruction Tuning\nGemma 3 on Hugging Face - Googles multimodal Gemma model\nFastVLM Paper - FastVLM: Efficient Vision Encoding for Vision Language Models\nFastVLM on Hugging Face - Apples efficient on-device VLM\nFastVLM WebGPU Demo - Run FastVLM in your browser",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#gradio-for-image-applications",
    "href": "src/04/resources.html#gradio-for-image-applications",
    "title": "Resources",
    "section": "Gradio for Image Applications",
    "text": "Gradio for Image Applications\n\nGradio Image Components - Image input/output documentation\nGradio ImageEditor - Component for drawing and editing images\nGradio Sketchpad - Simple drawing canvas component",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#prompt-engineering-for-image-models",
    "href": "src/04/resources.html#prompt-engineering-for-image-models",
    "title": "Resources",
    "section": "Prompt Engineering for Image Models",
    "text": "Prompt Engineering for Image Models\n\nDALL-E 3 Prompt Guide - OpenAIs guide to image prompting\nStable Diffusion Prompt Guide - Community guide to effective prompts\nLexica - Search engine for Stable Diffusion prompts and images",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#citations",
    "href": "src/04/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/05/assignment.html",
    "href": "src/05/assignment.html",
    "title": "Module 5 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Assignment"
    ]
  },
  {
    "objectID": "src/05/assignment.html#assignment",
    "href": "src/05/assignment.html#assignment",
    "title": "Module 5 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Assignment"
    ]
  },
  {
    "objectID": "src/05/slides.html#recap",
    "href": "src/05/slides.html#recap",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Recap",
    "text": "Recap\n\nUnderstood the fundamentals and history of diffuser models\nExplored and used models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet\nSetup and used Replicate to create a custom pipeline of production-grade models\nUnderstood the fundamentals and history of Vision Encoders and VLMs\nImplemented/tested a local VLM model for on-device inference",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#lesson-objectives",
    "href": "src/05/slides.html#lesson-objectives",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile\nUnderstand hardware requirements and architectures for model inference - e.g., CUDA vs.ONNX vs.MLX vs.WebGPU\nExplore how quantization works and understand techniques and formats for quantizing existing models\nUse llama.cpp to quantize and run an SLM on local hardware/gaming PC\nIntegrate a quantized model within Unity/Unreal/WebAssembly",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#why-local-models-1",
    "href": "src/05/slides.html#why-local-models-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Why Local Models?",
    "text": "Why Local Models?\n\nPrivacy\n\nEvery call you make to OpenAI/Claude/OpenRouter may (or may not) get logged and/or be used for training purposes\nMany organizations dont want their customer/financial data logged with an AI vendor\nThere may also be legal regulations/restrictions controlling this",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#why-local-models-2",
    "href": "src/05/slides.html#why-local-models-2",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Why Local Models?",
    "text": "Why Local Models?\n\nOffline\n\nEvery call you make to OpenAI/Claude/OpenRouter needs an Internet connection\nThats not always guaranteed!\nEducation is a good example - remote school in India and/or rural districts here in the US",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#why-local-models-3",
    "href": "src/05/slides.html#why-local-models-3",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Why Local Models?",
    "text": "Why Local Models?\n\nLatency\n\nEven with a network connection, calls can suffer from increased latency\nCan be a challenge if your application needs frequent, quick responses\ne.g., using a VLM to determine the contents of a video stream for a user with vision impairments",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#why-local-models-4",
    "href": "src/05/slides.html#why-local-models-4",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Why Local Models?",
    "text": "Why Local Models?\n\nCost\n\nWhile per-API costs are fractions of a cent, these can grow out of control with exponential growth\nMore pronounced for long conversation threads (think call center)\nOr agents with verbose tool call JSON requests/responses",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#whats-your-hardware-1",
    "href": "src/05/slides.html#whats-your-hardware-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Whats Your Hardware?",
    "text": "Whats Your Hardware?\n\nNVIDIA (CUDA)\nAMD (ROCm)\nApple Silicon\nVarious NPU (Neural Processing Unit) vendors",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#nvidia-cuda",
    "href": "src/05/slides.html#nvidia-cuda",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "NVIDIA CUDA",
    "text": "NVIDIA CUDA\n\nCUDA (Compute Unified Device Architecture)\n\nLaunched in 2006 to introduce programming on GPUs (GPGPUs or General Purpose GPUs)\nA C-like programming interface\nPerfectly timed for the deep learning revolution of the 2010s\nAdditional libraries (e.g., cuBLAS, cuDNN) make CUDA the defacto standard today",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-cuda-works",
    "href": "src/05/slides.html#how-cuda-works",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How CUDA Works",
    "text": "How CUDA Works\n\nMassive parallelism: CUDA exploits thousands of GPU cores simultaneously, making it ideal for matrix operations.\nMemory hierarchy: A tiered memory system with global, shared, and registers.\nKernel execution: Programs can launch kernels - same function that can operate on different data.",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#nvidia-cuda---hardware-support",
    "href": "src/05/slides.html#nvidia-cuda---hardware-support",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "NVIDIA CUDA - Hardware Support",
    "text": "NVIDIA CUDA - Hardware Support\n\nConsumer: RTX 40- and 50- cards with various VRAM options (8Gb - 24Gb) for local inference and small fine-tuning tasks. RTX 30- series still popular for education.\nLaptop: RTX 40- and 50- series also available on laptops (although less performant than discrete cards)\nWorkstation: DGX Spark launch in 2025, with GB10 and 128Gb of unified memory for medium fine-tuning tasks\nDatacenter GPUs: A/H series and GB-series for datacenters. NVLink for multi-GPU interconnectivity.",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#sidebar-tops",
    "href": "src/05/slides.html#sidebar-tops",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Sidebar: TOPS",
    "text": "Sidebar: TOPS\n\nTOPS (Terra Operations Per Second)\n\nHow many trillion operations a processor can perform per second\nOften qualified with the data type\n64 INT8 TOPS == 64 trillion 8-bit operations per second\n\nTFLOPS (Terra Floating-Point Operations Per Second)\n\n1 TFLOPS == 1 FP32 (32-bit floating point) TOPS",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#sidebar-tops-1",
    "href": "src/05/slides.html#sidebar-tops-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Sidebar: TOPS",
    "text": "Sidebar: TOPS\n\nRough Throughput Calculations\n\nYou have an NVIDIA 3090 (advertized at 35 TFLOPS)\nAssume a 7B param model with FP32 weights\nEach token generation requires ~2 FLOPs per parameter\nEach token generation ~= 14B FLOPs (7B params  2)\nTheoretical max = 35T FLOPs/sec  14B FLOPs/token  2,500 tokens/sec\nReality: 10-100 tokens/sec typical due to memory bandwidth bottlenecks",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#amd-rocm",
    "href": "src/05/slides.html#amd-rocm",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "AMD ROCm",
    "text": "AMD ROCm\n\nROCm (Radeon Open Compute)\n\nLaunched in 2016 as an open-source alternative to CUDA\nEmbraced open standards (e.g., OpenCL), positioning as avoiding vendor lock-in, although this fragmentation initially hurt adoption\nHas evolved significantly since (e.g., rocBLAS) although ecosystem gaps compared to CUDA persist",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#amd-rocm---hardware-support",
    "href": "src/05/slides.html#amd-rocm---hardware-support",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "AMD ROCm - Hardware Support",
    "text": "AMD ROCm - Hardware Support\n\nConsumer: RX7000 series offer sustantial VRAM (up to 24Gb) at competitive prices compared to NVIDIA RTX\nLaptop: Some laptop options for AMD-based machines\nWorkstation: Strix Halo, competitor to DGX Spark, with RX8060S and 128Gb unified memory\nSoftware support: Linux only with no Windows support (some via WSL)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#apple-silicon",
    "href": "src/05/slides.html#apple-silicon",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Apple Silicon",
    "text": "Apple Silicon\n\nApple Silicon\n\nMetal, a low-level graphics and compute API, launched in 2014 and later expanded for general GPU compute tasks\nMPS (Metal Performance Shaders) introduced in 2017 and optimized primitives for neural networks. PyTorch added MPS device support in 2022.\nMLX released in 2023, providing NumPy-like API for Apple Silicon hardware",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#apple-silicon---hardware-support",
    "href": "src/05/slides.html#apple-silicon---hardware-support",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Apple Silicon - Hardware Support",
    "text": "Apple Silicon - Hardware Support\n\nAvailable on all M-series hardware\nUnified memory by default, upto 128Gb on laptops and 512Gb for the Mac Studio with M3 Ultra\nNon portable models. (MLX uses safetensors/npz format and MLX-specific code for Metal.)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#sidebar-unified-memory",
    "href": "src/05/slides.html#sidebar-unified-memory",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Sidebar: Unified Memory",
    "text": "Sidebar: Unified Memory\n\nGPUs have historically had separate memory (VRAM)\nUnified memory is a process to share memory between CPU and GPU\n\nFor Apple/MLX, its a true SoC (System on a Chip); NVIDIA DGX Spark, two physical components connected via NVLink-C2C\n\nHigher memory availability, but lower memory bandwidth\n\n~275Gb/s for Spark/MLX; ~1TB/s for 5080; ~1.7TB/s for 5090; ~3TBs for H100",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#npus",
    "href": "src/05/slides.html#npus",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "NPUs",
    "text": "NPUs\n\nNPUs (Neural Processing Units) are specialized AI accelerators, designed for lower power consumption\nOptimized specifically for NN operations (e.g., matmul, convolutions, activations)\nCommonly found in edge devices (smartphones, IoT, embedded systems)\n15-80 TOPS common for NPUs (~10x less that desktop PCI-based GPUs)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#npu-vendors",
    "href": "src/05/slides.html#npu-vendors",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "NPU Vendors",
    "text": "NPU Vendors\n\nIntel: Acquired Movidius in 2016; released Myriad X in 2017 and a neural compute stick. Superceded by NPUs in Core Ultra processors.\nQualcomm: Snapdragon 865 range in 2019; now Snapdragon X and 8 ranges. Used in Windows/ARM devices. Popular with Android, although Google recently moved to their own TPUs.\nAMD: XDNA formerly Xilinx; Windows Copilot PC range competing with SnapDragon and Intel Core Ultra.\nApple: ANE (Apple Neural Engine) to support CoreML workloads on iPhone, iPad devices",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#compute-and-memory-challenges-1",
    "href": "src/05/slides.html#compute-and-memory-challenges-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Compute and Memory Challenges",
    "text": "Compute and Memory Challenges\n\nCompute Challenges\n\nLocal GPUs significantly slower than datacenter-class GPUs\n4060 Ti (~350 TOPS) vs.H100 (~3500+ TOPS)\nOne local GPU vs.interconnected datacenter-class GPUs (using NVLink)\n\nLess TOPS = slower inference (tokens/second)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#compute-and-memory-challenges-2",
    "href": "src/05/slides.html#compute-and-memory-challenges-2",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Compute and Memory Challenges",
    "text": "Compute and Memory Challenges\n\nMemory Challenges\n\nConsumer-grade GPUs and NPUs have significanly less memory than datacenter-class GPUs\n8Gb/16Gb VRAM on 4060 Ti vs.80/94Gb for H100\n(Also memory bandwidth is typically 10x-20x)\n(Combined memory architecture using NVLink)\n\nLess memory = only smaller models can run\n\nRougly speaking, size of model must be smaller than available VRAM",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#overcoming-compute-challenges-1",
    "href": "src/05/slides.html#overcoming-compute-challenges-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Overcoming Compute Challenges",
    "text": "Overcoming Compute Challenges\n\nMoE (Mixture of Experts)\n\nOriginal concept dates back to 1991. Jacobs et al.publish Adaptive Mixture of Local Experts showing subnetworks and a gating mechanism\nIn 2017, Shazeer et al.(Google) publish Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer demonstrating activating only a small subset of experts per input\n2022: Google releases Switch Transformers (Fedus et al.), simplifying MoE by routing each token to just a single expert\n2023: Mixtral 8x7B (Mistral AI) brings high-quality open-source MoE to the mainstream, becoming a standard architecture for efficient large-scale models",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#popular-moe-models",
    "href": "src/05/slides.html#popular-moe-models",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Popular MoE Models",
    "text": "Popular MoE Models\n\nMixtral (Mistral)\nQwen MoE (Alibaba)\nDeepSeek MoE\nPhi MoE (Microsofts SLMs)\nNemotron (NVIDIA)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#overcoming-compute-challenges-2",
    "href": "src/05/slides.html#overcoming-compute-challenges-2",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Overcoming Compute Challenges",
    "text": "Overcoming Compute Challenges\n\nWhy MoE?\n\nFaster Inference (token/second): Because you are only using a subset of experts for each token.\nAllows you to use larger models, where you would be otherwise compute-bound\n\nDense 13B model ~= 13Gb VRAM\nMixtral 8x7B ~= 47GB VRAM (87B experts + shared layers), but faster inference than dense 47B (only ~2 experts active per token)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moe-models-work",
    "href": "src/05/slides.html#how-do-moe-models-work",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoE Models Work?",
    "text": "How do MoE Models Work?\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-tiny-MoE-instruct\",\n    device_map=\"auto\",\n    dtype=torch.float16,\n    trust_remote_code=True,\n    output_router_logits=True  # Enable router outputs\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"microsoft/Phi-tiny-MoE-instruct\",\n    trust_remote_code=True\n)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moe-models-work-1",
    "href": "src/05/slides.html#how-do-moe-models-work-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoE Models Work?",
    "text": "How do MoE Models Work?\n\n\nPROMPT = \"What is hello in Japanese?\"\n\ngate_layers = []\nfor name, module in model.named_modules():\n    if 'block_sparse_moe.gate' in name:\n        gate_layers.append((name, module))\n\nprint(f\"Found {len(gate_layers)} gate layers\\n\")\n\nrouter_outputs = []\n\ndef router_hook(module, input, output):\n    router_outputs.append(output.detach().cpu())\n\n# Register hooks\nhooks = []\nfor name, module in gate_layers:\n    hooks.append(module.register_forward_hook(router_hook))\n\ninputs = tokenizer(PROMPT, return_tensors=\"pt\").to(model.device)\n\n# Run forward pass\nrouter_outputs.clear()\nwith torch.no_grad():\n    _ = model(**inputs, use_cache=False)\n\n# Analyze first layer\nrouter_probs = torch.softmax(router_outputs[0], dim=-1)\nprint(\"First layer routing:\")\nprint(f\"  Number of experts: {router_probs.shape[-1]}\")\nprint(f\"  Shape: {router_probs.shape}\")\n\n# Show top-k experts for first 5 tokens\nfor tok_idx in range(min(5, router_probs.shape[0])):\n    top_k = torch.topk(router_probs[tok_idx], k=4)\n    print(f\"  Token {tok_idx}: top experts {top_k.indices.tolist()} \"\n          f\"with probs {[f'{p:.3f}' for p in top_k.values.tolist()]}\")",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moe-models-work-2",
    "href": "src/05/slides.html#how-do-moe-models-work-2",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoE Models Work?",
    "text": "How do MoE Models Work?\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nPROMPT = \"What is hello in Japanese?\"\n\ndef capture_routing(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n\n    router_outputs.clear()\n    with torch.no_grad():\n        _ = model(**inputs, use_cache=False)\n\n    return router_outputs[:32], tokens\n\ndef routing_heatmap(router_logits, tokens, layer_idx=0):\n    # Get routing probabilities\n    router_probs = torch.softmax(router_logits[layer_idx], dim=-1).numpy()\n    num_tokens, num_experts = router_probs.shape\n\n    fig, axes = plt.subplots(1, 1, figsize=(8, 5))\n\n    # Use seaborn to display routing probabilities\n    sns.heatmap(\n        router_probs.T,\n        cmap='YlOrRd',\n        ax=axes,\n        cbar_kws={'label': 'Probability'},\n        xticklabels=tokens[:num_tokens],\n        yticklabels=[f'E{i}' for i in range(num_experts)]\n    )\n    axes.set_title(f'Layer {layer_idx}: Routing Probabilities per Token')\n    axes.set_xlabel('Token')\n    axes.set_ylabel('Expert')\n    plt.setp(axes.get_xticklabels(), rotation=45, ha='right')\n\noutputs, tokens = capture_routing(PROMPT)\n\n# Display heatmap for layer 0\nrouting_heatmap(outputs, tokens, layer_idx=0)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moe-models-work-3",
    "href": "src/05/slides.html#how-do-moe-models-work-3",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoE Models Work?",
    "text": "How do MoE Models Work?",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moe-models-work-4",
    "href": "src/05/slides.html#how-do-moe-models-work-4",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoE Models Work?",
    "text": "How do MoE Models Work?",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moe-models-work-5",
    "href": "src/05/slides.html#how-do-moe-models-work-5",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoE Models Work?",
    "text": "How do MoE Models Work?",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moe-models-work-6",
    "href": "src/05/slides.html#how-do-moe-models-work-6",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoE Models Work?",
    "text": "How do MoE Models Work?",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moe-models-work-7",
    "href": "src/05/slides.html#how-do-moe-models-work-7",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoE Models Work?",
    "text": "How do MoE Models Work?",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moe-models-work-8",
    "href": "src/05/slides.html#how-do-moe-models-work-8",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoE Models Work",
    "text": "How do MoE Models Work\n\nWhat the Heatmaps Show\n\nDifferent tokens activate different experts (token-level routing)\nRouting changes across layers (layer 0 vs.layer 31)\n\nEarly layers: syntax/grammar\nLate layers: semantics/reasoning\n\nNot all experts used equally (specialization emerges)\nRouter makes soft decisions (distributes probability across multiple experts)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moes-get-trained",
    "href": "src/05/slides.html#how-do-moes-get-trained",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoEs Get Trained?",
    "text": "How do MoEs Get Trained?\n\nFrom scratch\n\nTrain the entire model (experts and router)\nRouter networks learn which expert to select\nExperts learn their specializations\nExample: Mixtral 8x7B\n\nStart from a trained dense model (called upcycling)\n\nReplicate each layer into multiple experts; add router networks\nTypically faster and more stable vs.starting from scratch\nExample: Qwen1.5-MoE",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moes-get-trained-1",
    "href": "src/05/slides.html#how-do-moes-get-trained-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoEs Get Trained?",
    "text": "How do MoEs Get Trained?\n\nCombine multiple models (FrankenMoE / MoErge)\n\nNewer, community-driven approach\nTake specialized models (e.g., math, coding, chat) and use their FFN layers as separate experts; add router network\nTrain only the router (experts frozen initially)\nCheaper than training from scratch, but often lower quality\nExample: Beyonder-4x7B",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-do-moes-get-trained-2",
    "href": "src/05/slides.html#how-do-moes-get-trained-2",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How do MoEs Get Trained?",
    "text": "How do MoEs Get Trained?\n\nKey challenge: Load balancing / Expert collapse\nHealthy MoE:\n\nExpert 0: 8% of tokens\nExpert 1: 7% of tokens\nExpert 2: 9% of tokens\n(all experts get reasonable usage)\n\nCollapsed MoE:\n\nExpert 0: 45% of tokens\nExpert 1: 40% of tokens\nExpert 2-7: &lt;15% combined (essentially dead)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#overcoming-compute-challenges-3",
    "href": "src/05/slides.html#overcoming-compute-challenges-3",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Overcoming Compute Challenges",
    "text": "Overcoming Compute Challenges\n\nMoE gives us speed with quality\n\nBut we still cant fit it on consumer hardware\nPopular MoE models often exceed the amount of available VRAM\nWe need a way to reduce the memory footprint of the model",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#introducing-quantization",
    "href": "src/05/slides.html#introducing-quantization",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Introducing Quantization",
    "text": "Introducing Quantization\n\nProcess of reducing the precision of a models weights and activations\n\nFor example, converting 16-bit numbers to 4-bit\n\nLess precision significantly reduces memory needs\nFor accuracy, parameter count matters more than precision\n\nA 70B parameter model at 4-bit often beats a 13B model at bf16\nThe models knowledge remains largely intact\nOften the extra precision doesnt meaningfully improve outputs",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#quantization-formats",
    "href": "src/05/slides.html#quantization-formats",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Quantization Formats",
    "text": "Quantization Formats\n\nGPTQ (GPT Quantization):\n\nIntroduced in the paper GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\nFirst widely adopted method (in 2022) for agressive (4-bit) quantization\nCUDA only; distributed via HF Transformers\n\nGGML (Georgi Gerganov Machine Learning):\n\nA C/C++ library (llama.cpp released in Mar 2023) designed for CPU inference of models\nOptimized for CPUs (using SIMD) with a custom binary format\nDemocratized access to LLMs overnight (ability to run Llama 7B on CPU)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#quantization-formats-1",
    "href": "src/05/slides.html#quantization-formats-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Quantization Formats",
    "text": "Quantization Formats\n\nGGUF (GPT-Generated Unified Format):\n\nReplaced GGML in late 2023, adding extensibility\nBetter metadata support (model architecture, tokenizer info, quantization details)\nSingle file architecture\nCan offload layers to GPU/NPU, if available, increasing performance\nMultiple quantization schemes (Q4_K_M, Q5_K_S, Q6_K, etc.)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#sidebar-quantization-schemes",
    "href": "src/05/slides.html#sidebar-quantization-schemes",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Sidebar: Quantization Schemes",
    "text": "Sidebar: Quantization Schemes\n\nWhat does Q4_K_M mean?\n\nNumber (Q4, Q5, Q6, etc.) is the average bits per weight\nLetter suffix (K, 0, 1) is the quantization strategy\nSize suffix (S, M, L) for variants within that method",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#sidebar-quantization-schemes-1",
    "href": "src/05/slides.html#sidebar-quantization-schemes-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Sidebar: Quantization Schemes",
    "text": "Sidebar: Quantization Schemes\n\nQ2_K = Agressive quantization, smallest files, noticable loss\nQ4_K_M = Best quality/size trade off; most popular\nQ6_K = Very close to full precision, but much larger files (almost double Q4_K_M)\nQ8_0 = Essentially lossless compared to FP16",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#sidebar-quantization-schemes-2",
    "href": "src/05/slides.html#sidebar-quantization-schemes-2",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Sidebar: Quantization Schemes",
    "text": "Sidebar: Quantization Schemes\n\nK-Quant strategy\n\nA mixed quantization strategy; quantize different parts of the model at different bit depths depending on sensitivity\ne.g., critical weights (e.g., attention layers) get higher precision; less sensitive weights get more aggressive quantization\n\n0 and 1 strategy\n\nUniform quantization across all layers (1 strategy adds a zero point)\nLess optimal than using K strategy (and rarely used)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#running-llama.cpp",
    "href": "src/05/slides.html#running-llama.cpp",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Running llama.cpp",
    "text": "Running llama.cpp\n\nhttps://github.com/ggml-org/llama.cpp\nC/C++ library; download (brew, nix, winget) or compile from source\nInitially just a CLI, but now ships with Web UI and OpenAI API compatible server\nCan reference a locally downloaded .gguf file or pull one from HF",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#llama.cpp-wrappers",
    "href": "src/05/slides.html#llama.cpp-wrappers",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "llama.cpp Wrappers",
    "text": "llama.cpp Wrappers\n\nOllama (https://ollama.com)\n\nSimple CLI wrapper around llama.cpp\nUses a container-like Modelfile to customize system prompts, parameters, etc.\nCurated collection of models on ollama.com/library\n\nLM Studio\n\nDesktop GUI wrapper around llama.cpp\nSupports browsing/downloading of models from HF - i.e., iTunes for LLMs\nIn-built chat interface and API server",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#mlx-apples-ml-framework",
    "href": "src/05/slides.html#mlx-apples-ml-framework",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "MLX (Apples ML Framework)",
    "text": "MLX (Apples ML Framework)\n\nML framework created by Apple (released Dec 2023)\nDesigned specifically for Apple Silicon (M-series processors)\nPython API similar to NumPy/PyTorch, but optimized for Metal (Apples GPU API)\nCan run on non-Apple hardware - CPU and CUDA PyPi packages",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#mlx-apples-ml-framework-1",
    "href": "src/05/slides.html#mlx-apples-ml-framework-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "MLX (Apples ML Framework)",
    "text": "MLX (Apples ML Framework)\n\nSupports various levels of quantization (4bit and 8bit)\n.npz or .safetensor format (with MLX metadata)\nBetter performance on Mac (compared to GGUF)\nmlx-community on HF hosts MLX-quantized versions of popular models (separate repos)\n\nhttps://huggingface.co/models?library=mlx",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#onnx-open-neural-network-exchange",
    "href": "src/05/slides.html#onnx-open-neural-network-exchange",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "ONNX (Open Neural Network eXchange)",
    "text": "ONNX (Open Neural Network eXchange)\n\nModel interchange format, created by Microsoft and Facebook in 2017 to enable model portability\nModels can be converted: PyTorch -&gt; ONNX; TensorFlow -&gt; ONNX; etc.\nCommon export format, used in many CNNs and RNNs\nPopular for models running on mobile devices and in-browser (e.g., Transformers.js)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#onnx-open-neural-network-exchange-1",
    "href": "src/05/slides.html#onnx-open-neural-network-exchange-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "ONNX (Open Neural Network eXchange)",
    "text": "ONNX (Open Neural Network eXchange)\n\nSupports dynamic quantization (typically int8)\n.onnx file format (uses protobuf format) includes graph structure, weights, and metadata\nBroad portability, but at the expense of performance\nONNX models tend to live in separate folders on HF\n\nhttps://huggingface.co/models?library=onnx",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-to-quantize-a-model-1",
    "href": "src/05/slides.html#how-to-quantize-a-model-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How to Quantize a Model",
    "text": "How to Quantize a Model\n\nPopular models have already been quantized\n\nMany available on HF, contributed by Unsloth AI\n\nIf the model isnt already quantized (or youve fine tuned your own), its easy to do",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-to-quantize-a-model-2",
    "href": "src/05/slides.html#how-to-quantize-a-model-2",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How to Quantize a Model",
    "text": "How to Quantize a Model\n\n\n\n\nCloning into 'llama.cpp'...\n\nremote: Enumerating objects: 77965, done.\n\nremote: Counting objects: 100% (228/228), done.\n\nremote: Compressing objects: 100% (156/156), done.\n\nremote: Total 77965 (delta 141), reused 76 (delta 72), pack-reused 77737 (from 4)\n\nReceiving objects: 100% (77965/77965), 286.82 MiB | 15.73 MiB/s, done.\n\nResolving deltas: 100% (56335/56335), done.\n\n-- The C compiler identification is GNU 11.4.0\n\n-- The CXX compiler identification is GNU 11.4.0\n\n-- Detecting C compiler ABI info\n\n-- Detecting C compiler ABI info - done\n\n-- Check for working C compiler: /usr/bin/cc - skipped\n\n-- Detecting C compile features\n\n-- Detecting C compile features - done\n\n-- Detecting CXX compiler ABI info\n\n-- Detecting CXX compiler ABI info - done\n\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n\n-- Detecting CXX compile features\n\n-- Detecting CXX compile features - done\n\nCMAKE_BUILD_TYPE=Release\n\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n\n-- The ASM compiler identification is GNU\n\n-- Found assembler: /usr/bin/cc\n\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n\n-- Found Threads: TRUE\n\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n\n-- GGML_SYSTEM_ARCH: x86\n\n-- Including CPU backend\n\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n\n-- Found OpenMP: TRUE (found version \"4.5\")\n\n-- x86 detected\n\n-- Adding CPU backend variant ggml-cpu: -march=native \n\n-- ggml version: 0.9.5\n\n-- ggml commit:  0dfcd3b60\n\n-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"3.0.2\")\n\n-- Performing Test OPENSSL_VERSION_SUPPORTED\n\n-- Performing Test OPENSSL_VERSION_SUPPORTED - Success\n\n-- OpenSSL found: 3.0.2\n\n-- Generating embedded license file for target: common\n\n-- Configuring done (1.6s)\n\n-- Generating done (0.3s)\n\n-- Build files have been written to: /content/tmp/llama.cpp/build\n\n[  0%] Building CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\n\n[  0%] Linking CXX static library libcpp-httplib.a\n\n[  0%] Built target cpp-httplib\n\n[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\n\n[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\n\n[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n\n[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\n\n[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\n\n[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\n\n[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n\n[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\n\n[  4%] Linking CXX shared library ../../bin/libggml-base.so\n\n[  4%] Built target ggml-base\n\n[  4%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\n\n[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\n\n[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\n\n[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\n\n[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\n\n[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\n\n[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\n\n[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\n\n[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\n\n[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\n\n[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\n\n[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\n\n[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n\n[ 13%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\n\n[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\n\n[ 13%] Linking CXX shared library ../../bin/libggml-cpu.so\n\n[ 13%] Built target ggml-cpu\n\n[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o\n\n[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\n\n[ 15%] Linking CXX shared library ../../bin/libggml.so\n\n[ 15%] Built target ggml\n\n[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o\n\n[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\n\n[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\n\n[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\n\n[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\n\n[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\n\n[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\n\n[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\n\n[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\n\n[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\n\n[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\n\n[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\n\n[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\n\n[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\n\n[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\n\n[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\n\n[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o\n\n[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\n\n[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\n\n[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\n\n[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\n\n[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\n\n[ 26%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\n\n[ 28%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\n\n[ 28%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\n\n[ 28%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\n\n[ 28%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\n\n[ 31%] Building CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\n\n[ 31%] Building CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\n\n[ 31%] Building CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\n\n[ 31%] Building CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\n\n[ 33%] Building CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\n\n[ 33%] Building CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\n\n[ 33%] Building CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\n\n[ 33%] Building CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\n\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\n\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\n\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\n\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\n\n[ 35%] Building CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\n\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\n\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\n\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\n\n[ 37%] Building CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\n\n[ 40%] Building CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\n\n[ 40%] Building CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\n\n[ 40%] Building CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\n\n[ 40%] Building CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\n\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\n\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\n\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\n\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\n\n[ 44%] Building CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\n\n[ 44%] Building CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\n\n[ 44%] Building CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o\n\n[ 44%] Building CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\n\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\n\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\n\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\n\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\n\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\n\n[ 48%] Building CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\n\n[ 48%] Building CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\n\n[ 48%] Building CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\n\n[ 48%] Building CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\n\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\n\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\n\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\n\n[ 51%] Building CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\n\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\n\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\n\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\n\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\n\n[ 55%] Building CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\n\n[ 55%] Building CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\n\n[ 55%] Building CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\n\n[ 55%] Building CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\n\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\n\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\n\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\n\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\n\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\n\n[ 60%] Building CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\n\n[ 60%] Building CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\n\n[ 60%] Building CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\n\n[ 60%] Building CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\n\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\n\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\n\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\n\n[ 62%] Building CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\n\n[ 64%] Building CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\n\n[ 64%] Building CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\n\n[ 64%] Building CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\n\n[ 64%] Building CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\n\n[ 66%] Building CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\n\n[ 66%] Building CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\n\n[ 66%] Building CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\n\n[ 66%] Building CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\n\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\n\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\n\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\n\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\n\n[ 68%] Building CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\n\n[ 71%] Building CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\n\n[ 71%] Building CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\n\n[ 71%] Building CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\n\n[ 71%] Building CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\n\n[ 73%] Building CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\n\n[ 73%] Building CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\n\n[ 73%] Building CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\n\n[ 73%] Building CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\n\n[ 75%] Building CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\n\n[ 75%] Building CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\n\n[ 75%] Building CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\n\n[ 75%] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\n\n[ 77%] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\n\n[ 77%] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\n\n[ 77%] Building CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\n\n[ 77%] Building CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\n\n[ 77%] Building CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\n\n[ 80%] Building CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\n\n[ 80%] Building CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\n\n[ 80%] Building CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\n\n[ 80%] Building CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\n\n[ 82%] Building CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\n\n[ 82%] Building CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\n\n[ 82%] Building CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\n\n[ 82%] Building CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\n\n[ 84%] Building CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\n\n[ 84%] Building CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\n\n[ 84%] Building CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\n\n[ 84%] Linking CXX shared library ../bin/libllama.so\n\n[ 84%] Built target llama\n\n[ 84%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\n\n[ 84%] Built target build_info\n\n[ 84%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o\n\n[ 84%] Building CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\n\n[ 84%] Building CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\n\n[ 86%] Building CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\n\n[ 86%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o\n\n[ 86%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o\n\n[ 86%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o\n\n[ 88%] Building CXX object common/CMakeFiles/common.dir/debug.cpp.o\n\n[ 88%] Building CXX object common/CMakeFiles/common.dir/download.cpp.o\n\n[ 88%] Building CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\n\n[ 88%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n\n[ 91%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\n\n[ 91%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o\n\n[ 91%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\n\n[ 91%] Building CXX object common/CMakeFiles/common.dir/ngram-map.cpp.o\n\n[ 93%] Building CXX object common/CMakeFiles/common.dir/ngram-mod.cpp.o\n\n[ 93%] Building CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\n\n[ 93%] Building CXX object common/CMakeFiles/common.dir/preset.cpp.o\n\n[ 93%] Building CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\n\n[ 93%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o\n\n[ 95%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o\n\n[ 95%] Building CXX object common/CMakeFiles/common.dir/unicode.cpp.o\n\n[ 95%] Building CXX object common/CMakeFiles/common.dir/jinja/lexer.cpp.o\n\n[ 95%] Building CXX object common/CMakeFiles/common.dir/jinja/parser.cpp.o\n\n[ 97%] Building CXX object common/CMakeFiles/common.dir/jinja/runtime.cpp.o\n\n[ 97%] Building CXX object common/CMakeFiles/common.dir/jinja/value.cpp.o\n\n[ 97%] Building CXX object common/CMakeFiles/common.dir/jinja/string.cpp.o\n\n[ 97%] Building CXX object common/CMakeFiles/common.dir/jinja/caps.cpp.o\n\n[100%] Building CXX object common/CMakeFiles/common.dir/__/license.cpp.o\n\n[100%] Linking CXX static library libcommon.a\n\n[100%] Built target common\n\n[100%] Building CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\n\n[100%] Linking CXX executable ../../bin/llama-quantize\n\n[100%] Built target llama-quantize",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-to-quantize-a-model-3",
    "href": "src/05/slides.html#how-to-quantize-a-model-3",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How to Quantize a Model",
    "text": "How to Quantize a Model\n\n\n\n\n\n\n\n\n\nINFO:hf-to-gguf:Loading model: Qwen3-0.6B\nINFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\nINFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:output.weight,             torch.bfloat16 --&gt; F16, shape = {1024, 151936}\nINFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --&gt; F16, shape = {1024, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}\nINFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}\nINFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --&gt; F32, shape = {1024}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 40960\nINFO:hf-to-gguf:gguf: embedding length = 1024\nINFO:hf-to-gguf:gguf: feed forward length = 3072\nINFO:hf-to-gguf:gguf: head count = 16\nINFO:hf-to-gguf:gguf: key-value head count = 8\nWARNING:hf-to-gguf:Unknown RoPE type: default\nINFO:hf-to-gguf:gguf: rope scaling type = NONE\nINFO:hf-to-gguf:gguf: rope theta = 1000000\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nINFO:numexpr.utils:NumExpr defaulting to 12 threads.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting add_bos_token to False\nINFO:gguf.vocab:Setting chat_template to {%- if tools %}\n    {{- '&lt;|im_start|&gt;system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\\n&lt;tools&gt;\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n&lt;/tools&gt;\\n\\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\\n&lt;tool_call&gt;\\n{\\\"name\\\": &lt;function-name&gt;, \\\"arguments\\\": &lt;args-json-object&gt;}\\n&lt;/tool_call&gt;&lt;|im_end|&gt;\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '&lt;|im_start|&gt;system\\n' + messages[0].content + '&lt;|im_end|&gt;\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('&lt;tool_response&gt;') and message.content.endswith('&lt;/tool_response&gt;')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content + '&lt;|im_end|&gt;' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '&lt;/think&gt;' in content %}\n                {%- set reasoning_content = content.split('&lt;/think&gt;')[0].rstrip('\\n').split('&lt;think&gt;')[-1].lstrip('\\n') %}\n                {%- set content = content.split('&lt;/think&gt;')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 &gt; ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '&lt;|im_start|&gt;' + message.role + '\\n&lt;think&gt;\\n' + reasoning_content.strip('\\n') + '\\n&lt;/think&gt;\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '&lt;|im_start|&gt;' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '&lt;tool_call&gt;\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n&lt;/tool_call&gt;' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '&lt;|im_end|&gt;\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '&lt;|im_start|&gt;user' }}\n        {%- endif %}\n        {{- '\\n&lt;tool_response&gt;\\n' }}\n        {{- content }}\n        {{- '\\n&lt;/tool_response&gt;' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '&lt;|im_end|&gt;\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '&lt;|im_start|&gt;assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '&lt;think&gt;\\n\\n&lt;/think&gt;\\n\\n' }}\n    {%- endif %}\n{%- endif %}\nINFO:gguf.gguf_writer:Writing the following files:\nINFO:gguf.gguf_writer:/content/ggufs/Qwen3-0.6B-F16.gguf: n_tensors = 311, total_size = 1.5G\nWriting: 100% 1.50G/1.50G [00:05&lt;00:00, 287Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to /content/ggufs/Qwen3-0.6B-F16.gguf\nmain: build = 7916 (0dfcd3b60)\nmain: built with GNU 11.4.0 for Linux x86_64\nmain: quantizing '/content/ggufs/Qwen3-0.6B-F16.gguf' to '/content/ggufs/Qwen3-0.6B-Q4_K_M.gguf' as Q4_K_M\nllama_model_loader: loaded meta data with 37 key-value pairs and 311 tensors from /content/ggufs/Qwen3-0.6B-F16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                     general.sampling.top_k i32              = 20\nllama_model_loader: - kv   3:                     general.sampling.top_p f32              = 0.950000\nllama_model_loader: - kv   4:                      general.sampling.temp f32              = 0.600000\nllama_model_loader: - kv   5:                               general.name str              = Qwen3 0.6B\nllama_model_loader: - kv   6:                           general.basename str              = Qwen3\nllama_model_loader: - kv   7:                         general.size_label str              = 0.6B\nllama_model_loader: - kv   8:                            general.license str              = apache-2.0\nllama_model_loader: - kv   9:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-0.6...\nllama_model_loader: - kv  10:                   general.base_model.count u32              = 1\nllama_model_loader: - kv  11:                  general.base_model.0.name str              = Qwen3 0.6B Base\nllama_model_loader: - kv  12:          general.base_model.0.organization str              = Qwen\nllama_model_loader: - kv  13:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-0.6...\nllama_model_loader: - kv  14:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv  15:                          qwen3.block_count u32              = 28\nllama_model_loader: - kv  16:                       qwen3.context_length u32              = 40960\nllama_model_loader: - kv  17:                     qwen3.embedding_length u32              = 1024\nllama_model_loader: - kv  18:                  qwen3.feed_forward_length u32              = 3072\nllama_model_loader: - kv  19:                 qwen3.attention.head_count u32              = 16\nllama_model_loader: - kv  20:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  21:                       qwen3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  22:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  23:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  24:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  25:                          general.file_type u32              = 1\nllama_model_loader: - kv  26:               general.quantization_version u32              = 2\nllama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,151387]  = [\" \", \" \", \"i n\", \" t\",...\nllama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  34:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  36:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '&lt;|im_start|&gt;...\nllama_model_loader: - type  f32:  113 tensors\nllama_model_loader: - type  f16:  198 tensors\n[   1/ 311]                        output.weight - [ 1024, 151936,     1,     1], type =    f16, converting to q6_K .. size =   296.75 MiB -&gt;   121.71 MiB\n[   2/ 311]                   output_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[   3/ 311]                    token_embd.weight - [ 1024, 151936,     1,     1], type =    f16, converting to q4_K .. size =   296.75 MiB -&gt;    83.46 MiB\n[   4/ 311]                  blk.0.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[   5/ 311]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[   6/ 311]               blk.0.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[   7/ 311]             blk.0.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[   8/ 311]                  blk.0.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[   9/ 311]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  10/ 311]                  blk.0.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[  11/ 311]                blk.0.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[  12/ 311]                blk.0.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  13/ 311]                blk.0.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  14/ 311]                  blk.0.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  15/ 311]                  blk.1.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  16/ 311]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  17/ 311]               blk.1.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  18/ 311]             blk.1.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  19/ 311]                  blk.1.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  20/ 311]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  21/ 311]                  blk.1.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[  22/ 311]                blk.1.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[  23/ 311]                blk.1.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  24/ 311]                blk.1.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  25/ 311]                  blk.1.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  26/ 311]                  blk.2.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  27/ 311]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  28/ 311]               blk.2.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  29/ 311]             blk.2.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  30/ 311]                  blk.2.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  31/ 311]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  32/ 311]                  blk.2.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[  33/ 311]                blk.2.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[  34/ 311]                blk.2.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  35/ 311]                blk.2.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  36/ 311]                  blk.2.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  37/ 311]                  blk.3.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  38/ 311]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  39/ 311]               blk.3.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  40/ 311]             blk.3.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  41/ 311]                  blk.3.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  42/ 311]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  43/ 311]                  blk.3.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  44/ 311]                blk.3.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  45/ 311]                blk.3.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  46/ 311]                blk.3.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  47/ 311]                  blk.3.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  48/ 311]                  blk.4.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  49/ 311]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  50/ 311]               blk.4.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  51/ 311]             blk.4.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  52/ 311]                  blk.4.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  53/ 311]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  54/ 311]                  blk.4.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  55/ 311]                blk.4.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  56/ 311]                blk.4.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  57/ 311]                blk.4.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  58/ 311]                  blk.4.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  59/ 311]                  blk.5.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  60/ 311]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  61/ 311]               blk.5.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  62/ 311]             blk.5.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  63/ 311]                  blk.5.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  64/ 311]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  65/ 311]                  blk.5.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[  66/ 311]                blk.5.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[  67/ 311]                blk.5.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  68/ 311]                blk.5.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  69/ 311]                  blk.5.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  70/ 311]                  blk.6.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  71/ 311]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  72/ 311]               blk.6.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  73/ 311]             blk.6.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  74/ 311]                  blk.6.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  75/ 311]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  76/ 311]                  blk.6.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  77/ 311]                blk.6.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  78/ 311]                blk.6.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  79/ 311]                blk.6.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  80/ 311]                  blk.6.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  81/ 311]                  blk.7.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  82/ 311]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  83/ 311]               blk.7.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  84/ 311]             blk.7.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  85/ 311]                  blk.7.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  86/ 311]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  87/ 311]                  blk.7.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  88/ 311]                blk.7.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  89/ 311]                blk.7.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  90/ 311]                blk.7.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  91/ 311]                  blk.7.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[  92/ 311]                  blk.8.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[  93/ 311]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  94/ 311]               blk.8.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[  95/ 311]             blk.8.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  96/ 311]                  blk.8.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[  97/ 311]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[  98/ 311]                  blk.8.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[  99/ 311]                blk.8.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 100/ 311]                blk.8.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 101/ 311]                blk.8.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 102/ 311]                  blk.8.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 103/ 311]                  blk.9.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 104/ 311]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 105/ 311]               blk.9.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 106/ 311]             blk.9.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 107/ 311]                  blk.9.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 108/ 311]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 109/ 311]                  blk.9.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 110/ 311]                blk.9.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 111/ 311]                blk.9.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 112/ 311]                blk.9.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 113/ 311]                  blk.9.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 114/ 311]                 blk.10.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 115/ 311]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 116/ 311]              blk.10.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 117/ 311]            blk.10.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 118/ 311]                 blk.10.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 119/ 311]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 120/ 311]                 blk.10.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 121/ 311]               blk.10.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 122/ 311]               blk.10.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 123/ 311]               blk.10.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 124/ 311]                 blk.10.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 125/ 311]                 blk.11.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 126/ 311]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 127/ 311]              blk.11.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 128/ 311]            blk.11.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 129/ 311]                 blk.11.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 130/ 311]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 131/ 311]                 blk.11.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 132/ 311]               blk.11.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 133/ 311]               blk.11.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 134/ 311]               blk.11.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 135/ 311]                 blk.11.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 136/ 311]                 blk.12.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 137/ 311]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 138/ 311]              blk.12.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 139/ 311]            blk.12.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 140/ 311]                 blk.12.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 141/ 311]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 142/ 311]                 blk.12.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 143/ 311]               blk.12.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 144/ 311]               blk.12.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 145/ 311]               blk.12.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 146/ 311]                 blk.12.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 147/ 311]                 blk.13.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 148/ 311]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 149/ 311]              blk.13.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 150/ 311]            blk.13.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 151/ 311]                 blk.13.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 152/ 311]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 153/ 311]                 blk.13.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 154/ 311]               blk.13.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 155/ 311]               blk.13.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 156/ 311]               blk.13.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 157/ 311]                 blk.13.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 158/ 311]                 blk.14.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 159/ 311]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 160/ 311]              blk.14.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 161/ 311]            blk.14.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 162/ 311]                 blk.14.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 163/ 311]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 164/ 311]                 blk.14.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 165/ 311]               blk.14.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 166/ 311]               blk.14.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 167/ 311]               blk.14.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 168/ 311]                 blk.14.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 169/ 311]                 blk.15.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 170/ 311]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 171/ 311]              blk.15.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 172/ 311]            blk.15.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 173/ 311]                 blk.15.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 174/ 311]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 175/ 311]                 blk.15.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 176/ 311]               blk.15.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 177/ 311]               blk.15.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 178/ 311]               blk.15.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 179/ 311]                 blk.15.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 180/ 311]                 blk.16.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 181/ 311]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 182/ 311]              blk.16.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 183/ 311]            blk.16.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 184/ 311]                 blk.16.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 185/ 311]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 186/ 311]                 blk.16.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 187/ 311]               blk.16.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 188/ 311]               blk.16.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 189/ 311]               blk.16.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 190/ 311]                 blk.16.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 191/ 311]                 blk.17.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 192/ 311]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 193/ 311]              blk.17.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 194/ 311]            blk.17.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 195/ 311]                 blk.17.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 196/ 311]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 197/ 311]                 blk.17.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 198/ 311]               blk.17.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 199/ 311]               blk.17.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 200/ 311]               blk.17.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 201/ 311]                 blk.17.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 202/ 311]                 blk.18.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 203/ 311]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 204/ 311]              blk.18.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 205/ 311]            blk.18.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 206/ 311]                 blk.18.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 207/ 311]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 208/ 311]                 blk.18.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 209/ 311]               blk.18.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 210/ 311]               blk.18.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 211/ 311]               blk.18.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 212/ 311]                 blk.18.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 213/ 311]                 blk.19.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 214/ 311]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 215/ 311]              blk.19.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 216/ 311]            blk.19.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 217/ 311]                 blk.19.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 218/ 311]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 219/ 311]                 blk.19.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 220/ 311]               blk.19.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 221/ 311]               blk.19.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 222/ 311]               blk.19.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 223/ 311]                 blk.19.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 224/ 311]                 blk.20.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 225/ 311]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 226/ 311]              blk.20.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 227/ 311]            blk.20.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 228/ 311]                 blk.20.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 229/ 311]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 230/ 311]                 blk.20.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 231/ 311]               blk.20.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 232/ 311]               blk.20.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 233/ 311]               blk.20.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 234/ 311]                 blk.20.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 235/ 311]                 blk.21.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 236/ 311]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 237/ 311]              blk.21.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 238/ 311]            blk.21.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 239/ 311]                 blk.21.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 240/ 311]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 241/ 311]                 blk.21.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 242/ 311]               blk.21.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 243/ 311]               blk.21.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 244/ 311]               blk.21.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 245/ 311]                 blk.21.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 246/ 311]                 blk.22.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 247/ 311]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 248/ 311]              blk.22.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 249/ 311]            blk.22.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 250/ 311]                 blk.22.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 251/ 311]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 252/ 311]                 blk.22.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 253/ 311]               blk.22.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 254/ 311]               blk.22.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 255/ 311]               blk.22.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 256/ 311]                 blk.22.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 257/ 311]                 blk.23.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 258/ 311]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 259/ 311]              blk.23.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 260/ 311]            blk.23.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 261/ 311]                 blk.23.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 262/ 311]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 263/ 311]                 blk.23.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 264/ 311]               blk.23.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 265/ 311]               blk.23.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 266/ 311]               blk.23.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 267/ 311]                 blk.23.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 268/ 311]                 blk.24.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 269/ 311]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 270/ 311]              blk.24.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 271/ 311]            blk.24.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 272/ 311]                 blk.24.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 273/ 311]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 274/ 311]                 blk.24.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 275/ 311]               blk.24.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 276/ 311]               blk.24.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 277/ 311]               blk.24.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 278/ 311]                 blk.24.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 279/ 311]                 blk.25.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 280/ 311]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 281/ 311]              blk.25.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 282/ 311]            blk.25.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 283/ 311]                 blk.25.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 284/ 311]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 285/ 311]                 blk.25.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 286/ 311]               blk.25.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 287/ 311]               blk.25.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 288/ 311]               blk.25.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 289/ 311]                 blk.25.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 290/ 311]                 blk.26.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 291/ 311]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 292/ 311]              blk.26.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 293/ 311]            blk.26.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 294/ 311]                 blk.26.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 295/ 311]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 296/ 311]                 blk.26.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 297/ 311]               blk.26.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 298/ 311]               blk.26.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 299/ 311]               blk.26.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 300/ 311]                 blk.26.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 301/ 311]                 blk.27.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB\n[ 302/ 311]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 303/ 311]              blk.27.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 304/ 311]            blk.27.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 305/ 311]                 blk.27.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB\n[ 306/ 311]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n[ 307/ 311]                 blk.27.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB\n[ 308/ 311]               blk.27.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB\n[ 309/ 311]               blk.27.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\n[ 310/ 311]               blk.27.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n[ 311/ 311]                 blk.27.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB\nllama_model_quantize_impl: model size  =  1433.75 MiB\nllama_model_quantize_impl: quant size  =   456.11 MiB\n\nmain: quantize time = 17247.52 ms\nmain:    total time = 17247.52 ms",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#calling-llama.cpp-from-apps-1",
    "href": "src/05/slides.html#calling-llama.cpp-from-apps-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Calling llama.cpp from Apps",
    "text": "Calling llama.cpp from Apps\n\nThree ways to interact with a local GGUF model:\n\nUse llama.cpp to serve the model via a local web server\n\n\nEasier to get running, but the overhead of a local web server\n\n\nIf your app is C++, call llama.cpp library directly\n\n\nIf your app is not C++, use llama.cpp bindings\n\n(2 and 3 are more challenging to get setup, but no web server and better performance)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#calling-llama.cpp-from-unreal-engine-1",
    "href": "src/05/slides.html#calling-llama.cpp-from-unreal-engine-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Calling llama.cpp from Unreal Engine",
    "text": "Calling llama.cpp from Unreal Engine\n\nC++, so can call llama.cpp directly\n\nBuild the llama.cpp libraries for your platform\nCreate and compile an Unreal plug-in\nCreate a Blueprint that calls the plug-in\nIntegrate within scene",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#llama.cpp-bindings-1",
    "href": "src/05/slides.html#llama.cpp-bindings-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "llama.cpp Bindings",
    "text": "llama.cpp Bindings",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#what-are-llama.cpp-bindings",
    "href": "src/05/slides.html#what-are-llama.cpp-bindings",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "What Are llama.cpp Bindings?",
    "text": "What Are llama.cpp Bindings?\n\nWrappers around the llama.cpp C/C++ libraries\nEnable you to use models via the language/platform of your choice\nWhat languages/platforms are supported?\n\nhttps://github.com/ggml-org/llama.cpp#description\nClick on the Bindings drop-down",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#python-binding",
    "href": "src/05/slides.html#python-binding",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Python Binding",
    "text": "Python Binding\n\nllama-cpp-python\nTBD: Overview",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#c-binding",
    "href": "src/05/slides.html#c-binding",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "C# Binding",
    "text": "C# Binding\n\nhttps://github.com/SciSharp/LLamaSharp\nTBD: Overview",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#using-the-c-binding-in-unity",
    "href": "src/05/slides.html#using-the-c-binding-in-unity",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Using the C# Binding in Unity",
    "text": "Using the C# Binding in Unity",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#demo-c-client---gemma-3-27b-local",
    "href": "src/05/slides.html#demo-c-client---gemma-3-27b-local",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Demo: C# Client <-> Gemma 3 27B Local",
    "text": "Demo: C# Client &lt;-&gt; Gemma 3 27B Local\ndemos/01/lmstudio-client/LMStudioClient.csproj",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#hosting-models-in-unity",
    "href": "src/05/slides.html#hosting-models-in-unity",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Hosting Models in Unity",
    "text": "Hosting Models in Unity\n\nDownload the GGUF model locally to Assets/StreamingAssets folder\nUse llama.cpp bindings for C# to host\n\nLLAMASharp: https://github.com/SciSharp/LLamaSharp\n\nUse OpenAI SDK (or similar) as client\nUnity Demo\n\nhttps://github.com/eublefar/LLAMASharpUnityDemo",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#looking-ahead-1",
    "href": "src/05/slides.html#looking-ahead-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis weeks assignment!\nTBD",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#references-1",
    "href": "src/05/slides.html#references-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/resources.html",
    "href": "src/06/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Resources"
    ]
  },
  {
    "objectID": "src/06/resources.html#citations",
    "href": "src/06/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Resources"
    ]
  },
  {
    "objectID": "src/07/assignment.html",
    "href": "src/07/assignment.html",
    "title": "Module 7 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/07/assignment.html#assignment",
    "href": "src/07/assignment.html#assignment",
    "title": "Module 7 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/07/slides.html#recap",
    "href": "src/07/slides.html#recap",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "Recap",
    "text": "Recap\n\nUnderstood model training, dataset curation, what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy\nExplored use cases, advantages, and disadvantages of prompt engineering\nIntroduced and implemented RAG (Retrieval Augmented Generation) to increase the accuracy of a limited SLM\nStarted the exploration of how to fine-tune models using LoRA (Low Ranked Adaptation)\nUsed a foundational model to generate synthetic data for fine-tuning a 1B parameter model",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/slides.html#lesson-objectives",
    "href": "src/07/slides.html#lesson-objectives",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUse generated synthetic data to fine-tune a 1B parameter model\nUse W&B (Weights and Biases) to observe parameters during the training run\nPost-training, use W&B to use cosine similarity and LLM-as-a-Judge to evaluate the accuracy of our trained model\nTrain smaller models (270M parameters) and compare the results\nUnderstand and create a model card, upload the model to Hugging Face and share",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/slides.html#looking-ahead-1",
    "href": "src/07/slides.html#looking-ahead-1",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis weeks assignment!\nTBD",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/slides.html#references-1",
    "href": "src/07/slides.html#references-1",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/resources.html",
    "href": "src/08/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Resources"
    ]
  },
  {
    "objectID": "src/08/resources.html#citations",
    "href": "src/08/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/notebooks/GPT-2.html",
    "href": "src/01/notebooks/GPT-2.html",
    "title": "Pre-trained GPT-2 Notebook",
    "section": "",
    "text": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load pre-trained GPT-2 model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n# Set pad token\ntokenizer.pad_token = tokenizer.eos_token\n\n\nimport torch\n\ndef autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n    # Encode the prompt with attention mask\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    # Generate continuation\n    with torch.no_grad():\n        output = model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n\n\nprompt = \"Mary had a little lamb\"\ncompletion = autocomplete(prompt, max_length=80)\nprint(completion)\n\nMary had a little lamb, and the young woman asked her for a little lamb, and they gave it to her.\n\n\"Oh, my child, it is good to have a little lamb,\" said he, \"but it is not to be bought, for it is hard to make, and it is much more difficult to make.\n\n\"When you have a little lamb, it\n\n\n\nprompts = [\n    \"Mary had a little lamb\",\n    \"The future of artificial intelligence\",\n    \"In a galaxy far, far away\",\n    \"DigiPen is a place where\",\n    \"def calculate_fibonacci(n):\"\n]\n\nfor prompt in prompts:\n    print(f\"\\nPrompt: {prompt}\")\n    print(\"-\" * 50)\n    completion = autocomplete(prompt, max_length=80)\n    print(f\"Output: {completion}\\n\")\n\n\nPrompt: Mary had a little lamb\n--------------------------------------------------\nOutput: Mary had a little lamb, and the child was very hungry, and so he took a small lamb and brought it to her, and she and the child were very merry. So the child went home and the lamb was brought to her. So she and the child went to the priest and he gave her a piece of bread and said to her, \"This is good bread for you, but what\n\n\nPrompt: The future of artificial intelligence\n--------------------------------------------------\nOutput: The future of artificial intelligence is uncertain, but its future is bright.\n\nAnd so, we are all waiting for a breakthrough.\n\nAnd that's why I think that it's important to understand how AI is coming to the table.\n\nOne of the big questions we have right now is how AI will be able to take over a world, and how it will be able to take\n\n\nPrompt: In a galaxy far, far away\n--------------------------------------------------\nOutput: In a galaxy far, far away, there is only one thing that matters. The fate of our galaxy.\n\nAnd it matters only to you.\n\nA New Frontier for Space\n\nIt's been almost two years since I first wrote a post about this book. And that's because I've been busy.\n\nIn the last month or so, I've been working on an\n\n\nPrompt: DigiPen is a place where\n--------------------------------------------------\nOutput: DigiPen is a place where you can share your creations.\n\nDon't let the name fool you. This is the place to share your creations and to share your creativity.\n\nDon't let the name fool you. This is the place to share your creations and to share your creativity.\n\nDon't let the name fool you. This is the place to share your creations and\n\n\nPrompt: def calculate_fibonacci(n):\n--------------------------------------------------\nOutput: def calculate_fibonacci(n):\n\nfibonacci(n) = 0.01\n\nreturn f(n)\n\ndef calculate_fibonacci(n):\n\nfibonacci(n) = 0.01\n\nreturn f(n)\n\ndef calculate_fibonacci(n):\n\nfibonacci",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "GPT-2.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html",
    "href": "src/01/notebooks/translation-transformer.html",
    "title": "Translation Transformer",
    "section": "",
    "text": "In this notebook, we use a small transformer (Helsinki-NLP/opus-mt-fr-en) to translate from French to English.",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#load-model",
    "href": "src/01/notebooks/translation-transformer.html#load-model",
    "title": "Translation Transformer",
    "section": "Load model",
    "text": "Load model\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"Helsinki-NLP/opus-mt-fr-en\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n\n\n\n\n\n\n\n\n\n/Users/simon/Dev/CS-394/.venv/lib/python3.13/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#tokenize",
    "href": "src/01/notebooks/translation-transformer.html#tokenize",
    "title": "Translation Transformer",
    "section": "Tokenize",
    "text": "Tokenize\n\nfrench_text = \"Bonjour, comment allez-vous?\"\ninput_ids = tokenizer.encode(french_text, return_tensors=\"pt\")\nprint(input_ids[0])\nprint(\"Tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n\ntensor([8703,    2, 1027, 5682,   21,  682,   54,    0])\nTokens: ['Bonjour', ',', 'comment', 'allez', '-', 'vous', '?', '&lt;/s&gt;']\n\n\n\n# @title Demonstrate contextual vectors using the encoder\n\n# French: \"Bonjour , comment allez  - vous  ?\"\n#                                     \n# Encoder: [v1]   [v2] [v3]  [v4] [v5][v6][v7]   7 vectors, each 512-dim\n#          \n\nencoder = model.get_encoder()\nencoder_output = encoder(input_ids)\nprint(\"Encoder output shape:\", encoder_output.last_hidden_state.shape)\nprint(\"Encoder output:\", encoder_output)\n\nEncoder output shape: torch.Size([1, 8, 512])\nEncoder output: BaseModelOutput(last_hidden_state=tensor([[[-0.3943,  0.4660,  0.0190,  ..., -0.5069,  0.2120, -0.3190],\n         [ 0.0957,  0.0780,  0.1918,  ..., -0.0854,  0.2138,  0.1528],\n         [-0.6160,  0.0295,  0.1918,  ..., -0.3886,  0.0770,  0.2311],\n         ...,\n         [-0.1839, -0.3798,  0.1832,  ..., -0.0041, -0.3633, -0.5455],\n         [ 0.0153,  0.0264,  0.1122,  ...,  0.1966, -0.3027, -0.3659],\n         [-0.0484,  0.0147,  0.0078,  ..., -0.1359, -0.0295, -0.0799]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), hidden_states=None, attentions=None)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#run-through-tokenizer",
    "href": "src/01/notebooks/translation-transformer.html#run-through-tokenizer",
    "title": "Translation Transformer",
    "section": "Run through tokenizer",
    "text": "Run through tokenizer\n\noutput_ids = model.generate(input_ids)\nprint(output_ids)\n\ntensor([[59513, 10537,     2,   541,    52,    55,    54,     0]])",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#decode-back-to-tokens-to-complete-the-translation",
    "href": "src/01/notebooks/translation-transformer.html#decode-back-to-tokens-to-complete-the-translation",
    "title": "Translation Transformer",
    "section": "Decode back to tokens to complete the translation",
    "text": "Decode back to tokens to complete the translation\n\nenglish_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(\"Translation:\", english_text)\n\nTranslation: Hello, how are you?",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openai.html#or-grab-the-openai-api-key-if-running-locally",
    "href": "src/02/notebooks/chat-completion-openai.html#or-grab-the-openai-api-key-if-running-locally",
    "title": "Chat Completion API (via OpenAI)",
    "section": "(Or grab the OpenAI API key if running locally)",
    "text": "(Or grab the OpenAI API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openai.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openai.html#logging-function-to-print-the-api-request-to-the-console",
    "href": "src/02/notebooks/chat-completion-openai.html#logging-function-to-print-the-api-request-to-the-console",
    "title": "Chat Completion API (via OpenAI)",
    "section": "Logging function to print the API request to the console",
    "text": "Logging function to print the API request to the console\n\nimport json\n\ndef log_request(request):\n  print(\"\\n=== REQUEST ===\")\n  print(f\"URL: {request.url}\")\n  print(f\"Method: {request.method}\")\n\n  if request.content:\n    try:\n      body = json.loads(request.content.decode('utf-8'))\n      print(\"\\nBody:\")\n      print(json.dumps(body, indent=2))\n    except:\n      print(\"\\nBody:\")\n      print(request.content.decode('utf-8'))\n  print(\"=\" * 50)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openai.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openai.html#call-openai-via-the-sdk",
    "href": "src/02/notebooks/chat-completion-openai.html#call-openai-via-the-sdk",
    "title": "Chat Completion API (via OpenAI)",
    "section": "Call OpenAI via the SDK",
    "text": "Call OpenAI via the SDK\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    api_key=OPENAI_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-5\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://api.openai.com/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"gpt-5\"\n}\n==================================================\n\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"chatcmpl-CuVn7EYuGJUEUEQ18Cl0SM2nNz9Mj\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Awesome! I can tailor a plan, but a few quick questions help:\\n- When are you going and for how many days?\\n- First time in Paris?\\n- Main interests (art, food, fashion, history, photography, nightlife, kid-friendly, etc.) and preferred pace (relaxed vs. packed)?\\n- Any must-sees or hard nos?\\n- Rough budget and food needs (vegetarian, kosher/halal, allergies)?\\n- Where are you staying (neighborhood) and are day trips okay (Versailles, Champagne, Giverny, Disneyland)?\\n\\nIf you want a quick starter plan, heres a flexible 4-day outline you can reshuffle by weather and museum closures:\\n\\nDay 1  Islands + Latin Quarter\\n- le de la Cit: Notre-Dame exterior, Sainte-Chapelle (timed ticket), Conciergerie.\\n- Stroll the Latin Quarter: Shakespeare & Company, Sorbonne, Luxembourg Gardens.\\n- Evening: Seine cruise or sunset along the river.\\n\\nDay 2  Louvre to Arc de Triomphe\\n- Morning: Louvre (timed entry). Tuileries and Palais-Royal gardens.\\n- Covered passages (Vronique/Grand Cerf/Jouffroy) and Opra Garnier.\\n- Sunset view: Arc de Triomphe rooftop or Galeries Lafayette/Printemps terrace.\\n\\nDay 3  Montmartre + Left Bank art\\n- Montmartre: Sacr-Cur, Place du Tertre, quieter backstreets (Rue de lAbreuvoir).\\n- Afternoon: Muse dOrsay and/or Orangerie.\\n- Evening: Saint-Germain wine bar or jazz.\\n\\nDay 4  Le Marais or Day Trip\\n- Marais walk: Place des Vosges, Muse Carnavalet, Picasso Museum (check hours), Jewish quarter, trendy boutiques.\\n- Optional day trip: Versailles (palace + gardens; get the timed passport ticket).\\n- Night: Eiffel Tower area (view from Trocadro or Champ de Mars; book tower tickets if going up).\\n\\nOther great adds by interest\\n- Art/architecture: Rodin Museum; Bourse de Commerce; Fondation Louis Vuitton. Note: check Centre Pompidous renovation status.\\n- Food: Morning market (Aligre or Rue Cler), cheese/wine tasting, pastry crawl, bistro lunch, cooking class.\\n- Unique: Catacombs (book ahead), Pre Lachaise Cemetery, Canal Saint-Martin, covered markets (Le March des Enfants Rouges).\\n- With kids: Jardin des Plantes (zoo + galleries), Cit des Sciences, Jardin dAcclimatation, Parc de la Villette.\\n- Day trips: Giverny (AprOct), Reims/Epernay for Champagne, Fontainebleau, Auvers-sur-Oise, Disneyland Paris.\\n\\nBook these in advance\\n- Eiffel Tower, Louvre, Sainte-Chapelle, Catacombs, Versailles, Palais Garnier tours, popular restaurants.\\n- Consider the Paris Museum Pass (2/4/6 days) if youll visit several museums; the Louvre still needs a timed reservation even with the pass.\\n\\nPractical tips\\n- Closures: Many museums close one day/week (e.g., Orsay Mon, some Tue). Check hours.\\n- Getting around: The Mtro is fastest. Use a contactless bank card to tap in, or get a reloadable Navigo Easy. For a MondaySunday stay with lots of rides, a Navigo Dcouverte weekly pass can be good value.\\n- Dining: Reserve for dinner, especially weekends. Tipping is minimal (service included); round up or leave 510% for great service.\\n- Safety: Watch for pickpockets in crowded areas and on the Metro.\\n\\nShare your dates, length of stay, and interests, and Ill turn this into a detailed day-by-day plan with mapped routes and restaurant picks near each stop.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": [],\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1767584609,\n  \"model\": \"gpt-5-2025-08-07\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": \"default\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 2224,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 2268,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"reasoning_tokens\": 1408,\n      \"rejected_prediction_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0\n    }\n  }\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openai.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#install-required-packages",
    "href": "src/02/notebooks/gradio.html#install-required-packages",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Install required packages",
    "text": "Install required packages\n\n!uv pip install gradio==5.49.1 openai\n\n\nUsing Python 3.13.1 environment at: /Users/simon/Dev/CS-394/.venv\n\nAudited 2 packages in 14ms",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/gradio.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/gradio.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#initialize-the-openai-client",
    "href": "src/02/notebooks/gradio.html#initialize-the-openai-client",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Initialize the OpenAI client",
    "text": "Initialize the OpenAI client\n\nimport openai\n\n# Initialize OpenAI client\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#example-1-basic-gradio-interface",
    "href": "src/02/notebooks/gradio.html#example-1-basic-gradio-interface",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Example 1: Basic Gradio interface",
    "text": "Example 1: Basic Gradio interface\n\nimport gradio as gr\n\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\n\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\")\ndemo.launch()\n\n* Running on local URL:  http://127.0.0.1:7862\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#example-2-basic-chat-interface-with-conversation-history",
    "href": "src/02/notebooks/gradio.html#example-2-basic-chat-interface-with-conversation-history",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Example 2: Basic chat interface with conversation history",
    "text": "Example 2: Basic chat interface with conversation history\n\nimport gradio as gr\n\ndef chat_with_history(message, history):\n    # Add current message\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Get response from API\n    response = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n    )\n    \n    return response.choices[0].message.content\n\n# Create a chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_history,\n    title=\"Basic Chat with Conversation History\",\n    type=\"messages\"\n)\n\ndemo.launch()\n\n* Running on local URL:  http://127.0.0.1:7863\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#example-3-streaming-chat-interface",
    "href": "src/02/notebooks/gradio.html#example-3-streaming-chat-interface",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Example 3: Streaming chat interface",
    "text": "Example 3: Streaming chat interface\n\nimport gradio as gr\n\ndef chat_with_streaming(message, history):\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Stream the response\n    stream = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n        stream=True,\n    )\n    \n    response_text = \"\"\n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            token = chunk.choices[0].delta.content\n            response_text += token\n            yield response_text\n\n# Create streaming chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_streaming,\n    title=\"AI Chat with Streaming\",\n    type=\"messages\"\n)\n\ndemo.launch()\n\n* Running on local URL:  http://127.0.0.1:7864\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/structured-outputs.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Structured Outputs",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/structured-outputs.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Structured Outputs",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#create-the-openai-client-with-openrouter-url",
    "href": "src/02/notebooks/structured-outputs.html#create-the-openai-client-with-openrouter-url",
    "title": "Structured Outputs",
    "section": "Create the OpenAI client with OpenRouter URL",
    "text": "Create the OpenAI client with OpenRouter URL\n\nimport openai\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#create-the-location-type",
    "href": "src/02/notebooks/structured-outputs.html#create-the-location-type",
    "title": "Structured Outputs",
    "section": "Create the Location type",
    "text": "Create the Location type\n\nfrom pydantic import BaseModel\n\n# Define the model for a geographic location\nclass Location(BaseModel):\n  name: str\n  country: str\n  latitude: float\n  longitude: float",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#send-request-with-response_format-set",
    "href": "src/02/notebooks/structured-outputs.html#send-request-with-response_format-set",
    "title": "Structured Outputs",
    "section": "Send request with response_format set",
    "text": "Send request with response_format set\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.parse(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"What are the GPS coordinates for Paris?\"},\n    ],\n    response_format=Location\n)\n\ncompletion = response.choices[0].message\nprint(completion)\n\nParsedChatCompletionMessage[Location](content='{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=Location(name='Paris', country='France', latitude=48.8566, longitude=2.3522), reasoning=None)\n\n\n\n# Display the JSON repesentation\nprint(completion.content)\n\n# Display the parsed type\nprint(completion.parsed)\n\n# Pretty-print\nif completion.parsed:\n  location: Location = completion.parsed\n  print(f\"{location.name}, {location.country} has GPS coordinates of {location.latitude}, {location.longitude}\")\n\n{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}\nname='Paris' country='France' latitude=48.8566 longitude=2.3522\nParis, France has GPS coordinates of 48.8566, 2.3522",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#install-pre-requisites",
    "href": "src/03/notebooks/campus-agent.html#install-pre-requisites",
    "title": "DigiPen Campus Agent",
    "section": "Install pre-requisites",
    "text": "Install pre-requisites\n\n!uv pip install openai-agents==0.4.2 gradio==5.49.1\n\n\nResolved 190 packages in 1ms\n\nInstalled 10 packages in 33ms12.0                           \n\n + colorama==0.4.6\n\n + cryptography==46.0.3\n\n + griffe==1.15.0\n\n + httpx-sse==0.4.3\n\n + mcp==1.25.0\n\n + openai-agents==0.4.2\n\n + pydantic-settings==2.12.0\n\n + pyjwt==2.10.1\n\n + sse-starlette==3.1.2\n\n + types-requests==2.32.4.20260107",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#set-the-openai-api-key-environment-variable",
    "href": "src/03/notebooks/campus-agent.html#set-the-openai-api-key-environment-variable",
    "title": "DigiPen Campus Agent",
    "section": "Set the OpenAI API Key Environment Variable",
    "text": "Set the OpenAI API Key Environment Variable\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nelse:\n  load_dotenv()",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#agents",
    "href": "src/03/notebooks/campus-agent.html#agents",
    "title": "DigiPen Campus Agent",
    "section": "Agents",
    "text": "Agents\n\nfrom agents import Agent, Runner, FileSearchTool\n\n\nCafe Agent\n\nfrom agents import function_tool\n\n@function_tool\ndef get_bytes_cafe_menu(date: str) -&gt; any:\n    return {\n        f\"{date}\": {\n            \"daily byte\": {\n                \"name\": \"Steak Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Flank steak, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"vegetarian\": {\n                \"name\": \"Impossible Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Impossible plant based product, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"international\": {\n                \"name\": \"Chicken Curry\",\n                \"price\": 12,\n                \"description\": \"Chicken thighs, onion, carrot, potato, curry sauce served over rice\",\n            },\n        }\n    }\n\n\ncafe_agent = Agent(\n    name=\"Cafe Agent\",\n    instructions=\"You help students locate and provide information about the Bytes Cafe.\",\n    tools=[\n        get_bytes_cafe_menu,\n    ],\n)\n\n\n\nSet Vector Store\n\nVECTOR_STORE_ID = \"vs_6896d8c959008191981d645850b42313\"\n\n\n\nBuilding Agent\n\nbuilding_agent = Agent(\n    name=\"Building Agent\",\n    instructions=\"You help students locate and provide information about buildings and rooms on campus. Be descriptive when giving locations.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=3,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)\n\n\n\nCourse Agent\n\ncourse_agent = Agent(\n    name=\"Course Agent\",\n    instructions=\"You help students find out information about courses held at DigiPen.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)\n\n\n\nHandbook Agent\n\nhandbook_agent = Agent(\n    name=\"Handbook Agent\",\n    instructions=\"You help students navigate the school handbook, providing information about campus policies and student conduct.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)\n\n\n\nCampus Agent\n\nagent = Agent(\n    name=\"DigiPen Campus Agent\",\n    instructions=\"You are a helpful campus agent that can plan and execute tasks for students at DigiPen. Please be concise and accurate in handing off tasks to other agents as needed.\",\n    handoffs=[building_agent, course_agent, handbook_agent, cafe_agent],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#gradio-interface",
    "href": "src/03/notebooks/campus-agent.html#gradio-interface",
    "title": "DigiPen Campus Agent",
    "section": "Gradio Interface",
    "text": "Gradio Interface\n\nChat function and display of tool calls\n\nfrom gradio import ChatMessage\n\nasync def chat_with_agent(user_msg: str, history: list):\n    messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in history]\n    messages.append({\"role\": \"user\", \"content\": user_msg})\n    responses = []\n    reply_created = False\n    active_agent = None\n\n    result = Runner.run_streamed(agent, messages)\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\":\n            if event.data.type == \"response.output_text.delta\":\n                if not reply_created:\n                    responses.append(ChatMessage(role=\"assistant\", content=\"\"))\n                    reply_created = True\n                responses[-1].content += event.data.delta\n        elif event.type == \"agent_updated_stream_event\":\n            active_agent = event.new_agent.name\n            responses.append(\n                ChatMessage(\n                    content=event.new_agent.name,\n                    metadata={\"title\": \"Agent Now Running\", \"id\": active_agent},\n                )\n            )\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                if event.item.raw_item.type == \"file_search_call\":\n                    responses.append(\n                        ChatMessage(\n                            content=f\"Query used: {event.item.raw_item.queries}\",\n                            metadata={\n                                \"title\": \"File Search Completed\",\n                                \"parent_id\": active_agent,\n                            },\n                        )\n                    )\n                else:\n                    tool_name = getattr(event.item.raw_item, \"name\", \"unknown_tool\")\n                    tool_args = getattr(event.item.raw_item, \"arguments\", {})\n                    responses.append(\n                        ChatMessage(\n                            content=f\"Calling tool {tool_name} with arguments {tool_args}\",\n                            metadata={\"title\": \"Tool Call\", \"parent_id\": active_agent},\n                        )\n                    )\n            if event.item.type == \"tool_call_output_item\":\n                responses.append(\n                    ChatMessage(\n                        content=f\"Tool output: '{event.item.raw_item['output']}'\",\n                        metadata={\"title\": \"Tool Output\", \"parent_id\": active_agent},\n                    )\n                )\n            if event.item.type == \"handoff_call_item\":\n                responses.append(\n                    ChatMessage(\n                        content=f\"Name: {event.item.raw_item.name}\",\n                        metadata={\n                            \"title\": \"Handing Off Request\",\n                            \"parent_id\": active_agent,\n                        },\n                    )\n                )\n        yield responses\n\n\n\nLaunch Gradio\n\nimport gradio as gr\n\ndemo = gr.ChatInterface(\n    chat_with_agent,\n    title=\"DigiPen Campus Agent\",\n    theme=gr.themes.Soft(\n        primary_hue=\"red\", secondary_hue=\"slate\", font=[gr.themes.GoogleFont(\"Inter\")]\n    ),\n    examples=[\n        \"I'm trying to find the WANIC classrooms. Can you help?\",\n        \"What's the policy for eating in auditoriums?\",\n        \"What's today's vegetarian dish at the Bytes Cafe?\",\n        \"What are the prerequisites for FLM201?\"\n    ],\n    submit_btn=True,\n    flagging_mode=\"manual\",\n    flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n    type=\"messages\",\n    save_history=False,\n)\n\ndemo.launch(share=False, debug=True)\n\n* Running on local URL:  http://127.0.0.1:7863\n* To create a public link, set `share=True` in `launch()`.\n\n\n\n\n\nKeyboard interruption in main thread... closing server.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/open-meteo-mcp.html#note-to-run-this-notebook-on-colab",
    "href": "src/03/notebooks/open-meteo-mcp.html#note-to-run-this-notebook-on-colab",
    "title": "OpenAI Agents SDK with MCP Server",
    "section": "NOTE: To run this notebook on Colab",
    "text": "NOTE: To run this notebook on Colab\n\nRun the pre-requisites cell below to install libraries and NodeJS\nOpen up the Terminal (bottom left of window in Colab)\nRun the following command: TRANSPORT=http npx -y open-meteo-mcp-server - this will start the MCP Server\nRun the rest of the cells",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "open-meteo-mcp.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/open-meteo-mcp.html#install-pre-requisites-updated-openai-agents-sdk-and-nodejs",
    "href": "src/03/notebooks/open-meteo-mcp.html#install-pre-requisites-updated-openai-agents-sdk-and-nodejs",
    "title": "OpenAI Agents SDK with MCP Server",
    "section": "Install pre-requisites (Updated OpenAI Agents SDK and NodeJS)",
    "text": "Install pre-requisites (Updated OpenAI Agents SDK and NodeJS)\n\n!uv pip install -q openai-agents==0.7.0\n!curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "open-meteo-mcp.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/open-meteo-mcp.html#set-the-openai-api-key-environment-variable",
    "href": "src/03/notebooks/open-meteo-mcp.html#set-the-openai-api-key-environment-variable",
    "title": "OpenAI Agents SDK with MCP Server",
    "section": "Set the OpenAI API Key environment variable",
    "text": "Set the OpenAI API Key environment variable\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nelse:\n  load_dotenv()",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "open-meteo-mcp.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/open-meteo-mcp.html#mcp-server-setup",
    "href": "src/03/notebooks/open-meteo-mcp.html#mcp-server-setup",
    "title": "OpenAI Agents SDK with MCP Server",
    "section": "MCP Server Setup",
    "text": "MCP Server Setup\n\nimport logging\n\n# Suppress MCP client warnings about non-JSON messages from the server\nlogging.getLogger(\"mcp\").setLevel(logging.CRITICAL)\n\nfrom agents import Agent, Runner\nfrom agents.mcp import MCPServerStdio, MCPServerStreamableHttp\n\n\nList Available Tools\n\ntry:\n  async with MCPServerStreamableHttp(\n      params = {\"url\": \"http://localhost:3000/mcp\"}\n      ) as server:\n    tools = await server.list_tools()\n    print(f\"Available tools: {[tool.name for tool in tools]}\")\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nAvailable tools: ['weather_forecast', 'weather_archive', 'air_quality', 'marine_weather', 'elevation', 'flood_forecast', 'seasonal_forecast', 'climate_projection', 'ensemble_forecast', 'geocoding', 'dwd_icon_forecast', 'gfs_forecast', 'meteofrance_forecast', 'ecmwf_forecast', 'jma_forecast', 'metno_forecast', 'gem_forecast']\n\n\n\n\nSimple Weather Query\n\ntry:\n  async with MCPServerStreamableHttp(\n    params = {\"url\": \"http://localhost:3000/mcp\"}\n    ) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n    result = await Runner.run(agent, \"What's the weather forecast for MinneapolisSt. Paul this week?\")\n    print(result.final_output)\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nMinneapolisSt. Paul (Twin Cities) forecast for the next 7 days (America/Chicago):\n\n- **Fri Jan 23:** Partly cloudy. **High -8F / Low -20F**. Precip **0**. Wind up to **12 mph**.  \n- **Sat Jan 24:** Partly cloudy. **High 0F / Low -16F**. Precip **0**. Wind up to **6 mph**.  \n- **Sun Jan 25:** Partly cloudy. **High 8F / Low -7F**. Precip **0**. Wind up to **9 mph**.  \n- **Mon Jan 26:** Partly cloudy. **High 13F / Low -7F**. Precip **0**. Wind up to **9 mph**.  \n- **Tue Jan 27:** Partly cloudy. **High 12F / Low 3F**. Precip **0**. Wind up to **12 mph**.  \n- **Wed Jan 28:** Partly cloudy. **High 8F / Low 0F**. Precip **0**. Wind up to **8 mph**.  \n- **Thu Jan 29:** Partly cloudy. **High 10F / Low 2F**. Precip **0**. Wind up to **5 mph**.\n\nOverall: **cold, mostly partly cloudy, and dry all week** (no measurable precipitation in the forecast).\n\n\n\n\nAir Quality Query\n\ntry:\n  async with MCPServerStreamableHttp(\n    params = {\"url\": \"http://localhost:3000/mcp\"}\n    ) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n\n    result = await Runner.run(agent, \"What's the current air quality in Los Angeles?\")\n    print(result.final_output)\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nLos Angeles air quality (latest available hour in the model: **2026-01-23 00:00**, America/Los_Angeles):\n\n- **PM2.5:** 48.4 g/m  \n- **PM10:** 50.1 g/m  \n- **Ozone (O):** 0 g/m  \n- **Nitrogen dioxide (NO):** 71.5 g/m  \n- **Carbon monoxide (CO):** 896 g/m  \n- **Sulphur dioxide (SO):** 18.8 g/m  \n\nIf you tell me your nearest neighborhood (e.g., Downtown, Hollywood, Santa Monica), I can check a more location-specific point.\n\n\n\n\nHistorical Weather Data\n\ntry:\n  async with MCPServerStreamableHttp(\n    params = {\"url\": \"http://localhost:3000/mcp\"}\n    ) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n\n    result = await Runner.run(agent, \"What was the average temperature in Tokyo during January 2024?\")\n    print(result.final_output)\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nUsing ERA5 reanalysis for **Tokyo (35.6895, 139.6917)**, the **average temperature in January 2024** (computed as the mean of each days \\((T_\\max + T_\\min)/2\\) over Jan 131) was:\n\n** 6.1C**.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "open-meteo-mcp.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/depth-map-as-context.html#install-dependencies",
    "href": "src/04/notebooks/depth-map-as-context.html#install-dependencies",
    "title": "Depth Map as Context",
    "section": "Install Dependencies",
    "text": "Install Dependencies\n\n!uv pip install replicate -q",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "depth-map-as-context.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/depth-map-as-context.html#set-replicate-api-token",
    "href": "src/04/notebooks/depth-map-as-context.html#set-replicate-api-token",
    "title": "Depth Map as Context",
    "section": "Set Replicate API Token",
    "text": "Set Replicate API Token\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  os.environ['REPLICATE_API_TOKEN'] = userdata.get('REPLICATE_API_TOKEN')\n  print(\"Replicate API Token set for Colab\")\nelse:\n  load_dotenv()\n  print(\"Loaded env vars from .env\")",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "depth-map-as-context.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/depth-map-as-context.html#control-image",
    "href": "src/04/notebooks/depth-map-as-context.html#control-image",
    "title": "Depth Map as Context",
    "section": "Control Image",
    "text": "Control Image\n\nfrom IPython.display import Image\n\nCONTROL_IMAGE = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/campus-depth.png\"\n\nImage(url=CONTROL_IMAGE)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "depth-map-as-context.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/depth-map-as-context.html#image-to-image-with-guiding-prompt-using-flux-depth-pro",
    "href": "src/04/notebooks/depth-map-as-context.html#image-to-image-with-guiding-prompt-using-flux-depth-pro",
    "title": "Depth Map as Context",
    "section": "Image-to-Image with guiding prompt using Flux Depth Pro",
    "text": "Image-to-Image with guiding prompt using Flux Depth Pro\n\nimport replicate\n\nMODEL = \"black-forest-labs/flux-depth-pro\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"control_image\": CONTROL_IMAGE,\n      \"output_format\": \"png\",\n      \"seed\": 12345,\n      \"prompt\": \"A futuristic building set in 2050, neon lighting, night shot, dynamic\"\n  },\n)\n\n\nimport io\nfrom PIL import Image\n\noutput_bytes = io.BytesIO(output.read())\noutput_image = Image.open(output_bytes)\noutput_image\n\n\n\n\n\n\n\n\n\nimport replicate\n\nMODEL = \"black-forest-labs/flux-depth-pro\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"control_image\": CONTROL_IMAGE,\n      \"output_format\": \"png\",\n      \"seed\": 12345,\n      \"prompt\": \"A historical castle set in medieval England, clear day, partially cloudy sky\"\n  },\n)\n\n\nimport io\nfrom PIL import Image\n\noutput_bytes = io.BytesIO(output.read())\noutput_image = Image.open(output_bytes)\noutput_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "depth-map-as-context.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "",
    "text": "# @title Install dependencies\n!uv pip install -q diffusers[\"torch\"]==0.35.1 transformers==4.56.2 accelerate==1.10.1",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#load-sd-1.5-model-using-pipeline",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#load-sd-1.5-model-using-pipeline",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Load SD 1.5 model using pipeline",
    "text": "Load SD 1.5 model using pipeline\n\nimport torch\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\n# Load the img2img pipeline\nprint(\"Loading Stable Diffusion img2img pipeline...\")\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\nLoading Stable Diffusion img2img pipeline...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeyword arguments {'dtype': torch.float16} are not expected by StableDiffusionImg2ImgPipeline and will be ignored.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#generate-intermediate-images",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#generate-intermediate-images",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Generate Intermediate Images",
    "text": "Generate Intermediate Images\n\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nIMAGE_URL = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/luna.jpg\"\nSEED = 128763\nPROMPT = \"a goldendoodle wearing sunglasses, high quality, detailed\"\nNEGATIVE_PROMPT = \"blurry, low quality, distorted\"\n\nresponse = requests.get(IMAGE_URL)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n# Resize to standard size for faster processing\ninit_image = init_image.resize((512, 712))\n\ndisplay(init_image)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#function-to-generate-using-strength-param",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#function-to-generate-using-strength-param",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Function to generate using strength param",
    "text": "Function to generate using strength param\n\ndef generate_image(strength):\n  return pipe(\n      prompt=PROMPT,\n      negative_prompt=NEGATIVE_PROMPT,\n      image=init_image,\n      strength=strength,\n      guidance_scale=7.5,\n      num_inference_steps=30,\n      generator=torch.Generator().manual_seed(SEED) if SEED != -1 else None,\n  ).images[0]",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.3",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.3",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Strength 0.3",
    "text": "Strength 0.3\n\ndisplay(generate_image(0.3))",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.5",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.5",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Strength 0.5",
    "text": "Strength 0.5\n\ndisplay(generate_image(0.5))",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.7",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.7",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Strength 0.7",
    "text": "Strength 0.7\n\ndisplay(generate_image(0.7))",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.9",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.9",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Strength 0.9",
    "text": "Strength 0.9\n\ndisplay(generate_image(0.9))",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/outpainting.html",
    "href": "src/04/notebooks/outpainting.html",
    "title": "Outpainting using black-forest-labs/flux-fill-pro",
    "section": "",
    "text": "!uv pip install replicate",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "outpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/outpainting.html#set-replicate-api-token",
    "href": "src/04/notebooks/outpainting.html#set-replicate-api-token",
    "title": "Outpainting using black-forest-labs/flux-fill-pro",
    "section": "Set Replicate API Token",
    "text": "Set Replicate API Token\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  os.environ['REPLICATE_API_TOKEN'] = userdata.get('REPLICATE_API_TOKEN')\n  print(\"Replicate API Token set for Colab\")\nelse:\n  load_dotenv()\n  print(\"Loaded env vars from .env\")",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "outpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/outpainting.html#input-image",
    "href": "src/04/notebooks/outpainting.html#input-image",
    "title": "Outpainting using black-forest-labs/flux-fill-pro",
    "section": "Input Image",
    "text": "Input Image\n\nfrom IPython.display import Image\n\nINPUT_IMAGE = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/campus.png\"\n\nImage(url=INPUT_IMAGE)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "outpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/outpainting.html#call-black-forest-labsflux-fill-pro-with-outpainting-parameters",
    "href": "src/04/notebooks/outpainting.html#call-black-forest-labsflux-fill-pro-with-outpainting-parameters",
    "title": "Outpainting using black-forest-labs/flux-fill-pro",
    "section": "Call black-forest-labs/flux-fill-pro with outpainting parameters",
    "text": "Call black-forest-labs/flux-fill-pro with outpainting parameters\n\nimport replicate\n\n# Call the Replicate API\noutput = replicate.run(\n    \"black-forest-labs/flux-fill-pro\",\n    input={\n        \"image\": INPUT_IMAGE,\n        \"prompt\": \"The main building of a technical college, no text\",\n        \"seed\": 123456,\n        \"steps\": 50,\n        \"guidance\": 60,\n        \"outpaint\": \"Zoom out 2x\",\n        \"output_format\": \"jpg\",\n        \"safety_tolerance\": 2,\n        \"prompt_upsampling\": False\n    }\n)\n\n\nimport io\nfrom PIL import Image\n\noutput_bytes = io.BytesIO(output.read())\noutput_image = Image.open(output_bytes)\noutput_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "outpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-replicate.html#install-dependencies",
    "href": "src/04/notebooks/text-to-image-replicate.html#install-dependencies",
    "title": "Text-to-Image using Replicate",
    "section": "Install Dependencies",
    "text": "Install Dependencies\n\n!uv pip install replicate -q",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-replicate.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-replicate.html#set-replicate-api-token",
    "href": "src/04/notebooks/text-to-image-replicate.html#set-replicate-api-token",
    "title": "Text-to-Image using Replicate",
    "section": "Set Replicate API Token",
    "text": "Set Replicate API Token\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  os.environ['REPLICATE_API_TOKEN'] = userdata.get('REPLICATE_API_TOKEN')\n  print(\"Replicate API Token set for Colab\")\nelse:\n  load_dotenv()\n  print(\"Loaded env vars from .env\")",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-replicate.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-replicate.html#text-to-image",
    "href": "src/04/notebooks/text-to-image-replicate.html#text-to-image",
    "title": "Text-to-Image using Replicate",
    "section": "Text-to-Image",
    "text": "Text-to-Image\n\nimport replicate\noutput = replicate.run(\n  \"black-forest-labs/flux-pro\",\n  input={\n      \"steps\": 28,\n      \"prompt\": \"lemon cupcake spelling out the words 'DigiPen' with sparklers, tasty, food photography, dynamic shot\",\n      \"seed\": 1564435,\n      \"output_format\": \"png\",\n      \"safety_tolerance\": 2,\n      \"prompt_upsampling\": False\n  },\n)\n\n\nimport io\nfrom PIL import Image\n\noutput_bytes = io.BytesIO(output.read())\noutput_image = Image.open(output_bytes)\noutput_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-replicate.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/vlm-gemma-3-4b.html",
    "href": "src/04/notebooks/vlm-gemma-3-4b.html",
    "title": "Using Gemma 3 (4B) to identify images",
    "section": "",
    "text": "from transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-4b-it\",\n    device=\"cuda\",\n    dtype=torch.bfloat16\n)\n\n\nfrom IPython.display import Image\n\nIMAGE_URL = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"\n\nImage(url=IMAGE_URL, width=500)\n\n\n\n\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": IMAGE_URL},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"},\n        ],\n    },\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n\nOkay, let's take a look!\n\nBased on the image, the animal on the candy is a **turtle**. You can see the shell pattern clearly. \n\nWould you like to know anything else about these candies?",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "vlm-gemma-3-4b.ipynb"
    ]
  },
  {
    "objectID": "src/05/notebooks/python-llamacpp-binding.html",
    "href": "src/05/notebooks/python-llamacpp-binding.html",
    "title": "Using llama.cpp Binding for Python",
    "section": "",
    "text": "Note: Before running this notebook, you should follow README.md to first download the GGUF model."
  },
  {
    "objectID": "src/05/notebooks/python-llamacpp-binding.html#install-the-llama-cpp-python-binding",
    "href": "src/05/notebooks/python-llamacpp-binding.html#install-the-llama-cpp-python-binding",
    "title": "Using llama.cpp Binding for Python",
    "section": "Install the llama-cpp-python binding",
    "text": "Install the llama-cpp-python binding\n\n!uv pip install llama-cpp-python\n\n\nUsing Python 3.13.1 environment at: /Users/simon/Dev/CS-394/.venv\n\nResolved 6 packages in 110ms                                         \n\nInstalled 2 packages in 4ms3.16                             \n\n + diskcache==5.6.3\n\n + llama-cpp-python==0.3.16"
  },
  {
    "objectID": "src/05/notebooks/python-llamacpp-binding.html#load-the-local-gguf-model",
    "href": "src/05/notebooks/python-llamacpp-binding.html#load-the-local-gguf-model",
    "title": "Using llama.cpp Binding for Python",
    "section": "Load the local gguf model",
    "text": "Load the local gguf model\n\nfrom llama_cpp import Llama\n\nGGUF_MODEL = f\"../code/gguf/gemma-3-1b-it-Q4_K_M.gguf\"\n\nllm = Llama(\n      model_path=GGUF_MODEL,\n      chat_format=\"gemma\"\n)\n\nllama_model_load_from_file_impl: using device Metal (Apple M2 Max) - 79620 MiB free\nllama_model_loader: loaded meta data with 38 key-value pairs and 340 tensors from ../code/gguf/gemma-3-1b-it-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma-3-1B-It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = Gemma-3-1B-It\nllama_model_loader: - kv   5:                       general.quantized_by str              = Unsloth\nllama_model_loader: - kv   6:                         general.size_label str              = 1B\nllama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth\nllama_model_loader: - kv   8:                      gemma3.context_length u32              = 32768\nllama_model_loader: - kv   9:                    gemma3.embedding_length u32              = 1152\nllama_model_loader: - kv  10:                         gemma3.block_count u32              = 26\nllama_model_loader: - kv  11:                 gemma3.feed_forward_length u32              = 6912\nllama_model_loader: - kv  12:                gemma3.attention.head_count u32              = 4\nllama_model_loader: - kv  13:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  15:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  16:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  17:            gemma3.attention.sliding_window u32              = 512\nllama_model_loader: - kv  18:             gemma3.attention.head_count_kv u32              = 1\nllama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,262144]  = [\"&lt;pad&gt;\", \"&lt;eos&gt;\", \"&lt;bos&gt;\", \"&lt;unk&gt;\", ...\nllama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,262144]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 106\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  32:               general.quantization_version u32              = 2\nllama_model_loader: - kv  33:                          general.file_type u32              = 15\nllama_model_loader: - kv  34:                      quantize.imatrix.file str              = gemma-3-1b-it-GGUF/imatrix_unsloth.dat\nllama_model_loader: - kv  35:                   quantize.imatrix.dataset str              = unsloth_calibration_gemma-3-1b-it.txt\nllama_model_loader: - kv  36:             quantize.imatrix.entries_count i32              = 182\nllama_model_loader: - kv  37:              quantize.imatrix.chunks_count i32              = 663\nllama_model_loader: - type  f32:  157 tensors\nllama_model_loader: - type q5_0:  117 tensors\nllama_model_loader: - type q8_0:   14 tensors\nllama_model_loader: - type q4_K:   39 tensors\nllama_model_loader: - type q6_K:   13 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 762.49 MiB (6.40 BPW) \ninit_tokenizer: initializing tokenizer for type 1\nload: control token: 262141 '&lt;unused6239&gt;' is not marked as EOG\nload: control token: 262140 '&lt;unused6238&gt;' is not marked as EOG\nload: control token: 262137 '&lt;unused6235&gt;' is not marked as EOG\nload: control token: 262136 '&lt;unused6234&gt;' is not marked as EOG\nload: control token: 262135 '&lt;unused6233&gt;' is not marked as EOG\nload: control token: 262134 '&lt;unused6232&gt;' is not marked as EOG\nload: control token: 262133 '&lt;unused6231&gt;' is not marked as EOG\nload: control token: 262127 '&lt;unused6225&gt;' is not marked as EOG\nload: control token: 262126 '&lt;unused6224&gt;' is not marked as EOG\nload: control token: 262125 '&lt;unused6223&gt;' is not marked as EOG\nload: control token: 262124 '&lt;unused6222&gt;' is not marked as EOG\nload: control token: 262122 '&lt;unused6220&gt;' is not marked as EOG\nload: control token: 262119 '&lt;unused6217&gt;' is not marked as EOG\nload: control token: 262117 '&lt;unused6215&gt;' is not marked as EOG\nload: control token: 262115 '&lt;unused6213&gt;' is not marked as EOG\nload: control token: 262114 '&lt;unused6212&gt;' is not marked as EOG\nload: control token: 262113 '&lt;unused6211&gt;' is not marked as EOG\nload: control token: 262112 '&lt;unused6210&gt;' is not marked as EOG\nload: control token: 262111 '&lt;unused6209&gt;' is not marked as EOG\nload: control token: 262110 '&lt;unused6208&gt;' is not marked as EOG\nload: control token: 262109 '&lt;unused6207&gt;' is not marked as EOG\nload: control token: 262108 '&lt;unused6206&gt;' is not marked as EOG\nload: control token: 262107 '&lt;unused6205&gt;' is not marked as EOG\nload: control token: 262105 '&lt;unused6203&gt;' is not marked as EOG\nload: control token: 262104 '&lt;unused6202&gt;' is not marked as EOG\nload: control token: 262103 '&lt;unused6201&gt;' is not marked as EOG\nload: control token: 262102 '&lt;unused6200&gt;' is not marked as EOG\nload: control token: 262099 '&lt;unused6197&gt;' is not marked as EOG\nload: control token: 262098 '&lt;unused6196&gt;' is not marked as EOG\nload: control token: 262097 '&lt;unused6195&gt;' is not marked as EOG\nload: control token: 262095 '&lt;unused6193&gt;' is not marked as EOG\nload: control token: 262094 '&lt;unused6192&gt;' is not marked as EOG\nload: control token: 262093 '&lt;unused6191&gt;' is not marked as EOG\nload: control token: 262092 '&lt;unused6190&gt;' is not marked as EOG\nload: control token: 262091 '&lt;unused6189&gt;' is not marked as EOG\nload: control token: 262090 '&lt;unused6188&gt;' is not marked as EOG\nload: control token: 262089 '&lt;unused6187&gt;' is not marked as EOG\nload: control token: 262085 '&lt;unused6183&gt;' is not marked as EOG\nload: control token: 262083 '&lt;unused6181&gt;' is not marked as EOG\nload: control token: 262082 '&lt;unused6180&gt;' is not marked as EOG\nload: control token: 262080 '&lt;unused6178&gt;' is not marked as EOG\nload: control token: 262079 '&lt;unused6177&gt;' is not marked as EOG\nload: control token: 262078 '&lt;unused6176&gt;' is not marked as EOG\nload: control token: 262077 '&lt;unused6175&gt;' is not marked as EOG\nload: control token: 262070 '&lt;unused6168&gt;' is not marked as EOG\nload: control token: 262068 '&lt;unused6166&gt;' is not marked as EOG\nload: control token: 262067 '&lt;unused6165&gt;' is not marked as EOG\nload: control token: 262064 '&lt;unused6162&gt;' is not marked as EOG\nload: control token: 262063 '&lt;unused6161&gt;' is not marked as EOG\nload: control token: 262062 '&lt;unused6160&gt;' is not marked as EOG\nload: control token: 262060 '&lt;unused6158&gt;' is not marked as EOG\nload: control token: 262058 '&lt;unused6156&gt;' is not marked as EOG\nload: control token: 262053 '&lt;unused6151&gt;' is not marked as EOG\nload: control token: 262052 '&lt;unused6150&gt;' is not marked as EOG\nload: control token: 262051 '&lt;unused6149&gt;' is not marked as EOG\nload: control token: 262048 '&lt;unused6146&gt;' is not marked as EOG\nload: control token: 262046 '&lt;unused6144&gt;' is not marked as EOG\nload: control token: 262044 '&lt;unused6142&gt;' is not marked as EOG\nload: control token: 262043 '&lt;unused6141&gt;' is not marked as EOG\nload: control token: 262041 '&lt;unused6139&gt;' is not marked as EOG\nload: control token: 262040 '&lt;unused6138&gt;' is not marked as EOG\nload: control token: 262039 '&lt;unused6137&gt;' is not marked as EOG\nload: control token: 262038 '&lt;unused6136&gt;' is not marked as EOG\nload: control token: 262037 '&lt;unused6135&gt;' is not marked as EOG\nload: control token: 262035 '&lt;unused6133&gt;' is not marked as EOG\nload: control token: 262034 '&lt;unused6132&gt;' is not marked as EOG\nload: control token: 262033 '&lt;unused6131&gt;' is not marked as EOG\nload: control token: 262032 '&lt;unused6130&gt;' is not marked as EOG\nload: control token: 262031 '&lt;unused6129&gt;' is not marked as EOG\nload: control token: 262030 '&lt;unused6128&gt;' is not marked as EOG\nload: control token: 262027 '&lt;unused6125&gt;' is not marked as EOG\nload: control token: 262026 '&lt;unused6124&gt;' is not marked as EOG\nload: control token: 262022 '&lt;unused6120&gt;' is not marked as EOG\nload: control token: 262020 '&lt;unused6118&gt;' is not marked as EOG\nload: control token: 262014 '&lt;unused6112&gt;' is not marked as EOG\nload: control token: 262013 '&lt;unused6111&gt;' is not marked as EOG\nload: control token: 262011 '&lt;unused6109&gt;' is not marked as EOG\nload: control token: 262009 '&lt;unused6107&gt;' is not marked as EOG\nload: control token: 262007 '&lt;unused6105&gt;' is not marked as EOG\nload: control token: 261998 '&lt;unused6096&gt;' is not marked as EOG\nload: control token: 261995 '&lt;unused6093&gt;' is not marked as EOG\nload: control token: 261990 '&lt;unused6088&gt;' is not marked as EOG\nload: control token: 261988 '&lt;unused6086&gt;' is not marked as EOG\nload: control token: 261986 '&lt;unused6084&gt;' is not marked as EOG\nload: control token: 261985 '&lt;unused6083&gt;' is not marked as EOG\nload: control token: 261984 '&lt;unused6082&gt;' is not marked as EOG\nload: control token: 261982 '&lt;unused6080&gt;' is not marked as EOG\nload: control token: 261981 '&lt;unused6079&gt;' is not marked as EOG\nload: control token: 261979 '&lt;unused6077&gt;' is not marked as EOG\nload: control token: 261978 '&lt;unused6076&gt;' is not marked as EOG\nload: control token: 261977 '&lt;unused6075&gt;' is not marked as EOG\nload: control token: 261976 '&lt;unused6074&gt;' is not marked as EOG\nload: control token: 261974 '&lt;unused6072&gt;' is not marked as EOG\nload: control token: 261973 '&lt;unused6071&gt;' is not marked as EOG\nload: control token: 261970 '&lt;unused6068&gt;' is not marked as EOG\nload: control token: 261968 '&lt;unused6066&gt;' is not marked as EOG\nload: control token: 261967 '&lt;unused6065&gt;' is not marked as EOG\nload: control token: 261964 '&lt;unused6062&gt;' is not marked as EOG\nload: control token: 261963 '&lt;unused6061&gt;' is not marked as EOG\nload: control token: 261961 '&lt;unused6059&gt;' is not marked as EOG\nload: control token: 261959 '&lt;unused6057&gt;' is not marked as EOG\nload: control token: 261954 '&lt;unused6052&gt;' is not marked as EOG\nload: control token: 261951 '&lt;unused6049&gt;' is not marked as EOG\nload: control token: 261946 '&lt;unused6044&gt;' is not marked as EOG\nload: control token: 261945 '&lt;unused6043&gt;' is not marked as EOG\nload: control token: 261943 '&lt;unused6041&gt;' is not marked as EOG\nload: control token: 261940 '&lt;unused6038&gt;' is not marked as EOG\nload: control token: 261938 '&lt;unused6036&gt;' is not marked as EOG\nload: control token: 261937 '&lt;unused6035&gt;' is not marked as EOG\nload: control token: 261936 '&lt;unused6034&gt;' is not marked as EOG\nload: control token: 261935 '&lt;unused6033&gt;' is not marked as EOG\nload: control token: 261934 '&lt;unused6032&gt;' is not marked as EOG\nload: control token: 261932 '&lt;unused6030&gt;' is not marked as EOG\nload: control token: 261930 '&lt;unused6028&gt;' is not marked as EOG\nload: control token: 261928 '&lt;unused6026&gt;' is not marked as EOG\nload: control token: 261927 '&lt;unused6025&gt;' is not marked as EOG\nload: control token: 261926 '&lt;unused6024&gt;' is not marked as EOG\nload: control token: 261925 '&lt;unused6023&gt;' is not marked as EOG\nload: control token: 261924 '&lt;unused6022&gt;' is not marked as EOG\nload: control token: 261922 '&lt;unused6020&gt;' is not marked as EOG\nload: control token: 261919 '&lt;unused6017&gt;' is not marked as EOG\nload: control token: 261918 '&lt;unused6016&gt;' is not marked as EOG\nload: control token: 261917 '&lt;unused6015&gt;' is not marked as EOG\nload: control token: 261916 '&lt;unused6014&gt;' is not marked as EOG\nload: control token: 261915 '&lt;unused6013&gt;' is not marked as EOG\nload: control token: 261909 '&lt;unused6007&gt;' is not marked as EOG\nload: control token: 261908 '&lt;unused6006&gt;' is not marked as EOG\nload: control token: 261904 '&lt;unused6002&gt;' is not marked as EOG\nload: control token: 261903 '&lt;unused6001&gt;' is not marked as EOG\nload: control token: 261901 '&lt;unused5999&gt;' is not marked as EOG\nload: control token: 261900 '&lt;unused5998&gt;' is not marked as EOG\nload: control token: 261897 '&lt;unused5995&gt;' is not marked as EOG\nload: control token: 261890 '&lt;unused5988&gt;' is not marked as EOG\nload: control token: 261888 '&lt;unused5986&gt;' is not marked as EOG\nload: control token: 261886 '&lt;unused5984&gt;' is not marked as EOG\nload: control token: 261885 '&lt;unused5983&gt;' is not marked as EOG\nload: control token: 261883 '&lt;unused5981&gt;' is not marked as EOG\nload: control token: 261882 '&lt;unused5980&gt;' is not marked as EOG\nload: control token: 261879 '&lt;unused5977&gt;' is not marked as EOG\nload: control token: 261878 '&lt;unused5976&gt;' is not marked as EOG\nload: control token: 261877 '&lt;unused5975&gt;' is not marked as EOG\nload: control token: 261875 '&lt;unused5973&gt;' is not marked as EOG\nload: control token: 261874 '&lt;unused5972&gt;' is not marked as EOG\nload: control token: 261873 '&lt;unused5971&gt;' is not marked as EOG\nload: control token: 261869 '&lt;unused5967&gt;' is not marked as EOG\nload: control token: 261868 '&lt;unused5966&gt;' is not marked as EOG\nload: control token: 261866 '&lt;unused5964&gt;' is not marked as EOG\nload: control token: 261865 '&lt;unused5963&gt;' is not marked as EOG\nload: control token: 261861 '&lt;unused5959&gt;' is not marked as EOG\nload: control token: 261860 '&lt;unused5958&gt;' is not marked as EOG\nload: control token: 261857 '&lt;unused5955&gt;' is not marked as EOG\nload: control token: 261856 '&lt;unused5954&gt;' is not marked as EOG\nload: control token: 261854 '&lt;unused5952&gt;' is not marked as EOG\nload: control token: 261853 '&lt;unused5951&gt;' is not marked as EOG\nload: control token: 261852 '&lt;unused5950&gt;' is not marked as EOG\nload: control token: 261850 '&lt;unused5948&gt;' is not marked as EOG\nload: control token: 261849 '&lt;unused5947&gt;' is not marked as EOG\nload: control token: 261846 '&lt;unused5944&gt;' is not marked as EOG\nload: control token: 261845 '&lt;unused5943&gt;' is not marked as EOG\nload: control token: 261843 '&lt;unused5941&gt;' is not marked as EOG\nload: control token: 261841 '&lt;unused5939&gt;' is not marked as EOG\nload: control token: 261839 '&lt;unused5937&gt;' is not marked as EOG\nload: control token: 261835 '&lt;unused5933&gt;' is not marked as EOG\nload: control token: 261834 '&lt;unused5932&gt;' is not marked as EOG\nload: control token: 261833 '&lt;unused5931&gt;' is not marked as EOG\nload: control token: 261832 '&lt;unused5930&gt;' is not marked as EOG\nload: control token: 261831 '&lt;unused5929&gt;' is not marked as EOG\nload: control token: 261830 '&lt;unused5928&gt;' is not marked as EOG\nload: control token: 261829 '&lt;unused5927&gt;' is not marked as EOG\nload: control token: 261828 '&lt;unused5926&gt;' is not marked as EOG\nload: control token: 261827 '&lt;unused5925&gt;' is not marked as EOG\nload: control token: 261825 '&lt;unused5923&gt;' is not marked as EOG\nload: control token: 261824 '&lt;unused5922&gt;' is not marked as EOG\nload: control token: 261819 '&lt;unused5917&gt;' is not marked as EOG\nload: control token: 261818 '&lt;unused5916&gt;' is not marked as EOG\nload: control token: 261817 '&lt;unused5915&gt;' is not marked as EOG\nload: control token: 261815 '&lt;unused5913&gt;' is not marked as EOG\nload: control token: 261812 '&lt;unused5910&gt;' is not marked as EOG\nload: control token: 261811 '&lt;unused5909&gt;' is not marked as EOG\nload: control token: 261810 '&lt;unused5908&gt;' is not marked as EOG\nload: control token: 261807 '&lt;unused5905&gt;' is not marked as EOG\nload: control token: 261806 '&lt;unused5904&gt;' is not marked as EOG\nload: control token: 261804 '&lt;unused5902&gt;' is not marked as EOG\nload: control token: 261803 '&lt;unused5901&gt;' is not marked as EOG\nload: control token: 261801 '&lt;unused5899&gt;' is not marked as EOG\nload: control token: 261800 '&lt;unused5898&gt;' is not marked as EOG\nload: control token: 261799 '&lt;unused5897&gt;' is not marked as EOG\nload: control token: 261798 '&lt;unused5896&gt;' is not marked as EOG\nload: control token: 261796 '&lt;unused5894&gt;' is not marked as EOG\nload: control token: 261795 '&lt;unused5893&gt;' is not marked as EOG\nload: control token: 261794 '&lt;unused5892&gt;' is not marked as EOG\nload: control token: 261793 '&lt;unused5891&gt;' is not marked as EOG\nload: control token: 261792 '&lt;unused5890&gt;' is not marked as EOG\nload: control token: 261791 '&lt;unused5889&gt;' is not marked as EOG\nload: control token: 261790 '&lt;unused5888&gt;' is not marked as EOG\nload: control token: 261786 '&lt;unused5884&gt;' is not marked as EOG\nload: control token: 261785 '&lt;unused5883&gt;' is not marked as EOG\nload: control token: 261782 '&lt;unused5880&gt;' is not marked as EOG\nload: control token: 261781 '&lt;unused5879&gt;' is not marked as EOG\nload: control token: 261779 '&lt;unused5877&gt;' is not marked as EOG\nload: control token: 261774 '&lt;unused5872&gt;' is not marked as EOG\nload: control token: 261772 '&lt;unused5870&gt;' is not marked as EOG\nload: control token: 261769 '&lt;unused5867&gt;' is not marked as EOG\nload: control token: 261767 '&lt;unused5865&gt;' is not marked as EOG\nload: control token: 261765 '&lt;unused5863&gt;' is not marked as EOG\nload: control token: 261762 '&lt;unused5860&gt;' is not marked as EOG\nload: control token: 261760 '&lt;unused5858&gt;' is not marked as EOG\nload: control token: 261759 '&lt;unused5857&gt;' is not marked as EOG\nload: control token: 261757 '&lt;unused5855&gt;' is not marked as EOG\nload: control token: 261755 '&lt;unused5853&gt;' is not marked as EOG\nload: control token: 261753 '&lt;unused5851&gt;' is not marked as EOG\nload: control token: 261752 '&lt;unused5850&gt;' is not marked as EOG\nload: control token: 261747 '&lt;unused5845&gt;' is not marked as EOG\nload: control token: 261745 '&lt;unused5843&gt;' is not marked as EOG\nload: control token: 261741 '&lt;unused5839&gt;' is not marked as EOG\nload: control token: 261740 '&lt;unused5838&gt;' is not marked as EOG\nload: control token: 261738 '&lt;unused5836&gt;' is not marked as EOG\nload: control token: 261737 '&lt;unused5835&gt;' is not marked as EOG\nload: control token: 261736 '&lt;unused5834&gt;' is not marked as EOG\nload: control token: 261733 '&lt;unused5831&gt;' is not marked as EOG\nload: control token: 261731 '&lt;unused5829&gt;' is not marked as EOG\nload: control token: 261729 '&lt;unused5827&gt;' is not marked as EOG\nload: control token: 261727 '&lt;unused5825&gt;' is not marked as EOG\nload: control token: 261726 '&lt;unused5824&gt;' is not marked as EOG\nload: control token: 261723 '&lt;unused5821&gt;' is not marked as EOG\nload: control token: 261722 '&lt;unused5820&gt;' is not marked as EOG\nload: control token: 261721 '&lt;unused5819&gt;' is not marked as EOG\nload: control token: 261720 '&lt;unused5818&gt;' is not marked as EOG\nload: control token: 261718 '&lt;unused5816&gt;' is not marked as EOG\nload: control token: 261716 '&lt;unused5814&gt;' is not marked as EOG\nload: control token: 261715 '&lt;unused5813&gt;' is not marked as EOG\nload: control token: 261712 '&lt;unused5810&gt;' is not marked as EOG\nload: control token: 261711 '&lt;unused5809&gt;' is not marked as EOG\nload: control token: 261710 '&lt;unused5808&gt;' is not marked as EOG\nload: control token: 261705 '&lt;unused5803&gt;' is not marked as EOG\nload: control token: 261704 '&lt;unused5802&gt;' is not marked as EOG\nload: control token: 261703 '&lt;unused5801&gt;' is not marked as EOG\nload: control token: 261702 '&lt;unused5800&gt;' is not marked as EOG\nload: control token: 261701 '&lt;unused5799&gt;' is not marked as EOG\nload: control token: 261698 '&lt;unused5796&gt;' is not marked as EOG\nload: control token: 261693 '&lt;unused5791&gt;' is not marked as EOG\nload: control token: 261692 '&lt;unused5790&gt;' is not marked as EOG\nload: control token: 261691 '&lt;unused5789&gt;' is not marked as EOG\nload: control token: 261690 '&lt;unused5788&gt;' is not marked as EOG\nload: control token: 261689 '&lt;unused5787&gt;' is not marked as EOG\nload: control token: 261687 '&lt;unused5785&gt;' is not marked as EOG\nload: control token: 261685 '&lt;unused5783&gt;' is not marked as EOG\nload: control token: 261680 '&lt;unused5778&gt;' is not marked as EOG\nload: control token: 261678 '&lt;unused5776&gt;' is not marked as EOG\nload: control token: 261674 '&lt;unused5772&gt;' is not marked as EOG\nload: control token: 261673 '&lt;unused5771&gt;' is not marked as EOG\nload: control token: 261670 '&lt;unused5768&gt;' is not marked as EOG\nload: control token: 261667 '&lt;unused5765&gt;' is not marked as EOG\nload: control token: 261665 '&lt;unused5763&gt;' is not marked as EOG\nload: control token: 261664 '&lt;unused5762&gt;' is not marked as EOG\nload: control token: 261662 '&lt;unused5760&gt;' is not marked as EOG\nload: control token: 261660 '&lt;unused5758&gt;' is not marked as EOG\nload: control token: 261659 '&lt;unused5757&gt;' is not marked as EOG\nload: control token: 261657 '&lt;unused5755&gt;' is not marked as EOG\nload: control token: 261654 '&lt;unused5752&gt;' is not marked as EOG\nload: control token: 261653 '&lt;unused5751&gt;' is not marked as EOG\nload: control token: 261649 '&lt;unused5747&gt;' is not marked as EOG\nload: control token: 261647 '&lt;unused5745&gt;' is not marked as EOG\nload: control token: 261643 '&lt;unused5741&gt;' is not marked as EOG\nload: control token: 261638 '&lt;unused5736&gt;' is not marked as EOG\nload: control token: 261637 '&lt;unused5735&gt;' is not marked as EOG\nload: control token: 261635 '&lt;unused5733&gt;' is not marked as EOG\nload: control token: 261633 '&lt;unused5731&gt;' is not marked as EOG\nload: control token: 261632 '&lt;unused5730&gt;' is not marked as EOG\nload: control token: 261630 '&lt;unused5728&gt;' is not marked as EOG\nload: control token: 261629 '&lt;unused5727&gt;' is not marked as EOG\nload: control token: 261625 '&lt;unused5723&gt;' is not marked as EOG\nload: control token: 261622 '&lt;unused5720&gt;' is not marked as EOG\nload: control token: 261621 '&lt;unused5719&gt;' is not marked as EOG\nload: control token: 261619 '&lt;unused5717&gt;' is not marked as EOG\nload: control token: 261612 '&lt;unused5710&gt;' is not marked as EOG\nload: control token: 261611 '&lt;unused5709&gt;' is not marked as EOG\nload: control token: 261610 '&lt;unused5708&gt;' is not marked as EOG\nload: control token: 261609 '&lt;unused5707&gt;' is not marked as EOG\nload: control token: 261608 '&lt;unused5706&gt;' is not marked as EOG\nload: control token: 261607 '&lt;unused5705&gt;' is not marked as EOG\nload: control token: 261606 '&lt;unused5704&gt;' is not marked as EOG\nload: control token: 261605 '&lt;unused5703&gt;' is not marked as EOG\nload: control token: 261603 '&lt;unused5701&gt;' is not marked as EOG\nload: control token: 261602 '&lt;unused5700&gt;' is not marked as EOG\nload: control token: 261600 '&lt;unused5698&gt;' is not marked as EOG\nload: control token: 261597 '&lt;unused5695&gt;' is not marked as EOG\nload: control token: 261595 '&lt;unused5693&gt;' is not marked as EOG\nload: control token: 261594 '&lt;unused5692&gt;' is not marked as EOG\nload: control token: 261593 '&lt;unused5691&gt;' is not marked as EOG\nload: control token: 261592 '&lt;unused5690&gt;' is not marked as EOG\nload: control token: 261590 '&lt;unused5688&gt;' is not marked as EOG\nload: control token: 261588 '&lt;unused5686&gt;' is not marked as EOG\nload: control token: 261583 '&lt;unused5681&gt;' is not marked as EOG\nload: control token: 261582 '&lt;unused5680&gt;' is not marked as EOG\nload: control token: 261581 '&lt;unused5679&gt;' is not marked as EOG\nload: control token: 261579 '&lt;unused5677&gt;' is not marked as EOG\nload: control token: 261578 '&lt;unused5676&gt;' is not marked as EOG\nload: control token: 261577 '&lt;unused5675&gt;' is not marked as EOG\nload: control token: 261576 '&lt;unused5674&gt;' is not marked as EOG\nload: control token: 261573 '&lt;unused5671&gt;' is not marked as EOG\nload: control token: 261572 '&lt;unused5670&gt;' is not marked as EOG\nload: control token: 261570 '&lt;unused5668&gt;' is not marked as EOG\nload: control token: 261568 '&lt;unused5666&gt;' is not marked as EOG\nload: control token: 261566 '&lt;unused5664&gt;' is not marked as EOG\nload: control token: 261564 '&lt;unused5662&gt;' is not marked as EOG\nload: control token: 261563 '&lt;unused5661&gt;' is not marked as EOG\nload: control token: 261562 '&lt;unused5660&gt;' is not marked as EOG\nload: control token: 261560 '&lt;unused5658&gt;' is not marked as EOG\nload: control token: 261555 '&lt;unused5653&gt;' is not marked as EOG\nload: control token: 261552 '&lt;unused5650&gt;' is not marked as EOG\nload: control token: 261551 '&lt;unused5649&gt;' is not marked as EOG\nload: control token: 261550 '&lt;unused5648&gt;' is not marked as EOG\nload: control token: 261549 '&lt;unused5647&gt;' is not marked as EOG\nload: control token: 261546 '&lt;unused5644&gt;' is not marked as EOG\nload: control token: 261545 '&lt;unused5643&gt;' is not marked as EOG\nload: control token: 261544 '&lt;unused5642&gt;' is not marked as EOG\nload: control token: 261541 '&lt;unused5639&gt;' is not marked as EOG\nload: control token: 261539 '&lt;unused5637&gt;' is not marked as EOG\nload: control token: 261537 '&lt;unused5635&gt;' is not marked as EOG\nload: control token: 261535 '&lt;unused5633&gt;' is not marked as EOG\nload: control token: 261533 '&lt;unused5631&gt;' is not marked as EOG\nload: control token: 261529 '&lt;unused5627&gt;' is not marked as EOG\nload: control token: 261528 '&lt;unused5626&gt;' is not marked as EOG\nload: control token: 261527 '&lt;unused5625&gt;' is not marked as EOG\nload: control token: 261523 '&lt;unused5621&gt;' is not marked as EOG\nload: control token: 261522 '&lt;unused5620&gt;' is not marked as EOG\nload: control token: 261518 '&lt;unused5616&gt;' is not marked as EOG\nload: control token: 261516 '&lt;unused5614&gt;' is not marked as EOG\nload: control token: 261514 '&lt;unused5612&gt;' is not marked as EOG\nload: control token: 261512 '&lt;unused5610&gt;' is not marked as EOG\nload: control token: 261511 '&lt;unused5609&gt;' is not marked as EOG\nload: control token: 261506 '&lt;unused5604&gt;' is not marked as EOG\nload: control token: 261505 '&lt;unused5603&gt;' is not marked as EOG\nload: control token: 261504 '&lt;unused5602&gt;' is not marked as EOG\nload: control token: 261503 '&lt;unused5601&gt;' is not marked as EOG\nload: control token: 261502 '&lt;unused5600&gt;' is not marked as EOG\nload: control token: 261501 '&lt;unused5599&gt;' is not marked as EOG\nload: control token: 261499 '&lt;unused5597&gt;' is not marked as EOG\nload: control token: 261497 '&lt;unused5595&gt;' is not marked as EOG\nload: control token: 261495 '&lt;unused5593&gt;' is not marked as EOG\nload: control token: 261494 '&lt;unused5592&gt;' is not marked as EOG\nload: control token: 261492 '&lt;unused5590&gt;' is not marked as EOG\nload: control token: 261490 '&lt;unused5588&gt;' is not marked as EOG\nload: control token: 261488 '&lt;unused5586&gt;' is not marked as EOG\nload: control token: 261487 '&lt;unused5585&gt;' is not marked as EOG\nload: control token: 261486 '&lt;unused5584&gt;' is not marked as EOG\nload: control token: 261485 '&lt;unused5583&gt;' is not marked as EOG\nload: control token: 261482 '&lt;unused5580&gt;' is not marked as EOG\nload: control token: 261481 '&lt;unused5579&gt;' is not marked as EOG\nload: control token: 261478 '&lt;unused5576&gt;' is not marked as EOG\nload: control token: 261477 '&lt;unused5575&gt;' is not marked as EOG\nload: control token: 261476 '&lt;unused5574&gt;' is not marked as EOG\nload: control token: 261475 '&lt;unused5573&gt;' is not marked as EOG\nload: control token: 261474 '&lt;unused5572&gt;' is not marked as EOG\nload: control token: 261472 '&lt;unused5570&gt;' is not marked as EOG\nload: control token: 261471 '&lt;unused5569&gt;' is not marked as EOG\nload: control token: 261470 '&lt;unused5568&gt;' is not marked as EOG\nload: control token: 261468 '&lt;unused5566&gt;' is not marked as EOG\nload: control token: 261467 '&lt;unused5565&gt;' is not marked as EOG\nload: control token: 261462 '&lt;unused5560&gt;' is not marked as EOG\nload: control token: 261461 '&lt;unused5559&gt;' is not marked as EOG\nload: control token: 261456 '&lt;unused5554&gt;' is not marked as EOG\nload: control token: 261455 '&lt;unused5553&gt;' is not marked as EOG\nload: control token: 261453 '&lt;unused5551&gt;' is not marked as EOG\nload: control token: 261451 '&lt;unused5549&gt;' is not marked as EOG\nload: control token: 261449 '&lt;unused5547&gt;' is not marked as EOG\nload: control token: 261448 '&lt;unused5546&gt;' is not marked as EOG\nload: control token: 261447 '&lt;unused5545&gt;' is not marked as EOG\nload: control token: 261446 '&lt;unused5544&gt;' is not marked as EOG\nload: control token: 261444 '&lt;unused5542&gt;' is not marked as EOG\nload: control token: 261443 '&lt;unused5541&gt;' is not marked as EOG\nload: control token: 261441 '&lt;unused5539&gt;' is not marked as EOG\nload: control token: 261440 '&lt;unused5538&gt;' is not marked as EOG\nload: control token: 261438 '&lt;unused5536&gt;' is not marked as EOG\nload: control token: 261435 '&lt;unused5533&gt;' is not marked as EOG\nload: control token: 261432 '&lt;unused5530&gt;' is not marked as EOG\nload: control token: 261431 '&lt;unused5529&gt;' is not marked as EOG\nload: control token: 261429 '&lt;unused5527&gt;' is not marked as EOG\nload: control token: 261427 '&lt;unused5525&gt;' is not marked as EOG\nload: control token: 261425 '&lt;unused5523&gt;' is not marked as EOG\nload: control token: 261424 '&lt;unused5522&gt;' is not marked as EOG\nload: control token: 261422 '&lt;unused5520&gt;' is not marked as EOG\nload: control token: 261420 '&lt;unused5518&gt;' is not marked as EOG\nload: control token: 261419 '&lt;unused5517&gt;' is not marked as EOG\nload: control token: 261418 '&lt;unused5516&gt;' is not marked as EOG\nload: control token: 261417 '&lt;unused5515&gt;' is not marked as EOG\nload: control token: 261416 '&lt;unused5514&gt;' is not marked as EOG\nload: control token: 261415 '&lt;unused5513&gt;' is not marked as EOG\nload: control token: 261413 '&lt;unused5511&gt;' is not marked as EOG\nload: control token: 261409 '&lt;unused5507&gt;' is not marked as EOG\nload: control token: 261407 '&lt;unused5505&gt;' is not marked as EOG\nload: control token: 261406 '&lt;unused5504&gt;' is not marked as EOG\nload: control token: 261405 '&lt;unused5503&gt;' is not marked as EOG\nload: control token: 261403 '&lt;unused5501&gt;' is not marked as EOG\nload: control token: 261402 '&lt;unused5500&gt;' is not marked as EOG\nload: control token: 261401 '&lt;unused5499&gt;' is not marked as EOG\nload: control token: 261399 '&lt;unused5497&gt;' is not marked as EOG\nload: control token: 261398 '&lt;unused5496&gt;' is not marked as EOG\nload: control token: 261394 '&lt;unused5492&gt;' is not marked as EOG\nload: control token: 261393 '&lt;unused5491&gt;' is not marked as EOG\nload: control token: 261392 '&lt;unused5490&gt;' is not marked as EOG\nload: control token: 261388 '&lt;unused5486&gt;' is not marked as EOG\nload: control token: 261387 '&lt;unused5485&gt;' is not marked as EOG\nload: control token: 261386 '&lt;unused5484&gt;' is not marked as EOG\nload: control token: 261385 '&lt;unused5483&gt;' is not marked as EOG\nload: control token: 261384 '&lt;unused5482&gt;' is not marked as EOG\nload: control token: 261383 '&lt;unused5481&gt;' is not marked as EOG\nload: control token: 261382 '&lt;unused5480&gt;' is not marked as EOG\nload: control token: 261380 '&lt;unused5478&gt;' is not marked as EOG\nload: control token: 261376 '&lt;unused5474&gt;' is not marked as EOG\nload: control token: 261375 '&lt;unused5473&gt;' is not marked as EOG\nload: control token: 261374 '&lt;unused5472&gt;' is not marked as EOG\nload: control token: 261371 '&lt;unused5469&gt;' is not marked as EOG\nload: control token: 261369 '&lt;unused5467&gt;' is not marked as EOG\nload: control token: 261367 '&lt;unused5465&gt;' is not marked as EOG\nload: control token: 261366 '&lt;unused5464&gt;' is not marked as EOG\nload: control token: 261364 '&lt;unused5462&gt;' is not marked as EOG\nload: control token: 261360 '&lt;unused5458&gt;' is not marked as EOG\nload: control token: 261359 '&lt;unused5457&gt;' is not marked as EOG\nload: control token: 261356 '&lt;unused5454&gt;' is not marked as EOG\nload: control token: 261354 '&lt;unused5452&gt;' is not marked as EOG\nload: control token: 261353 '&lt;unused5451&gt;' is not marked as EOG\nload: control token: 261352 '&lt;unused5450&gt;' is not marked as EOG\nload: control token: 261351 '&lt;unused5449&gt;' is not marked as EOG\nload: control token: 261350 '&lt;unused5448&gt;' is not marked as EOG\nload: control token: 261349 '&lt;unused5447&gt;' is not marked as EOG\nload: control token: 261348 '&lt;unused5446&gt;' is not marked as EOG\nload: control token: 261347 '&lt;unused5445&gt;' is not marked as EOG\nload: control token: 261346 '&lt;unused5444&gt;' is not marked as EOG\nload: control token: 261345 '&lt;unused5443&gt;' is not marked as EOG\nload: control token: 261342 '&lt;unused5440&gt;' is not marked as EOG\nload: control token: 261339 '&lt;unused5437&gt;' is not marked as EOG\nload: control token: 261338 '&lt;unused5436&gt;' is not marked as EOG\nload: control token: 261337 '&lt;unused5435&gt;' is not marked as EOG\nload: control token: 261336 '&lt;unused5434&gt;' is not marked as EOG\nload: control token: 261335 '&lt;unused5433&gt;' is not marked as EOG\nload: control token: 261331 '&lt;unused5429&gt;' is not marked as EOG\nload: control token: 261330 '&lt;unused5428&gt;' is not marked as EOG\nload: control token: 261328 '&lt;unused5426&gt;' is not marked as EOG\nload: control token: 261327 '&lt;unused5425&gt;' is not marked as EOG\nload: control token: 261326 '&lt;unused5424&gt;' is not marked as EOG\nload: control token: 261324 '&lt;unused5422&gt;' is not marked as EOG\nload: control token: 261323 '&lt;unused5421&gt;' is not marked as EOG\nload: control token: 261322 '&lt;unused5420&gt;' is not marked as EOG\nload: control token: 261321 '&lt;unused5419&gt;' is not marked as EOG\nload: control token: 261320 '&lt;unused5418&gt;' is not marked as EOG\nload: control token: 261317 '&lt;unused5415&gt;' is not marked as EOG\nload: control token: 261315 '&lt;unused5413&gt;' is not marked as EOG\nload: control token: 261313 '&lt;unused5411&gt;' is not marked as EOG\nload: control token: 261310 '&lt;unused5408&gt;' is not marked as EOG\nload: control token: 261307 '&lt;unused5405&gt;' is not marked as EOG\nload: control token: 261306 '&lt;unused5404&gt;' is not marked as EOG\nload: control token: 261305 '&lt;unused5403&gt;' is not marked as EOG\nload: control token: 261303 '&lt;unused5401&gt;' is not marked as EOG\nload: control token: 261300 '&lt;unused5398&gt;' is not marked as EOG\nload: control token: 261299 '&lt;unused5397&gt;' is not marked as EOG\nload: control token: 261292 '&lt;unused5390&gt;' is not marked as EOG\nload: control token: 261287 '&lt;unused5385&gt;' is not marked as EOG\nload: control token: 261284 '&lt;unused5382&gt;' is not marked as EOG\nload: control token: 261283 '&lt;unused5381&gt;' is not marked as EOG\nload: control token: 261281 '&lt;unused5379&gt;' is not marked as EOG\nload: control token: 261279 '&lt;unused5377&gt;' is not marked as EOG\nload: control token: 261277 '&lt;unused5375&gt;' is not marked as EOG\nload: control token: 261276 '&lt;unused5374&gt;' is not marked as EOG\nload: control token: 261274 '&lt;unused5372&gt;' is not marked as EOG\nload: control token: 261273 '&lt;unused5371&gt;' is not marked as EOG\nload: control token: 261272 '&lt;unused5370&gt;' is not marked as EOG\nload: control token: 261269 '&lt;unused5367&gt;' is not marked as EOG\nload: control token: 261268 '&lt;unused5366&gt;' is not marked as EOG\nload: control token: 261267 '&lt;unused5365&gt;' is not marked as EOG\nload: control token: 261265 '&lt;unused5363&gt;' is not marked as EOG\nload: control token: 261264 '&lt;unused5362&gt;' is not marked as EOG\nload: control token: 261263 '&lt;unused5361&gt;' is not marked as EOG\nload: control token: 261262 '&lt;unused5360&gt;' is not marked as EOG\nload: control token: 261260 '&lt;unused5358&gt;' is not marked as EOG\nload: control token: 261257 '&lt;unused5355&gt;' is not marked as EOG\nload: control token: 261256 '&lt;unused5354&gt;' is not marked as EOG\nload: control token: 261255 '&lt;unused5353&gt;' is not marked as EOG\nload: control token: 261252 '&lt;unused5350&gt;' is not marked as EOG\nload: control token: 261251 '&lt;unused5349&gt;' is not marked as EOG\nload: control token: 261249 '&lt;unused5347&gt;' is not marked as EOG\nload: control token: 261248 '&lt;unused5346&gt;' is not marked as EOG\nload: control token: 261247 '&lt;unused5345&gt;' is not marked as EOG\nload: control token: 261244 '&lt;unused5342&gt;' is not marked as EOG\nload: control token: 261241 '&lt;unused5339&gt;' is not marked as EOG\nload: control token: 261237 '&lt;unused5335&gt;' is not marked as EOG\nload: control token: 261236 '&lt;unused5334&gt;' is not marked as EOG\nload: control token: 261234 '&lt;unused5332&gt;' is not marked as EOG\nload: control token: 261232 '&lt;unused5330&gt;' is not marked as EOG\nload: control token: 261230 '&lt;unused5328&gt;' is not marked as EOG\nload: control token: 261229 '&lt;unused5327&gt;' is not marked as EOG\nload: control token: 261228 '&lt;unused5326&gt;' is not marked as EOG\nload: control token: 261227 '&lt;unused5325&gt;' is not marked as EOG\nload: control token: 261226 '&lt;unused5324&gt;' is not marked as EOG\nload: control token: 261224 '&lt;unused5322&gt;' is not marked as EOG\nload: control token: 261222 '&lt;unused5320&gt;' is not marked as EOG\nload: control token: 261221 '&lt;unused5319&gt;' is not marked as EOG\nload: control token: 261219 '&lt;unused5317&gt;' is not marked as EOG\nload: control token: 261217 '&lt;unused5315&gt;' is not marked as EOG\nload: control token: 261212 '&lt;unused5310&gt;' is not marked as EOG\nload: control token: 261211 '&lt;unused5309&gt;' is not marked as EOG\nload: control token: 261210 '&lt;unused5308&gt;' is not marked as EOG\nload: control token: 261209 '&lt;unused5307&gt;' is not marked as EOG\nload: control token: 261208 '&lt;unused5306&gt;' is not marked as EOG\nload: control token: 261207 '&lt;unused5305&gt;' is not marked as EOG\nload: control token: 261206 '&lt;unused5304&gt;' is not marked as EOG\nload: control token: 261205 '&lt;unused5303&gt;' is not marked as EOG\nload: control token: 261203 '&lt;unused5301&gt;' is not marked as EOG\nload: control token: 261202 '&lt;unused5300&gt;' is not marked as EOG\nload: control token: 261198 '&lt;unused5296&gt;' is not marked as EOG\nload: control token: 261197 '&lt;unused5295&gt;' is not marked as EOG\nload: control token: 261194 '&lt;unused5292&gt;' is not marked as EOG\nload: control token: 261193 '&lt;unused5291&gt;' is not marked as EOG\nload: control token: 261192 '&lt;unused5290&gt;' is not marked as EOG\nload: control token: 261190 '&lt;unused5288&gt;' is not marked as EOG\nload: control token: 261186 '&lt;unused5284&gt;' is not marked as EOG\nload: control token: 261182 '&lt;unused5280&gt;' is not marked as EOG\nload: control token: 261179 '&lt;unused5277&gt;' is not marked as EOG\nload: control token: 261175 '&lt;unused5273&gt;' is not marked as EOG\nload: control token: 261172 '&lt;unused5270&gt;' is not marked as EOG\nload: control token: 261171 '&lt;unused5269&gt;' is not marked as EOG\nload: control token: 261170 '&lt;unused5268&gt;' is not marked as EOG\nload: control token: 261168 '&lt;unused5266&gt;' is not marked as EOG\nload: control token: 261164 '&lt;unused5262&gt;' is not marked as EOG\nload: control token: 261163 '&lt;unused5261&gt;' is not marked as EOG\nload: control token: 261162 '&lt;unused5260&gt;' is not marked as EOG\nload: control token: 261161 '&lt;unused5259&gt;' is not marked as EOG\nload: control token: 261159 '&lt;unused5257&gt;' is not marked as EOG\nload: control token: 261156 '&lt;unused5254&gt;' is not marked as EOG\nload: control token: 261155 '&lt;unused5253&gt;' is not marked as EOG\nload: control token: 261153 '&lt;unused5251&gt;' is not marked as EOG\nload: control token: 261152 '&lt;unused5250&gt;' is not marked as EOG\nload: control token: 261151 '&lt;unused5249&gt;' is not marked as EOG\nload: control token: 261150 '&lt;unused5248&gt;' is not marked as EOG\nload: control token: 261146 '&lt;unused5244&gt;' is not marked as EOG\nload: control token: 261145 '&lt;unused5243&gt;' is not marked as EOG\nload: control token: 261144 '&lt;unused5242&gt;' is not marked as EOG\nload: control token: 261142 '&lt;unused5240&gt;' is not marked as EOG\nload: control token: 261141 '&lt;unused5239&gt;' is not marked as EOG\nload: control token: 261140 '&lt;unused5238&gt;' is not marked as EOG\nload: control token: 261139 '&lt;unused5237&gt;' is not marked as EOG\nload: control token: 261138 '&lt;unused5236&gt;' is not marked as EOG\nload: control token: 261137 '&lt;unused5235&gt;' is not marked as EOG\nload: control token: 261136 '&lt;unused5234&gt;' is not marked as EOG\nload: control token: 261135 '&lt;unused5233&gt;' is not marked as EOG\nload: control token: 261134 '&lt;unused5232&gt;' is not marked as EOG\nload: control token: 261133 '&lt;unused5231&gt;' is not marked as EOG\nload: control token: 261131 '&lt;unused5229&gt;' is not marked as EOG\nload: control token: 261130 '&lt;unused5228&gt;' is not marked as EOG\nload: control token: 261127 '&lt;unused5225&gt;' is not marked as EOG\nload: control token: 261126 '&lt;unused5224&gt;' is not marked as EOG\nload: control token: 261124 '&lt;unused5222&gt;' is not marked as EOG\nload: control token: 261123 '&lt;unused5221&gt;' is not marked as EOG\nload: control token: 261122 '&lt;unused5220&gt;' is not marked as EOG\nload: control token: 261119 '&lt;unused5217&gt;' is not marked as EOG\nload: control token: 261117 '&lt;unused5215&gt;' is not marked as EOG\nload: control token: 261116 '&lt;unused5214&gt;' is not marked as EOG\nload: control token: 261115 '&lt;unused5213&gt;' is not marked as EOG\nload: control token: 261112 '&lt;unused5210&gt;' is not marked as EOG\nload: control token: 261111 '&lt;unused5209&gt;' is not marked as EOG\nload: control token: 261109 '&lt;unused5207&gt;' is not marked as EOG\nload: control token: 261105 '&lt;unused5203&gt;' is not marked as EOG\nload: control token: 261104 '&lt;unused5202&gt;' is not marked as EOG\nload: control token: 261102 '&lt;unused5200&gt;' is not marked as EOG\nload: control token: 261101 '&lt;unused5199&gt;' is not marked as EOG\nload: control token: 261100 '&lt;unused5198&gt;' is not marked as EOG\nload: control token: 261099 '&lt;unused5197&gt;' is not marked as EOG\nload: control token: 261095 '&lt;unused5193&gt;' is not marked as EOG\nload: control token: 261094 '&lt;unused5192&gt;' is not marked as EOG\nload: control token: 261092 '&lt;unused5190&gt;' is not marked as EOG\nload: control token: 261091 '&lt;unused5189&gt;' is not marked as EOG\nload: control token: 261090 '&lt;unused5188&gt;' is not marked as EOG\nload: control token: 261089 '&lt;unused5187&gt;' is not marked as EOG\nload: control token: 261088 '&lt;unused5186&gt;' is not marked as EOG\nload: control token: 261087 '&lt;unused5185&gt;' is not marked as EOG\nload: control token: 261086 '&lt;unused5184&gt;' is not marked as EOG\nload: control token: 261085 '&lt;unused5183&gt;' is not marked as EOG\nload: control token: 261081 '&lt;unused5179&gt;' is not marked as EOG\nload: control token: 261079 '&lt;unused5177&gt;' is not marked as EOG\nload: control token: 261076 '&lt;unused5174&gt;' is not marked as EOG\nload: control token: 261073 '&lt;unused5171&gt;' is not marked as EOG\nload: control token: 261071 '&lt;unused5169&gt;' is not marked as EOG\nload: control token: 261069 '&lt;unused5167&gt;' is not marked as EOG\nload: control token: 261066 '&lt;unused5164&gt;' is not marked as EOG\nload: control token: 261065 '&lt;unused5163&gt;' is not marked as EOG\nload: control token: 261063 '&lt;unused5161&gt;' is not marked as EOG\nload: control token: 261062 '&lt;unused5160&gt;' is not marked as EOG\nload: control token: 261061 '&lt;unused5159&gt;' is not marked as EOG\nload: control token: 261060 '&lt;unused5158&gt;' is not marked as EOG\nload: control token: 261058 '&lt;unused5156&gt;' is not marked as EOG\nload: control token: 261057 '&lt;unused5155&gt;' is not marked as EOG\nload: control token: 261056 '&lt;unused5154&gt;' is not marked as EOG\nload: control token: 261054 '&lt;unused5152&gt;' is not marked as EOG\nload: control token: 261052 '&lt;unused5150&gt;' is not marked as EOG\nload: control token: 261049 '&lt;unused5147&gt;' is not marked as EOG\nload: control token: 261046 '&lt;unused5144&gt;' is not marked as EOG\nload: control token: 261045 '&lt;unused5143&gt;' is not marked as EOG\nload: control token: 261043 '&lt;unused5141&gt;' is not marked as EOG\nload: control token: 261042 '&lt;unused5140&gt;' is not marked as EOG\nload: control token: 261039 '&lt;unused5137&gt;' is not marked as EOG\nload: control token: 261038 '&lt;unused5136&gt;' is not marked as EOG\nload: control token: 261036 '&lt;unused5134&gt;' is not marked as EOG\nload: control token: 261035 '&lt;unused5133&gt;' is not marked as EOG\nload: control token: 261032 '&lt;unused5130&gt;' is not marked as EOG\nload: control token: 261029 '&lt;unused5127&gt;' is not marked as EOG\nload: control token: 261028 '&lt;unused5126&gt;' is not marked as EOG\nload: control token: 261027 '&lt;unused5125&gt;' is not marked as EOG\nload: control token: 261026 '&lt;unused5124&gt;' is not marked as EOG\nload: control token: 261024 '&lt;unused5122&gt;' is not marked as EOG\nload: control token: 261023 '&lt;unused5121&gt;' is not marked as EOG\nload: control token: 261019 '&lt;unused5117&gt;' is not marked as EOG\nload: control token: 261018 '&lt;unused5116&gt;' is not marked as EOG\nload: control token: 261017 '&lt;unused5115&gt;' is not marked as EOG\nload: control token: 261016 '&lt;unused5114&gt;' is not marked as EOG\nload: control token: 261015 '&lt;unused5113&gt;' is not marked as EOG\nload: control token: 261011 '&lt;unused5109&gt;' is not marked as EOG\nload: control token: 261008 '&lt;unused5106&gt;' is not marked as EOG\nload: control token: 261006 '&lt;unused5104&gt;' is not marked as EOG\nload: control token: 260999 '&lt;unused5097&gt;' is not marked as EOG\nload: control token: 260998 '&lt;unused5096&gt;' is not marked as EOG\nload: control token: 260996 '&lt;unused5094&gt;' is not marked as EOG\nload: control token: 260995 '&lt;unused5093&gt;' is not marked as EOG\nload: control token: 260992 '&lt;unused5090&gt;' is not marked as EOG\nload: control token: 260987 '&lt;unused5085&gt;' is not marked as EOG\nload: control token: 260986 '&lt;unused5084&gt;' is not marked as EOG\nload: control token: 260985 '&lt;unused5083&gt;' is not marked as EOG\nload: control token: 260978 '&lt;unused5076&gt;' is not marked as EOG\nload: control token: 260977 '&lt;unused5075&gt;' is not marked as EOG\nload: control token: 260976 '&lt;unused5074&gt;' is not marked as EOG\nload: control token: 260975 '&lt;unused5073&gt;' is not marked as EOG\nload: control token: 260968 '&lt;unused5066&gt;' is not marked as EOG\nload: control token: 260966 '&lt;unused5064&gt;' is not marked as EOG\nload: control token: 260964 '&lt;unused5062&gt;' is not marked as EOG\nload: control token: 260963 '&lt;unused5061&gt;' is not marked as EOG\nload: control token: 260962 '&lt;unused5060&gt;' is not marked as EOG\nload: control token: 260959 '&lt;unused5057&gt;' is not marked as EOG\nload: control token: 260958 '&lt;unused5056&gt;' is not marked as EOG\nload: control token: 260957 '&lt;unused5055&gt;' is not marked as EOG\nload: control token: 260955 '&lt;unused5053&gt;' is not marked as EOG\nload: control token: 260954 '&lt;unused5052&gt;' is not marked as EOG\nload: control token: 260951 '&lt;unused5049&gt;' is not marked as EOG\nload: control token: 260949 '&lt;unused5047&gt;' is not marked as EOG\nload: control token: 260946 '&lt;unused5044&gt;' is not marked as EOG\nload: control token: 260945 '&lt;unused5043&gt;' is not marked as EOG\nload: control token: 260943 '&lt;unused5041&gt;' is not marked as EOG\nload: control token: 260942 '&lt;unused5040&gt;' is not marked as EOG\nload: control token: 260941 '&lt;unused5039&gt;' is not marked as EOG\nload: control token: 260938 '&lt;unused5036&gt;' is not marked as EOG\nload: control token: 260937 '&lt;unused5035&gt;' is not marked as EOG\nload: control token: 260936 '&lt;unused5034&gt;' is not marked as EOG\nload: control token: 260934 '&lt;unused5032&gt;' is not marked as EOG\nload: control token: 260933 '&lt;unused5031&gt;' is not marked as EOG\nload: control token: 260932 '&lt;unused5030&gt;' is not marked as EOG\nload: control token: 260929 '&lt;unused5027&gt;' is not marked as EOG\nload: control token: 260928 '&lt;unused5026&gt;' is not marked as EOG\nload: control token: 260925 '&lt;unused5023&gt;' is not marked as EOG\nload: control token: 260922 '&lt;unused5020&gt;' is not marked as EOG\nload: control token: 260921 '&lt;unused5019&gt;' is not marked as EOG\nload: control token: 260920 '&lt;unused5018&gt;' is not marked as EOG\nload: control token: 260919 '&lt;unused5017&gt;' is not marked as EOG\nload: control token: 260917 '&lt;unused5015&gt;' is not marked as EOG\nload: control token: 260915 '&lt;unused5013&gt;' is not marked as EOG\nload: control token: 260913 '&lt;unused5011&gt;' is not marked as EOG\nload: control token: 260912 '&lt;unused5010&gt;' is not marked as EOG\nload: control token: 260911 '&lt;unused5009&gt;' is not marked as EOG\nload: control token: 260909 '&lt;unused5007&gt;' is not marked as EOG\nload: control token: 260905 '&lt;unused5003&gt;' is not marked as EOG\nload: control token: 260904 '&lt;unused5002&gt;' is not marked as EOG\nload: control token: 260902 '&lt;unused5000&gt;' is not marked as EOG\nload: control token: 260901 '&lt;unused4999&gt;' is not marked as EOG\nload: control token: 260897 '&lt;unused4995&gt;' is not marked as EOG\nload: control token: 260895 '&lt;unused4993&gt;' is not marked as EOG\nload: control token: 260893 '&lt;unused4991&gt;' is not marked as EOG\nload: control token: 260892 '&lt;unused4990&gt;' is not marked as EOG\nload: control token: 260890 '&lt;unused4988&gt;' is not marked as EOG\nload: control token: 260889 '&lt;unused4987&gt;' is not marked as EOG\nload: control token: 260883 '&lt;unused4981&gt;' is not marked as EOG\nload: control token: 260882 '&lt;unused4980&gt;' is not marked as EOG\nload: control token: 260881 '&lt;unused4979&gt;' is not marked as EOG\nload: control token: 260880 '&lt;unused4978&gt;' is not marked as EOG\nload: control token: 260873 '&lt;unused4971&gt;' is not marked as EOG\nload: control token: 260872 '&lt;unused4970&gt;' is not marked as EOG\nload: control token: 260871 '&lt;unused4969&gt;' is not marked as EOG\nload: control token: 260870 '&lt;unused4968&gt;' is not marked as EOG\nload: control token: 260868 '&lt;unused4966&gt;' is not marked as EOG\nload: control token: 260867 '&lt;unused4965&gt;' is not marked as EOG\nload: control token: 260866 '&lt;unused4964&gt;' is not marked as EOG\nload: control token: 260864 '&lt;unused4962&gt;' is not marked as EOG\nload: control token: 260863 '&lt;unused4961&gt;' is not marked as EOG\nload: control token: 260861 '&lt;unused4959&gt;' is not marked as EOG\nload: control token: 260856 '&lt;unused4954&gt;' is not marked as EOG\nload: control token: 260855 '&lt;unused4953&gt;' is not marked as EOG\nload: control token: 260853 '&lt;unused4951&gt;' is not marked as EOG\nload: control token: 260851 '&lt;unused4949&gt;' is not marked as EOG\nload: control token: 260847 '&lt;unused4945&gt;' is not marked as EOG\nload: control token: 260846 '&lt;unused4944&gt;' is not marked as EOG\nload: control token: 260844 '&lt;unused4942&gt;' is not marked as EOG\nload: control token: 260843 '&lt;unused4941&gt;' is not marked as EOG\nload: control token: 260842 '&lt;unused4940&gt;' is not marked as EOG\nload: control token: 260841 '&lt;unused4939&gt;' is not marked as EOG\nload: control token: 260839 '&lt;unused4937&gt;' is not marked as EOG\nload: control token: 260838 '&lt;unused4936&gt;' is not marked as EOG\nload: control token: 260835 '&lt;unused4933&gt;' is not marked as EOG\nload: control token: 260834 '&lt;unused4932&gt;' is not marked as EOG\nload: control token: 260831 '&lt;unused4929&gt;' is not marked as EOG\nload: control token: 260830 '&lt;unused4928&gt;' is not marked as EOG\nload: control token: 260829 '&lt;unused4927&gt;' is not marked as EOG\nload: control token: 260828 '&lt;unused4926&gt;' is not marked as EOG\nload: control token: 260826 '&lt;unused4924&gt;' is not marked as EOG\nload: control token: 260825 '&lt;unused4923&gt;' is not marked as EOG\nload: control token: 260824 '&lt;unused4922&gt;' is not marked as EOG\nload: control token: 260823 '&lt;unused4921&gt;' is not marked as EOG\nload: control token: 260821 '&lt;unused4919&gt;' is not marked as EOG\nload: control token: 260819 '&lt;unused4917&gt;' is not marked as EOG\nload: control token: 261070 '&lt;unused5168&gt;' is not marked as EOG\nload: control token: 260818 '&lt;unused4916&gt;' is not marked as EOG\nload: control token: 260817 '&lt;unused4915&gt;' is not marked as EOG\nload: control token: 260815 '&lt;unused4913&gt;' is not marked as EOG\nload: control token: 260814 '&lt;unused4912&gt;' is not marked as EOG\nload: control token: 260811 '&lt;unused4909&gt;' is not marked as EOG\nload: control token: 260810 '&lt;unused4908&gt;' is not marked as EOG\nload: control token: 260809 '&lt;unused4907&gt;' is not marked as EOG\nload: control token: 260808 '&lt;unused4906&gt;' is not marked as EOG\nload: control token: 260807 '&lt;unused4905&gt;' is not marked as EOG\nload: control token: 260806 '&lt;unused4904&gt;' is not marked as EOG\nload: control token: 260804 '&lt;unused4902&gt;' is not marked as EOG\nload: control token: 260802 '&lt;unused4900&gt;' is not marked as EOG\nload: control token: 260801 '&lt;unused4899&gt;' is not marked as EOG\nload: control token: 260800 '&lt;unused4898&gt;' is not marked as EOG\nload: control token: 260799 '&lt;unused4897&gt;' is not marked as EOG\nload: control token: 260798 '&lt;unused4896&gt;' is not marked as EOG\nload: control token: 260795 '&lt;unused4893&gt;' is not marked as EOG\nload: control token: 260792 '&lt;unused4890&gt;' is not marked as EOG\nload: control token: 260791 '&lt;unused4889&gt;' is not marked as EOG\nload: control token: 260789 '&lt;unused4887&gt;' is not marked as EOG\nload: control token: 260788 '&lt;unused4886&gt;' is not marked as EOG\nload: control token: 260785 '&lt;unused4883&gt;' is not marked as EOG\nload: control token: 260784 '&lt;unused4882&gt;' is not marked as EOG\nload: control token: 260783 '&lt;unused4881&gt;' is not marked as EOG\nload: control token: 260781 '&lt;unused4879&gt;' is not marked as EOG\nload: control token: 260779 '&lt;unused4877&gt;' is not marked as EOG\nload: control token: 260778 '&lt;unused4876&gt;' is not marked as EOG\nload: control token: 260777 '&lt;unused4875&gt;' is not marked as EOG\nload: control token: 260774 '&lt;unused4872&gt;' is not marked as EOG\nload: control token: 260773 '&lt;unused4871&gt;' is not marked as EOG\nload: control token: 260772 '&lt;unused4870&gt;' is not marked as EOG\nload: control token: 260771 '&lt;unused4869&gt;' is not marked as EOG\nload: control token: 260770 '&lt;unused4868&gt;' is not marked as EOG\nload: control token: 260768 '&lt;unused4866&gt;' is not marked as EOG\nload: control token: 260767 '&lt;unused4865&gt;' is not marked as EOG\nload: control token: 260766 '&lt;unused4864&gt;' is not marked as EOG\nload: control token: 260764 '&lt;unused4862&gt;' is not marked as EOG\nload: control token: 260763 '&lt;unused4861&gt;' is not marked as EOG\nload: control token: 260762 '&lt;unused4860&gt;' is not marked as EOG\nload: control token: 260761 '&lt;unused4859&gt;' is not marked as EOG\nload: control token: 260759 '&lt;unused4857&gt;' is not marked as EOG\nload: control token: 260754 '&lt;unused4852&gt;' is not marked as EOG\nload: control token: 260752 '&lt;unused4850&gt;' is not marked as EOG\nload: control token: 260743 '&lt;unused4841&gt;' is not marked as EOG\nload: control token: 260739 '&lt;unused4837&gt;' is not marked as EOG\nload: control token: 260736 '&lt;unused4834&gt;' is not marked as EOG\nload: control token: 260735 '&lt;unused4833&gt;' is not marked as EOG\nload: control token: 260730 '&lt;unused4828&gt;' is not marked as EOG\nload: control token: 260729 '&lt;unused4827&gt;' is not marked as EOG\nload: control token: 260728 '&lt;unused4826&gt;' is not marked as EOG\nload: control token: 260727 '&lt;unused4825&gt;' is not marked as EOG\nload: control token: 260726 '&lt;unused4824&gt;' is not marked as EOG\nload: control token: 260721 '&lt;unused4819&gt;' is not marked as EOG\nload: control token: 260719 '&lt;unused4817&gt;' is not marked as EOG\nload: control token: 260718 '&lt;unused4816&gt;' is not marked as EOG\nload: control token: 260716 '&lt;unused4814&gt;' is not marked as EOG\nload: control token: 260715 '&lt;unused4813&gt;' is not marked as EOG\nload: control token: 260713 '&lt;unused4811&gt;' is not marked as EOG\nload: control token: 260709 '&lt;unused4807&gt;' is not marked as EOG\nload: control token: 260708 '&lt;unused4806&gt;' is not marked as EOG\nload: control token: 260705 '&lt;unused4803&gt;' is not marked as EOG\nload: control token: 260701 '&lt;unused4799&gt;' is not marked as EOG\nload: control token: 260698 '&lt;unused4796&gt;' is not marked as EOG\nload: control token: 260696 '&lt;unused4794&gt;' is not marked as EOG\nload: control token: 260693 '&lt;unused4791&gt;' is not marked as EOG\nload: control token: 260692 '&lt;unused4790&gt;' is not marked as EOG\nload: control token: 260691 '&lt;unused4789&gt;' is not marked as EOG\nload: control token: 260690 '&lt;unused4788&gt;' is not marked as EOG\nload: control token: 260688 '&lt;unused4786&gt;' is not marked as EOG\nload: control token: 260686 '&lt;unused4784&gt;' is not marked as EOG\nload: control token: 260683 '&lt;unused4781&gt;' is not marked as EOG\nload: control token: 260682 '&lt;unused4780&gt;' is not marked as EOG\nload: control token: 260678 '&lt;unused4776&gt;' is not marked as EOG\nload: control token: 260675 '&lt;unused4773&gt;' is not marked as EOG\nload: control token: 260673 '&lt;unused4771&gt;' is not marked as EOG\nload: control token: 260671 '&lt;unused4769&gt;' is not marked as EOG\nload: control token: 260670 '&lt;unused4768&gt;' is not marked as EOG\nload: control token: 260669 '&lt;unused4767&gt;' is not marked as EOG\nload: control token: 260667 '&lt;unused4765&gt;' is not marked as EOG\nload: control token: 260665 '&lt;unused4763&gt;' is not marked as EOG\nload: control token: 260662 '&lt;unused4760&gt;' is not marked as EOG\nload: control token: 260660 '&lt;unused4758&gt;' is not marked as EOG\nload: control token: 260658 '&lt;unused4756&gt;' is not marked as EOG\nload: control token: 260657 '&lt;unused4755&gt;' is not marked as EOG\nload: control token: 260655 '&lt;unused4753&gt;' is not marked as EOG\nload: control token: 260654 '&lt;unused4752&gt;' is not marked as EOG\nload: control token: 260653 '&lt;unused4751&gt;' is not marked as EOG\nload: control token: 260651 '&lt;unused4749&gt;' is not marked as EOG\nload: control token: 260650 '&lt;unused4748&gt;' is not marked as EOG\nload: control token: 260649 '&lt;unused4747&gt;' is not marked as EOG\nload: control token: 260648 '&lt;unused4746&gt;' is not marked as EOG\nload: control token: 261666 '&lt;unused5764&gt;' is not marked as EOG\nload: control token: 260647 '&lt;unused4745&gt;' is not marked as EOG\nload: control token: 260646 '&lt;unused4744&gt;' is not marked as EOG\nload: control token: 260645 '&lt;unused4743&gt;' is not marked as EOG\nload: control token: 260644 '&lt;unused4742&gt;' is not marked as EOG\nload: control token: 260643 '&lt;unused4741&gt;' is not marked as EOG\nload: control token: 260641 '&lt;unused4739&gt;' is not marked as EOG\nload: control token: 260640 '&lt;unused4738&gt;' is not marked as EOG\nload: control token: 260639 '&lt;unused4737&gt;' is not marked as EOG\nload: control token: 260637 '&lt;unused4735&gt;' is not marked as EOG\nload: control token: 260636 '&lt;unused4734&gt;' is not marked as EOG\nload: control token: 260635 '&lt;unused4733&gt;' is not marked as EOG\nload: control token: 260634 '&lt;unused4732&gt;' is not marked as EOG\nload: control token: 260631 '&lt;unused4729&gt;' is not marked as EOG\nload: control token: 260628 '&lt;unused4726&gt;' is not marked as EOG\nload: control token: 260626 '&lt;unused4724&gt;' is not marked as EOG\nload: control token: 260625 '&lt;unused4723&gt;' is not marked as EOG\nload: control token: 260624 '&lt;unused4722&gt;' is not marked as EOG\nload: control token: 260623 '&lt;unused4721&gt;' is not marked as EOG\nload: control token: 260622 '&lt;unused4720&gt;' is not marked as EOG\nload: control token: 260621 '&lt;unused4719&gt;' is not marked as EOG\nload: control token: 260619 '&lt;unused4717&gt;' is not marked as EOG\nload: control token: 260617 '&lt;unused4715&gt;' is not marked as EOG\nload: control token: 260615 '&lt;unused4713&gt;' is not marked as EOG\nload: control token: 260608 '&lt;unused4706&gt;' is not marked as EOG\nload: control token: 260607 '&lt;unused4705&gt;' is not marked as EOG\nload: control token: 260606 '&lt;unused4704&gt;' is not marked as EOG\nload: control token: 260605 '&lt;unused4703&gt;' is not marked as EOG\nload: control token: 260603 '&lt;unused4701&gt;' is not marked as EOG\nload: control token: 260602 '&lt;unused4700&gt;' is not marked as EOG\nload: control token: 260601 '&lt;unused4699&gt;' is not marked as EOG\nload: control token: 260600 '&lt;unused4698&gt;' is not marked as EOG\nload: control token: 260599 '&lt;unused4697&gt;' is not marked as EOG\nload: control token: 260597 '&lt;unused4695&gt;' is not marked as EOG\nload: control token: 260593 '&lt;unused4691&gt;' is not marked as EOG\nload: control token: 260591 '&lt;unused4689&gt;' is not marked as EOG\nload: control token: 260590 '&lt;unused4688&gt;' is not marked as EOG\nload: control token: 260589 '&lt;unused4687&gt;' is not marked as EOG\nload: control token: 260588 '&lt;unused4686&gt;' is not marked as EOG\nload: control token: 260587 '&lt;unused4685&gt;' is not marked as EOG\nload: control token: 260585 '&lt;unused4683&gt;' is not marked as EOG\nload: control token: 260584 '&lt;unused4682&gt;' is not marked as EOG\nload: control token: 260579 '&lt;unused4677&gt;' is not marked as EOG\nload: control token: 260578 '&lt;unused4676&gt;' is not marked as EOG\nload: control token: 260573 '&lt;unused4671&gt;' is not marked as EOG\nload: control token: 260572 '&lt;unused4670&gt;' is not marked as EOG\nload: control token: 260571 '&lt;unused4669&gt;' is not marked as EOG\nload: control token: 260569 '&lt;unused4667&gt;' is not marked as EOG\nload: control token: 260568 '&lt;unused4666&gt;' is not marked as EOG\nload: control token: 260567 '&lt;unused4665&gt;' is not marked as EOG\nload: control token: 260565 '&lt;unused4663&gt;' is not marked as EOG\nload: control token: 260563 '&lt;unused4661&gt;' is not marked as EOG\nload: control token: 260561 '&lt;unused4659&gt;' is not marked as EOG\nload: control token: 260560 '&lt;unused4658&gt;' is not marked as EOG\nload: control token: 260558 '&lt;unused4656&gt;' is not marked as EOG\nload: control token: 260556 '&lt;unused4654&gt;' is not marked as EOG\nload: control token: 260555 '&lt;unused4653&gt;' is not marked as EOG\nload: control token: 260553 '&lt;unused4651&gt;' is not marked as EOG\nload: control token: 260551 '&lt;unused4649&gt;' is not marked as EOG\nload: control token: 260547 '&lt;unused4645&gt;' is not marked as EOG\nload: control token: 260546 '&lt;unused4644&gt;' is not marked as EOG\nload: control token: 260544 '&lt;unused4642&gt;' is not marked as EOG\nload: control token: 260543 '&lt;unused4641&gt;' is not marked as EOG\nload: control token: 260542 '&lt;unused4640&gt;' is not marked as EOG\nload: control token: 260540 '&lt;unused4638&gt;' is not marked as EOG\nload: control token: 260538 '&lt;unused4636&gt;' is not marked as EOG\nload: control token: 260537 '&lt;unused4635&gt;' is not marked as EOG\nload: control token: 260534 '&lt;unused4632&gt;' is not marked as EOG\nload: control token: 260533 '&lt;unused4631&gt;' is not marked as EOG\nload: control token: 260532 '&lt;unused4630&gt;' is not marked as EOG\nload: control token: 260531 '&lt;unused4629&gt;' is not marked as EOG\nload: control token: 260530 '&lt;unused4628&gt;' is not marked as EOG\nload: control token: 260527 '&lt;unused4625&gt;' is not marked as EOG\nload: control token: 260521 '&lt;unused4619&gt;' is not marked as EOG\nload: control token: 260519 '&lt;unused4617&gt;' is not marked as EOG\nload: control token: 260517 '&lt;unused4615&gt;' is not marked as EOG\nload: control token: 260514 '&lt;unused4612&gt;' is not marked as EOG\nload: control token: 260513 '&lt;unused4611&gt;' is not marked as EOG\nload: control token: 260512 '&lt;unused4610&gt;' is not marked as EOG\nload: control token: 260510 '&lt;unused4608&gt;' is not marked as EOG\nload: control token: 260508 '&lt;unused4606&gt;' is not marked as EOG\nload: control token: 260504 '&lt;unused4602&gt;' is not marked as EOG\nload: control token: 260502 '&lt;unused4600&gt;' is not marked as EOG\nload: control token: 260501 '&lt;unused4599&gt;' is not marked as EOG\nload: control token: 260500 '&lt;unused4598&gt;' is not marked as EOG\nload: control token: 260498 '&lt;unused4596&gt;' is not marked as EOG\nload: control token: 260497 '&lt;unused4595&gt;' is not marked as EOG\nload: control token: 260496 '&lt;unused4594&gt;' is not marked as EOG\nload: control token: 260494 '&lt;unused4592&gt;' is not marked as EOG\nload: control token: 260492 '&lt;unused4590&gt;' is not marked as EOG\nload: control token: 260490 '&lt;unused4588&gt;' is not marked as EOG\nload: control token: 260485 '&lt;unused4583&gt;' is not marked as EOG\nload: control token: 260482 '&lt;unused4580&gt;' is not marked as EOG\nload: control token: 260481 '&lt;unused4579&gt;' is not marked as EOG\nload: control token: 260479 '&lt;unused4577&gt;' is not marked as EOG\nload: control token: 260477 '&lt;unused4575&gt;' is not marked as EOG\nload: control token: 260476 '&lt;unused4574&gt;' is not marked as EOG\nload: control token: 260474 '&lt;unused4572&gt;' is not marked as EOG\nload: control token: 260473 '&lt;unused4571&gt;' is not marked as EOG\nload: control token: 260472 '&lt;unused4570&gt;' is not marked as EOG\nload: control token: 260471 '&lt;unused4569&gt;' is not marked as EOG\nload: control token: 260468 '&lt;unused4566&gt;' is not marked as EOG\nload: control token: 260467 '&lt;unused4565&gt;' is not marked as EOG\nload: control token: 260466 '&lt;unused4564&gt;' is not marked as EOG\nload: control token: 260465 '&lt;unused4563&gt;' is not marked as EOG\nload: control token: 260464 '&lt;unused4562&gt;' is not marked as EOG\nload: control token: 260463 '&lt;unused4561&gt;' is not marked as EOG\nload: control token: 260462 '&lt;unused4560&gt;' is not marked as EOG\nload: control token: 260461 '&lt;unused4559&gt;' is not marked as EOG\nload: control token: 260460 '&lt;unused4558&gt;' is not marked as EOG\nload: control token: 260459 '&lt;unused4557&gt;' is not marked as EOG\nload: control token: 260458 '&lt;unused4556&gt;' is not marked as EOG\nload: control token: 260455 '&lt;unused4553&gt;' is not marked as EOG\nload: control token: 260454 '&lt;unused4552&gt;' is not marked as EOG\nload: control token: 260453 '&lt;unused4551&gt;' is not marked as EOG\nload: control token: 260452 '&lt;unused4550&gt;' is not marked as EOG\nload: control token: 260449 '&lt;unused4547&gt;' is not marked as EOG\nload: control token: 260448 '&lt;unused4546&gt;' is not marked as EOG\nload: control token: 260446 '&lt;unused4544&gt;' is not marked as EOG\nload: control token: 260443 '&lt;unused4541&gt;' is not marked as EOG\nload: control token: 260441 '&lt;unused4539&gt;' is not marked as EOG\nload: control token: 260440 '&lt;unused4538&gt;' is not marked as EOG\nload: control token: 260439 '&lt;unused4537&gt;' is not marked as EOG\nload: control token: 260436 '&lt;unused4534&gt;' is not marked as EOG\nload: control token: 260433 '&lt;unused4531&gt;' is not marked as EOG\nload: control token: 260432 '&lt;unused4530&gt;' is not marked as EOG\nload: control token: 260430 '&lt;unused4528&gt;' is not marked as EOG\nload: control token: 260429 '&lt;unused4527&gt;' is not marked as EOG\nload: control token: 260424 '&lt;unused4522&gt;' is not marked as EOG\nload: control token: 260423 '&lt;unused4521&gt;' is not marked as EOG\nload: control token: 260420 '&lt;unused4518&gt;' is not marked as EOG\nload: control token: 260415 '&lt;unused4513&gt;' is not marked as EOG\nload: control token: 260411 '&lt;unused4509&gt;' is not marked as EOG\nload: control token: 260408 '&lt;unused4506&gt;' is not marked as EOG\nload: control token: 260407 '&lt;unused4505&gt;' is not marked as EOG\nload: control token: 260406 '&lt;unused4504&gt;' is not marked as EOG\nload: control token: 260403 '&lt;unused4501&gt;' is not marked as EOG\nload: control token: 260402 '&lt;unused4500&gt;' is not marked as EOG\nload: control token: 260400 '&lt;unused4498&gt;' is not marked as EOG\nload: control token: 260399 '&lt;unused4497&gt;' is not marked as EOG\nload: control token: 260398 '&lt;unused4496&gt;' is not marked as EOG\nload: control token: 260395 '&lt;unused4493&gt;' is not marked as EOG\nload: control token: 260394 '&lt;unused4492&gt;' is not marked as EOG\nload: control token: 260393 '&lt;unused4491&gt;' is not marked as EOG\nload: control token: 260392 '&lt;unused4490&gt;' is not marked as EOG\nload: control token: 260388 '&lt;unused4486&gt;' is not marked as EOG\nload: control token: 260387 '&lt;unused4485&gt;' is not marked as EOG\nload: control token: 260386 '&lt;unused4484&gt;' is not marked as EOG\nload: control token: 260385 '&lt;unused4483&gt;' is not marked as EOG\nload: control token: 260383 '&lt;unused4481&gt;' is not marked as EOG\nload: control token: 260380 '&lt;unused4478&gt;' is not marked as EOG\nload: control token: 260378 '&lt;unused4476&gt;' is not marked as EOG\nload: control token: 260377 '&lt;unused4475&gt;' is not marked as EOG\nload: control token: 260374 '&lt;unused4472&gt;' is not marked as EOG\nload: control token: 260373 '&lt;unused4471&gt;' is not marked as EOG\nload: control token: 260365 '&lt;unused4463&gt;' is not marked as EOG\nload: control token: 260364 '&lt;unused4462&gt;' is not marked as EOG\nload: control token: 260362 '&lt;unused4460&gt;' is not marked as EOG\nload: control token: 260360 '&lt;unused4458&gt;' is not marked as EOG\nload: control token: 260357 '&lt;unused4455&gt;' is not marked as EOG\nload: control token: 260355 '&lt;unused4453&gt;' is not marked as EOG\nload: control token: 260351 '&lt;unused4449&gt;' is not marked as EOG\nload: control token: 260350 '&lt;unused4448&gt;' is not marked as EOG\nload: control token: 260346 '&lt;unused4444&gt;' is not marked as EOG\nload: control token: 260342 '&lt;unused4440&gt;' is not marked as EOG\nload: control token: 260338 '&lt;unused4436&gt;' is not marked as EOG\nload: control token: 260337 '&lt;unused4435&gt;' is not marked as EOG\nload: control token: 260336 '&lt;unused4434&gt;' is not marked as EOG\nload: control token: 260335 '&lt;unused4433&gt;' is not marked as EOG\nload: control token: 260333 '&lt;unused4431&gt;' is not marked as EOG\nload: control token: 260332 '&lt;unused4430&gt;' is not marked as EOG\nload: control token: 260331 '&lt;unused4429&gt;' is not marked as EOG\nload: control token: 260330 '&lt;unused4428&gt;' is not marked as EOG\nload: control token: 260329 '&lt;unused4427&gt;' is not marked as EOG\nload: control token: 260328 '&lt;unused4426&gt;' is not marked as EOG\nload: control token: 260327 '&lt;unused4425&gt;' is not marked as EOG\nload: control token: 260326 '&lt;unused4424&gt;' is not marked as EOG\nload: control token: 260325 '&lt;unused4423&gt;' is not marked as EOG\nload: control token: 260320 '&lt;unused4418&gt;' is not marked as EOG\nload: control token: 260319 '&lt;unused4417&gt;' is not marked as EOG\nload: control token: 260318 '&lt;unused4416&gt;' is not marked as EOG\nload: control token: 260316 '&lt;unused4414&gt;' is not marked as EOG\nload: control token: 260311 '&lt;unused4409&gt;' is not marked as EOG\nload: control token: 260307 '&lt;unused4405&gt;' is not marked as EOG\nload: control token: 260304 '&lt;unused4402&gt;' is not marked as EOG\nload: control token: 260303 '&lt;unused4401&gt;' is not marked as EOG\nload: control token: 260300 '&lt;unused4398&gt;' is not marked as EOG\nload: control token: 260299 '&lt;unused4397&gt;' is not marked as EOG\nload: control token: 260298 '&lt;unused4396&gt;' is not marked as EOG\nload: control token: 260297 '&lt;unused4395&gt;' is not marked as EOG\nload: control token: 260291 '&lt;unused4389&gt;' is not marked as EOG\nload: control token: 260289 '&lt;unused4387&gt;' is not marked as EOG\nload: control token: 260288 '&lt;unused4386&gt;' is not marked as EOG\nload: control token: 260286 '&lt;unused4384&gt;' is not marked as EOG\nload: control token: 260285 '&lt;unused4383&gt;' is not marked as EOG\nload: control token: 260284 '&lt;unused4382&gt;' is not marked as EOG\nload: control token: 260283 '&lt;unused4381&gt;' is not marked as EOG\nload: control token: 260278 '&lt;unused4376&gt;' is not marked as EOG\nload: control token: 260275 '&lt;unused4373&gt;' is not marked as EOG\nload: control token: 260272 '&lt;unused4370&gt;' is not marked as EOG\nload: control token: 260271 '&lt;unused4369&gt;' is not marked as EOG\nload: control token: 260270 '&lt;unused4368&gt;' is not marked as EOG\nload: control token: 260264 '&lt;unused4362&gt;' is not marked as EOG\nload: control token: 260262 '&lt;unused4360&gt;' is not marked as EOG\nload: control token: 260261 '&lt;unused4359&gt;' is not marked as EOG\nload: control token: 260259 '&lt;unused4357&gt;' is not marked as EOG\nload: control token: 260257 '&lt;unused4355&gt;' is not marked as EOG\nload: control token: 260256 '&lt;unused4354&gt;' is not marked as EOG\nload: control token: 260254 '&lt;unused4352&gt;' is not marked as EOG\nload: control token: 261290 '&lt;unused5388&gt;' is not marked as EOG\nload: control token: 260253 '&lt;unused4351&gt;' is not marked as EOG\nload: control token: 260252 '&lt;unused4350&gt;' is not marked as EOG\nload: control token: 260251 '&lt;unused4349&gt;' is not marked as EOG\nload: control token: 260250 '&lt;unused4348&gt;' is not marked as EOG\nload: control token: 260249 '&lt;unused4347&gt;' is not marked as EOG\nload: control token: 260248 '&lt;unused4346&gt;' is not marked as EOG\nload: control token: 260246 '&lt;unused4344&gt;' is not marked as EOG\nload: control token: 260243 '&lt;unused4341&gt;' is not marked as EOG\nload: control token: 260242 '&lt;unused4340&gt;' is not marked as EOG\nload: control token: 260239 '&lt;unused4337&gt;' is not marked as EOG\nload: control token: 260236 '&lt;unused4334&gt;' is not marked as EOG\nload: control token: 260235 '&lt;unused4333&gt;' is not marked as EOG\nload: control token: 260233 '&lt;unused4331&gt;' is not marked as EOG\nload: control token: 260232 '&lt;unused4330&gt;' is not marked as EOG\nload: control token: 260228 '&lt;unused4326&gt;' is not marked as EOG\nload: control token: 260226 '&lt;unused4324&gt;' is not marked as EOG\nload: control token: 260222 '&lt;unused4320&gt;' is not marked as EOG\nload: control token: 260219 '&lt;unused4317&gt;' is not marked as EOG\nload: control token: 260218 '&lt;unused4316&gt;' is not marked as EOG\nload: control token: 260217 '&lt;unused4315&gt;' is not marked as EOG\nload: control token: 260215 '&lt;unused4313&gt;' is not marked as EOG\nload: control token: 260214 '&lt;unused4312&gt;' is not marked as EOG\nload: control token: 260213 '&lt;unused4311&gt;' is not marked as EOG\nload: control token: 260212 '&lt;unused4310&gt;' is not marked as EOG\nload: control token: 260210 '&lt;unused4308&gt;' is not marked as EOG\nload: control token: 260206 '&lt;unused4304&gt;' is not marked as EOG\nload: control token: 260204 '&lt;unused4302&gt;' is not marked as EOG\nload: control token: 260201 '&lt;unused4299&gt;' is not marked as EOG\nload: control token: 260200 '&lt;unused4298&gt;' is not marked as EOG\nload: control token: 260199 '&lt;unused4297&gt;' is not marked as EOG\nload: control token: 260197 '&lt;unused4295&gt;' is not marked as EOG\nload: control token: 260196 '&lt;unused4294&gt;' is not marked as EOG\nload: control token: 260194 '&lt;unused4292&gt;' is not marked as EOG\nload: control token: 260192 '&lt;unused4290&gt;' is not marked as EOG\nload: control token: 260190 '&lt;unused4288&gt;' is not marked as EOG\nload: control token: 260186 '&lt;unused4284&gt;' is not marked as EOG\nload: control token: 260185 '&lt;unused4283&gt;' is not marked as EOG\nload: control token: 260181 '&lt;unused4279&gt;' is not marked as EOG\nload: control token: 260178 '&lt;unused4276&gt;' is not marked as EOG\nload: control token: 260177 '&lt;unused4275&gt;' is not marked as EOG\nload: control token: 260176 '&lt;unused4274&gt;' is not marked as EOG\nload: control token: 260175 '&lt;unused4273&gt;' is not marked as EOG\nload: control token: 260174 '&lt;unused4272&gt;' is not marked as EOG\nload: control token: 260172 '&lt;unused4270&gt;' is not marked as EOG\nload: control token: 260171 '&lt;unused4269&gt;' is not marked as EOG\nload: control token: 260168 '&lt;unused4266&gt;' is not marked as EOG\nload: control token: 260167 '&lt;unused4265&gt;' is not marked as EOG\nload: control token: 260165 '&lt;unused4263&gt;' is not marked as EOG\nload: control token: 260164 '&lt;unused4262&gt;' is not marked as EOG\nload: control token: 260161 '&lt;unused4259&gt;' is not marked as EOG\nload: control token: 260159 '&lt;unused4257&gt;' is not marked as EOG\nload: control token: 260158 '&lt;unused4256&gt;' is not marked as EOG\nload: control token: 260157 '&lt;unused4255&gt;' is not marked as EOG\nload: control token: 260156 '&lt;unused4254&gt;' is not marked as EOG\nload: control token: 260153 '&lt;unused4251&gt;' is not marked as EOG\nload: control token: 260151 '&lt;unused4249&gt;' is not marked as EOG\nload: control token: 260150 '&lt;unused4248&gt;' is not marked as EOG\nload: control token: 260147 '&lt;unused4245&gt;' is not marked as EOG\nload: control token: 260146 '&lt;unused4244&gt;' is not marked as EOG\nload: control token: 260145 '&lt;unused4243&gt;' is not marked as EOG\nload: control token: 260143 '&lt;unused4241&gt;' is not marked as EOG\nload: control token: 260141 '&lt;unused4239&gt;' is not marked as EOG\nload: control token: 260140 '&lt;unused4238&gt;' is not marked as EOG\nload: control token: 260136 '&lt;unused4234&gt;' is not marked as EOG\nload: control token: 260135 '&lt;unused4233&gt;' is not marked as EOG\nload: control token: 260133 '&lt;unused4231&gt;' is not marked as EOG\nload: control token: 260131 '&lt;unused4229&gt;' is not marked as EOG\nload: control token: 260130 '&lt;unused4228&gt;' is not marked as EOG\nload: control token: 260129 '&lt;unused4227&gt;' is not marked as EOG\nload: control token: 260127 '&lt;unused4225&gt;' is not marked as EOG\nload: control token: 260126 '&lt;unused4224&gt;' is not marked as EOG\nload: control token: 260125 '&lt;unused4223&gt;' is not marked as EOG\nload: control token: 260124 '&lt;unused4222&gt;' is not marked as EOG\nload: control token: 260120 '&lt;unused4218&gt;' is not marked as EOG\nload: control token: 260115 '&lt;unused4213&gt;' is not marked as EOG\nload: control token: 260112 '&lt;unused4210&gt;' is not marked as EOG\nload: control token: 260111 '&lt;unused4209&gt;' is not marked as EOG\nload: control token: 260110 '&lt;unused4208&gt;' is not marked as EOG\nload: control token: 260109 '&lt;unused4207&gt;' is not marked as EOG\nload: control token: 260108 '&lt;unused4206&gt;' is not marked as EOG\nload: control token: 260107 '&lt;unused4205&gt;' is not marked as EOG\nload: control token: 260106 '&lt;unused4204&gt;' is not marked as EOG\nload: control token: 260102 '&lt;unused4200&gt;' is not marked as EOG\nload: control token: 260099 '&lt;unused4197&gt;' is not marked as EOG\nload: control token: 260098 '&lt;unused4196&gt;' is not marked as EOG\nload: control token: 260097 '&lt;unused4195&gt;' is not marked as EOG\nload: control token: 260093 '&lt;unused4191&gt;' is not marked as EOG\nload: control token: 260092 '&lt;unused4190&gt;' is not marked as EOG\nload: control token: 260091 '&lt;unused4189&gt;' is not marked as EOG\nload: control token: 260087 '&lt;unused4185&gt;' is not marked as EOG\nload: control token: 260085 '&lt;unused4183&gt;' is not marked as EOG\nload: control token: 260084 '&lt;unused4182&gt;' is not marked as EOG\nload: control token: 260083 '&lt;unused4181&gt;' is not marked as EOG\nload: control token: 260079 '&lt;unused4177&gt;' is not marked as EOG\nload: control token: 260078 '&lt;unused4176&gt;' is not marked as EOG\nload: control token: 260077 '&lt;unused4175&gt;' is not marked as EOG\nload: control token: 260076 '&lt;unused4174&gt;' is not marked as EOG\nload: control token: 260075 '&lt;unused4173&gt;' is not marked as EOG\nload: control token: 260074 '&lt;unused4172&gt;' is not marked as EOG\nload: control token: 260070 '&lt;unused4168&gt;' is not marked as EOG\nload: control token: 260069 '&lt;unused4167&gt;' is not marked as EOG\nload: control token: 260068 '&lt;unused4166&gt;' is not marked as EOG\nload: control token: 260066 '&lt;unused4164&gt;' is not marked as EOG\nload: control token: 260054 '&lt;unused4152&gt;' is not marked as EOG\nload: control token: 260045 '&lt;unused4143&gt;' is not marked as EOG\nload: control token: 260044 '&lt;unused4142&gt;' is not marked as EOG\nload: control token: 260043 '&lt;unused4141&gt;' is not marked as EOG\nload: control token: 260042 '&lt;unused4140&gt;' is not marked as EOG\nload: control token: 260040 '&lt;unused4138&gt;' is not marked as EOG\nload: control token: 260039 '&lt;unused4137&gt;' is not marked as EOG\nload: control token: 260038 '&lt;unused4136&gt;' is not marked as EOG\nload: control token: 260035 '&lt;unused4133&gt;' is not marked as EOG\nload: control token: 260034 '&lt;unused4132&gt;' is not marked as EOG\nload: control token: 260032 '&lt;unused4130&gt;' is not marked as EOG\nload: control token: 260031 '&lt;unused4129&gt;' is not marked as EOG\nload: control token: 260030 '&lt;unused4128&gt;' is not marked as EOG\nload: control token: 260029 '&lt;unused4127&gt;' is not marked as EOG\nload: control token: 260028 '&lt;unused4126&gt;' is not marked as EOG\nload: control token: 260026 '&lt;unused4124&gt;' is not marked as EOG\nload: control token: 260025 '&lt;unused4123&gt;' is not marked as EOG\nload: control token: 260024 '&lt;unused4122&gt;' is not marked as EOG\nload: control token: 260023 '&lt;unused4121&gt;' is not marked as EOG\nload: control token: 260022 '&lt;unused4120&gt;' is not marked as EOG\nload: control token: 260021 '&lt;unused4119&gt;' is not marked as EOG\nload: control token: 260020 '&lt;unused4118&gt;' is not marked as EOG\nload: control token: 260019 '&lt;unused4117&gt;' is not marked as EOG\nload: control token: 260018 '&lt;unused4116&gt;' is not marked as EOG\nload: control token: 260017 '&lt;unused4115&gt;' is not marked as EOG\nload: control token: 260016 '&lt;unused4114&gt;' is not marked as EOG\nload: control token: 260014 '&lt;unused4112&gt;' is not marked as EOG\nload: control token: 260012 '&lt;unused4110&gt;' is not marked as EOG\nload: control token: 260011 '&lt;unused4109&gt;' is not marked as EOG\nload: control token: 260010 '&lt;unused4108&gt;' is not marked as EOG\nload: control token: 260008 '&lt;unused4106&gt;' is not marked as EOG\nload: control token: 260007 '&lt;unused4105&gt;' is not marked as EOG\nload: control token: 260006 '&lt;unused4104&gt;' is not marked as EOG\nload: control token: 260005 '&lt;unused4103&gt;' is not marked as EOG\nload: control token: 260004 '&lt;unused4102&gt;' is not marked as EOG\nload: control token: 260002 '&lt;unused4100&gt;' is not marked as EOG\nload: control token: 260001 '&lt;unused4099&gt;' is not marked as EOG\nload: control token: 259999 '&lt;unused4097&gt;' is not marked as EOG\nload: control token: 259998 '&lt;unused4096&gt;' is not marked as EOG\nload: control token: 259997 '&lt;unused4095&gt;' is not marked as EOG\nload: control token: 259994 '&lt;unused4092&gt;' is not marked as EOG\nload: control token: 259993 '&lt;unused4091&gt;' is not marked as EOG\nload: control token: 259992 '&lt;unused4090&gt;' is not marked as EOG\nload: control token: 259990 '&lt;unused4088&gt;' is not marked as EOG\nload: control token: 259987 '&lt;unused4085&gt;' is not marked as EOG\nload: control token: 259986 '&lt;unused4084&gt;' is not marked as EOG\nload: control token: 259985 '&lt;unused4083&gt;' is not marked as EOG\nload: control token: 259981 '&lt;unused4079&gt;' is not marked as EOG\nload: control token: 259980 '&lt;unused4078&gt;' is not marked as EOG\nload: control token: 259978 '&lt;unused4076&gt;' is not marked as EOG\nload: control token: 259975 '&lt;unused4073&gt;' is not marked as EOG\nload: control token: 259966 '&lt;unused4064&gt;' is not marked as EOG\nload: control token: 259964 '&lt;unused4062&gt;' is not marked as EOG\nload: control token: 259963 '&lt;unused4061&gt;' is not marked as EOG\nload: control token: 259958 '&lt;unused4056&gt;' is not marked as EOG\nload: control token: 259955 '&lt;unused4053&gt;' is not marked as EOG\nload: control token: 259954 '&lt;unused4052&gt;' is not marked as EOG\nload: control token: 259951 '&lt;unused4049&gt;' is not marked as EOG\nload: control token: 259950 '&lt;unused4048&gt;' is not marked as EOG\nload: control token: 259948 '&lt;unused4046&gt;' is not marked as EOG\nload: control token: 259947 '&lt;unused4045&gt;' is not marked as EOG\nload: control token: 259946 '&lt;unused4044&gt;' is not marked as EOG\nload: control token: 259943 '&lt;unused4041&gt;' is not marked as EOG\nload: control token: 259941 '&lt;unused4039&gt;' is not marked as EOG\nload: control token: 259940 '&lt;unused4038&gt;' is not marked as EOG\nload: control token: 259939 '&lt;unused4037&gt;' is not marked as EOG\nload: control token: 259938 '&lt;unused4036&gt;' is not marked as EOG\nload: control token: 259936 '&lt;unused4034&gt;' is not marked as EOG\nload: control token: 259935 '&lt;unused4033&gt;' is not marked as EOG\nload: control token: 259934 '&lt;unused4032&gt;' is not marked as EOG\nload: control token: 259933 '&lt;unused4031&gt;' is not marked as EOG\nload: control token: 259931 '&lt;unused4029&gt;' is not marked as EOG\nload: control token: 259930 '&lt;unused4028&gt;' is not marked as EOG\nload: control token: 259928 '&lt;unused4026&gt;' is not marked as EOG\nload: control token: 259927 '&lt;unused4025&gt;' is not marked as EOG\nload: control token: 259926 '&lt;unused4024&gt;' is not marked as EOG\nload: control token: 259924 '&lt;unused4022&gt;' is not marked as EOG\nload: control token: 259922 '&lt;unused4020&gt;' is not marked as EOG\nload: control token: 259918 '&lt;unused4016&gt;' is not marked as EOG\nload: control token: 259913 '&lt;unused4011&gt;' is not marked as EOG\nload: control token: 259911 '&lt;unused4009&gt;' is not marked as EOG\nload: control token: 259910 '&lt;unused4008&gt;' is not marked as EOG\nload: control token: 259909 '&lt;unused4007&gt;' is not marked as EOG\nload: control token: 259907 '&lt;unused4005&gt;' is not marked as EOG\nload: control token: 259906 '&lt;unused4004&gt;' is not marked as EOG\nload: control token: 259905 '&lt;unused4003&gt;' is not marked as EOG\nload: control token: 259902 '&lt;unused4000&gt;' is not marked as EOG\nload: control token: 259901 '&lt;unused3999&gt;' is not marked as EOG\nload: control token: 259898 '&lt;unused3996&gt;' is not marked as EOG\nload: control token: 259897 '&lt;unused3995&gt;' is not marked as EOG\nload: control token: 259894 '&lt;unused3992&gt;' is not marked as EOG\nload: control token: 259893 '&lt;unused3991&gt;' is not marked as EOG\nload: control token: 259890 '&lt;unused3988&gt;' is not marked as EOG\nload: control token: 259889 '&lt;unused3987&gt;' is not marked as EOG\nload: control token: 259888 '&lt;unused3986&gt;' is not marked as EOG\nload: control token: 259887 '&lt;unused3985&gt;' is not marked as EOG\nload: control token: 259885 '&lt;unused3983&gt;' is not marked as EOG\nload: control token: 259883 '&lt;unused3981&gt;' is not marked as EOG\nload: control token: 259882 '&lt;unused3980&gt;' is not marked as EOG\nload: control token: 259879 '&lt;unused3977&gt;' is not marked as EOG\nload: control token: 259874 '&lt;unused3972&gt;' is not marked as EOG\nload: control token: 259872 '&lt;unused3970&gt;' is not marked as EOG\nload: control token: 259871 '&lt;unused3969&gt;' is not marked as EOG\nload: control token: 259861 '&lt;unused3959&gt;' is not marked as EOG\nload: control token: 259860 '&lt;unused3958&gt;' is not marked as EOG\nload: control token: 259859 '&lt;unused3957&gt;' is not marked as EOG\nload: control token: 259858 '&lt;unused3956&gt;' is not marked as EOG\nload: control token: 259855 '&lt;unused3953&gt;' is not marked as EOG\nload: control token: 259854 '&lt;unused3952&gt;' is not marked as EOG\nload: control token: 259853 '&lt;unused3951&gt;' is not marked as EOG\nload: control token: 259852 '&lt;unused3950&gt;' is not marked as EOG\nload: control token: 259850 '&lt;unused3948&gt;' is not marked as EOG\nload: control token: 259848 '&lt;unused3946&gt;' is not marked as EOG\nload: control token: 259847 '&lt;unused3945&gt;' is not marked as EOG\nload: control token: 259846 '&lt;unused3944&gt;' is not marked as EOG\nload: control token: 259844 '&lt;unused3942&gt;' is not marked as EOG\nload: control token: 259842 '&lt;unused3940&gt;' is not marked as EOG\nload: control token: 259840 '&lt;unused3938&gt;' is not marked as EOG\nload: control token: 259837 '&lt;unused3935&gt;' is not marked as EOG\nload: control token: 259835 '&lt;unused3933&gt;' is not marked as EOG\nload: control token: 259834 '&lt;unused3932&gt;' is not marked as EOG\nload: control token: 259832 '&lt;unused3930&gt;' is not marked as EOG\nload: control token: 259831 '&lt;unused3929&gt;' is not marked as EOG\nload: control token: 259829 '&lt;unused3927&gt;' is not marked as EOG\nload: control token: 259828 '&lt;unused3926&gt;' is not marked as EOG\nload: control token: 259826 '&lt;unused3924&gt;' is not marked as EOG\nload: control token: 259825 '&lt;unused3923&gt;' is not marked as EOG\nload: control token: 259824 '&lt;unused3922&gt;' is not marked as EOG\nload: control token: 259823 '&lt;unused3921&gt;' is not marked as EOG\nload: control token: 259822 '&lt;unused3920&gt;' is not marked as EOG\nload: control token: 259821 '&lt;unused3919&gt;' is not marked as EOG\nload: control token: 259820 '&lt;unused3918&gt;' is not marked as EOG\nload: control token: 259819 '&lt;unused3917&gt;' is not marked as EOG\nload: control token: 259817 '&lt;unused3915&gt;' is not marked as EOG\nload: control token: 259816 '&lt;unused3914&gt;' is not marked as EOG\nload: control token: 259815 '&lt;unused3913&gt;' is not marked as EOG\nload: control token: 259814 '&lt;unused3912&gt;' is not marked as EOG\nload: control token: 259813 '&lt;unused3911&gt;' is not marked as EOG\nload: control token: 259810 '&lt;unused3908&gt;' is not marked as EOG\nload: control token: 259809 '&lt;unused3907&gt;' is not marked as EOG\nload: control token: 259805 '&lt;unused3903&gt;' is not marked as EOG\nload: control token: 259804 '&lt;unused3902&gt;' is not marked as EOG\nload: control token: 259802 '&lt;unused3900&gt;' is not marked as EOG\nload: control token: 259800 '&lt;unused3898&gt;' is not marked as EOG\nload: control token: 259797 '&lt;unused3895&gt;' is not marked as EOG\nload: control token: 259793 '&lt;unused3891&gt;' is not marked as EOG\nload: control token: 259792 '&lt;unused3890&gt;' is not marked as EOG\nload: control token: 259791 '&lt;unused3889&gt;' is not marked as EOG\nload: control token: 259789 '&lt;unused3887&gt;' is not marked as EOG\nload: control token: 259788 '&lt;unused3886&gt;' is not marked as EOG\nload: control token: 259787 '&lt;unused3885&gt;' is not marked as EOG\nload: control token: 259786 '&lt;unused3884&gt;' is not marked as EOG\nload: control token: 259781 '&lt;unused3879&gt;' is not marked as EOG\nload: control token: 259780 '&lt;unused3878&gt;' is not marked as EOG\nload: control token: 259777 '&lt;unused3875&gt;' is not marked as EOG\nload: control token: 259776 '&lt;unused3874&gt;' is not marked as EOG\nload: control token: 259775 '&lt;unused3873&gt;' is not marked as EOG\nload: control token: 259774 '&lt;unused3872&gt;' is not marked as EOG\nload: control token: 259772 '&lt;unused3870&gt;' is not marked as EOG\nload: control token: 259771 '&lt;unused3869&gt;' is not marked as EOG\nload: control token: 259769 '&lt;unused3867&gt;' is not marked as EOG\nload: control token: 259766 '&lt;unused3864&gt;' is not marked as EOG\nload: control token: 259762 '&lt;unused3860&gt;' is not marked as EOG\nload: control token: 259758 '&lt;unused3856&gt;' is not marked as EOG\nload: control token: 259756 '&lt;unused3854&gt;' is not marked as EOG\nload: control token: 259755 '&lt;unused3853&gt;' is not marked as EOG\nload: control token: 259754 '&lt;unused3852&gt;' is not marked as EOG\nload: control token: 259751 '&lt;unused3849&gt;' is not marked as EOG\nload: control token: 259748 '&lt;unused3846&gt;' is not marked as EOG\nload: control token: 259746 '&lt;unused3844&gt;' is not marked as EOG\nload: control token: 259744 '&lt;unused3842&gt;' is not marked as EOG\nload: control token: 259742 '&lt;unused3840&gt;' is not marked as EOG\nload: control token: 259738 '&lt;unused3836&gt;' is not marked as EOG\nload: control token: 259736 '&lt;unused3834&gt;' is not marked as EOG\nload: control token: 259732 '&lt;unused3830&gt;' is not marked as EOG\nload: control token: 259727 '&lt;unused3825&gt;' is not marked as EOG\nload: control token: 259726 '&lt;unused3824&gt;' is not marked as EOG\nload: control token: 259724 '&lt;unused3822&gt;' is not marked as EOG\nload: control token: 259723 '&lt;unused3821&gt;' is not marked as EOG\nload: control token: 259721 '&lt;unused3819&gt;' is not marked as EOG\nload: control token: 259720 '&lt;unused3818&gt;' is not marked as EOG\nload: control token: 259717 '&lt;unused3815&gt;' is not marked as EOG\nload: control token: 259710 '&lt;unused3808&gt;' is not marked as EOG\nload: control token: 259708 '&lt;unused3806&gt;' is not marked as EOG\nload: control token: 259702 '&lt;unused3800&gt;' is not marked as EOG\nload: control token: 259701 '&lt;unused3799&gt;' is not marked as EOG\nload: control token: 259700 '&lt;unused3798&gt;' is not marked as EOG\nload: control token: 259699 '&lt;unused3797&gt;' is not marked as EOG\nload: control token: 259696 '&lt;unused3794&gt;' is not marked as EOG\nload: control token: 259693 '&lt;unused3791&gt;' is not marked as EOG\nload: control token: 259692 '&lt;unused3790&gt;' is not marked as EOG\nload: control token: 259691 '&lt;unused3789&gt;' is not marked as EOG\nload: control token: 259690 '&lt;unused3788&gt;' is not marked as EOG\nload: control token: 259689 '&lt;unused3787&gt;' is not marked as EOG\nload: control token: 259684 '&lt;unused3782&gt;' is not marked as EOG\nload: control token: 261585 '&lt;unused5683&gt;' is not marked as EOG\nload: control token: 259682 '&lt;unused3780&gt;' is not marked as EOG\nload: control token: 259681 '&lt;unused3779&gt;' is not marked as EOG\nload: control token: 259680 '&lt;unused3778&gt;' is not marked as EOG\nload: control token: 259679 '&lt;unused3777&gt;' is not marked as EOG\nload: control token: 259676 '&lt;unused3774&gt;' is not marked as EOG\nload: control token: 259675 '&lt;unused3773&gt;' is not marked as EOG\nload: control token: 259674 '&lt;unused3772&gt;' is not marked as EOG\nload: control token: 259672 '&lt;unused3770&gt;' is not marked as EOG\nload: control token: 259669 '&lt;unused3767&gt;' is not marked as EOG\nload: control token: 259666 '&lt;unused3764&gt;' is not marked as EOG\nload: control token: 259664 '&lt;unused3762&gt;' is not marked as EOG\nload: control token: 259661 '&lt;unused3759&gt;' is not marked as EOG\nload: control token: 259660 '&lt;unused3758&gt;' is not marked as EOG\nload: control token: 259654 '&lt;unused3752&gt;' is not marked as EOG\nload: control token: 259653 '&lt;unused3751&gt;' is not marked as EOG\nload: control token: 259651 '&lt;unused3749&gt;' is not marked as EOG\nload: control token: 259650 '&lt;unused3748&gt;' is not marked as EOG\nload: control token: 259646 '&lt;unused3744&gt;' is not marked as EOG\nload: control token: 259645 '&lt;unused3743&gt;' is not marked as EOG\nload: control token: 259643 '&lt;unused3741&gt;' is not marked as EOG\nload: control token: 259642 '&lt;unused3740&gt;' is not marked as EOG\nload: control token: 259640 '&lt;unused3738&gt;' is not marked as EOG\nload: control token: 259638 '&lt;unused3736&gt;' is not marked as EOG\nload: control token: 259637 '&lt;unused3735&gt;' is not marked as EOG\nload: control token: 259636 '&lt;unused3734&gt;' is not marked as EOG\nload: control token: 259635 '&lt;unused3733&gt;' is not marked as EOG\nload: control token: 259634 '&lt;unused3732&gt;' is not marked as EOG\nload: control token: 259633 '&lt;unused3731&gt;' is not marked as EOG\nload: control token: 259632 '&lt;unused3730&gt;' is not marked as EOG\nload: control token: 259630 '&lt;unused3728&gt;' is not marked as EOG\nload: control token: 259627 '&lt;unused3725&gt;' is not marked as EOG\nload: control token: 259624 '&lt;unused3722&gt;' is not marked as EOG\nload: control token: 259623 '&lt;unused3721&gt;' is not marked as EOG\nload: control token: 259620 '&lt;unused3718&gt;' is not marked as EOG\nload: control token: 259619 '&lt;unused3717&gt;' is not marked as EOG\nload: control token: 259618 '&lt;unused3716&gt;' is not marked as EOG\nload: control token: 259616 '&lt;unused3714&gt;' is not marked as EOG\nload: control token: 259615 '&lt;unused3713&gt;' is not marked as EOG\nload: control token: 259612 '&lt;unused3710&gt;' is not marked as EOG\nload: control token: 259611 '&lt;unused3709&gt;' is not marked as EOG\nload: control token: 259610 '&lt;unused3708&gt;' is not marked as EOG\nload: control token: 259609 '&lt;unused3707&gt;' is not marked as EOG\nload: control token: 259608 '&lt;unused3706&gt;' is not marked as EOG\nload: control token: 259607 '&lt;unused3705&gt;' is not marked as EOG\nload: control token: 259606 '&lt;unused3704&gt;' is not marked as EOG\nload: control token: 259604 '&lt;unused3702&gt;' is not marked as EOG\nload: control token: 259600 '&lt;unused3698&gt;' is not marked as EOG\nload: control token: 259597 '&lt;unused3695&gt;' is not marked as EOG\nload: control token: 259596 '&lt;unused3694&gt;' is not marked as EOG\nload: control token: 259595 '&lt;unused3693&gt;' is not marked as EOG\nload: control token: 259593 '&lt;unused3691&gt;' is not marked as EOG\nload: control token: 259589 '&lt;unused3687&gt;' is not marked as EOG\nload: control token: 259587 '&lt;unused3685&gt;' is not marked as EOG\nload: control token: 259586 '&lt;unused3684&gt;' is not marked as EOG\nload: control token: 259584 '&lt;unused3682&gt;' is not marked as EOG\nload: control token: 259583 '&lt;unused3681&gt;' is not marked as EOG\nload: control token: 259581 '&lt;unused3679&gt;' is not marked as EOG\nload: control token: 259579 '&lt;unused3677&gt;' is not marked as EOG\nload: control token: 259578 '&lt;unused3676&gt;' is not marked as EOG\nload: control token: 259576 '&lt;unused3674&gt;' is not marked as EOG\nload: control token: 259575 '&lt;unused3673&gt;' is not marked as EOG\nload: control token: 259574 '&lt;unused3672&gt;' is not marked as EOG\nload: control token: 259571 '&lt;unused3669&gt;' is not marked as EOG\nload: control token: 259568 '&lt;unused3666&gt;' is not marked as EOG\nload: control token: 259566 '&lt;unused3664&gt;' is not marked as EOG\nload: control token: 261725 '&lt;unused5823&gt;' is not marked as EOG\nload: control token: 259563 '&lt;unused3661&gt;' is not marked as EOG\nload: control token: 259561 '&lt;unused3659&gt;' is not marked as EOG\nload: control token: 259560 '&lt;unused3658&gt;' is not marked as EOG\nload: control token: 259558 '&lt;unused3656&gt;' is not marked as EOG\nload: control token: 259555 '&lt;unused3653&gt;' is not marked as EOG\nload: control token: 259554 '&lt;unused3652&gt;' is not marked as EOG\nload: control token: 259552 '&lt;unused3650&gt;' is not marked as EOG\nload: control token: 259551 '&lt;unused3649&gt;' is not marked as EOG\nload: control token: 259550 '&lt;unused3648&gt;' is not marked as EOG\nload: control token: 259547 '&lt;unused3645&gt;' is not marked as EOG\nload: control token: 259546 '&lt;unused3644&gt;' is not marked as EOG\nload: control token: 259545 '&lt;unused3643&gt;' is not marked as EOG\nload: control token: 259542 '&lt;unused3640&gt;' is not marked as EOG\nload: control token: 259541 '&lt;unused3639&gt;' is not marked as EOG\nload: control token: 259540 '&lt;unused3638&gt;' is not marked as EOG\nload: control token: 259539 '&lt;unused3637&gt;' is not marked as EOG\nload: control token: 259538 '&lt;unused3636&gt;' is not marked as EOG\nload: control token: 259537 '&lt;unused3635&gt;' is not marked as EOG\nload: control token: 259536 '&lt;unused3634&gt;' is not marked as EOG\nload: control token: 259535 '&lt;unused3633&gt;' is not marked as EOG\nload: control token: 259534 '&lt;unused3632&gt;' is not marked as EOG\nload: control token: 259529 '&lt;unused3627&gt;' is not marked as EOG\nload: control token: 259527 '&lt;unused3625&gt;' is not marked as EOG\nload: control token: 259526 '&lt;unused3624&gt;' is not marked as EOG\nload: control token: 259524 '&lt;unused3622&gt;' is not marked as EOG\nload: control token: 259523 '&lt;unused3621&gt;' is not marked as EOG\nload: control token: 259519 '&lt;unused3617&gt;' is not marked as EOG\nload: control token: 259518 '&lt;unused3616&gt;' is not marked as EOG\nload: control token: 259515 '&lt;unused3613&gt;' is not marked as EOG\nload: control token: 259513 '&lt;unused3611&gt;' is not marked as EOG\nload: control token: 259511 '&lt;unused3609&gt;' is not marked as EOG\nload: control token: 259505 '&lt;unused3603&gt;' is not marked as EOG\nload: control token: 259504 '&lt;unused3602&gt;' is not marked as EOG\nload: control token: 259503 '&lt;unused3601&gt;' is not marked as EOG\nload: control token: 259502 '&lt;unused3600&gt;' is not marked as EOG\nload: control token: 259501 '&lt;unused3599&gt;' is not marked as EOG\nload: control token: 259500 '&lt;unused3598&gt;' is not marked as EOG\nload: control token: 259499 '&lt;unused3597&gt;' is not marked as EOG\nload: control token: 259498 '&lt;unused3596&gt;' is not marked as EOG\nload: control token: 259497 '&lt;unused3595&gt;' is not marked as EOG\nload: control token: 259496 '&lt;unused3594&gt;' is not marked as EOG\nload: control token: 259495 '&lt;unused3593&gt;' is not marked as EOG\nload: control token: 259494 '&lt;unused3592&gt;' is not marked as EOG\nload: control token: 259491 '&lt;unused3589&gt;' is not marked as EOG\nload: control token: 259489 '&lt;unused3587&gt;' is not marked as EOG\nload: control token: 259488 '&lt;unused3586&gt;' is not marked as EOG\nload: control token: 259486 '&lt;unused3584&gt;' is not marked as EOG\nload: control token: 259484 '&lt;unused3582&gt;' is not marked as EOG\nload: control token: 259483 '&lt;unused3581&gt;' is not marked as EOG\nload: control token: 259481 '&lt;unused3579&gt;' is not marked as EOG\nload: control token: 259473 '&lt;unused3571&gt;' is not marked as EOG\nload: control token: 259471 '&lt;unused3569&gt;' is not marked as EOG\nload: control token: 259470 '&lt;unused3568&gt;' is not marked as EOG\nload: control token: 259467 '&lt;unused3565&gt;' is not marked as EOG\nload: control token: 259463 '&lt;unused3561&gt;' is not marked as EOG\nload: control token: 259462 '&lt;unused3560&gt;' is not marked as EOG\nload: control token: 259461 '&lt;unused3559&gt;' is not marked as EOG\nload: control token: 259459 '&lt;unused3557&gt;' is not marked as EOG\nload: control token: 259458 '&lt;unused3556&gt;' is not marked as EOG\nload: control token: 259457 '&lt;unused3555&gt;' is not marked as EOG\nload: control token: 259454 '&lt;unused3552&gt;' is not marked as EOG\nload: control token: 259453 '&lt;unused3551&gt;' is not marked as EOG\nload: control token: 259452 '&lt;unused3550&gt;' is not marked as EOG\nload: control token: 259451 '&lt;unused3549&gt;' is not marked as EOG\nload: control token: 259449 '&lt;unused3547&gt;' is not marked as EOG\nload: control token: 259448 '&lt;unused3546&gt;' is not marked as EOG\nload: control token: 259446 '&lt;unused3544&gt;' is not marked as EOG\nload: control token: 259440 '&lt;unused3538&gt;' is not marked as EOG\nload: control token: 259439 '&lt;unused3537&gt;' is not marked as EOG\nload: control token: 259437 '&lt;unused3535&gt;' is not marked as EOG\nload: control token: 259434 '&lt;unused3532&gt;' is not marked as EOG\nload: control token: 259433 '&lt;unused3531&gt;' is not marked as EOG\nload: control token: 259432 '&lt;unused3530&gt;' is not marked as EOG\nload: control token: 259429 '&lt;unused3527&gt;' is not marked as EOG\nload: control token: 259428 '&lt;unused3526&gt;' is not marked as EOG\nload: control token: 259423 '&lt;unused3521&gt;' is not marked as EOG\nload: control token: 259421 '&lt;unused3519&gt;' is not marked as EOG\nload: control token: 259419 '&lt;unused3517&gt;' is not marked as EOG\nload: control token: 259417 '&lt;unused3515&gt;' is not marked as EOG\nload: control token: 259416 '&lt;unused3514&gt;' is not marked as EOG\nload: control token: 259415 '&lt;unused3513&gt;' is not marked as EOG\nload: control token: 259411 '&lt;unused3509&gt;' is not marked as EOG\nload: control token: 259409 '&lt;unused3507&gt;' is not marked as EOG\nload: control token: 259407 '&lt;unused3505&gt;' is not marked as EOG\nload: control token: 259402 '&lt;unused3500&gt;' is not marked as EOG\nload: control token: 261617 '&lt;unused5715&gt;' is not marked as EOG\nload: control token: 259401 '&lt;unused3499&gt;' is not marked as EOG\nload: control token: 259399 '&lt;unused3497&gt;' is not marked as EOG\nload: control token: 259398 '&lt;unused3496&gt;' is not marked as EOG\nload: control token: 259396 '&lt;unused3494&gt;' is not marked as EOG\nload: control token: 259393 '&lt;unused3491&gt;' is not marked as EOG\nload: control token: 259392 '&lt;unused3490&gt;' is not marked as EOG\nload: control token: 259390 '&lt;unused3488&gt;' is not marked as EOG\nload: control token: 259389 '&lt;unused3487&gt;' is not marked as EOG\nload: control token: 259386 '&lt;unused3484&gt;' is not marked as EOG\nload: control token: 259381 '&lt;unused3479&gt;' is not marked as EOG\nload: control token: 259379 '&lt;unused3477&gt;' is not marked as EOG\nload: control token: 259378 '&lt;unused3476&gt;' is not marked as EOG\nload: control token: 259376 '&lt;unused3474&gt;' is not marked as EOG\nload: control token: 259374 '&lt;unused3472&gt;' is not marked as EOG\nload: control token: 259370 '&lt;unused3468&gt;' is not marked as EOG\nload: control token: 259369 '&lt;unused3467&gt;' is not marked as EOG\nload: control token: 259367 '&lt;unused3465&gt;' is not marked as EOG\nload: control token: 259366 '&lt;unused3464&gt;' is not marked as EOG\nload: control token: 259364 '&lt;unused3462&gt;' is not marked as EOG\nload: control token: 259362 '&lt;unused3460&gt;' is not marked as EOG\nload: control token: 259361 '&lt;unused3459&gt;' is not marked as EOG\nload: control token: 259359 '&lt;unused3457&gt;' is not marked as EOG\nload: control token: 259357 '&lt;unused3455&gt;' is not marked as EOG\nload: control token: 259355 '&lt;unused3453&gt;' is not marked as EOG\nload: control token: 259354 '&lt;unused3452&gt;' is not marked as EOG\nload: control token: 259352 '&lt;unused3450&gt;' is not marked as EOG\nload: control token: 259351 '&lt;unused3449&gt;' is not marked as EOG\nload: control token: 259349 '&lt;unused3447&gt;' is not marked as EOG\nload: control token: 259348 '&lt;unused3446&gt;' is not marked as EOG\nload: control token: 259347 '&lt;unused3445&gt;' is not marked as EOG\nload: control token: 259345 '&lt;unused3443&gt;' is not marked as EOG\nload: control token: 259344 '&lt;unused3442&gt;' is not marked as EOG\nload: control token: 259343 '&lt;unused3441&gt;' is not marked as EOG\nload: control token: 259338 '&lt;unused3436&gt;' is not marked as EOG\nload: control token: 259336 '&lt;unused3434&gt;' is not marked as EOG\nload: control token: 259335 '&lt;unused3433&gt;' is not marked as EOG\nload: control token: 259334 '&lt;unused3432&gt;' is not marked as EOG\nload: control token: 259333 '&lt;unused3431&gt;' is not marked as EOG\nload: control token: 259332 '&lt;unused3430&gt;' is not marked as EOG\nload: control token: 259331 '&lt;unused3429&gt;' is not marked as EOG\nload: control token: 259330 '&lt;unused3428&gt;' is not marked as EOG\nload: control token: 259328 '&lt;unused3426&gt;' is not marked as EOG\nload: control token: 259327 '&lt;unused3425&gt;' is not marked as EOG\nload: control token: 259326 '&lt;unused3424&gt;' is not marked as EOG\nload: control token: 259325 '&lt;unused3423&gt;' is not marked as EOG\nload: control token: 259323 '&lt;unused3421&gt;' is not marked as EOG\nload: control token: 259320 '&lt;unused3418&gt;' is not marked as EOG\nload: control token: 259317 '&lt;unused3415&gt;' is not marked as EOG\nload: control token: 259313 '&lt;unused3411&gt;' is not marked as EOG\nload: control token: 259312 '&lt;unused3410&gt;' is not marked as EOG\nload: control token: 259311 '&lt;unused3409&gt;' is not marked as EOG\nload: control token: 259309 '&lt;unused3407&gt;' is not marked as EOG\nload: control token: 259306 '&lt;unused3404&gt;' is not marked as EOG\nload: control token: 259304 '&lt;unused3402&gt;' is not marked as EOG\nload: control token: 259303 '&lt;unused3401&gt;' is not marked as EOG\nload: control token: 259302 '&lt;unused3400&gt;' is not marked as EOG\nload: control token: 259298 '&lt;unused3396&gt;' is not marked as EOG\nload: control token: 259297 '&lt;unused3395&gt;' is not marked as EOG\nload: control token: 259296 '&lt;unused3394&gt;' is not marked as EOG\nload: control token: 259294 '&lt;unused3392&gt;' is not marked as EOG\nload: control token: 259293 '&lt;unused3391&gt;' is not marked as EOG\nload: control token: 259291 '&lt;unused3389&gt;' is not marked as EOG\nload: control token: 259290 '&lt;unused3388&gt;' is not marked as EOG\nload: control token: 259289 '&lt;unused3387&gt;' is not marked as EOG\nload: control token: 259286 '&lt;unused3384&gt;' is not marked as EOG\nload: control token: 259285 '&lt;unused3383&gt;' is not marked as EOG\nload: control token: 259284 '&lt;unused3382&gt;' is not marked as EOG\nload: control token: 259283 '&lt;unused3381&gt;' is not marked as EOG\nload: control token: 259282 '&lt;unused3380&gt;' is not marked as EOG\nload: control token: 259281 '&lt;unused3379&gt;' is not marked as EOG\nload: control token: 259280 '&lt;unused3378&gt;' is not marked as EOG\nload: control token: 259278 '&lt;unused3376&gt;' is not marked as EOG\nload: control token: 259274 '&lt;unused3372&gt;' is not marked as EOG\nload: control token: 259273 '&lt;unused3371&gt;' is not marked as EOG\nload: control token: 259267 '&lt;unused3365&gt;' is not marked as EOG\nload: control token: 259266 '&lt;unused3364&gt;' is not marked as EOG\nload: control token: 259265 '&lt;unused3363&gt;' is not marked as EOG\nload: control token: 259264 '&lt;unused3362&gt;' is not marked as EOG\nload: control token: 259263 '&lt;unused3361&gt;' is not marked as EOG\nload: control token: 259262 '&lt;unused3360&gt;' is not marked as EOG\nload: control token: 259261 '&lt;unused3359&gt;' is not marked as EOG\nload: control token: 259260 '&lt;unused3358&gt;' is not marked as EOG\nload: control token: 259259 '&lt;unused3357&gt;' is not marked as EOG\nload: control token: 259256 '&lt;unused3354&gt;' is not marked as EOG\nload: control token: 259253 '&lt;unused3351&gt;' is not marked as EOG\nload: control token: 259248 '&lt;unused3346&gt;' is not marked as EOG\nload: control token: 259247 '&lt;unused3345&gt;' is not marked as EOG\nload: control token: 259245 '&lt;unused3343&gt;' is not marked as EOG\nload: control token: 259241 '&lt;unused3339&gt;' is not marked as EOG\nload: control token: 259240 '&lt;unused3338&gt;' is not marked as EOG\nload: control token: 259239 '&lt;unused3337&gt;' is not marked as EOG\nload: control token: 259238 '&lt;unused3336&gt;' is not marked as EOG\nload: control token: 259236 '&lt;unused3334&gt;' is not marked as EOG\nload: control token: 259234 '&lt;unused3332&gt;' is not marked as EOG\nload: control token: 259233 '&lt;unused3331&gt;' is not marked as EOG\nload: control token: 259231 '&lt;unused3329&gt;' is not marked as EOG\nload: control token: 259229 '&lt;unused3327&gt;' is not marked as EOG\nload: control token: 259226 '&lt;unused3324&gt;' is not marked as EOG\nload: control token: 259225 '&lt;unused3323&gt;' is not marked as EOG\nload: control token: 259221 '&lt;unused3319&gt;' is not marked as EOG\nload: control token: 259219 '&lt;unused3317&gt;' is not marked as EOG\nload: control token: 259217 '&lt;unused3315&gt;' is not marked as EOG\nload: control token: 259215 '&lt;unused3313&gt;' is not marked as EOG\nload: control token: 259212 '&lt;unused3310&gt;' is not marked as EOG\nload: control token: 259208 '&lt;unused3306&gt;' is not marked as EOG\nload: control token: 259205 '&lt;unused3303&gt;' is not marked as EOG\nload: control token: 259203 '&lt;unused3301&gt;' is not marked as EOG\nload: control token: 259202 '&lt;unused3300&gt;' is not marked as EOG\nload: control token: 259201 '&lt;unused3299&gt;' is not marked as EOG\nload: control token: 259200 '&lt;unused3298&gt;' is not marked as EOG\nload: control token: 259197 '&lt;unused3295&gt;' is not marked as EOG\nload: control token: 259196 '&lt;unused3294&gt;' is not marked as EOG\nload: control token: 259193 '&lt;unused3291&gt;' is not marked as EOG\nload: control token: 259189 '&lt;unused3287&gt;' is not marked as EOG\nload: control token: 259186 '&lt;unused3284&gt;' is not marked as EOG\nload: control token: 259184 '&lt;unused3282&gt;' is not marked as EOG\nload: control token: 259183 '&lt;unused3281&gt;' is not marked as EOG\nload: control token: 259180 '&lt;unused3278&gt;' is not marked as EOG\nload: control token: 259176 '&lt;unused3274&gt;' is not marked as EOG\nload: control token: 259174 '&lt;unused3272&gt;' is not marked as EOG\nload: control token: 259173 '&lt;unused3271&gt;' is not marked as EOG\nload: control token: 259171 '&lt;unused3269&gt;' is not marked as EOG\nload: control token: 259169 '&lt;unused3267&gt;' is not marked as EOG\nload: control token: 259168 '&lt;unused3266&gt;' is not marked as EOG\nload: control token: 259166 '&lt;unused3264&gt;' is not marked as EOG\nload: control token: 259164 '&lt;unused3262&gt;' is not marked as EOG\nload: control token: 259162 '&lt;unused3260&gt;' is not marked as EOG\nload: control token: 259160 '&lt;unused3258&gt;' is not marked as EOG\nload: control token: 259159 '&lt;unused3257&gt;' is not marked as EOG\nload: control token: 259158 '&lt;unused3256&gt;' is not marked as EOG\nload: control token: 259157 '&lt;unused3255&gt;' is not marked as EOG\nload: control token: 259156 '&lt;unused3254&gt;' is not marked as EOG\nload: control token: 259153 '&lt;unused3251&gt;' is not marked as EOG\nload: control token: 259151 '&lt;unused3249&gt;' is not marked as EOG\nload: control token: 259150 '&lt;unused3248&gt;' is not marked as EOG\nload: control token: 259146 '&lt;unused3244&gt;' is not marked as EOG\nload: control token: 259143 '&lt;unused3241&gt;' is not marked as EOG\nload: control token: 259137 '&lt;unused3235&gt;' is not marked as EOG\nload: control token: 259135 '&lt;unused3233&gt;' is not marked as EOG\nload: control token: 259134 '&lt;unused3232&gt;' is not marked as EOG\nload: control token: 259131 '&lt;unused3229&gt;' is not marked as EOG\nload: control token: 259130 '&lt;unused3228&gt;' is not marked as EOG\nload: control token: 259128 '&lt;unused3226&gt;' is not marked as EOG\nload: control token: 259127 '&lt;unused3225&gt;' is not marked as EOG\nload: control token: 259125 '&lt;unused3223&gt;' is not marked as EOG\nload: control token: 259121 '&lt;unused3219&gt;' is not marked as EOG\nload: control token: 259120 '&lt;unused3218&gt;' is not marked as EOG\nload: control token: 259119 '&lt;unused3217&gt;' is not marked as EOG\nload: control token: 259117 '&lt;unused3215&gt;' is not marked as EOG\nload: control token: 259109 '&lt;unused3207&gt;' is not marked as EOG\nload: control token: 259108 '&lt;unused3206&gt;' is not marked as EOG\nload: control token: 259107 '&lt;unused3205&gt;' is not marked as EOG\nload: control token: 259102 '&lt;unused3200&gt;' is not marked as EOG\nload: control token: 259099 '&lt;unused3197&gt;' is not marked as EOG\nload: control token: 259098 '&lt;unused3196&gt;' is not marked as EOG\nload: control token: 259097 '&lt;unused3195&gt;' is not marked as EOG\nload: control token: 259096 '&lt;unused3194&gt;' is not marked as EOG\nload: control token: 259095 '&lt;unused3193&gt;' is not marked as EOG\nload: control token: 259093 '&lt;unused3191&gt;' is not marked as EOG\nload: control token: 259092 '&lt;unused3190&gt;' is not marked as EOG\nload: control token: 259089 '&lt;unused3187&gt;' is not marked as EOG\nload: control token: 259088 '&lt;unused3186&gt;' is not marked as EOG\nload: control token: 259086 '&lt;unused3184&gt;' is not marked as EOG\nload: control token: 259085 '&lt;unused3183&gt;' is not marked as EOG\nload: control token: 259084 '&lt;unused3182&gt;' is not marked as EOG\nload: control token: 259082 '&lt;unused3180&gt;' is not marked as EOG\nload: control token: 259081 '&lt;unused3179&gt;' is not marked as EOG\nload: control token: 259078 '&lt;unused3176&gt;' is not marked as EOG\nload: control token: 259076 '&lt;unused3174&gt;' is not marked as EOG\nload: control token: 259073 '&lt;unused3171&gt;' is not marked as EOG\nload: control token: 259072 '&lt;unused3170&gt;' is not marked as EOG\nload: control token: 259071 '&lt;unused3169&gt;' is not marked as EOG\nload: control token: 259065 '&lt;unused3163&gt;' is not marked as EOG\nload: control token: 259064 '&lt;unused3162&gt;' is not marked as EOG\nload: control token: 259063 '&lt;unused3161&gt;' is not marked as EOG\nload: control token: 259061 '&lt;unused3159&gt;' is not marked as EOG\nload: control token: 259058 '&lt;unused3156&gt;' is not marked as EOG\nload: control token: 259057 '&lt;unused3155&gt;' is not marked as EOG\nload: control token: 259055 '&lt;unused3153&gt;' is not marked as EOG\nload: control token: 259054 '&lt;unused3152&gt;' is not marked as EOG\nload: control token: 259053 '&lt;unused3151&gt;' is not marked as EOG\nload: control token: 259052 '&lt;unused3150&gt;' is not marked as EOG\nload: control token: 259051 '&lt;unused3149&gt;' is not marked as EOG\nload: control token: 259049 '&lt;unused3147&gt;' is not marked as EOG\nload: control token: 259048 '&lt;unused3146&gt;' is not marked as EOG\nload: control token: 259047 '&lt;unused3145&gt;' is not marked as EOG\nload: control token: 259045 '&lt;unused3143&gt;' is not marked as EOG\nload: control token: 259041 '&lt;unused3139&gt;' is not marked as EOG\nload: control token: 259039 '&lt;unused3137&gt;' is not marked as EOG\nload: control token: 259038 '&lt;unused3136&gt;' is not marked as EOG\nload: control token: 259036 '&lt;unused3134&gt;' is not marked as EOG\nload: control token: 259030 '&lt;unused3128&gt;' is not marked as EOG\nload: control token: 259029 '&lt;unused3127&gt;' is not marked as EOG\nload: control token: 259025 '&lt;unused3123&gt;' is not marked as EOG\nload: control token: 259022 '&lt;unused3120&gt;' is not marked as EOG\nload: control token: 259021 '&lt;unused3119&gt;' is not marked as EOG\nload: control token: 259017 '&lt;unused3115&gt;' is not marked as EOG\nload: control token: 259016 '&lt;unused3114&gt;' is not marked as EOG\nload: control token: 259014 '&lt;unused3112&gt;' is not marked as EOG\nload: control token: 259011 '&lt;unused3109&gt;' is not marked as EOG\nload: control token: 259009 '&lt;unused3107&gt;' is not marked as EOG\nload: control token: 259008 '&lt;unused3106&gt;' is not marked as EOG\nload: control token: 259006 '&lt;unused3104&gt;' is not marked as EOG\nload: control token: 259005 '&lt;unused3103&gt;' is not marked as EOG\nload: control token: 259004 '&lt;unused3102&gt;' is not marked as EOG\nload: control token: 258999 '&lt;unused3097&gt;' is not marked as EOG\nload: control token: 258998 '&lt;unused3096&gt;' is not marked as EOG\nload: control token: 258996 '&lt;unused3094&gt;' is not marked as EOG\nload: control token: 258995 '&lt;unused3093&gt;' is not marked as EOG\nload: control token: 258994 '&lt;unused3092&gt;' is not marked as EOG\nload: control token: 258987 '&lt;unused3085&gt;' is not marked as EOG\nload: control token: 258984 '&lt;unused3082&gt;' is not marked as EOG\nload: control token: 258983 '&lt;unused3081&gt;' is not marked as EOG\nload: control token: 258982 '&lt;unused3080&gt;' is not marked as EOG\nload: control token: 258980 '&lt;unused3078&gt;' is not marked as EOG\nload: control token: 258979 '&lt;unused3077&gt;' is not marked as EOG\nload: control token: 258977 '&lt;unused3075&gt;' is not marked as EOG\nload: control token: 258973 '&lt;unused3071&gt;' is not marked as EOG\nload: control token: 258970 '&lt;unused3068&gt;' is not marked as EOG\nload: control token: 258968 '&lt;unused3066&gt;' is not marked as EOG\nload: control token: 258967 '&lt;unused3065&gt;' is not marked as EOG\nload: control token: 258965 '&lt;unused3063&gt;' is not marked as EOG\nload: control token: 258964 '&lt;unused3062&gt;' is not marked as EOG\nload: control token: 258961 '&lt;unused3059&gt;' is not marked as EOG\nload: control token: 258960 '&lt;unused3058&gt;' is not marked as EOG\nload: control token: 258958 '&lt;unused3056&gt;' is not marked as EOG\nload: control token: 258956 '&lt;unused3054&gt;' is not marked as EOG\nload: control token: 258955 '&lt;unused3053&gt;' is not marked as EOG\nload: control token: 258952 '&lt;unused3050&gt;' is not marked as EOG\nload: control token: 258951 '&lt;unused3049&gt;' is not marked as EOG\nload: control token: 258950 '&lt;unused3048&gt;' is not marked as EOG\nload: control token: 258949 '&lt;unused3047&gt;' is not marked as EOG\nload: control token: 258948 '&lt;unused3046&gt;' is not marked as EOG\nload: control token: 258947 '&lt;unused3045&gt;' is not marked as EOG\nload: control token: 258946 '&lt;unused3044&gt;' is not marked as EOG\nload: control token: 258945 '&lt;unused3043&gt;' is not marked as EOG\nload: control token: 258941 '&lt;unused3039&gt;' is not marked as EOG\nload: control token: 258936 '&lt;unused3034&gt;' is not marked as EOG\nload: control token: 258935 '&lt;unused3033&gt;' is not marked as EOG\nload: control token: 258934 '&lt;unused3032&gt;' is not marked as EOG\nload: control token: 258932 '&lt;unused3030&gt;' is not marked as EOG\nload: control token: 258931 '&lt;unused3029&gt;' is not marked as EOG\nload: control token: 258929 '&lt;unused3027&gt;' is not marked as EOG\nload: control token: 258928 '&lt;unused3026&gt;' is not marked as EOG\nload: control token: 258922 '&lt;unused3020&gt;' is not marked as EOG\nload: control token: 258921 '&lt;unused3019&gt;' is not marked as EOG\nload: control token: 258920 '&lt;unused3018&gt;' is not marked as EOG\nload: control token: 258919 '&lt;unused3017&gt;' is not marked as EOG\nload: control token: 258918 '&lt;unused3016&gt;' is not marked as EOG\nload: control token: 258916 '&lt;unused3014&gt;' is not marked as EOG\nload: control token: 258915 '&lt;unused3013&gt;' is not marked as EOG\nload: control token: 258912 '&lt;unused3010&gt;' is not marked as EOG\nload: control token: 258910 '&lt;unused3008&gt;' is not marked as EOG\nload: control token: 258909 '&lt;unused3007&gt;' is not marked as EOG\nload: control token: 258906 '&lt;unused3004&gt;' is not marked as EOG\nload: control token: 258905 '&lt;unused3003&gt;' is not marked as EOG\nload: control token: 258904 '&lt;unused3002&gt;' is not marked as EOG\nload: control token: 258903 '&lt;unused3001&gt;' is not marked as EOG\nload: control token: 258902 '&lt;unused3000&gt;' is not marked as EOG\nload: control token: 258901 '&lt;unused2999&gt;' is not marked as EOG\nload: control token: 258900 '&lt;unused2998&gt;' is not marked as EOG\nload: control token: 258894 '&lt;unused2992&gt;' is not marked as EOG\nload: control token: 258892 '&lt;unused2990&gt;' is not marked as EOG\nload: control token: 258888 '&lt;unused2986&gt;' is not marked as EOG\nload: control token: 258884 '&lt;unused2982&gt;' is not marked as EOG\nload: control token: 258882 '&lt;unused2980&gt;' is not marked as EOG\nload: control token: 258879 '&lt;unused2977&gt;' is not marked as EOG\nload: control token: 258878 '&lt;unused2976&gt;' is not marked as EOG\nload: control token: 258877 '&lt;unused2975&gt;' is not marked as EOG\nload: control token: 258876 '&lt;unused2974&gt;' is not marked as EOG\nload: control token: 258874 '&lt;unused2972&gt;' is not marked as EOG\nload: control token: 258872 '&lt;unused2970&gt;' is not marked as EOG\nload: control token: 258869 '&lt;unused2967&gt;' is not marked as EOG\nload: control token: 258866 '&lt;unused2964&gt;' is not marked as EOG\nload: control token: 258865 '&lt;unused2963&gt;' is not marked as EOG\nload: control token: 258864 '&lt;unused2962&gt;' is not marked as EOG\nload: control token: 258861 '&lt;unused2959&gt;' is not marked as EOG\nload: control token: 258860 '&lt;unused2958&gt;' is not marked as EOG\nload: control token: 258858 '&lt;unused2956&gt;' is not marked as EOG\nload: control token: 258856 '&lt;unused2954&gt;' is not marked as EOG\nload: control token: 258854 '&lt;unused2952&gt;' is not marked as EOG\nload: control token: 258853 '&lt;unused2951&gt;' is not marked as EOG\nload: control token: 258852 '&lt;unused2950&gt;' is not marked as EOG\nload: control token: 258851 '&lt;unused2949&gt;' is not marked as EOG\nload: control token: 258850 '&lt;unused2948&gt;' is not marked as EOG\nload: control token: 258849 '&lt;unused2947&gt;' is not marked as EOG\nload: control token: 258848 '&lt;unused2946&gt;' is not marked as EOG\nload: control token: 258843 '&lt;unused2941&gt;' is not marked as EOG\nload: control token: 258841 '&lt;unused2939&gt;' is not marked as EOG\nload: control token: 258838 '&lt;unused2936&gt;' is not marked as EOG\nload: control token: 258837 '&lt;unused2935&gt;' is not marked as EOG\nload: control token: 258835 '&lt;unused2933&gt;' is not marked as EOG\nload: control token: 258833 '&lt;unused2931&gt;' is not marked as EOG\nload: control token: 258832 '&lt;unused2930&gt;' is not marked as EOG\nload: control token: 258831 '&lt;unused2929&gt;' is not marked as EOG\nload: control token: 258830 '&lt;unused2928&gt;' is not marked as EOG\nload: control token: 258828 '&lt;unused2926&gt;' is not marked as EOG\nload: control token: 258826 '&lt;unused2924&gt;' is not marked as EOG\nload: control token: 258825 '&lt;unused2923&gt;' is not marked as EOG\nload: control token: 258824 '&lt;unused2922&gt;' is not marked as EOG\nload: control token: 258823 '&lt;unused2921&gt;' is not marked as EOG\nload: control token: 258822 '&lt;unused2920&gt;' is not marked as EOG\nload: control token: 258821 '&lt;unused2919&gt;' is not marked as EOG\nload: control token: 258820 '&lt;unused2918&gt;' is not marked as EOG\nload: control token: 258819 '&lt;unused2917&gt;' is not marked as EOG\nload: control token: 258817 '&lt;unused2915&gt;' is not marked as EOG\nload: control token: 258816 '&lt;unused2914&gt;' is not marked as EOG\nload: control token: 258815 '&lt;unused2913&gt;' is not marked as EOG\nload: control token: 258812 '&lt;unused2910&gt;' is not marked as EOG\nload: control token: 258811 '&lt;unused2909&gt;' is not marked as EOG\nload: control token: 258810 '&lt;unused2908&gt;' is not marked as EOG\nload: control token: 258806 '&lt;unused2904&gt;' is not marked as EOG\nload: control token: 258803 '&lt;unused2901&gt;' is not marked as EOG\nload: control token: 258802 '&lt;unused2900&gt;' is not marked as EOG\nload: control token: 258796 '&lt;unused2894&gt;' is not marked as EOG\nload: control token: 258795 '&lt;unused2893&gt;' is not marked as EOG\nload: control token: 258794 '&lt;unused2892&gt;' is not marked as EOG\nload: control token: 258792 '&lt;unused2890&gt;' is not marked as EOG\nload: control token: 258791 '&lt;unused2889&gt;' is not marked as EOG\nload: control token: 258790 '&lt;unused2888&gt;' is not marked as EOG\nload: control token: 258789 '&lt;unused2887&gt;' is not marked as EOG\nload: control token: 258788 '&lt;unused2886&gt;' is not marked as EOG\nload: control token: 258787 '&lt;unused2885&gt;' is not marked as EOG\nload: control token: 258786 '&lt;unused2884&gt;' is not marked as EOG\nload: control token: 258784 '&lt;unused2882&gt;' is not marked as EOG\nload: control token: 258782 '&lt;unused2880&gt;' is not marked as EOG\nload: control token: 258780 '&lt;unused2878&gt;' is not marked as EOG\nload: control token: 258777 '&lt;unused2875&gt;' is not marked as EOG\nload: control token: 258775 '&lt;unused2873&gt;' is not marked as EOG\nload: control token: 258774 '&lt;unused2872&gt;' is not marked as EOG\nload: control token: 259532 '&lt;unused3630&gt;' is not marked as EOG\nload: control token: 258772 '&lt;unused2870&gt;' is not marked as EOG\nload: control token: 258771 '&lt;unused2869&gt;' is not marked as EOG\nload: control token: 258768 '&lt;unused2866&gt;' is not marked as EOG\nload: control token: 258767 '&lt;unused2865&gt;' is not marked as EOG\nload: control token: 258765 '&lt;unused2863&gt;' is not marked as EOG\nload: control token: 258764 '&lt;unused2862&gt;' is not marked as EOG\nload: control token: 258763 '&lt;unused2861&gt;' is not marked as EOG\nload: control token: 258762 '&lt;unused2860&gt;' is not marked as EOG\nload: control token: 258760 '&lt;unused2858&gt;' is not marked as EOG\nload: control token: 258756 '&lt;unused2854&gt;' is not marked as EOG\nload: control token: 258755 '&lt;unused2853&gt;' is not marked as EOG\nload: control token: 258754 '&lt;unused2852&gt;' is not marked as EOG\nload: control token: 258752 '&lt;unused2850&gt;' is not marked as EOG\nload: control token: 258750 '&lt;unused2848&gt;' is not marked as EOG\nload: control token: 258749 '&lt;unused2847&gt;' is not marked as EOG\nload: control token: 258746 '&lt;unused2844&gt;' is not marked as EOG\nload: control token: 258745 '&lt;unused2843&gt;' is not marked as EOG\nload: control token: 258742 '&lt;unused2840&gt;' is not marked as EOG\nload: control token: 258741 '&lt;unused2839&gt;' is not marked as EOG\nload: control token: 258739 '&lt;unused2837&gt;' is not marked as EOG\nload: control token: 258738 '&lt;unused2836&gt;' is not marked as EOG\nload: control token: 258736 '&lt;unused2834&gt;' is not marked as EOG\nload: control token: 258734 '&lt;unused2832&gt;' is not marked as EOG\nload: control token: 258730 '&lt;unused2828&gt;' is not marked as EOG\nload: control token: 258729 '&lt;unused2827&gt;' is not marked as EOG\nload: control token: 258728 '&lt;unused2826&gt;' is not marked as EOG\nload: control token: 258727 '&lt;unused2825&gt;' is not marked as EOG\nload: control token: 258726 '&lt;unused2824&gt;' is not marked as EOG\nload: control token: 258725 '&lt;unused2823&gt;' is not marked as EOG\nload: control token: 258723 '&lt;unused2821&gt;' is not marked as EOG\nload: control token: 258721 '&lt;unused2819&gt;' is not marked as EOG\nload: control token: 258719 '&lt;unused2817&gt;' is not marked as EOG\nload: control token: 258718 '&lt;unused2816&gt;' is not marked as EOG\nload: control token: 258717 '&lt;unused2815&gt;' is not marked as EOG\nload: control token: 258716 '&lt;unused2814&gt;' is not marked as EOG\nload: control token: 258714 '&lt;unused2812&gt;' is not marked as EOG\nload: control token: 258713 '&lt;unused2811&gt;' is not marked as EOG\nload: control token: 258710 '&lt;unused2808&gt;' is not marked as EOG\nload: control token: 258709 '&lt;unused2807&gt;' is not marked as EOG\nload: control token: 258708 '&lt;unused2806&gt;' is not marked as EOG\nload: control token: 258707 '&lt;unused2805&gt;' is not marked as EOG\nload: control token: 258704 '&lt;unused2802&gt;' is not marked as EOG\nload: control token: 258700 '&lt;unused2798&gt;' is not marked as EOG\nload: control token: 258698 '&lt;unused2796&gt;' is not marked as EOG\nload: control token: 258697 '&lt;unused2795&gt;' is not marked as EOG\nload: control token: 258695 '&lt;unused2793&gt;' is not marked as EOG\nload: control token: 258692 '&lt;unused2790&gt;' is not marked as EOG\nload: control token: 258689 '&lt;unused2787&gt;' is not marked as EOG\nload: control token: 258688 '&lt;unused2786&gt;' is not marked as EOG\nload: control token: 258687 '&lt;unused2785&gt;' is not marked as EOG\nload: control token: 258686 '&lt;unused2784&gt;' is not marked as EOG\nload: control token: 258684 '&lt;unused2782&gt;' is not marked as EOG\nload: control token: 258683 '&lt;unused2781&gt;' is not marked as EOG\nload: control token: 258682 '&lt;unused2780&gt;' is not marked as EOG\nload: control token: 258680 '&lt;unused2778&gt;' is not marked as EOG\nload: control token: 258678 '&lt;unused2776&gt;' is not marked as EOG\nload: control token: 258675 '&lt;unused2773&gt;' is not marked as EOG\nload: control token: 258670 '&lt;unused2768&gt;' is not marked as EOG\nload: control token: 258668 '&lt;unused2766&gt;' is not marked as EOG\nload: control token: 258667 '&lt;unused2765&gt;' is not marked as EOG\nload: control token: 258665 '&lt;unused2763&gt;' is not marked as EOG\nload: control token: 258663 '&lt;unused2761&gt;' is not marked as EOG\nload: control token: 258662 '&lt;unused2760&gt;' is not marked as EOG\nload: control token: 258661 '&lt;unused2759&gt;' is not marked as EOG\nload: control token: 258660 '&lt;unused2758&gt;' is not marked as EOG\nload: control token: 258656 '&lt;unused2754&gt;' is not marked as EOG\nload: control token: 258655 '&lt;unused2753&gt;' is not marked as EOG\nload: control token: 258653 '&lt;unused2751&gt;' is not marked as EOG\nload: control token: 258652 '&lt;unused2750&gt;' is not marked as EOG\nload: control token: 258651 '&lt;unused2749&gt;' is not marked as EOG\nload: control token: 258649 '&lt;unused2747&gt;' is not marked as EOG\nload: control token: 258646 '&lt;unused2744&gt;' is not marked as EOG\nload: control token: 258645 '&lt;unused2743&gt;' is not marked as EOG\nload: control token: 258644 '&lt;unused2742&gt;' is not marked as EOG\nload: control token: 258634 '&lt;unused2732&gt;' is not marked as EOG\nload: control token: 258633 '&lt;unused2731&gt;' is not marked as EOG\nload: control token: 258632 '&lt;unused2730&gt;' is not marked as EOG\nload: control token: 258631 '&lt;unused2729&gt;' is not marked as EOG\nload: control token: 258627 '&lt;unused2725&gt;' is not marked as EOG\nload: control token: 258625 '&lt;unused2723&gt;' is not marked as EOG\nload: control token: 258624 '&lt;unused2722&gt;' is not marked as EOG\nload: control token: 258623 '&lt;unused2721&gt;' is not marked as EOG\nload: control token: 258616 '&lt;unused2714&gt;' is not marked as EOG\nload: control token: 258615 '&lt;unused2713&gt;' is not marked as EOG\nload: control token: 258614 '&lt;unused2712&gt;' is not marked as EOG\nload: control token: 258613 '&lt;unused2711&gt;' is not marked as EOG\nload: control token: 258609 '&lt;unused2707&gt;' is not marked as EOG\nload: control token: 258606 '&lt;unused2704&gt;' is not marked as EOG\nload: control token: 258603 '&lt;unused2701&gt;' is not marked as EOG\nload: control token: 258602 '&lt;unused2700&gt;' is not marked as EOG\nload: control token: 258601 '&lt;unused2699&gt;' is not marked as EOG\nload: control token: 258600 '&lt;unused2698&gt;' is not marked as EOG\nload: control token: 258599 '&lt;unused2697&gt;' is not marked as EOG\nload: control token: 258597 '&lt;unused2695&gt;' is not marked as EOG\nload: control token: 258595 '&lt;unused2693&gt;' is not marked as EOG\nload: control token: 258594 '&lt;unused2692&gt;' is not marked as EOG\nload: control token: 258592 '&lt;unused2690&gt;' is not marked as EOG\nload: control token: 258589 '&lt;unused2687&gt;' is not marked as EOG\nload: control token: 258588 '&lt;unused2686&gt;' is not marked as EOG\nload: control token: 258583 '&lt;unused2681&gt;' is not marked as EOG\nload: control token: 258582 '&lt;unused2680&gt;' is not marked as EOG\nload: control token: 258579 '&lt;unused2677&gt;' is not marked as EOG\nload: control token: 258577 '&lt;unused2675&gt;' is not marked as EOG\nload: control token: 258576 '&lt;unused2674&gt;' is not marked as EOG\nload: control token: 258573 '&lt;unused2671&gt;' is not marked as EOG\nload: control token: 258571 '&lt;unused2669&gt;' is not marked as EOG\nload: control token: 258570 '&lt;unused2668&gt;' is not marked as EOG\nload: control token: 258569 '&lt;unused2667&gt;' is not marked as EOG\nload: control token: 258568 '&lt;unused2666&gt;' is not marked as EOG\nload: control token: 258567 '&lt;unused2665&gt;' is not marked as EOG\nload: control token: 258565 '&lt;unused2663&gt;' is not marked as EOG\nload: control token: 258563 '&lt;unused2661&gt;' is not marked as EOG\nload: control token: 258562 '&lt;unused2660&gt;' is not marked as EOG\nload: control token: 258559 '&lt;unused2657&gt;' is not marked as EOG\nload: control token: 258558 '&lt;unused2656&gt;' is not marked as EOG\nload: control token: 258557 '&lt;unused2655&gt;' is not marked as EOG\nload: control token: 258556 '&lt;unused2654&gt;' is not marked as EOG\nload: control token: 258555 '&lt;unused2653&gt;' is not marked as EOG\nload: control token: 258554 '&lt;unused2652&gt;' is not marked as EOG\nload: control token: 258552 '&lt;unused2650&gt;' is not marked as EOG\nload: control token: 258549 '&lt;unused2647&gt;' is not marked as EOG\nload: control token: 258547 '&lt;unused2645&gt;' is not marked as EOG\nload: control token: 258546 '&lt;unused2644&gt;' is not marked as EOG\nload: control token: 258544 '&lt;unused2642&gt;' is not marked as EOG\nload: control token: 258543 '&lt;unused2641&gt;' is not marked as EOG\nload: control token: 258540 '&lt;unused2638&gt;' is not marked as EOG\nload: control token: 258536 '&lt;unused2634&gt;' is not marked as EOG\nload: control token: 258533 '&lt;unused2631&gt;' is not marked as EOG\nload: control token: 258529 '&lt;unused2627&gt;' is not marked as EOG\nload: control token: 258528 '&lt;unused2626&gt;' is not marked as EOG\nload: control token: 258527 '&lt;unused2625&gt;' is not marked as EOG\nload: control token: 258526 '&lt;unused2624&gt;' is not marked as EOG\nload: control token: 258524 '&lt;unused2622&gt;' is not marked as EOG\nload: control token: 258522 '&lt;unused2620&gt;' is not marked as EOG\nload: control token: 258521 '&lt;unused2619&gt;' is not marked as EOG\nload: control token: 258519 '&lt;unused2617&gt;' is not marked as EOG\nload: control token: 258518 '&lt;unused2616&gt;' is not marked as EOG\nload: control token: 258517 '&lt;unused2615&gt;' is not marked as EOG\nload: control token: 258516 '&lt;unused2614&gt;' is not marked as EOG\nload: control token: 258514 '&lt;unused2612&gt;' is not marked as EOG\nload: control token: 258513 '&lt;unused2611&gt;' is not marked as EOG\nload: control token: 258508 '&lt;unused2606&gt;' is not marked as EOG\nload: control token: 258503 '&lt;unused2601&gt;' is not marked as EOG\nload: control token: 258502 '&lt;unused2600&gt;' is not marked as EOG\nload: control token: 258501 '&lt;unused2599&gt;' is not marked as EOG\nload: control token: 258500 '&lt;unused2598&gt;' is not marked as EOG\nload: control token: 258498 '&lt;unused2596&gt;' is not marked as EOG\nload: control token: 258497 '&lt;unused2595&gt;' is not marked as EOG\nload: control token: 258494 '&lt;unused2592&gt;' is not marked as EOG\nload: control token: 258492 '&lt;unused2590&gt;' is not marked as EOG\nload: control token: 258491 '&lt;unused2589&gt;' is not marked as EOG\nload: control token: 258489 '&lt;unused2587&gt;' is not marked as EOG\nload: control token: 258487 '&lt;unused2585&gt;' is not marked as EOG\nload: control token: 258486 '&lt;unused2584&gt;' is not marked as EOG\nload: control token: 258485 '&lt;unused2583&gt;' is not marked as EOG\nload: control token: 258484 '&lt;unused2582&gt;' is not marked as EOG\nload: control token: 258483 '&lt;unused2581&gt;' is not marked as EOG\nload: control token: 258481 '&lt;unused2579&gt;' is not marked as EOG\nload: control token: 258480 '&lt;unused2578&gt;' is not marked as EOG\nload: control token: 258477 '&lt;unused2575&gt;' is not marked as EOG\nload: control token: 258476 '&lt;unused2574&gt;' is not marked as EOG\nload: control token: 258475 '&lt;unused2573&gt;' is not marked as EOG\nload: control token: 258471 '&lt;unused2569&gt;' is not marked as EOG\nload: control token: 258469 '&lt;unused2567&gt;' is not marked as EOG\nload: control token: 258467 '&lt;unused2565&gt;' is not marked as EOG\nload: control token: 258465 '&lt;unused2563&gt;' is not marked as EOG\nload: control token: 258463 '&lt;unused2561&gt;' is not marked as EOG\nload: control token: 258461 '&lt;unused2559&gt;' is not marked as EOG\nload: control token: 258460 '&lt;unused2558&gt;' is not marked as EOG\nload: control token: 258457 '&lt;unused2555&gt;' is not marked as EOG\nload: control token: 258456 '&lt;unused2554&gt;' is not marked as EOG\nload: control token: 258452 '&lt;unused2550&gt;' is not marked as EOG\nload: control token: 258451 '&lt;unused2549&gt;' is not marked as EOG\nload: control token: 258447 '&lt;unused2545&gt;' is not marked as EOG\nload: control token: 258446 '&lt;unused2544&gt;' is not marked as EOG\nload: control token: 258445 '&lt;unused2543&gt;' is not marked as EOG\nload: control token: 258444 '&lt;unused2542&gt;' is not marked as EOG\nload: control token: 258442 '&lt;unused2540&gt;' is not marked as EOG\nload: control token: 258441 '&lt;unused2539&gt;' is not marked as EOG\nload: control token: 258440 '&lt;unused2538&gt;' is not marked as EOG\nload: control token: 258439 '&lt;unused2537&gt;' is not marked as EOG\nload: control token: 258437 '&lt;unused2535&gt;' is not marked as EOG\nload: control token: 258433 '&lt;unused2531&gt;' is not marked as EOG\nload: control token: 258428 '&lt;unused2526&gt;' is not marked as EOG\nload: control token: 258427 '&lt;unused2525&gt;' is not marked as EOG\nload: control token: 258425 '&lt;unused2523&gt;' is not marked as EOG\nload: control token: 258422 '&lt;unused2520&gt;' is not marked as EOG\nload: control token: 258420 '&lt;unused2518&gt;' is not marked as EOG\nload: control token: 258419 '&lt;unused2517&gt;' is not marked as EOG\nload: control token: 258417 '&lt;unused2515&gt;' is not marked as EOG\nload: control token: 258411 '&lt;unused2509&gt;' is not marked as EOG\nload: control token: 258410 '&lt;unused2508&gt;' is not marked as EOG\nload: control token: 258408 '&lt;unused2506&gt;' is not marked as EOG\nload: control token: 258406 '&lt;unused2504&gt;' is not marked as EOG\nload: control token: 258403 '&lt;unused2501&gt;' is not marked as EOG\nload: control token: 258401 '&lt;unused2499&gt;' is not marked as EOG\nload: control token: 258399 '&lt;unused2497&gt;' is not marked as EOG\nload: control token: 258396 '&lt;unused2494&gt;' is not marked as EOG\nload: control token: 258395 '&lt;unused2493&gt;' is not marked as EOG\nload: control token: 258394 '&lt;unused2492&gt;' is not marked as EOG\nload: control token: 258392 '&lt;unused2490&gt;' is not marked as EOG\nload: control token: 258391 '&lt;unused2489&gt;' is not marked as EOG\nload: control token: 258390 '&lt;unused2488&gt;' is not marked as EOG\nload: control token: 258387 '&lt;unused2485&gt;' is not marked as EOG\nload: control token: 258386 '&lt;unused2484&gt;' is not marked as EOG\nload: control token: 258382 '&lt;unused2480&gt;' is not marked as EOG\nload: control token: 258377 '&lt;unused2475&gt;' is not marked as EOG\nload: control token: 258376 '&lt;unused2474&gt;' is not marked as EOG\nload: control token: 258373 '&lt;unused2471&gt;' is not marked as EOG\nload: control token: 258371 '&lt;unused2469&gt;' is not marked as EOG\nload: control token: 258370 '&lt;unused2468&gt;' is not marked as EOG\nload: control token: 258368 '&lt;unused2466&gt;' is not marked as EOG\nload: control token: 258367 '&lt;unused2465&gt;' is not marked as EOG\nload: control token: 258365 '&lt;unused2463&gt;' is not marked as EOG\nload: control token: 258364 '&lt;unused2462&gt;' is not marked as EOG\nload: control token: 258363 '&lt;unused2461&gt;' is not marked as EOG\nload: control token: 258362 '&lt;unused2460&gt;' is not marked as EOG\nload: control token: 258358 '&lt;unused2456&gt;' is not marked as EOG\nload: control token: 258357 '&lt;unused2455&gt;' is not marked as EOG\nload: control token: 258352 '&lt;unused2450&gt;' is not marked as EOG\nload: control token: 258349 '&lt;unused2447&gt;' is not marked as EOG\nload: control token: 258348 '&lt;unused2446&gt;' is not marked as EOG\nload: control token: 258346 '&lt;unused2444&gt;' is not marked as EOG\nload: control token: 258345 '&lt;unused2443&gt;' is not marked as EOG\nload: control token: 258344 '&lt;unused2442&gt;' is not marked as EOG\nload: control token: 258343 '&lt;unused2441&gt;' is not marked as EOG\nload: control token: 258342 '&lt;unused2440&gt;' is not marked as EOG\nload: control token: 258341 '&lt;unused2439&gt;' is not marked as EOG\nload: control token: 258339 '&lt;unused2437&gt;' is not marked as EOG\nload: control token: 258338 '&lt;unused2436&gt;' is not marked as EOG\nload: control token: 258337 '&lt;unused2435&gt;' is not marked as EOG\nload: control token: 258336 '&lt;unused2434&gt;' is not marked as EOG\nload: control token: 258333 '&lt;unused2431&gt;' is not marked as EOG\nload: control token: 258332 '&lt;unused2430&gt;' is not marked as EOG\nload: control token: 258331 '&lt;unused2429&gt;' is not marked as EOG\nload: control token: 258330 '&lt;unused2428&gt;' is not marked as EOG\nload: control token: 258328 '&lt;unused2426&gt;' is not marked as EOG\nload: control token: 258327 '&lt;unused2425&gt;' is not marked as EOG\nload: control token: 258323 '&lt;unused2421&gt;' is not marked as EOG\nload: control token: 258322 '&lt;unused2420&gt;' is not marked as EOG\nload: control token: 258321 '&lt;unused2419&gt;' is not marked as EOG\nload: control token: 258320 '&lt;unused2418&gt;' is not marked as EOG\nload: control token: 258313 '&lt;unused2411&gt;' is not marked as EOG\nload: control token: 258311 '&lt;unused2409&gt;' is not marked as EOG\nload: control token: 258310 '&lt;unused2408&gt;' is not marked as EOG\nload: control token: 258308 '&lt;unused2406&gt;' is not marked as EOG\nload: control token: 258306 '&lt;unused2404&gt;' is not marked as EOG\nload: control token: 258304 '&lt;unused2402&gt;' is not marked as EOG\nload: control token: 258303 '&lt;unused2401&gt;' is not marked as EOG\nload: control token: 258302 '&lt;unused2400&gt;' is not marked as EOG\nload: control token: 258301 '&lt;unused2399&gt;' is not marked as EOG\nload: control token: 258300 '&lt;unused2398&gt;' is not marked as EOG\nload: control token: 258299 '&lt;unused2397&gt;' is not marked as EOG\nload: control token: 258298 '&lt;unused2396&gt;' is not marked as EOG\nload: control token: 258297 '&lt;unused2395&gt;' is not marked as EOG\nload: control token: 258296 '&lt;unused2394&gt;' is not marked as EOG\nload: control token: 258294 '&lt;unused2392&gt;' is not marked as EOG\nload: control token: 258293 '&lt;unused2391&gt;' is not marked as EOG\nload: control token: 258291 '&lt;unused2389&gt;' is not marked as EOG\nload: control token: 258290 '&lt;unused2388&gt;' is not marked as EOG\nload: control token: 258289 '&lt;unused2387&gt;' is not marked as EOG\nload: control token: 258288 '&lt;unused2386&gt;' is not marked as EOG\nload: control token: 258283 '&lt;unused2381&gt;' is not marked as EOG\nload: control token: 258280 '&lt;unused2378&gt;' is not marked as EOG\nload: control token: 258277 '&lt;unused2375&gt;' is not marked as EOG\nload: control token: 258276 '&lt;unused2374&gt;' is not marked as EOG\nload: control token: 259784 '&lt;unused3882&gt;' is not marked as EOG\nload: control token: 258274 '&lt;unused2372&gt;' is not marked as EOG\nload: control token: 258273 '&lt;unused2371&gt;' is not marked as EOG\nload: control token: 258272 '&lt;unused2370&gt;' is not marked as EOG\nload: control token: 258270 '&lt;unused2368&gt;' is not marked as EOG\nload: control token: 258267 '&lt;unused2365&gt;' is not marked as EOG\nload: control token: 258265 '&lt;unused2363&gt;' is not marked as EOG\nload: control token: 258263 '&lt;unused2361&gt;' is not marked as EOG\nload: control token: 258262 '&lt;unused2360&gt;' is not marked as EOG\nload: control token: 258261 '&lt;unused2359&gt;' is not marked as EOG\nload: control token: 258258 '&lt;unused2356&gt;' is not marked as EOG\nload: control token: 258256 '&lt;unused2354&gt;' is not marked as EOG\nload: control token: 258255 '&lt;unused2353&gt;' is not marked as EOG\nload: control token: 258254 '&lt;unused2352&gt;' is not marked as EOG\nload: control token: 258250 '&lt;unused2348&gt;' is not marked as EOG\nload: control token: 258249 '&lt;unused2347&gt;' is not marked as EOG\nload: control token: 258247 '&lt;unused2345&gt;' is not marked as EOG\nload: control token: 258246 '&lt;unused2344&gt;' is not marked as EOG\nload: control token: 258245 '&lt;unused2343&gt;' is not marked as EOG\nload: control token: 258244 '&lt;unused2342&gt;' is not marked as EOG\nload: control token: 258243 '&lt;unused2341&gt;' is not marked as EOG\nload: control token: 258240 '&lt;unused2338&gt;' is not marked as EOG\nload: control token: 258239 '&lt;unused2337&gt;' is not marked as EOG\nload: control token: 258237 '&lt;unused2335&gt;' is not marked as EOG\nload: control token: 258232 '&lt;unused2330&gt;' is not marked as EOG\nload: control token: 258231 '&lt;unused2329&gt;' is not marked as EOG\nload: control token: 258230 '&lt;unused2328&gt;' is not marked as EOG\nload: control token: 258226 '&lt;unused2324&gt;' is not marked as EOG\nload: control token: 258224 '&lt;unused2322&gt;' is not marked as EOG\nload: control token: 258222 '&lt;unused2320&gt;' is not marked as EOG\nload: control token: 258221 '&lt;unused2319&gt;' is not marked as EOG\nload: control token: 258220 '&lt;unused2318&gt;' is not marked as EOG\nload: control token: 258217 '&lt;unused2315&gt;' is not marked as EOG\nload: control token: 258215 '&lt;unused2313&gt;' is not marked as EOG\nload: control token: 258214 '&lt;unused2312&gt;' is not marked as EOG\nload: control token: 258213 '&lt;unused2311&gt;' is not marked as EOG\nload: control token: 258208 '&lt;unused2306&gt;' is not marked as EOG\nload: control token: 258205 '&lt;unused2303&gt;' is not marked as EOG\nload: control token: 258204 '&lt;unused2302&gt;' is not marked as EOG\nload: control token: 258201 '&lt;unused2299&gt;' is not marked as EOG\nload: control token: 258200 '&lt;unused2298&gt;' is not marked as EOG\nload: control token: 258198 '&lt;unused2296&gt;' is not marked as EOG\nload: control token: 258197 '&lt;unused2295&gt;' is not marked as EOG\nload: control token: 258196 '&lt;unused2294&gt;' is not marked as EOG\nload: control token: 258192 '&lt;unused2290&gt;' is not marked as EOG\nload: control token: 258190 '&lt;unused2288&gt;' is not marked as EOG\nload: control token: 258189 '&lt;unused2287&gt;' is not marked as EOG\nload: control token: 258187 '&lt;unused2285&gt;' is not marked as EOG\nload: control token: 258186 '&lt;unused2284&gt;' is not marked as EOG\nload: control token: 258185 '&lt;unused2283&gt;' is not marked as EOG\nload: control token: 258184 '&lt;unused2282&gt;' is not marked as EOG\nload: control token: 258183 '&lt;unused2281&gt;' is not marked as EOG\nload: control token: 258182 '&lt;unused2280&gt;' is not marked as EOG\nload: control token: 258181 '&lt;unused2279&gt;' is not marked as EOG\nload: control token: 258180 '&lt;unused2278&gt;' is not marked as EOG\nload: control token: 258179 '&lt;unused2277&gt;' is not marked as EOG\nload: control token: 258178 '&lt;unused2276&gt;' is not marked as EOG\nload: control token: 258177 '&lt;unused2275&gt;' is not marked as EOG\nload: control token: 258175 '&lt;unused2273&gt;' is not marked as EOG\nload: control token: 258168 '&lt;unused2266&gt;' is not marked as EOG\nload: control token: 258166 '&lt;unused2264&gt;' is not marked as EOG\nload: control token: 258165 '&lt;unused2263&gt;' is not marked as EOG\nload: control token: 258164 '&lt;unused2262&gt;' is not marked as EOG\nload: control token: 258162 '&lt;unused2260&gt;' is not marked as EOG\nload: control token: 258161 '&lt;unused2259&gt;' is not marked as EOG\nload: control token: 258158 '&lt;unused2256&gt;' is not marked as EOG\nload: control token: 258157 '&lt;unused2255&gt;' is not marked as EOG\nload: control token: 258156 '&lt;unused2254&gt;' is not marked as EOG\nload: control token: 258155 '&lt;unused2253&gt;' is not marked as EOG\nload: control token: 258152 '&lt;unused2250&gt;' is not marked as EOG\nload: control token: 258150 '&lt;unused2248&gt;' is not marked as EOG\nload: control token: 258148 '&lt;unused2246&gt;' is not marked as EOG\nload: control token: 258147 '&lt;unused2245&gt;' is not marked as EOG\nload: control token: 258146 '&lt;unused2244&gt;' is not marked as EOG\nload: control token: 258139 '&lt;unused2237&gt;' is not marked as EOG\nload: control token: 258137 '&lt;unused2235&gt;' is not marked as EOG\nload: control token: 258136 '&lt;unused2234&gt;' is not marked as EOG\nload: control token: 258135 '&lt;unused2233&gt;' is not marked as EOG\nload: control token: 258133 '&lt;unused2231&gt;' is not marked as EOG\nload: control token: 258130 '&lt;unused2228&gt;' is not marked as EOG\nload: control token: 258129 '&lt;unused2227&gt;' is not marked as EOG\nload: control token: 258127 '&lt;unused2225&gt;' is not marked as EOG\nload: control token: 258126 '&lt;unused2224&gt;' is not marked as EOG\nload: control token: 258124 '&lt;unused2222&gt;' is not marked as EOG\nload: control token: 258123 '&lt;unused2221&gt;' is not marked as EOG\nload: control token: 258122 '&lt;unused2220&gt;' is not marked as EOG\nload: control token: 258121 '&lt;unused2219&gt;' is not marked as EOG\nload: control token: 258115 '&lt;unused2213&gt;' is not marked as EOG\nload: control token: 258113 '&lt;unused2211&gt;' is not marked as EOG\nload: control token: 258111 '&lt;unused2209&gt;' is not marked as EOG\nload: control token: 258110 '&lt;unused2208&gt;' is not marked as EOG\nload: control token: 258109 '&lt;unused2207&gt;' is not marked as EOG\nload: control token: 258104 '&lt;unused2202&gt;' is not marked as EOG\nload: control token: 258103 '&lt;unused2201&gt;' is not marked as EOG\nload: control token: 258100 '&lt;unused2198&gt;' is not marked as EOG\nload: control token: 258099 '&lt;unused2197&gt;' is not marked as EOG\nload: control token: 258098 '&lt;unused2196&gt;' is not marked as EOG\nload: control token: 258095 '&lt;unused2193&gt;' is not marked as EOG\nload: control token: 258094 '&lt;unused2192&gt;' is not marked as EOG\nload: control token: 258093 '&lt;unused2191&gt;' is not marked as EOG\nload: control token: 258090 '&lt;unused2188&gt;' is not marked as EOG\nload: control token: 258087 '&lt;unused2185&gt;' is not marked as EOG\nload: control token: 258085 '&lt;unused2183&gt;' is not marked as EOG\nload: control token: 258083 '&lt;unused2181&gt;' is not marked as EOG\nload: control token: 258081 '&lt;unused2179&gt;' is not marked as EOG\nload: control token: 258080 '&lt;unused2178&gt;' is not marked as EOG\nload: control token: 258079 '&lt;unused2177&gt;' is not marked as EOG\nload: control token: 258077 '&lt;unused2175&gt;' is not marked as EOG\nload: control token: 258076 '&lt;unused2174&gt;' is not marked as EOG\nload: control token: 258074 '&lt;unused2172&gt;' is not marked as EOG\nload: control token: 258072 '&lt;unused2170&gt;' is not marked as EOG\nload: control token: 258068 '&lt;unused2166&gt;' is not marked as EOG\nload: control token: 258067 '&lt;unused2165&gt;' is not marked as EOG\nload: control token: 258066 '&lt;unused2164&gt;' is not marked as EOG\nload: control token: 258064 '&lt;unused2162&gt;' is not marked as EOG\nload: control token: 258063 '&lt;unused2161&gt;' is not marked as EOG\nload: control token: 258062 '&lt;unused2160&gt;' is not marked as EOG\nload: control token: 258059 '&lt;unused2157&gt;' is not marked as EOG\nload: control token: 258054 '&lt;unused2152&gt;' is not marked as EOG\nload: control token: 258053 '&lt;unused2151&gt;' is not marked as EOG\nload: control token: 258051 '&lt;unused2149&gt;' is not marked as EOG\nload: control token: 258050 '&lt;unused2148&gt;' is not marked as EOG\nload: control token: 258049 '&lt;unused2147&gt;' is not marked as EOG\nload: control token: 258048 '&lt;unused2146&gt;' is not marked as EOG\nload: control token: 258047 '&lt;unused2145&gt;' is not marked as EOG\nload: control token: 258042 '&lt;unused2140&gt;' is not marked as EOG\nload: control token: 258040 '&lt;unused2138&gt;' is not marked as EOG\nload: control token: 258039 '&lt;unused2137&gt;' is not marked as EOG\nload: control token: 258038 '&lt;unused2136&gt;' is not marked as EOG\nload: control token: 258035 '&lt;unused2133&gt;' is not marked as EOG\nload: control token: 258033 '&lt;unused2131&gt;' is not marked as EOG\nload: control token: 258032 '&lt;unused2130&gt;' is not marked as EOG\nload: control token: 258031 '&lt;unused2129&gt;' is not marked as EOG\nload: control token: 258030 '&lt;unused2128&gt;' is not marked as EOG\nload: control token: 258029 '&lt;unused2127&gt;' is not marked as EOG\nload: control token: 258025 '&lt;unused2123&gt;' is not marked as EOG\nload: control token: 258022 '&lt;unused2120&gt;' is not marked as EOG\nload: control token: 258021 '&lt;unused2119&gt;' is not marked as EOG\nload: control token: 258020 '&lt;unused2118&gt;' is not marked as EOG\nload: control token: 258017 '&lt;unused2115&gt;' is not marked as EOG\nload: control token: 258015 '&lt;unused2113&gt;' is not marked as EOG\nload: control token: 258014 '&lt;unused2112&gt;' is not marked as EOG\nload: control token: 258013 '&lt;unused2111&gt;' is not marked as EOG\nload: control token: 258012 '&lt;unused2110&gt;' is not marked as EOG\nload: control token: 258011 '&lt;unused2109&gt;' is not marked as EOG\nload: control token: 258010 '&lt;unused2108&gt;' is not marked as EOG\nload: control token: 258009 '&lt;unused2107&gt;' is not marked as EOG\nload: control token: 258004 '&lt;unused2102&gt;' is not marked as EOG\nload: control token: 258003 '&lt;unused2101&gt;' is not marked as EOG\nload: control token: 258002 '&lt;unused2100&gt;' is not marked as EOG\nload: control token: 257997 '&lt;unused2095&gt;' is not marked as EOG\nload: control token: 257996 '&lt;unused2094&gt;' is not marked as EOG\nload: control token: 257993 '&lt;unused2091&gt;' is not marked as EOG\nload: control token: 257992 '&lt;unused2090&gt;' is not marked as EOG\nload: control token: 257990 '&lt;unused2088&gt;' is not marked as EOG\nload: control token: 257985 '&lt;unused2083&gt;' is not marked as EOG\nload: control token: 257984 '&lt;unused2082&gt;' is not marked as EOG\nload: control token: 257983 '&lt;unused2081&gt;' is not marked as EOG\nload: control token: 257981 '&lt;unused2079&gt;' is not marked as EOG\nload: control token: 257980 '&lt;unused2078&gt;' is not marked as EOG\nload: control token: 257979 '&lt;unused2077&gt;' is not marked as EOG\nload: control token: 257978 '&lt;unused2076&gt;' is not marked as EOG\nload: control token: 257977 '&lt;unused2075&gt;' is not marked as EOG\nload: control token: 257976 '&lt;unused2074&gt;' is not marked as EOG\nload: control token: 257975 '&lt;unused2073&gt;' is not marked as EOG\nload: control token: 257974 '&lt;unused2072&gt;' is not marked as EOG\nload: control token: 257973 '&lt;unused2071&gt;' is not marked as EOG\nload: control token: 257969 '&lt;unused2067&gt;' is not marked as EOG\nload: control token: 257962 '&lt;unused2060&gt;' is not marked as EOG\nload: control token: 257960 '&lt;unused2058&gt;' is not marked as EOG\nload: control token: 257959 '&lt;unused2057&gt;' is not marked as EOG\nload: control token: 257958 '&lt;unused2056&gt;' is not marked as EOG\nload: control token: 257956 '&lt;unused2054&gt;' is not marked as EOG\nload: control token: 257955 '&lt;unused2053&gt;' is not marked as EOG\nload: control token: 257953 '&lt;unused2051&gt;' is not marked as EOG\nload: control token: 257952 '&lt;unused2050&gt;' is not marked as EOG\nload: control token: 257951 '&lt;unused2049&gt;' is not marked as EOG\nload: control token: 257949 '&lt;unused2047&gt;' is not marked as EOG\nload: control token: 257943 '&lt;unused2041&gt;' is not marked as EOG\nload: control token: 257940 '&lt;unused2038&gt;' is not marked as EOG\nload: control token: 257938 '&lt;unused2036&gt;' is not marked as EOG\nload: control token: 257936 '&lt;unused2034&gt;' is not marked as EOG\nload: control token: 257935 '&lt;unused2033&gt;' is not marked as EOG\nload: control token: 257933 '&lt;unused2031&gt;' is not marked as EOG\nload: control token: 257931 '&lt;unused2029&gt;' is not marked as EOG\nload: control token: 257929 '&lt;unused2027&gt;' is not marked as EOG\nload: control token: 257928 '&lt;unused2026&gt;' is not marked as EOG\nload: control token: 257923 '&lt;unused2021&gt;' is not marked as EOG\nload: control token: 257922 '&lt;unused2020&gt;' is not marked as EOG\nload: control token: 257921 '&lt;unused2019&gt;' is not marked as EOG\nload: control token: 257918 '&lt;unused2016&gt;' is not marked as EOG\nload: control token: 257914 '&lt;unused2012&gt;' is not marked as EOG\nload: control token: 257913 '&lt;unused2011&gt;' is not marked as EOG\nload: control token: 257912 '&lt;unused2010&gt;' is not marked as EOG\nload: control token: 257910 '&lt;unused2008&gt;' is not marked as EOG\nload: control token: 257909 '&lt;unused2007&gt;' is not marked as EOG\nload: control token: 257908 '&lt;unused2006&gt;' is not marked as EOG\nload: control token: 257907 '&lt;unused2005&gt;' is not marked as EOG\nload: control token: 257906 '&lt;unused2004&gt;' is not marked as EOG\nload: control token: 257904 '&lt;unused2002&gt;' is not marked as EOG\nload: control token: 257903 '&lt;unused2001&gt;' is not marked as EOG\nload: control token: 257901 '&lt;unused1999&gt;' is not marked as EOG\nload: control token: 257899 '&lt;unused1997&gt;' is not marked as EOG\nload: control token: 257898 '&lt;unused1996&gt;' is not marked as EOG\nload: control token: 257897 '&lt;unused1995&gt;' is not marked as EOG\nload: control token: 257895 '&lt;unused1993&gt;' is not marked as EOG\nload: control token: 257893 '&lt;unused1991&gt;' is not marked as EOG\nload: control token: 257891 '&lt;unused1989&gt;' is not marked as EOG\nload: control token: 257890 '&lt;unused1988&gt;' is not marked as EOG\nload: control token: 257889 '&lt;unused1987&gt;' is not marked as EOG\nload: control token: 257887 '&lt;unused1985&gt;' is not marked as EOG\nload: control token: 257886 '&lt;unused1984&gt;' is not marked as EOG\nload: control token: 257884 '&lt;unused1982&gt;' is not marked as EOG\nload: control token: 257883 '&lt;unused1981&gt;' is not marked as EOG\nload: control token: 257881 '&lt;unused1979&gt;' is not marked as EOG\nload: control token: 257878 '&lt;unused1976&gt;' is not marked as EOG\nload: control token: 257875 '&lt;unused1973&gt;' is not marked as EOG\nload: control token: 257873 '&lt;unused1971&gt;' is not marked as EOG\nload: control token: 257872 '&lt;unused1970&gt;' is not marked as EOG\nload: control token: 257869 '&lt;unused1967&gt;' is not marked as EOG\nload: control token: 257868 '&lt;unused1966&gt;' is not marked as EOG\nload: control token: 257867 '&lt;unused1965&gt;' is not marked as EOG\nload: control token: 257864 '&lt;unused1962&gt;' is not marked as EOG\nload: control token: 257860 '&lt;unused1958&gt;' is not marked as EOG\nload: control token: 257859 '&lt;unused1957&gt;' is not marked as EOG\nload: control token: 257858 '&lt;unused1956&gt;' is not marked as EOG\nload: control token: 257857 '&lt;unused1955&gt;' is not marked as EOG\nload: control token: 257855 '&lt;unused1953&gt;' is not marked as EOG\nload: control token: 257851 '&lt;unused1949&gt;' is not marked as EOG\nload: control token: 257850 '&lt;unused1948&gt;' is not marked as EOG\nload: control token: 257849 '&lt;unused1947&gt;' is not marked as EOG\nload: control token: 257848 '&lt;unused1946&gt;' is not marked as EOG\nload: control token: 257846 '&lt;unused1944&gt;' is not marked as EOG\nload: control token: 257844 '&lt;unused1942&gt;' is not marked as EOG\nload: control token: 257838 '&lt;unused1936&gt;' is not marked as EOG\nload: control token: 257837 '&lt;unused1935&gt;' is not marked as EOG\nload: control token: 257835 '&lt;unused1933&gt;' is not marked as EOG\nload: control token: 257834 '&lt;unused1932&gt;' is not marked as EOG\nload: control token: 257833 '&lt;unused1931&gt;' is not marked as EOG\nload: control token: 257832 '&lt;unused1930&gt;' is not marked as EOG\nload: control token: 257831 '&lt;unused1929&gt;' is not marked as EOG\nload: control token: 257830 '&lt;unused1928&gt;' is not marked as EOG\nload: control token: 257829 '&lt;unused1927&gt;' is not marked as EOG\nload: control token: 257827 '&lt;unused1925&gt;' is not marked as EOG\nload: control token: 257826 '&lt;unused1924&gt;' is not marked as EOG\nload: control token: 257825 '&lt;unused1923&gt;' is not marked as EOG\nload: control token: 257821 '&lt;unused1919&gt;' is not marked as EOG\nload: control token: 257819 '&lt;unused1917&gt;' is not marked as EOG\nload: control token: 257817 '&lt;unused1915&gt;' is not marked as EOG\nload: control token: 257816 '&lt;unused1914&gt;' is not marked as EOG\nload: control token: 257814 '&lt;unused1912&gt;' is not marked as EOG\nload: control token: 257811 '&lt;unused1909&gt;' is not marked as EOG\nload: control token: 257808 '&lt;unused1906&gt;' is not marked as EOG\nload: control token: 257806 '&lt;unused1904&gt;' is not marked as EOG\nload: control token: 257802 '&lt;unused1900&gt;' is not marked as EOG\nload: control token: 257801 '&lt;unused1899&gt;' is not marked as EOG\nload: control token: 257800 '&lt;unused1898&gt;' is not marked as EOG\nload: control token: 257798 '&lt;unused1896&gt;' is not marked as EOG\nload: control token: 257797 '&lt;unused1895&gt;' is not marked as EOG\nload: control token: 257796 '&lt;unused1894&gt;' is not marked as EOG\nload: control token: 257795 '&lt;unused1893&gt;' is not marked as EOG\nload: control token: 257793 '&lt;unused1891&gt;' is not marked as EOG\nload: control token: 257792 '&lt;unused1890&gt;' is not marked as EOG\nload: control token: 257791 '&lt;unused1889&gt;' is not marked as EOG\nload: control token: 257789 '&lt;unused1887&gt;' is not marked as EOG\nload: control token: 257788 '&lt;unused1886&gt;' is not marked as EOG\nload: control token: 257787 '&lt;unused1885&gt;' is not marked as EOG\nload: control token: 257785 '&lt;unused1883&gt;' is not marked as EOG\nload: control token: 257782 '&lt;unused1880&gt;' is not marked as EOG\nload: control token: 257781 '&lt;unused1879&gt;' is not marked as EOG\nload: control token: 257776 '&lt;unused1874&gt;' is not marked as EOG\nload: control token: 257774 '&lt;unused1872&gt;' is not marked as EOG\nload: control token: 257772 '&lt;unused1870&gt;' is not marked as EOG\nload: control token: 257771 '&lt;unused1869&gt;' is not marked as EOG\nload: control token: 257770 '&lt;unused1868&gt;' is not marked as EOG\nload: control token: 257768 '&lt;unused1866&gt;' is not marked as EOG\nload: control token: 257767 '&lt;unused1865&gt;' is not marked as EOG\nload: control token: 257766 '&lt;unused1864&gt;' is not marked as EOG\nload: control token: 257761 '&lt;unused1859&gt;' is not marked as EOG\nload: control token: 257760 '&lt;unused1858&gt;' is not marked as EOG\nload: control token: 257759 '&lt;unused1857&gt;' is not marked as EOG\nload: control token: 257758 '&lt;unused1856&gt;' is not marked as EOG\nload: control token: 257757 '&lt;unused1855&gt;' is not marked as EOG\nload: control token: 257756 '&lt;unused1854&gt;' is not marked as EOG\nload: control token: 257753 '&lt;unused1851&gt;' is not marked as EOG\nload: control token: 257749 '&lt;unused1847&gt;' is not marked as EOG\nload: control token: 257747 '&lt;unused1845&gt;' is not marked as EOG\nload: control token: 257745 '&lt;unused1843&gt;' is not marked as EOG\nload: control token: 257744 '&lt;unused1842&gt;' is not marked as EOG\nload: control token: 257743 '&lt;unused1841&gt;' is not marked as EOG\nload: control token: 257742 '&lt;unused1840&gt;' is not marked as EOG\nload: control token: 257741 '&lt;unused1839&gt;' is not marked as EOG\nload: control token: 257736 '&lt;unused1834&gt;' is not marked as EOG\nload: control token: 257734 '&lt;unused1832&gt;' is not marked as EOG\nload: control token: 257733 '&lt;unused1831&gt;' is not marked as EOG\nload: control token: 257730 '&lt;unused1828&gt;' is not marked as EOG\nload: control token: 257728 '&lt;unused1826&gt;' is not marked as EOG\nload: control token: 257726 '&lt;unused1824&gt;' is not marked as EOG\nload: control token: 257724 '&lt;unused1822&gt;' is not marked as EOG\nload: control token: 257722 '&lt;unused1820&gt;' is not marked as EOG\nload: control token: 257720 '&lt;unused1818&gt;' is not marked as EOG\nload: control token: 257719 '&lt;unused1817&gt;' is not marked as EOG\nload: control token: 257718 '&lt;unused1816&gt;' is not marked as EOG\nload: control token: 257717 '&lt;unused1815&gt;' is not marked as EOG\nload: control token: 257716 '&lt;unused1814&gt;' is not marked as EOG\nload: control token: 257715 '&lt;unused1813&gt;' is not marked as EOG\nload: control token: 257714 '&lt;unused1812&gt;' is not marked as EOG\nload: control token: 257711 '&lt;unused1809&gt;' is not marked as EOG\nload: control token: 257710 '&lt;unused1808&gt;' is not marked as EOG\nload: control token: 257708 '&lt;unused1806&gt;' is not marked as EOG\nload: control token: 257705 '&lt;unused1803&gt;' is not marked as EOG\nload: control token: 257698 '&lt;unused1796&gt;' is not marked as EOG\nload: control token: 257697 '&lt;unused1795&gt;' is not marked as EOG\nload: control token: 257693 '&lt;unused1791&gt;' is not marked as EOG\nload: control token: 257689 '&lt;unused1787&gt;' is not marked as EOG\nload: control token: 257687 '&lt;unused1785&gt;' is not marked as EOG\nload: control token: 257686 '&lt;unused1784&gt;' is not marked as EOG\nload: control token: 257684 '&lt;unused1782&gt;' is not marked as EOG\nload: control token: 257683 '&lt;unused1781&gt;' is not marked as EOG\nload: control token: 257681 '&lt;unused1779&gt;' is not marked as EOG\nload: control token: 257677 '&lt;unused1775&gt;' is not marked as EOG\nload: control token: 257675 '&lt;unused1773&gt;' is not marked as EOG\nload: control token: 257673 '&lt;unused1771&gt;' is not marked as EOG\nload: control token: 257672 '&lt;unused1770&gt;' is not marked as EOG\nload: control token: 257671 '&lt;unused1769&gt;' is not marked as EOG\nload: control token: 257669 '&lt;unused1767&gt;' is not marked as EOG\nload: control token: 257664 '&lt;unused1762&gt;' is not marked as EOG\nload: control token: 257663 '&lt;unused1761&gt;' is not marked as EOG\nload: control token: 257662 '&lt;unused1760&gt;' is not marked as EOG\nload: control token: 258366 '&lt;unused2464&gt;' is not marked as EOG\nload: control token: 257659 '&lt;unused1757&gt;' is not marked as EOG\nload: control token: 257658 '&lt;unused1756&gt;' is not marked as EOG\nload: control token: 257657 '&lt;unused1755&gt;' is not marked as EOG\nload: control token: 261377 '&lt;unused5475&gt;' is not marked as EOG\nload: control token: 257653 '&lt;unused1751&gt;' is not marked as EOG\nload: control token: 257652 '&lt;unused1750&gt;' is not marked as EOG\nload: control token: 257651 '&lt;unused1749&gt;' is not marked as EOG\nload: control token: 257650 '&lt;unused1748&gt;' is not marked as EOG\nload: control token: 257649 '&lt;unused1747&gt;' is not marked as EOG\nload: control token: 257648 '&lt;unused1746&gt;' is not marked as EOG\nload: control token: 257645 '&lt;unused1743&gt;' is not marked as EOG\nload: control token: 257644 '&lt;unused1742&gt;' is not marked as EOG\nload: control token: 257643 '&lt;unused1741&gt;' is not marked as EOG\nload: control token: 257641 '&lt;unused1739&gt;' is not marked as EOG\nload: control token: 257640 '&lt;unused1738&gt;' is not marked as EOG\nload: control token: 257639 '&lt;unused1737&gt;' is not marked as EOG\nload: control token: 257637 '&lt;unused1735&gt;' is not marked as EOG\nload: control token: 257636 '&lt;unused1734&gt;' is not marked as EOG\nload: control token: 257632 '&lt;unused1730&gt;' is not marked as EOG\nload: control token: 257627 '&lt;unused1725&gt;' is not marked as EOG\nload: control token: 257623 '&lt;unused1721&gt;' is not marked as EOG\nload: control token: 257622 '&lt;unused1720&gt;' is not marked as EOG\nload: control token: 257618 '&lt;unused1716&gt;' is not marked as EOG\nload: control token: 257617 '&lt;unused1715&gt;' is not marked as EOG\nload: control token: 257613 '&lt;unused1711&gt;' is not marked as EOG\nload: control token: 257611 '&lt;unused1709&gt;' is not marked as EOG\nload: control token: 257609 '&lt;unused1707&gt;' is not marked as EOG\nload: control token: 257606 '&lt;unused1704&gt;' is not marked as EOG\nload: control token: 257603 '&lt;unused1701&gt;' is not marked as EOG\nload: control token: 257600 '&lt;unused1698&gt;' is not marked as EOG\nload: control token: 257598 '&lt;unused1696&gt;' is not marked as EOG\nload: control token: 257597 '&lt;unused1695&gt;' is not marked as EOG\nload: control token: 257596 '&lt;unused1694&gt;' is not marked as EOG\nload: control token: 257594 '&lt;unused1692&gt;' is not marked as EOG\nload: control token: 257593 '&lt;unused1691&gt;' is not marked as EOG\nload: control token: 257586 '&lt;unused1684&gt;' is not marked as EOG\nload: control token: 257585 '&lt;unused1683&gt;' is not marked as EOG\nload: control token: 257583 '&lt;unused1681&gt;' is not marked as EOG\nload: control token: 257582 '&lt;unused1680&gt;' is not marked as EOG\nload: control token: 257581 '&lt;unused1679&gt;' is not marked as EOG\nload: control token: 257580 '&lt;unused1678&gt;' is not marked as EOG\nload: control token: 257579 '&lt;unused1677&gt;' is not marked as EOG\nload: control token: 257576 '&lt;unused1674&gt;' is not marked as EOG\nload: control token: 257573 '&lt;unused1671&gt;' is not marked as EOG\nload: control token: 257572 '&lt;unused1670&gt;' is not marked as EOG\nload: control token: 257571 '&lt;unused1669&gt;' is not marked as EOG\nload: control token: 257570 '&lt;unused1668&gt;' is not marked as EOG\nload: control token: 257569 '&lt;unused1667&gt;' is not marked as EOG\nload: control token: 257568 '&lt;unused1666&gt;' is not marked as EOG\nload: control token: 257566 '&lt;unused1664&gt;' is not marked as EOG\nload: control token: 257565 '&lt;unused1663&gt;' is not marked as EOG\nload: control token: 257561 '&lt;unused1659&gt;' is not marked as EOG\nload: control token: 261944 '&lt;unused6042&gt;' is not marked as EOG\nload: control token: 257560 '&lt;unused1658&gt;' is not marked as EOG\nload: control token: 257557 '&lt;unused1655&gt;' is not marked as EOG\nload: control token: 257556 '&lt;unused1654&gt;' is not marked as EOG\nload: control token: 257555 '&lt;unused1653&gt;' is not marked as EOG\nload: control token: 257552 '&lt;unused1650&gt;' is not marked as EOG\nload: control token: 257550 '&lt;unused1648&gt;' is not marked as EOG\nload: control token: 257549 '&lt;unused1647&gt;' is not marked as EOG\nload: control token: 257548 '&lt;unused1646&gt;' is not marked as EOG\nload: control token: 257547 '&lt;unused1645&gt;' is not marked as EOG\nload: control token: 257546 '&lt;unused1644&gt;' is not marked as EOG\nload: control token: 257545 '&lt;unused1643&gt;' is not marked as EOG\nload: control token: 257542 '&lt;unused1640&gt;' is not marked as EOG\nload: control token: 257541 '&lt;unused1639&gt;' is not marked as EOG\nload: control token: 257538 '&lt;unused1636&gt;' is not marked as EOG\nload: control token: 257536 '&lt;unused1634&gt;' is not marked as EOG\nload: control token: 257534 '&lt;unused1632&gt;' is not marked as EOG\nload: control token: 257532 '&lt;unused1630&gt;' is not marked as EOG\nload: control token: 257528 '&lt;unused1626&gt;' is not marked as EOG\nload: control token: 257527 '&lt;unused1625&gt;' is not marked as EOG\nload: control token: 257525 '&lt;unused1623&gt;' is not marked as EOG\nload: control token: 257524 '&lt;unused1622&gt;' is not marked as EOG\nload: control token: 257522 '&lt;unused1620&gt;' is not marked as EOG\nload: control token: 257520 '&lt;unused1618&gt;' is not marked as EOG\nload: control token: 257518 '&lt;unused1616&gt;' is not marked as EOG\nload: control token: 257517 '&lt;unused1615&gt;' is not marked as EOG\nload: control token: 257516 '&lt;unused1614&gt;' is not marked as EOG\nload: control token: 257512 '&lt;unused1610&gt;' is not marked as EOG\nload: control token: 257511 '&lt;unused1609&gt;' is not marked as EOG\nload: control token: 257506 '&lt;unused1604&gt;' is not marked as EOG\nload: control token: 257504 '&lt;unused1602&gt;' is not marked as EOG\nload: control token: 257503 '&lt;unused1601&gt;' is not marked as EOG\nload: control token: 257502 '&lt;unused1600&gt;' is not marked as EOG\nload: control token: 257500 '&lt;unused1598&gt;' is not marked as EOG\nload: control token: 257499 '&lt;unused1597&gt;' is not marked as EOG\nload: control token: 257498 '&lt;unused1596&gt;' is not marked as EOG\nload: control token: 257496 '&lt;unused1594&gt;' is not marked as EOG\nload: control token: 257495 '&lt;unused1593&gt;' is not marked as EOG\nload: control token: 257493 '&lt;unused1591&gt;' is not marked as EOG\nload: control token: 257492 '&lt;unused1590&gt;' is not marked as EOG\nload: control token: 257491 '&lt;unused1589&gt;' is not marked as EOG\nload: control token: 257490 '&lt;unused1588&gt;' is not marked as EOG\nload: control token: 257488 '&lt;unused1586&gt;' is not marked as EOG\nload: control token: 257485 '&lt;unused1583&gt;' is not marked as EOG\nload: control token: 257482 '&lt;unused1580&gt;' is not marked as EOG\nload: control token: 257479 '&lt;unused1577&gt;' is not marked as EOG\nload: control token: 257478 '&lt;unused1576&gt;' is not marked as EOG\nload: control token: 257477 '&lt;unused1575&gt;' is not marked as EOG\nload: control token: 257476 '&lt;unused1574&gt;' is not marked as EOG\nload: control token: 257475 '&lt;unused1573&gt;' is not marked as EOG\nload: control token: 257474 '&lt;unused1572&gt;' is not marked as EOG\nload: control token: 257471 '&lt;unused1569&gt;' is not marked as EOG\nload: control token: 257470 '&lt;unused1568&gt;' is not marked as EOG\nload: control token: 257469 '&lt;unused1567&gt;' is not marked as EOG\nload: control token: 257468 '&lt;unused1566&gt;' is not marked as EOG\nload: control token: 257467 '&lt;unused1565&gt;' is not marked as EOG\nload: control token: 257465 '&lt;unused1563&gt;' is not marked as EOG\nload: control token: 257461 '&lt;unused1559&gt;' is not marked as EOG\nload: control token: 257460 '&lt;unused1558&gt;' is not marked as EOG\nload: control token: 257456 '&lt;unused1554&gt;' is not marked as EOG\nload: control token: 257455 '&lt;unused1553&gt;' is not marked as EOG\nload: control token: 257451 '&lt;unused1549&gt;' is not marked as EOG\nload: control token: 257450 '&lt;unused1548&gt;' is not marked as EOG\nload: control token: 257447 '&lt;unused1545&gt;' is not marked as EOG\nload: control token: 257446 '&lt;unused1544&gt;' is not marked as EOG\nload: control token: 257445 '&lt;unused1543&gt;' is not marked as EOG\nload: control token: 257444 '&lt;unused1542&gt;' is not marked as EOG\nload: control token: 257442 '&lt;unused1540&gt;' is not marked as EOG\nload: control token: 257441 '&lt;unused1539&gt;' is not marked as EOG\nload: control token: 257438 '&lt;unused1536&gt;' is not marked as EOG\nload: control token: 257436 '&lt;unused1534&gt;' is not marked as EOG\nload: control token: 257435 '&lt;unused1533&gt;' is not marked as EOG\nload: control token: 257433 '&lt;unused1531&gt;' is not marked as EOG\nload: control token: 257431 '&lt;unused1529&gt;' is not marked as EOG\nload: control token: 257430 '&lt;unused1528&gt;' is not marked as EOG\nload: control token: 257429 '&lt;unused1527&gt;' is not marked as EOG\nload: control token: 257426 '&lt;unused1524&gt;' is not marked as EOG\nload: control token: 257425 '&lt;unused1523&gt;' is not marked as EOG\nload: control token: 257424 '&lt;unused1522&gt;' is not marked as EOG\nload: control token: 257423 '&lt;unused1521&gt;' is not marked as EOG\nload: control token: 257422 '&lt;unused1520&gt;' is not marked as EOG\nload: control token: 257420 '&lt;unused1518&gt;' is not marked as EOG\nload: control token: 257417 '&lt;unused1515&gt;' is not marked as EOG\nload: control token: 257416 '&lt;unused1514&gt;' is not marked as EOG\nload: control token: 257412 '&lt;unused1510&gt;' is not marked as EOG\nload: control token: 257411 '&lt;unused1509&gt;' is not marked as EOG\nload: control token: 257410 '&lt;unused1508&gt;' is not marked as EOG\nload: control token: 257408 '&lt;unused1506&gt;' is not marked as EOG\nload: control token: 257406 '&lt;unused1504&gt;' is not marked as EOG\nload: control token: 257404 '&lt;unused1502&gt;' is not marked as EOG\nload: control token: 257402 '&lt;unused1500&gt;' is not marked as EOG\nload: control token: 257401 '&lt;unused1499&gt;' is not marked as EOG\nload: control token: 257400 '&lt;unused1498&gt;' is not marked as EOG\nload: control token: 257399 '&lt;unused1497&gt;' is not marked as EOG\nload: control token: 257398 '&lt;unused1496&gt;' is not marked as EOG\nload: control token: 257393 '&lt;unused1491&gt;' is not marked as EOG\nload: control token: 257392 '&lt;unused1490&gt;' is not marked as EOG\nload: control token: 257391 '&lt;unused1489&gt;' is not marked as EOG\nload: control token: 258657 '&lt;unused2755&gt;' is not marked as EOG\nload: control token: 257389 '&lt;unused1487&gt;' is not marked as EOG\nload: control token: 257388 '&lt;unused1486&gt;' is not marked as EOG\nload: control token: 257387 '&lt;unused1485&gt;' is not marked as EOG\nload: control token: 257386 '&lt;unused1484&gt;' is not marked as EOG\nload: control token: 257383 '&lt;unused1481&gt;' is not marked as EOG\nload: control token: 257382 '&lt;unused1480&gt;' is not marked as EOG\nload: control token: 257381 '&lt;unused1479&gt;' is not marked as EOG\nload: control token: 257379 '&lt;unused1477&gt;' is not marked as EOG\nload: control token: 257378 '&lt;unused1476&gt;' is not marked as EOG\nload: control token: 257377 '&lt;unused1475&gt;' is not marked as EOG\nload: control token: 257376 '&lt;unused1474&gt;' is not marked as EOG\nload: control token: 257374 '&lt;unused1472&gt;' is not marked as EOG\nload: control token: 257373 '&lt;unused1471&gt;' is not marked as EOG\nload: control token: 257371 '&lt;unused1469&gt;' is not marked as EOG\nload: control token: 257368 '&lt;unused1466&gt;' is not marked as EOG\nload: control token: 257367 '&lt;unused1465&gt;' is not marked as EOG\nload: control token: 257363 '&lt;unused1461&gt;' is not marked as EOG\nload: control token: 257362 '&lt;unused1460&gt;' is not marked as EOG\nload: control token: 257361 '&lt;unused1459&gt;' is not marked as EOG\nload: control token: 257360 '&lt;unused1458&gt;' is not marked as EOG\nload: control token: 257359 '&lt;unused1457&gt;' is not marked as EOG\nload: control token: 257358 '&lt;unused1456&gt;' is not marked as EOG\nload: control token: 257356 '&lt;unused1454&gt;' is not marked as EOG\nload: control token: 257353 '&lt;unused1451&gt;' is not marked as EOG\nload: control token: 257352 '&lt;unused1450&gt;' is not marked as EOG\nload: control token: 257351 '&lt;unused1449&gt;' is not marked as EOG\nload: control token: 257345 '&lt;unused1443&gt;' is not marked as EOG\nload: control token: 257344 '&lt;unused1442&gt;' is not marked as EOG\nload: control token: 257343 '&lt;unused1441&gt;' is not marked as EOG\nload: control token: 257340 '&lt;unused1438&gt;' is not marked as EOG\nload: control token: 257338 '&lt;unused1436&gt;' is not marked as EOG\nload: control token: 257337 '&lt;unused1435&gt;' is not marked as EOG\nload: control token: 257336 '&lt;unused1434&gt;' is not marked as EOG\nload: control token: 257335 '&lt;unused1433&gt;' is not marked as EOG\nload: control token: 257333 '&lt;unused1431&gt;' is not marked as EOG\nload: control token: 257330 '&lt;unused1428&gt;' is not marked as EOG\nload: control token: 257329 '&lt;unused1427&gt;' is not marked as EOG\nload: control token: 257324 '&lt;unused1422&gt;' is not marked as EOG\nload: control token: 257322 '&lt;unused1420&gt;' is not marked as EOG\nload: control token: 257317 '&lt;unused1415&gt;' is not marked as EOG\nload: control token: 257315 '&lt;unused1413&gt;' is not marked as EOG\nload: control token: 257313 '&lt;unused1411&gt;' is not marked as EOG\nload: control token: 257311 '&lt;unused1409&gt;' is not marked as EOG\nload: control token: 257310 '&lt;unused1408&gt;' is not marked as EOG\nload: control token: 257308 '&lt;unused1406&gt;' is not marked as EOG\nload: control token: 257297 '&lt;unused1395&gt;' is not marked as EOG\nload: control token: 257296 '&lt;unused1394&gt;' is not marked as EOG\nload: control token: 257295 '&lt;unused1393&gt;' is not marked as EOG\nload: control token: 257294 '&lt;unused1392&gt;' is not marked as EOG\nload: control token: 257293 '&lt;unused1391&gt;' is not marked as EOG\nload: control token: 257291 '&lt;unused1389&gt;' is not marked as EOG\nload: control token: 257289 '&lt;unused1387&gt;' is not marked as EOG\nload: control token: 257288 '&lt;unused1386&gt;' is not marked as EOG\nload: control token: 257287 '&lt;unused1385&gt;' is not marked as EOG\nload: control token: 257286 '&lt;unused1384&gt;' is not marked as EOG\nload: control token: 257281 '&lt;unused1379&gt;' is not marked as EOG\nload: control token: 257280 '&lt;unused1378&gt;' is not marked as EOG\nload: control token: 257278 '&lt;unused1376&gt;' is not marked as EOG\nload: control token: 257277 '&lt;unused1375&gt;' is not marked as EOG\nload: control token: 257276 '&lt;unused1374&gt;' is not marked as EOG\nload: control token: 257275 '&lt;unused1373&gt;' is not marked as EOG\nload: control token: 257273 '&lt;unused1371&gt;' is not marked as EOG\nload: control token: 257272 '&lt;unused1370&gt;' is not marked as EOG\nload: control token: 257270 '&lt;unused1368&gt;' is not marked as EOG\nload: control token: 257269 '&lt;unused1367&gt;' is not marked as EOG\nload: control token: 257267 '&lt;unused1365&gt;' is not marked as EOG\nload: control token: 257266 '&lt;unused1364&gt;' is not marked as EOG\nload: control token: 257263 '&lt;unused1361&gt;' is not marked as EOG\nload: control token: 257260 '&lt;unused1358&gt;' is not marked as EOG\nload: control token: 257259 '&lt;unused1357&gt;' is not marked as EOG\nload: control token: 257258 '&lt;unused1356&gt;' is not marked as EOG\nload: control token: 257257 '&lt;unused1355&gt;' is not marked as EOG\nload: control token: 257256 '&lt;unused1354&gt;' is not marked as EOG\nload: control token: 257255 '&lt;unused1353&gt;' is not marked as EOG\nload: control token: 257253 '&lt;unused1351&gt;' is not marked as EOG\nload: control token: 257252 '&lt;unused1350&gt;' is not marked as EOG\nload: control token: 257251 '&lt;unused1349&gt;' is not marked as EOG\nload: control token: 257248 '&lt;unused1346&gt;' is not marked as EOG\nload: control token: 257246 '&lt;unused1344&gt;' is not marked as EOG\nload: control token: 257243 '&lt;unused1341&gt;' is not marked as EOG\nload: control token: 257238 '&lt;unused1336&gt;' is not marked as EOG\nload: control token: 257234 '&lt;unused1332&gt;' is not marked as EOG\nload: control token: 257231 '&lt;unused1329&gt;' is not marked as EOG\nload: control token: 257230 '&lt;unused1328&gt;' is not marked as EOG\nload: control token: 257228 '&lt;unused1326&gt;' is not marked as EOG\nload: control token: 257227 '&lt;unused1325&gt;' is not marked as EOG\nload: control token: 257224 '&lt;unused1322&gt;' is not marked as EOG\nload: control token: 257221 '&lt;unused1319&gt;' is not marked as EOG\nload: control token: 257216 '&lt;unused1314&gt;' is not marked as EOG\nload: control token: 257214 '&lt;unused1312&gt;' is not marked as EOG\nload: control token: 261220 '&lt;unused5318&gt;' is not marked as EOG\nload: control token: 257212 '&lt;unused1310&gt;' is not marked as EOG\nload: control token: 257211 '&lt;unused1309&gt;' is not marked as EOG\nload: control token: 257210 '&lt;unused1308&gt;' is not marked as EOG\nload: control token: 257207 '&lt;unused1305&gt;' is not marked as EOG\nload: control token: 257205 '&lt;unused1303&gt;' is not marked as EOG\nload: control token: 257202 '&lt;unused1300&gt;' is not marked as EOG\nload: control token: 257201 '&lt;unused1299&gt;' is not marked as EOG\nload: control token: 257200 '&lt;unused1298&gt;' is not marked as EOG\nload: control token: 257199 '&lt;unused1297&gt;' is not marked as EOG\nload: control token: 257197 '&lt;unused1295&gt;' is not marked as EOG\nload: control token: 257193 '&lt;unused1291&gt;' is not marked as EOG\nload: control token: 257191 '&lt;unused1289&gt;' is not marked as EOG\nload: control token: 257190 '&lt;unused1288&gt;' is not marked as EOG\nload: control token: 257186 '&lt;unused1284&gt;' is not marked as EOG\nload: control token: 257185 '&lt;unused1283&gt;' is not marked as EOG\nload: control token: 257184 '&lt;unused1282&gt;' is not marked as EOG\nload: control token: 257183 '&lt;unused1281&gt;' is not marked as EOG\nload: control token: 257182 '&lt;unused1280&gt;' is not marked as EOG\nload: control token: 257179 '&lt;unused1277&gt;' is not marked as EOG\nload: control token: 257173 '&lt;unused1271&gt;' is not marked as EOG\nload: control token: 257171 '&lt;unused1269&gt;' is not marked as EOG\nload: control token: 257169 '&lt;unused1267&gt;' is not marked as EOG\nload: control token: 257168 '&lt;unused1266&gt;' is not marked as EOG\nload: control token: 257167 '&lt;unused1265&gt;' is not marked as EOG\nload: control token: 257165 '&lt;unused1263&gt;' is not marked as EOG\nload: control token: 257164 '&lt;unused1262&gt;' is not marked as EOG\nload: control token: 257162 '&lt;unused1260&gt;' is not marked as EOG\nload: control token: 257160 '&lt;unused1258&gt;' is not marked as EOG\nload: control token: 257159 '&lt;unused1257&gt;' is not marked as EOG\nload: control token: 257158 '&lt;unused1256&gt;' is not marked as EOG\nload: control token: 257156 '&lt;unused1254&gt;' is not marked as EOG\nload: control token: 257153 '&lt;unused1251&gt;' is not marked as EOG\nload: control token: 257151 '&lt;unused1249&gt;' is not marked as EOG\nload: control token: 257150 '&lt;unused1248&gt;' is not marked as EOG\nload: control token: 257149 '&lt;unused1247&gt;' is not marked as EOG\nload: control token: 257148 '&lt;unused1246&gt;' is not marked as EOG\nload: control token: 257147 '&lt;unused1245&gt;' is not marked as EOG\nload: control token: 257146 '&lt;unused1244&gt;' is not marked as EOG\nload: control token: 257145 '&lt;unused1243&gt;' is not marked as EOG\nload: control token: 257144 '&lt;unused1242&gt;' is not marked as EOG\nload: control token: 257143 '&lt;unused1241&gt;' is not marked as EOG\nload: control token: 257142 '&lt;unused1240&gt;' is not marked as EOG\nload: control token: 257139 '&lt;unused1237&gt;' is not marked as EOG\nload: control token: 257138 '&lt;unused1236&gt;' is not marked as EOG\nload: control token: 257137 '&lt;unused1235&gt;' is not marked as EOG\nload: control token: 257136 '&lt;unused1234&gt;' is not marked as EOG\nload: control token: 257134 '&lt;unused1232&gt;' is not marked as EOG\nload: control token: 257133 '&lt;unused1231&gt;' is not marked as EOG\nload: control token: 257130 '&lt;unused1228&gt;' is not marked as EOG\nload: control token: 257128 '&lt;unused1226&gt;' is not marked as EOG\nload: control token: 257125 '&lt;unused1223&gt;' is not marked as EOG\nload: control token: 257124 '&lt;unused1222&gt;' is not marked as EOG\nload: control token: 257123 '&lt;unused1221&gt;' is not marked as EOG\nload: control token: 257122 '&lt;unused1220&gt;' is not marked as EOG\nload: control token: 257120 '&lt;unused1218&gt;' is not marked as EOG\nload: control token: 257119 '&lt;unused1217&gt;' is not marked as EOG\nload: control token: 257118 '&lt;unused1216&gt;' is not marked as EOG\nload: control token: 257117 '&lt;unused1215&gt;' is not marked as EOG\nload: control token: 257116 '&lt;unused1214&gt;' is not marked as EOG\nload: control token: 257112 '&lt;unused1210&gt;' is not marked as EOG\nload: control token: 257111 '&lt;unused1209&gt;' is not marked as EOG\nload: control token: 257108 '&lt;unused1206&gt;' is not marked as EOG\nload: control token: 257107 '&lt;unused1205&gt;' is not marked as EOG\nload: control token: 257105 '&lt;unused1203&gt;' is not marked as EOG\nload: control token: 257104 '&lt;unused1202&gt;' is not marked as EOG\nload: control token: 257101 '&lt;unused1199&gt;' is not marked as EOG\nload: control token: 257099 '&lt;unused1197&gt;' is not marked as EOG\nload: control token: 257097 '&lt;unused1195&gt;' is not marked as EOG\nload: control token: 257094 '&lt;unused1192&gt;' is not marked as EOG\nload: control token: 257093 '&lt;unused1191&gt;' is not marked as EOG\nload: control token: 257092 '&lt;unused1190&gt;' is not marked as EOG\nload: control token: 257091 '&lt;unused1189&gt;' is not marked as EOG\nload: control token: 257090 '&lt;unused1188&gt;' is not marked as EOG\nload: control token: 257088 '&lt;unused1186&gt;' is not marked as EOG\nload: control token: 257086 '&lt;unused1184&gt;' is not marked as EOG\nload: control token: 257085 '&lt;unused1183&gt;' is not marked as EOG\nload: control token: 257084 '&lt;unused1182&gt;' is not marked as EOG\nload: control token: 257083 '&lt;unused1181&gt;' is not marked as EOG\nload: control token: 257082 '&lt;unused1180&gt;' is not marked as EOG\nload: control token: 257080 '&lt;unused1178&gt;' is not marked as EOG\nload: control token: 257079 '&lt;unused1177&gt;' is not marked as EOG\nload: control token: 257078 '&lt;unused1176&gt;' is not marked as EOG\nload: control token: 257075 '&lt;unused1173&gt;' is not marked as EOG\nload: control token: 257074 '&lt;unused1172&gt;' is not marked as EOG\nload: control token: 257072 '&lt;unused1170&gt;' is not marked as EOG\nload: control token: 257071 '&lt;unused1169&gt;' is not marked as EOG\nload: control token: 257070 '&lt;unused1168&gt;' is not marked as EOG\nload: control token: 257069 '&lt;unused1167&gt;' is not marked as EOG\nload: control token: 257068 '&lt;unused1166&gt;' is not marked as EOG\nload: control token: 257067 '&lt;unused1165&gt;' is not marked as EOG\nload: control token: 257063 '&lt;unused1161&gt;' is not marked as EOG\nload: control token: 257062 '&lt;unused1160&gt;' is not marked as EOG\nload: control token: 257061 '&lt;unused1159&gt;' is not marked as EOG\nload: control token: 257060 '&lt;unused1158&gt;' is not marked as EOG\nload: control token: 257059 '&lt;unused1157&gt;' is not marked as EOG\nload: control token: 257057 '&lt;unused1155&gt;' is not marked as EOG\nload: control token: 257056 '&lt;unused1154&gt;' is not marked as EOG\nload: control token: 257053 '&lt;unused1151&gt;' is not marked as EOG\nload: control token: 257052 '&lt;unused1150&gt;' is not marked as EOG\nload: control token: 257050 '&lt;unused1148&gt;' is not marked as EOG\nload: control token: 257049 '&lt;unused1147&gt;' is not marked as EOG\nload: control token: 257048 '&lt;unused1146&gt;' is not marked as EOG\nload: control token: 257043 '&lt;unused1141&gt;' is not marked as EOG\nload: control token: 257042 '&lt;unused1140&gt;' is not marked as EOG\nload: control token: 257038 '&lt;unused1136&gt;' is not marked as EOG\nload: control token: 257036 '&lt;unused1134&gt;' is not marked as EOG\nload: control token: 257035 '&lt;unused1133&gt;' is not marked as EOG\nload: control token: 257034 '&lt;unused1132&gt;' is not marked as EOG\nload: control token: 257031 '&lt;unused1129&gt;' is not marked as EOG\nload: control token: 257029 '&lt;unused1127&gt;' is not marked as EOG\nload: control token: 257028 '&lt;unused1126&gt;' is not marked as EOG\nload: control token: 257027 '&lt;unused1125&gt;' is not marked as EOG\nload: control token: 257026 '&lt;unused1124&gt;' is not marked as EOG\nload: control token: 257024 '&lt;unused1122&gt;' is not marked as EOG\nload: control token: 257023 '&lt;unused1121&gt;' is not marked as EOG\nload: control token: 257022 '&lt;unused1120&gt;' is not marked as EOG\nload: control token: 257021 '&lt;unused1119&gt;' is not marked as EOG\nload: control token: 257018 '&lt;unused1116&gt;' is not marked as EOG\nload: control token: 257017 '&lt;unused1115&gt;' is not marked as EOG\nload: control token: 257014 '&lt;unused1112&gt;' is not marked as EOG\nload: control token: 257013 '&lt;unused1111&gt;' is not marked as EOG\nload: control token: 257010 '&lt;unused1108&gt;' is not marked as EOG\nload: control token: 257008 '&lt;unused1106&gt;' is not marked as EOG\nload: control token: 257006 '&lt;unused1104&gt;' is not marked as EOG\nload: control token: 257004 '&lt;unused1102&gt;' is not marked as EOG\nload: control token: 257001 '&lt;unused1099&gt;' is not marked as EOG\nload: control token: 257000 '&lt;unused1098&gt;' is not marked as EOG\nload: control token: 256999 '&lt;unused1097&gt;' is not marked as EOG\nload: control token: 256998 '&lt;unused1096&gt;' is not marked as EOG\nload: control token: 256997 '&lt;unused1095&gt;' is not marked as EOG\nload: control token: 256995 '&lt;unused1093&gt;' is not marked as EOG\nload: control token: 256994 '&lt;unused1092&gt;' is not marked as EOG\nload: control token: 256993 '&lt;unused1091&gt;' is not marked as EOG\nload: control token: 256992 '&lt;unused1090&gt;' is not marked as EOG\nload: control token: 256991 '&lt;unused1089&gt;' is not marked as EOG\nload: control token: 256990 '&lt;unused1088&gt;' is not marked as EOG\nload: control token: 256989 '&lt;unused1087&gt;' is not marked as EOG\nload: control token: 256988 '&lt;unused1086&gt;' is not marked as EOG\nload: control token: 256987 '&lt;unused1085&gt;' is not marked as EOG\nload: control token: 256985 '&lt;unused1083&gt;' is not marked as EOG\nload: control token: 256984 '&lt;unused1082&gt;' is not marked as EOG\nload: control token: 256980 '&lt;unused1078&gt;' is not marked as EOG\nload: control token: 256977 '&lt;unused1075&gt;' is not marked as EOG\nload: control token: 256976 '&lt;unused1074&gt;' is not marked as EOG\nload: control token: 256973 '&lt;unused1071&gt;' is not marked as EOG\nload: control token: 256969 '&lt;unused1067&gt;' is not marked as EOG\nload: control token: 256965 '&lt;unused1063&gt;' is not marked as EOG\nload: control token: 256962 '&lt;unused1060&gt;' is not marked as EOG\nload: control token: 256961 '&lt;unused1059&gt;' is not marked as EOG\nload: control token: 256959 '&lt;unused1057&gt;' is not marked as EOG\nload: control token: 256958 '&lt;unused1056&gt;' is not marked as EOG\nload: control token: 256957 '&lt;unused1055&gt;' is not marked as EOG\nload: control token: 256955 '&lt;unused1053&gt;' is not marked as EOG\nload: control token: 256954 '&lt;unused1052&gt;' is not marked as EOG\nload: control token: 256953 '&lt;unused1051&gt;' is not marked as EOG\nload: control token: 256952 '&lt;unused1050&gt;' is not marked as EOG\nload: control token: 256950 '&lt;unused1048&gt;' is not marked as EOG\nload: control token: 256949 '&lt;unused1047&gt;' is not marked as EOG\nload: control token: 256947 '&lt;unused1045&gt;' is not marked as EOG\nload: control token: 256945 '&lt;unused1043&gt;' is not marked as EOG\nload: control token: 256943 '&lt;unused1041&gt;' is not marked as EOG\nload: control token: 256942 '&lt;unused1040&gt;' is not marked as EOG\nload: control token: 256939 '&lt;unused1037&gt;' is not marked as EOG\nload: control token: 256938 '&lt;unused1036&gt;' is not marked as EOG\nload: control token: 256935 '&lt;unused1033&gt;' is not marked as EOG\nload: control token: 256931 '&lt;unused1029&gt;' is not marked as EOG\nload: control token: 256929 '&lt;unused1027&gt;' is not marked as EOG\nload: control token: 256926 '&lt;unused1024&gt;' is not marked as EOG\nload: control token: 256924 '&lt;unused1022&gt;' is not marked as EOG\nload: control token: 256922 '&lt;unused1020&gt;' is not marked as EOG\nload: control token: 256921 '&lt;unused1019&gt;' is not marked as EOG\nload: control token: 256920 '&lt;unused1018&gt;' is not marked as EOG\nload: control token: 256919 '&lt;unused1017&gt;' is not marked as EOG\nload: control token: 256917 '&lt;unused1015&gt;' is not marked as EOG\nload: control token: 256916 '&lt;unused1014&gt;' is not marked as EOG\nload: control token: 256915 '&lt;unused1013&gt;' is not marked as EOG\nload: control token: 256914 '&lt;unused1012&gt;' is not marked as EOG\nload: control token: 256913 '&lt;unused1011&gt;' is not marked as EOG\nload: control token: 256911 '&lt;unused1009&gt;' is not marked as EOG\nload: control token: 256909 '&lt;unused1007&gt;' is not marked as EOG\nload: control token: 256906 '&lt;unused1004&gt;' is not marked as EOG\nload: control token: 256905 '&lt;unused1003&gt;' is not marked as EOG\nload: control token: 256902 '&lt;unused1000&gt;' is not marked as EOG\nload: control token: 256899 '&lt;unused997&gt;' is not marked as EOG\nload: control token: 256898 '&lt;unused996&gt;' is not marked as EOG\nload: control token: 256897 '&lt;unused995&gt;' is not marked as EOG\nload: control token: 256893 '&lt;unused991&gt;' is not marked as EOG\nload: control token: 256892 '&lt;unused990&gt;' is not marked as EOG\nload: control token: 256891 '&lt;unused989&gt;' is not marked as EOG\nload: control token: 256888 '&lt;unused986&gt;' is not marked as EOG\nload: control token: 256887 '&lt;unused985&gt;' is not marked as EOG\nload: control token: 256885 '&lt;unused983&gt;' is not marked as EOG\nload: control token: 256883 '&lt;unused981&gt;' is not marked as EOG\nload: control token: 256881 '&lt;unused979&gt;' is not marked as EOG\nload: control token: 256880 '&lt;unused978&gt;' is not marked as EOG\nload: control token: 256879 '&lt;unused977&gt;' is not marked as EOG\nload: control token: 256878 '&lt;unused976&gt;' is not marked as EOG\nload: control token: 256877 '&lt;unused975&gt;' is not marked as EOG\nload: control token: 256876 '&lt;unused974&gt;' is not marked as EOG\nload: control token: 256874 '&lt;unused972&gt;' is not marked as EOG\nload: control token: 256868 '&lt;unused966&gt;' is not marked as EOG\nload: control token: 256867 '&lt;unused965&gt;' is not marked as EOG\nload: control token: 256866 '&lt;unused964&gt;' is not marked as EOG\nload: control token: 256862 '&lt;unused960&gt;' is not marked as EOG\nload: control token: 256861 '&lt;unused959&gt;' is not marked as EOG\nload: control token: 256859 '&lt;unused957&gt;' is not marked as EOG\nload: control token: 256855 '&lt;unused953&gt;' is not marked as EOG\nload: control token: 256852 '&lt;unused950&gt;' is not marked as EOG\nload: control token: 256851 '&lt;unused949&gt;' is not marked as EOG\nload: control token: 256850 '&lt;unused948&gt;' is not marked as EOG\nload: control token: 256849 '&lt;unused947&gt;' is not marked as EOG\nload: control token: 256848 '&lt;unused946&gt;' is not marked as EOG\nload: control token: 256847 '&lt;unused945&gt;' is not marked as EOG\nload: control token: 256846 '&lt;unused944&gt;' is not marked as EOG\nload: control token: 256845 '&lt;unused943&gt;' is not marked as EOG\nload: control token: 256844 '&lt;unused942&gt;' is not marked as EOG\nload: control token: 256842 '&lt;unused940&gt;' is not marked as EOG\nload: control token: 256841 '&lt;unused939&gt;' is not marked as EOG\nload: control token: 256840 '&lt;unused938&gt;' is not marked as EOG\nload: control token: 256838 '&lt;unused936&gt;' is not marked as EOG\nload: control token: 256837 '&lt;unused935&gt;' is not marked as EOG\nload: control token: 256836 '&lt;unused934&gt;' is not marked as EOG\nload: control token: 256835 '&lt;unused933&gt;' is not marked as EOG\nload: control token: 256834 '&lt;unused932&gt;' is not marked as EOG\nload: control token: 256832 '&lt;unused930&gt;' is not marked as EOG\nload: control token: 256830 '&lt;unused928&gt;' is not marked as EOG\nload: control token: 256829 '&lt;unused927&gt;' is not marked as EOG\nload: control token: 256827 '&lt;unused925&gt;' is not marked as EOG\nload: control token: 256825 '&lt;unused923&gt;' is not marked as EOG\nload: control token: 256824 '&lt;unused922&gt;' is not marked as EOG\nload: control token: 256823 '&lt;unused921&gt;' is not marked as EOG\nload: control token: 256820 '&lt;unused918&gt;' is not marked as EOG\nload: control token: 256818 '&lt;unused916&gt;' is not marked as EOG\nload: control token: 256816 '&lt;unused914&gt;' is not marked as EOG\nload: control token: 256815 '&lt;unused913&gt;' is not marked as EOG\nload: control token: 256813 '&lt;unused911&gt;' is not marked as EOG\nload: control token: 256807 '&lt;unused905&gt;' is not marked as EOG\nload: control token: 256804 '&lt;unused902&gt;' is not marked as EOG\nload: control token: 256803 '&lt;unused901&gt;' is not marked as EOG\nload: control token: 256801 '&lt;unused899&gt;' is not marked as EOG\nload: control token: 256800 '&lt;unused898&gt;' is not marked as EOG\nload: control token: 256797 '&lt;unused895&gt;' is not marked as EOG\nload: control token: 256796 '&lt;unused894&gt;' is not marked as EOG\nload: control token: 256795 '&lt;unused893&gt;' is not marked as EOG\nload: control token: 256793 '&lt;unused891&gt;' is not marked as EOG\nload: control token: 256792 '&lt;unused890&gt;' is not marked as EOG\nload: control token: 256790 '&lt;unused888&gt;' is not marked as EOG\nload: control token: 256788 '&lt;unused886&gt;' is not marked as EOG\nload: control token: 256787 '&lt;unused885&gt;' is not marked as EOG\nload: control token: 256786 '&lt;unused884&gt;' is not marked as EOG\nload: control token: 256785 '&lt;unused883&gt;' is not marked as EOG\nload: control token: 256781 '&lt;unused879&gt;' is not marked as EOG\nload: control token: 256779 '&lt;unused877&gt;' is not marked as EOG\nload: control token: 256778 '&lt;unused876&gt;' is not marked as EOG\nload: control token: 256771 '&lt;unused869&gt;' is not marked as EOG\nload: control token: 256770 '&lt;unused868&gt;' is not marked as EOG\nload: control token: 256766 '&lt;unused864&gt;' is not marked as EOG\nload: control token: 256764 '&lt;unused862&gt;' is not marked as EOG\nload: control token: 256761 '&lt;unused859&gt;' is not marked as EOG\nload: control token: 256760 '&lt;unused858&gt;' is not marked as EOG\nload: control token: 256757 '&lt;unused855&gt;' is not marked as EOG\nload: control token: 256755 '&lt;unused853&gt;' is not marked as EOG\nload: control token: 256754 '&lt;unused852&gt;' is not marked as EOG\nload: control token: 256753 '&lt;unused851&gt;' is not marked as EOG\nload: control token: 256751 '&lt;unused849&gt;' is not marked as EOG\nload: control token: 256746 '&lt;unused844&gt;' is not marked as EOG\nload: control token: 256742 '&lt;unused840&gt;' is not marked as EOG\nload: control token: 256741 '&lt;unused839&gt;' is not marked as EOG\nload: control token: 256739 '&lt;unused837&gt;' is not marked as EOG\nload: control token: 256738 '&lt;unused836&gt;' is not marked as EOG\nload: control token: 256737 '&lt;unused835&gt;' is not marked as EOG\nload: control token: 256734 '&lt;unused832&gt;' is not marked as EOG\nload: control token: 256733 '&lt;unused831&gt;' is not marked as EOG\nload: control token: 256731 '&lt;unused829&gt;' is not marked as EOG\nload: control token: 256723 '&lt;unused821&gt;' is not marked as EOG\nload: control token: 256720 '&lt;unused818&gt;' is not marked as EOG\nload: control token: 256719 '&lt;unused817&gt;' is not marked as EOG\nload: control token: 256717 '&lt;unused815&gt;' is not marked as EOG\nload: control token: 256716 '&lt;unused814&gt;' is not marked as EOG\nload: control token: 256713 '&lt;unused811&gt;' is not marked as EOG\nload: control token: 256712 '&lt;unused810&gt;' is not marked as EOG\nload: control token: 256710 '&lt;unused808&gt;' is not marked as EOG\nload: control token: 256701 '&lt;unused799&gt;' is not marked as EOG\nload: control token: 256700 '&lt;unused798&gt;' is not marked as EOG\nload: control token: 256699 '&lt;unused797&gt;' is not marked as EOG\nload: control token: 256693 '&lt;unused791&gt;' is not marked as EOG\nload: control token: 256692 '&lt;unused790&gt;' is not marked as EOG\nload: control token: 256691 '&lt;unused789&gt;' is not marked as EOG\nload: control token: 256690 '&lt;unused788&gt;' is not marked as EOG\nload: control token: 256688 '&lt;unused786&gt;' is not marked as EOG\nload: control token: 256681 '&lt;unused779&gt;' is not marked as EOG\nload: control token: 256677 '&lt;unused775&gt;' is not marked as EOG\nload: control token: 256675 '&lt;unused773&gt;' is not marked as EOG\nload: control token: 256672 '&lt;unused770&gt;' is not marked as EOG\nload: control token: 260447 '&lt;unused4545&gt;' is not marked as EOG\nload: control token: 256669 '&lt;unused767&gt;' is not marked as EOG\nload: control token: 256665 '&lt;unused763&gt;' is not marked as EOG\nload: control token: 256663 '&lt;unused761&gt;' is not marked as EOG\nload: control token: 256662 '&lt;unused760&gt;' is not marked as EOG\nload: control token: 256661 '&lt;unused759&gt;' is not marked as EOG\nload: control token: 256659 '&lt;unused757&gt;' is not marked as EOG\nload: control token: 256656 '&lt;unused754&gt;' is not marked as EOG\nload: control token: 256655 '&lt;unused753&gt;' is not marked as EOG\nload: control token: 256651 '&lt;unused749&gt;' is not marked as EOG\nload: control token: 256649 '&lt;unused747&gt;' is not marked as EOG\nload: control token: 256645 '&lt;unused743&gt;' is not marked as EOG\nload: control token: 256643 '&lt;unused741&gt;' is not marked as EOG\nload: control token: 256642 '&lt;unused740&gt;' is not marked as EOG\nload: control token: 256638 '&lt;unused736&gt;' is not marked as EOG\nload: control token: 256637 '&lt;unused735&gt;' is not marked as EOG\nload: control token: 256635 '&lt;unused733&gt;' is not marked as EOG\nload: control token: 256634 '&lt;unused732&gt;' is not marked as EOG\nload: control token: 256633 '&lt;unused731&gt;' is not marked as EOG\nload: control token: 256632 '&lt;unused730&gt;' is not marked as EOG\nload: control token: 256627 '&lt;unused725&gt;' is not marked as EOG\nload: control token: 256626 '&lt;unused724&gt;' is not marked as EOG\nload: control token: 256625 '&lt;unused723&gt;' is not marked as EOG\nload: control token: 256624 '&lt;unused722&gt;' is not marked as EOG\nload: control token: 256621 '&lt;unused719&gt;' is not marked as EOG\nload: control token: 256617 '&lt;unused715&gt;' is not marked as EOG\nload: control token: 256613 '&lt;unused711&gt;' is not marked as EOG\nload: control token: 256606 '&lt;unused704&gt;' is not marked as EOG\nload: control token: 256605 '&lt;unused703&gt;' is not marked as EOG\nload: control token: 256604 '&lt;unused702&gt;' is not marked as EOG\nload: control token: 256603 '&lt;unused701&gt;' is not marked as EOG\nload: control token: 256599 '&lt;unused697&gt;' is not marked as EOG\nload: control token: 256597 '&lt;unused695&gt;' is not marked as EOG\nload: control token: 256594 '&lt;unused692&gt;' is not marked as EOG\nload: control token: 256593 '&lt;unused691&gt;' is not marked as EOG\nload: control token: 256592 '&lt;unused690&gt;' is not marked as EOG\nload: control token: 256591 '&lt;unused689&gt;' is not marked as EOG\nload: control token: 256590 '&lt;unused688&gt;' is not marked as EOG\nload: control token: 256589 '&lt;unused687&gt;' is not marked as EOG\nload: control token: 256588 '&lt;unused686&gt;' is not marked as EOG\nload: control token: 256586 '&lt;unused684&gt;' is not marked as EOG\nload: control token: 256584 '&lt;unused682&gt;' is not marked as EOG\nload: control token: 256582 '&lt;unused680&gt;' is not marked as EOG\nload: control token: 256581 '&lt;unused679&gt;' is not marked as EOG\nload: control token: 256579 '&lt;unused677&gt;' is not marked as EOG\nload: control token: 256578 '&lt;unused676&gt;' is not marked as EOG\nload: control token: 256576 '&lt;unused674&gt;' is not marked as EOG\nload: control token: 256574 '&lt;unused672&gt;' is not marked as EOG\nload: control token: 256573 '&lt;unused671&gt;' is not marked as EOG\nload: control token: 256572 '&lt;unused670&gt;' is not marked as EOG\nload: control token: 256568 '&lt;unused666&gt;' is not marked as EOG\nload: control token: 256566 '&lt;unused664&gt;' is not marked as EOG\nload: control token: 256565 '&lt;unused663&gt;' is not marked as EOG\nload: control token: 256563 '&lt;unused661&gt;' is not marked as EOG\nload: control token: 256556 '&lt;unused654&gt;' is not marked as EOG\nload: control token: 256554 '&lt;unused652&gt;' is not marked as EOG\nload: control token: 256550 '&lt;unused648&gt;' is not marked as EOG\nload: control token: 259319 '&lt;unused3417&gt;' is not marked as EOG\nload: control token: 256549 '&lt;unused647&gt;' is not marked as EOG\nload: control token: 256544 '&lt;unused642&gt;' is not marked as EOG\nload: control token: 256539 '&lt;unused637&gt;' is not marked as EOG\nload: control token: 256538 '&lt;unused636&gt;' is not marked as EOG\nload: control token: 256537 '&lt;unused635&gt;' is not marked as EOG\nload: control token: 256535 '&lt;unused633&gt;' is not marked as EOG\nload: control token: 256534 '&lt;unused632&gt;' is not marked as EOG\nload: control token: 256532 '&lt;unused630&gt;' is not marked as EOG\nload: control token: 256531 '&lt;unused629&gt;' is not marked as EOG\nload: control token: 256530 '&lt;unused628&gt;' is not marked as EOG\nload: control token: 256527 '&lt;unused625&gt;' is not marked as EOG\nload: control token: 256526 '&lt;unused624&gt;' is not marked as EOG\nload: control token: 256525 '&lt;unused623&gt;' is not marked as EOG\nload: control token: 256524 '&lt;unused622&gt;' is not marked as EOG\nload: control token: 256523 '&lt;unused621&gt;' is not marked as EOG\nload: control token: 256522 '&lt;unused620&gt;' is not marked as EOG\nload: control token: 256521 '&lt;unused619&gt;' is not marked as EOG\nload: control token: 256520 '&lt;unused618&gt;' is not marked as EOG\nload: control token: 256513 '&lt;unused611&gt;' is not marked as EOG\nload: control token: 256512 '&lt;unused610&gt;' is not marked as EOG\nload: control token: 256511 '&lt;unused609&gt;' is not marked as EOG\nload: control token: 256510 '&lt;unused608&gt;' is not marked as EOG\nload: control token: 256508 '&lt;unused606&gt;' is not marked as EOG\nload: control token: 256506 '&lt;unused604&gt;' is not marked as EOG\nload: control token: 256502 '&lt;unused600&gt;' is not marked as EOG\nload: control token: 256501 '&lt;unused599&gt;' is not marked as EOG\nload: control token: 256500 '&lt;unused598&gt;' is not marked as EOG\nload: control token: 256497 '&lt;unused595&gt;' is not marked as EOG\nload: control token: 256493 '&lt;unused591&gt;' is not marked as EOG\nload: control token: 256490 '&lt;unused588&gt;' is not marked as EOG\nload: control token: 256489 '&lt;unused587&gt;' is not marked as EOG\nload: control token: 256486 '&lt;unused584&gt;' is not marked as EOG\nload: control token: 256485 '&lt;unused583&gt;' is not marked as EOG\nload: control token: 256483 '&lt;unused581&gt;' is not marked as EOG\nload: control token: 256482 '&lt;unused580&gt;' is not marked as EOG\nload: control token: 256476 '&lt;unused574&gt;' is not marked as EOG\nload: control token: 256473 '&lt;unused571&gt;' is not marked as EOG\nload: control token: 256472 '&lt;unused570&gt;' is not marked as EOG\nload: control token: 256468 '&lt;unused566&gt;' is not marked as EOG\nload: control token: 256467 '&lt;unused565&gt;' is not marked as EOG\nload: control token: 256466 '&lt;unused564&gt;' is not marked as EOG\nload: control token: 256465 '&lt;unused563&gt;' is not marked as EOG\nload: control token: 256464 '&lt;unused562&gt;' is not marked as EOG\nload: control token: 256463 '&lt;unused561&gt;' is not marked as EOG\nload: control token: 256458 '&lt;unused556&gt;' is not marked as EOG\nload: control token: 256457 '&lt;unused555&gt;' is not marked as EOG\nload: control token: 256453 '&lt;unused551&gt;' is not marked as EOG\nload: control token: 256452 '&lt;unused550&gt;' is not marked as EOG\nload: control token: 256451 '&lt;unused549&gt;' is not marked as EOG\nload: control token: 256449 '&lt;unused547&gt;' is not marked as EOG\nload: control token: 256447 '&lt;unused545&gt;' is not marked as EOG\nload: control token: 256446 '&lt;unused544&gt;' is not marked as EOG\nload: control token: 256444 '&lt;unused542&gt;' is not marked as EOG\nload: control token: 259628 '&lt;unused3726&gt;' is not marked as EOG\nload: control token: 256441 '&lt;unused539&gt;' is not marked as EOG\nload: control token: 256439 '&lt;unused537&gt;' is not marked as EOG\nload: control token: 256436 '&lt;unused534&gt;' is not marked as EOG\nload: control token: 256432 '&lt;unused530&gt;' is not marked as EOG\nload: control token: 256431 '&lt;unused529&gt;' is not marked as EOG\nload: control token: 256429 '&lt;unused527&gt;' is not marked as EOG\nload: control token: 256424 '&lt;unused522&gt;' is not marked as EOG\nload: control token: 256422 '&lt;unused520&gt;' is not marked as EOG\nload: control token: 256421 '&lt;unused519&gt;' is not marked as EOG\nload: control token: 256420 '&lt;unused518&gt;' is not marked as EOG\nload: control token: 256419 '&lt;unused517&gt;' is not marked as EOG\nload: control token: 256418 '&lt;unused516&gt;' is not marked as EOG\nload: control token: 256417 '&lt;unused515&gt;' is not marked as EOG\nload: control token: 256415 '&lt;unused513&gt;' is not marked as EOG\nload: control token: 256414 '&lt;unused512&gt;' is not marked as EOG\nload: control token: 256413 '&lt;unused511&gt;' is not marked as EOG\nload: control token: 256411 '&lt;unused509&gt;' is not marked as EOG\nload: control token: 256410 '&lt;unused508&gt;' is not marked as EOG\nload: control token: 256408 '&lt;unused506&gt;' is not marked as EOG\nload: control token: 256407 '&lt;unused505&gt;' is not marked as EOG\nload: control token: 256406 '&lt;unused504&gt;' is not marked as EOG\nload: control token: 262002 '&lt;unused6100&gt;' is not marked as EOG\nload: control token: 256404 '&lt;unused502&gt;' is not marked as EOG\nload: control token: 256401 '&lt;unused499&gt;' is not marked as EOG\nload: control token: 256400 '&lt;unused498&gt;' is not marked as EOG\nload: control token: 256399 '&lt;unused497&gt;' is not marked as EOG\nload: control token: 256398 '&lt;unused496&gt;' is not marked as EOG\nload: control token: 256396 '&lt;unused494&gt;' is not marked as EOG\nload: control token: 256395 '&lt;unused493&gt;' is not marked as EOG\nload: control token: 256394 '&lt;unused492&gt;' is not marked as EOG\nload: control token: 256393 '&lt;unused491&gt;' is not marked as EOG\nload: control token: 256392 '&lt;unused490&gt;' is not marked as EOG\nload: control token: 256390 '&lt;unused488&gt;' is not marked as EOG\nload: control token: 256387 '&lt;unused485&gt;' is not marked as EOG\nload: control token: 256385 '&lt;unused483&gt;' is not marked as EOG\nload: control token: 256384 '&lt;unused482&gt;' is not marked as EOG\nload: control token: 256382 '&lt;unused480&gt;' is not marked as EOG\nload: control token: 256381 '&lt;unused479&gt;' is not marked as EOG\nload: control token: 256380 '&lt;unused478&gt;' is not marked as EOG\nload: control token: 256379 '&lt;unused477&gt;' is not marked as EOG\nload: control token: 256378 '&lt;unused476&gt;' is not marked as EOG\nload: control token: 256376 '&lt;unused474&gt;' is not marked as EOG\nload: control token: 256375 '&lt;unused473&gt;' is not marked as EOG\nload: control token: 256374 '&lt;unused472&gt;' is not marked as EOG\nload: control token: 256371 '&lt;unused469&gt;' is not marked as EOG\nload: control token: 256368 '&lt;unused466&gt;' is not marked as EOG\nload: control token: 256366 '&lt;unused464&gt;' is not marked as EOG\nload: control token: 256364 '&lt;unused462&gt;' is not marked as EOG\nload: control token: 256363 '&lt;unused461&gt;' is not marked as EOG\nload: control token: 256361 '&lt;unused459&gt;' is not marked as EOG\nload: control token: 256350 '&lt;unused448&gt;' is not marked as EOG\nload: control token: 256349 '&lt;unused447&gt;' is not marked as EOG\nload: control token: 256347 '&lt;unused445&gt;' is not marked as EOG\nload: control token: 256343 '&lt;unused441&gt;' is not marked as EOG\nload: control token: 256342 '&lt;unused440&gt;' is not marked as EOG\nload: control token: 256341 '&lt;unused439&gt;' is not marked as EOG\nload: control token: 256336 '&lt;unused434&gt;' is not marked as EOG\nload: control token: 256333 '&lt;unused431&gt;' is not marked as EOG\nload: control token: 256332 '&lt;unused430&gt;' is not marked as EOG\nload: control token: 256331 '&lt;unused429&gt;' is not marked as EOG\nload: control token: 256330 '&lt;unused428&gt;' is not marked as EOG\nload: control token: 256325 '&lt;unused423&gt;' is not marked as EOG\nload: control token: 256323 '&lt;unused421&gt;' is not marked as EOG\nload: control token: 256322 '&lt;unused420&gt;' is not marked as EOG\nload: control token: 256321 '&lt;unused419&gt;' is not marked as EOG\nload: control token: 256319 '&lt;unused417&gt;' is not marked as EOG\nload: control token: 256313 '&lt;unused411&gt;' is not marked as EOG\nload: control token: 256311 '&lt;unused409&gt;' is not marked as EOG\nload: control token: 256310 '&lt;unused408&gt;' is not marked as EOG\nload: control token: 256309 '&lt;unused407&gt;' is not marked as EOG\nload: control token: 256308 '&lt;unused406&gt;' is not marked as EOG\nload: control token: 256307 '&lt;unused405&gt;' is not marked as EOG\nload: control token: 256306 '&lt;unused404&gt;' is not marked as EOG\nload: control token: 256305 '&lt;unused403&gt;' is not marked as EOG\nload: control token: 256303 '&lt;unused401&gt;' is not marked as EOG\nload: control token: 256297 '&lt;unused395&gt;' is not marked as EOG\nload: control token: 256294 '&lt;unused392&gt;' is not marked as EOG\nload: control token: 256292 '&lt;unused390&gt;' is not marked as EOG\nload: control token: 256291 '&lt;unused389&gt;' is not marked as EOG\nload: control token: 256290 '&lt;unused388&gt;' is not marked as EOG\nload: control token: 256289 '&lt;unused387&gt;' is not marked as EOG\nload: control token: 256283 '&lt;unused381&gt;' is not marked as EOG\nload: control token: 256280 '&lt;unused378&gt;' is not marked as EOG\nload: control token: 256278 '&lt;unused376&gt;' is not marked as EOG\nload: control token: 256270 '&lt;unused368&gt;' is not marked as EOG\nload: control token: 256269 '&lt;unused367&gt;' is not marked as EOG\nload: control token: 256265 '&lt;unused363&gt;' is not marked as EOG\nload: control token: 256262 '&lt;unused360&gt;' is not marked as EOG\nload: control token: 256261 '&lt;unused359&gt;' is not marked as EOG\nload: control token: 256257 '&lt;unused355&gt;' is not marked as EOG\nload: control token: 256256 '&lt;unused354&gt;' is not marked as EOG\nload: control token: 256255 '&lt;unused353&gt;' is not marked as EOG\nload: control token: 256254 '&lt;unused352&gt;' is not marked as EOG\nload: control token: 256253 '&lt;unused351&gt;' is not marked as EOG\nload: control token: 256250 '&lt;unused348&gt;' is not marked as EOG\nload: control token: 256249 '&lt;unused347&gt;' is not marked as EOG\nload: control token: 256247 '&lt;unused345&gt;' is not marked as EOG\nload: control token: 259075 '&lt;unused3173&gt;' is not marked as EOG\nload: control token: 256244 '&lt;unused342&gt;' is not marked as EOG\nload: control token: 256243 '&lt;unused341&gt;' is not marked as EOG\nload: control token: 256239 '&lt;unused337&gt;' is not marked as EOG\nload: control token: 256234 '&lt;unused332&gt;' is not marked as EOG\nload: control token: 256233 '&lt;unused331&gt;' is not marked as EOG\nload: control token: 256232 '&lt;unused330&gt;' is not marked as EOG\nload: control token: 256231 '&lt;unused329&gt;' is not marked as EOG\nload: control token: 256230 '&lt;unused328&gt;' is not marked as EOG\nload: control token: 256227 '&lt;unused325&gt;' is not marked as EOG\nload: control token: 256225 '&lt;unused323&gt;' is not marked as EOG\nload: control token: 256223 '&lt;unused321&gt;' is not marked as EOG\nload: control token: 256221 '&lt;unused319&gt;' is not marked as EOG\nload: control token: 256220 '&lt;unused318&gt;' is not marked as EOG\nload: control token: 256219 '&lt;unused317&gt;' is not marked as EOG\nload: control token: 256213 '&lt;unused311&gt;' is not marked as EOG\nload: control token: 256210 '&lt;unused308&gt;' is not marked as EOG\nload: control token: 256208 '&lt;unused306&gt;' is not marked as EOG\nload: control token: 256207 '&lt;unused305&gt;' is not marked as EOG\nload: control token: 256206 '&lt;unused304&gt;' is not marked as EOG\nload: control token: 256205 '&lt;unused303&gt;' is not marked as EOG\nload: control token: 256204 '&lt;unused302&gt;' is not marked as EOG\nload: control token: 256203 '&lt;unused301&gt;' is not marked as EOG\nload: control token: 256201 '&lt;unused299&gt;' is not marked as EOG\nload: control token: 256200 '&lt;unused298&gt;' is not marked as EOG\nload: control token: 256198 '&lt;unused296&gt;' is not marked as EOG\nload: control token: 256197 '&lt;unused295&gt;' is not marked as EOG\nload: control token: 256196 '&lt;unused294&gt;' is not marked as EOG\nload: control token: 256192 '&lt;unused290&gt;' is not marked as EOG\nload: control token: 256191 '&lt;unused289&gt;' is not marked as EOG\nload: control token: 256190 '&lt;unused288&gt;' is not marked as EOG\nload: control token: 256189 '&lt;unused287&gt;' is not marked as EOG\nload: control token: 256187 '&lt;unused285&gt;' is not marked as EOG\nload: control token: 256186 '&lt;unused284&gt;' is not marked as EOG\nload: control token: 256185 '&lt;unused283&gt;' is not marked as EOG\nload: control token: 256183 '&lt;unused281&gt;' is not marked as EOG\nload: control token: 256181 '&lt;unused279&gt;' is not marked as EOG\nload: control token: 256180 '&lt;unused278&gt;' is not marked as EOG\nload: control token: 256179 '&lt;unused277&gt;' is not marked as EOG\nload: control token: 256177 '&lt;unused275&gt;' is not marked as EOG\nload: control token: 256176 '&lt;unused274&gt;' is not marked as EOG\nload: control token: 256175 '&lt;unused273&gt;' is not marked as EOG\nload: control token: 256171 '&lt;unused269&gt;' is not marked as EOG\nload: control token: 256170 '&lt;unused268&gt;' is not marked as EOG\nload: control token: 256169 '&lt;unused267&gt;' is not marked as EOG\nload: control token: 256166 '&lt;unused264&gt;' is not marked as EOG\nload: control token: 256165 '&lt;unused263&gt;' is not marked as EOG\nload: control token: 256164 '&lt;unused262&gt;' is not marked as EOG\nload: control token: 256163 '&lt;unused261&gt;' is not marked as EOG\nload: control token: 256162 '&lt;unused260&gt;' is not marked as EOG\nload: control token: 256159 '&lt;unused257&gt;' is not marked as EOG\nload: control token: 256154 '&lt;unused252&gt;' is not marked as EOG\nload: control token: 256153 '&lt;unused251&gt;' is not marked as EOG\nload: control token: 256152 '&lt;unused250&gt;' is not marked as EOG\nload: control token: 256150 '&lt;unused248&gt;' is not marked as EOG\nload: control token: 256148 '&lt;unused246&gt;' is not marked as EOG\nload: control token: 256145 '&lt;unused243&gt;' is not marked as EOG\nload: control token: 256144 '&lt;unused242&gt;' is not marked as EOG\nload: control token: 256142 '&lt;unused240&gt;' is not marked as EOG\nload: control token: 256139 '&lt;unused237&gt;' is not marked as EOG\nload: control token: 256138 '&lt;unused236&gt;' is not marked as EOG\nload: control token: 256136 '&lt;unused234&gt;' is not marked as EOG\nload: control token: 256133 '&lt;unused231&gt;' is not marked as EOG\nload: control token: 256123 '&lt;unused221&gt;' is not marked as EOG\nload: control token: 256122 '&lt;unused220&gt;' is not marked as EOG\nload: control token: 256121 '&lt;unused219&gt;' is not marked as EOG\nload: control token: 256117 '&lt;unused215&gt;' is not marked as EOG\nload: control token: 256115 '&lt;unused213&gt;' is not marked as EOG\nload: control token: 256112 '&lt;unused210&gt;' is not marked as EOG\nload: control token: 256109 '&lt;unused207&gt;' is not marked as EOG\nload: control token: 256107 '&lt;unused205&gt;' is not marked as EOG\nload: control token: 256103 '&lt;unused201&gt;' is not marked as EOG\nload: control token: 256099 '&lt;unused197&gt;' is not marked as EOG\nload: control token: 261510 '&lt;unused5608&gt;' is not marked as EOG\nload: control token: 256097 '&lt;unused195&gt;' is not marked as EOG\nload: control token: 256092 '&lt;unused190&gt;' is not marked as EOG\nload: control token: 256089 '&lt;unused187&gt;' is not marked as EOG\nload: control token: 256086 '&lt;unused184&gt;' is not marked as EOG\nload: control token: 256085 '&lt;unused183&gt;' is not marked as EOG\nload: control token: 256084 '&lt;unused182&gt;' is not marked as EOG\nload: control token: 256082 '&lt;unused180&gt;' is not marked as EOG\nload: control token: 256081 '&lt;unused179&gt;' is not marked as EOG\nload: control token: 256080 '&lt;unused178&gt;' is not marked as EOG\nload: control token: 256078 '&lt;unused176&gt;' is not marked as EOG\nload: control token: 256074 '&lt;unused172&gt;' is not marked as EOG\nload: control token: 256073 '&lt;unused171&gt;' is not marked as EOG\nload: control token: 256072 '&lt;unused170&gt;' is not marked as EOG\nload: control token: 256071 '&lt;unused169&gt;' is not marked as EOG\nload: control token: 256070 '&lt;unused168&gt;' is not marked as EOG\nload: control token: 256069 '&lt;unused167&gt;' is not marked as EOG\nload: control token: 256068 '&lt;unused166&gt;' is not marked as EOG\nload: control token: 256067 '&lt;unused165&gt;' is not marked as EOG\nload: control token: 256066 '&lt;unused164&gt;' is not marked as EOG\nload: control token: 256065 '&lt;unused163&gt;' is not marked as EOG\nload: control token: 256064 '&lt;unused162&gt;' is not marked as EOG\nload: control token: 256063 '&lt;unused161&gt;' is not marked as EOG\nload: control token: 256060 '&lt;unused158&gt;' is not marked as EOG\nload: control token: 256058 '&lt;unused156&gt;' is not marked as EOG\nload: control token: 256056 '&lt;unused154&gt;' is not marked as EOG\nload: control token: 256055 '&lt;unused153&gt;' is not marked as EOG\nload: control token: 256053 '&lt;unused151&gt;' is not marked as EOG\nload: control token: 256052 '&lt;unused150&gt;' is not marked as EOG\nload: control token: 261513 '&lt;unused5611&gt;' is not marked as EOG\nload: control token: 256049 '&lt;unused147&gt;' is not marked as EOG\nload: control token: 256048 '&lt;unused146&gt;' is not marked as EOG\nload: control token: 256047 '&lt;unused145&gt;' is not marked as EOG\nload: control token: 256044 '&lt;unused142&gt;' is not marked as EOG\nload: control token: 256043 '&lt;unused141&gt;' is not marked as EOG\nload: control token: 256041 '&lt;unused139&gt;' is not marked as EOG\nload: control token: 256039 '&lt;unused137&gt;' is not marked as EOG\nload: control token: 256037 '&lt;unused135&gt;' is not marked as EOG\nload: control token: 256035 '&lt;unused133&gt;' is not marked as EOG\nload: control token: 256033 '&lt;unused131&gt;' is not marked as EOG\nload: control token: 256030 '&lt;unused128&gt;' is not marked as EOG\nload: control token: 256028 '&lt;unused126&gt;' is not marked as EOG\nload: control token: 256027 '&lt;unused125&gt;' is not marked as EOG\nload: control token: 256026 '&lt;unused124&gt;' is not marked as EOG\nload: control token: 256025 '&lt;unused123&gt;' is not marked as EOG\nload: control token: 256024 '&lt;unused122&gt;' is not marked as EOG\nload: control token: 256023 '&lt;unused121&gt;' is not marked as EOG\nload: control token: 256022 '&lt;unused120&gt;' is not marked as EOG\nload: control token: 256021 '&lt;unused119&gt;' is not marked as EOG\nload: control token: 256019 '&lt;unused117&gt;' is not marked as EOG\nload: control token: 256016 '&lt;unused114&gt;' is not marked as EOG\nload: control token: 256012 '&lt;unused110&gt;' is not marked as EOG\nload: control token: 256010 '&lt;unused108&gt;' is not marked as EOG\nload: control token: 256007 '&lt;unused105&gt;' is not marked as EOG\nload: control token: 256006 '&lt;unused104&gt;' is not marked as EOG\nload: control token: 256001 '&lt;unused99&gt;' is not marked as EOG\nload: control token: 255999 '&lt;start_of_image&gt;' is not marked as EOG\nload: control token: 260581 '&lt;unused4679&gt;' is not marked as EOG\nload: control token: 258610 '&lt;unused2708&gt;' is not marked as EOG\nload: control token: 258206 '&lt;unused2304&gt;' is not marked as EOG\nload: control token: 257301 '&lt;unused1399&gt;' is not marked as EOG\nload: control token: 258829 '&lt;unused2927&gt;' is not marked as EOG\nload: control token: 257481 '&lt;unused1579&gt;' is not marked as EOG\nload: control token: 261558 '&lt;unused5656&gt;' is not marked as EOG\nload: control token: 257203 '&lt;unused1301&gt;' is not marked as EOG\nload: control token: 260041 '&lt;unused4139&gt;' is not marked as EOG\nload: control token: 258216 '&lt;unused2314&gt;' is not marked as EOG\nload: control token: 257666 '&lt;unused1764&gt;' is not marked as EOG\nload: control token: 261802 '&lt;unused5900&gt;' is not marked as EOG\nload: control token: 258479 '&lt;unused2577&gt;' is not marked as EOG\nload: control token: 256966 '&lt;unused1064&gt;' is not marked as EOG\nload: control token: 260410 '&lt;unused4508&gt;' is not marked as EOG\nload: control token: 257738 '&lt;unused1836&gt;' is not marked as EOG\nload: control token: 261363 '&lt;unused5461&gt;' is not marked as EOG\nload: control token: 259315 '&lt;unused3413&gt;' is not marked as EOG\nload: control token: 259704 '&lt;unused3802&gt;' is not marked as EOG\nload: control token: 260535 '&lt;unused4633&gt;' is not marked as EOG\nload: control token: 261074 '&lt;unused5172&gt;' is not marked as EOG\nload: control token: 256469 '&lt;unused567&gt;' is not marked as EOG\nload: control token: 256744 '&lt;unused842&gt;' is not marked as EOG\nload: control token: 256426 '&lt;unused524&gt;' is not marked as EOG\nload: control token: 257384 '&lt;unused1482&gt;' is not marked as EOG\nload: control token: 257365 '&lt;unused1463&gt;' is not marked as EOG\nload: control token: 256831 '&lt;unused929&gt;' is not marked as EOG\nload: control token: 258468 '&lt;unused2566&gt;' is not marked as EOG\nload: control token: 262059 '&lt;unused6157&gt;' is not marked as EOG\nload: control token: 260550 '&lt;unused4648&gt;' is not marked as EOG\nload: control token: 260119 '&lt;unused4217&gt;' is not marked as EOG\nload: control token: 259506 '&lt;unused3604&gt;' is not marked as EOG\nload: control token: 257679 '&lt;unused1777&gt;' is not marked as EOG\nload: control token: 258118 '&lt;unused2216&gt;' is not marked as EOG\nload: control token: 257452 '&lt;unused1550&gt;' is not marked as EOG\nload: control token: 256668 '&lt;unused766&gt;' is not marked as EOG\nload: control token: 259207 '&lt;unused3305&gt;' is not marked as EOG\nload: control token: 258886 '&lt;unused2984&gt;' is not marked as EOG\nload: control token: 260965 '&lt;unused5063&gt;' is not marked as EOG\nload: control token: 261920 '&lt;unused6018&gt;' is not marked as EOG\nload: control token: 259970 '&lt;unused4068&gt;' is not marked as EOG\nload: control token: 257674 '&lt;unused1772&gt;' is not marked as EOG\nload: control token: 256202 '&lt;unused300&gt;' is not marked as EOG\nload: control token: 259003 '&lt;unused3101&gt;' is not marked as EOG\nload: control token: 260566 '&lt;unused4664&gt;' is not marked as EOG\nload: control token: 257957 '&lt;unused2055&gt;' is not marked as EOG\nload: control token: 260511 '&lt;unused4609&gt;' is not marked as EOG\nload: control token: 256900 '&lt;unused998&gt;' is not marked as EOG\nload: control token: 258474 '&lt;unused2572&gt;' is not marked as EOG\nload: control token: 256409 '&lt;unused507&gt;' is not marked as EOG\nload: control token: 259301 '&lt;unused3399&gt;' is not marked as EOG\nload: control token: 256819 '&lt;unused917&gt;' is not marked as EOG\nload: control token: 260489 '&lt;unused4587&gt;' is not marked as EOG\nload: control token: 257946 '&lt;unused2044&gt;' is not marked as EOG\nload: control token: 256077 '&lt;unused175&gt;' is not marked as EOG\nload: control token: 258027 '&lt;unused2125&gt;' is not marked as EOG\nload: control token: 256036 '&lt;unused134&gt;' is not marked as EOG\nload: control token: 257690 '&lt;unused1788&gt;' is not marked as EOG\nload: control token: 258807 '&lt;unused2905&gt;' is not marked as EOG\nload: control token: 258228 '&lt;unused2326&gt;' is not marked as EOG\nload: control token: 257466 '&lt;unused1564&gt;' is not marked as EOG\nload: control token: 256076 '&lt;unused174&gt;' is not marked as EOG\nload: control token: 256928 '&lt;unused1026&gt;' is not marked as EOG\nload: control token: 260244 '&lt;unused4342&gt;' is not marked as EOG\nload: control token: 258278 '&lt;unused2376&gt;' is not marked as EOG\nload: control token: 259292 '&lt;unused3390&gt;' is not marked as EOG\nload: control token: 257954 '&lt;unused2052&gt;' is not marked as EOG\nload: control token: 261821 '&lt;unused5919&gt;' is not marked as EOG\nload: control token: 261991 '&lt;unused6089&gt;' is not marked as EOG\nload: control token: 256157 '&lt;unused255&gt;' is not marked as EOG\nload: control token: 260714 '&lt;unused4812&gt;' is not marked as EOG\nload: control token: 261992 '&lt;unused6090&gt;' is not marked as EOG\nload: control token: 261639 '&lt;unused5737&gt;' is not marked as EOG\nload: control token: 260053 '&lt;unused4151&gt;' is not marked as EOG\nload: control token: 257701 '&lt;unused1799&gt;' is not marked as EOG\nload: control token: 256484 '&lt;unused582&gt;' is not marked as EOG\nload: control token: 260207 '&lt;unused4305&gt;' is not marked as EOG\nload: control token: 259533 '&lt;unused3631&gt;' is not marked as EOG\nload: control token: 256160 '&lt;unused258&gt;' is not marked as EOG\nload: control token: 258643 '&lt;unused2741&gt;' is not marked as EOG\nload: control token: 256370 '&lt;unused468&gt;' is not marked as EOG\nload: control token: 256545 '&lt;unused643&gt;' is not marked as EOG\nload: control token: 259530 '&lt;unused3628&gt;' is not marked as EOG\nload: control token: 261686 '&lt;unused5784&gt;' is not marked as EOG\nload: control token: 260758 '&lt;unused4856&gt;' is not marked as EOG\nload: control token: 260486 '&lt;unused4584&gt;' is not marked as EOG\nload: control token: 256585 '&lt;unused683&gt;' is not marked as EOG\nload: control token: 261077 '&lt;unused5175&gt;' is not marked as EOG\nload: control token: 261775 '&lt;unused5873&gt;' is not marked as EOG\nload: control token: 260295 '&lt;unused4393&gt;' is not marked as EOG\nload: control token: 259629 '&lt;unused3727&gt;' is not marked as EOG\nload: control token: 258398 '&lt;unused2496&gt;' is not marked as EOG\nload: control token: 259170 '&lt;unused3268&gt;' is not marked as EOG\nload: control token: 258312 '&lt;unused2410&gt;' is not marked as EOG\nload: control token: 256941 '&lt;unused1039&gt;' is not marked as EOG\nload: control token: 256519 '&lt;unused617&gt;' is not marked as EOG\nload: control token: 257845 '&lt;unused1943&gt;' is not marked as EOG\nload: control token: 260887 '&lt;unused4985&gt;' is not marked as EOG\nload: control token: 258413 '&lt;unused2511&gt;' is not marked as EOG\nload: control token: 261636 '&lt;unused5734&gt;' is not marked as EOG\nload: control token: 259929 '&lt;unused4027&gt;' is not marked as EOG\nload: control token: 261437 '&lt;unused5535&gt;' is not marked as EOG\nload: control token: 262084 '&lt;unused6182&gt;' is not marked as EOG\nload: control token: 259477 '&lt;unused3575&gt;' is not marked as EOG\nload: control token: 261030 '&lt;unused5128&gt;' is not marked as EOG\nload: control token: 258315 '&lt;unused2413&gt;' is not marked as EOG\nload: control token: 257754 '&lt;unused1852&gt;' is not marked as EOG\nload: control token: 259474 '&lt;unused3572&gt;' is not marked as EOG\nload: control token: 256666 '&lt;unused764&gt;' is not marked as EOG\nload: control token: 260723 '&lt;unused4821&gt;' is not marked as EOG\nload: control token: 256101 '&lt;unused199&gt;' is not marked as EOG\nload: control token: 259648 '&lt;unused3746&gt;' is not marked as EOG\nload: control token: 259258 '&lt;unused3356&gt;' is not marked as EOG\nload: control token: 261631 '&lt;unused5729&gt;' is not marked as EOG\nload: control token: 261748 '&lt;unused5846&gt;' is not marked as EOG\nload: control token: 257264 '&lt;unused1362&gt;' is not marked as EOG\nload: control token: 259763 '&lt;unused3861&gt;' is not marked as EOG\nload: control token: 260188 '&lt;unused4286&gt;' is not marked as EOG\nload: control token: 258426 '&lt;unused2524&gt;' is not marked as EOG\nload: control token: 258873 '&lt;unused2971&gt;' is not marked as EOG\nload: control token: 258389 '&lt;unused2487&gt;' is not marked as EOG\nload: control token: 256110 '&lt;unused208&gt;' is not marked as EOG\nload: control token: 260525 '&lt;unused4623&gt;' is not marked as EOG\nload: control token: 256575 '&lt;unused673&gt;' is not marked as EOG\nload: control token: 261231 '&lt;unused5329&gt;' is not marked as EOG\nload: control token: 256147 '&lt;unused245&gt;' is not marked as EOG\nload: control token: 261107 '&lt;unused5205&gt;' is not marked as EOG\nload: control token: 256134 '&lt;unused232&gt;' is not marked as EOG\nload: control token: 257366 '&lt;unused1464&gt;' is not marked as EOG\nload: control token: 256503 '&lt;unused601&gt;' is not marked as EOG\nload: control token: 260840 '&lt;unused4938&gt;' is not marked as EOG\nload: control token: 261532 '&lt;unused5630&gt;' is not marked as EOG\nload: control token: 259923 '&lt;unused4021&gt;' is not marked as EOG\nload: control token: 259572 '&lt;unused3670&gt;' is not marked as EOG\nload: control token: 261166 '&lt;unused5264&gt;' is not marked as EOG\nload: control token: 256548 '&lt;unused646&gt;' is not marked as EOG\nload: control token: 260065 '&lt;unused4163&gt;' is not marked as EOG\nload: control token: 257114 '&lt;unused1212&gt;' is not marked as EOG\nload: control token: 261770 '&lt;unused5868&gt;' is not marked as EOG\nload: control token: 261064 '&lt;unused5162&gt;' is not marked as EOG\nload: control token: 260348 '&lt;unused4446&gt;' is not marked as EOG\nload: control token: 260155 '&lt;unused4253&gt;' is not marked as EOG\nload: control token: 259122 '&lt;unused3220&gt;' is not marked as EOG\nload: control token: 262069 '&lt;unused6167&gt;' is not marked as EOG\nload: control token: 258842 '&lt;unused2940&gt;' is not marked as EOG\nload: control token: 257514 '&lt;unused1612&gt;' is not marked as EOG\nload: control token: 261706 '&lt;unused5804&gt;' is not marked as EOG\nload: control token: 259711 '&lt;unused3809&gt;' is not marked as EOG\nload: control token: 262072 '&lt;unused6170&gt;' is not marked as EOG\nload: control token: 259976 '&lt;unused4074&gt;' is not marked as EOG\nload: control token: 261894 '&lt;unused5992&gt;' is not marked as EOG\nload: control token: 259706 '&lt;unused3804&gt;' is not marked as EOG\nload: control token: 256434 '&lt;unused532&gt;' is not marked as EOG\nload: control token: 261836 '&lt;unused5934&gt;' is not marked as EOG\nload: control token: 260562 '&lt;unused4660&gt;' is not marked as EOG\nload: control token: 259154 '&lt;unused3252&gt;' is not marked as EOG\nload: control token: 259192 '&lt;unused3290&gt;' is not marked as EOG\nload: control token: 257847 '&lt;unused1945&gt;' is not marked as EOG\nload: control token: 258798 '&lt;unused2896&gt;' is not marked as EOG\nload: control token: 257233 '&lt;unused1331&gt;' is not marked as EOG\nload: control token: 256555 '&lt;unused653&gt;' is not marked as EOG\nload: control token: 259665 '&lt;unused3763&gt;' is not marked as EOG\nload: control token: 259865 '&lt;unused3963&gt;' is not marked as EOG\nload: control token: 259743 '&lt;unused3841&gt;' is not marked as EOG\nload: control token: 258145 '&lt;unused2243&gt;' is not marked as EOG\nload: control token: 257926 '&lt;unused2024&gt;' is not marked as EOG\nload: control token: 257209 '&lt;unused1307&gt;' is not marked as EOG\nload: control token: 260820 '&lt;unused4918&gt;' is not marked as EOG\nload: control token: 260520 '&lt;unused4618&gt;' is not marked as EOG\nload: control token: 260677 '&lt;unused4775&gt;' is not marked as EOG\nload: control token: 260989 '&lt;unused5087&gt;' is not marked as EOG\nload: control token: 261884 '&lt;unused5982&gt;' is not marked as EOG\nload: control token: 257804 '&lt;unused1902&gt;' is not marked as EOG\nload: control token: 256199 '&lt;unused297&gt;' is not marked as EOG\nload: control token: 257432 '&lt;unused1530&gt;' is not marked as EOG\nload: control token: 259655 '&lt;unused3753&gt;' is not marked as EOG\nload: control token: 259806 '&lt;unused3904&gt;' is not marked as EOG\nload: control token: 258551 '&lt;unused2649&gt;' is not marked as EOG\nload: control token: 258194 '&lt;unused2292&gt;' is not marked as EOG\nload: control token: 260341 '&lt;unused4439&gt;' is not marked as EOG\nload: control token: 260416 '&lt;unused4514&gt;' is not marked as EOG\nload: control token: 260674 '&lt;unused4772&gt;' is not marked as EOG\nload: control token: 260666 '&lt;unused4764&gt;' is not marked as EOG\nload: control token: 259316 '&lt;unused3414&gt;' is not marked as EOG\nload: control token: 258143 '&lt;unused2241&gt;' is not marked as EOG\nload: control token: 256971 '&lt;unused1069&gt;' is not marked as EOG\nload: control token: 261316 '&lt;unused5414&gt;' is not marked as EOG\nload: control token: 261587 '&lt;unused5685&gt;' is not marked as EOG\nload: control token: 260994 '&lt;unused5092&gt;' is not marked as EOG\nload: control token: 258862 '&lt;unused2960&gt;' is not marked as EOG\nload: control token: 260656 '&lt;unused4754&gt;' is not marked as EOG\nload: control token: 256216 '&lt;unused314&gt;' is not marked as EOG\nload: control token: 261343 '&lt;unused5441&gt;' is not marked as EOG\nload: control token: 260375 '&lt;unused4473&gt;' is not marked as EOG\nload: control token: 257369 '&lt;unused1467&gt;' is not marked as EOG\nload: control token: 261614 '&lt;unused5712&gt;' is not marked as EOG\nload: control token: 258266 '&lt;unused2364&gt;' is not marked as EOG\nload: control token: 257157 '&lt;unused1255&gt;' is not marked as EOG\nload: control token: 260923 '&lt;unused5021&gt;' is not marked as EOG\nload: control token: 257320 '&lt;unused1418&gt;' is not marked as EOG\nload: control token: 257064 '&lt;unused1162&gt;' is not marked as EOG\nload: control token: 259856 '&lt;unused3954&gt;' is not marked as EOG\nload: control token: 258119 '&lt;unused2217&gt;' is not marked as EOG\nload: control token: 259730 '&lt;unused3828&gt;' is not marked as EOG\nload: control token: 260874 '&lt;unused4972&gt;' is not marked as EOG\nload: control token: 261887 '&lt;unused5985&gt;' is not marked as EOG\nload: control token: 261780 '&lt;unused5878&gt;' is not marked as EOG\nload: control token: 257483 '&lt;unused1581&gt;' is not marked as EOG\nload: control token: 259426 '&lt;unused3524&gt;' is not marked as EOG\nload: control token: 258490 '&lt;unused2588&gt;' is not marked as EOG\nload: control token: 258793 '&lt;unused2891&gt;' is not marked as EOG\nload: control token: 256353 '&lt;unused451&gt;' is not marked as EOG\nload: control token: 261580 '&lt;unused5678&gt;' is not marked as EOG\nload: control token: 259182 '&lt;unused3280&gt;' is not marked as EOG\nload: control token: 258694 '&lt;unused2792&gt;' is not marked as EOG\nload: control token: 256369 '&lt;unused467&gt;' is not marked as EOG\nload: control token: 258875 '&lt;unused2973&gt;' is not marked as EOG\nload: control token: 259363 '&lt;unused3461&gt;' is not marked as EOG\nload: control token: 259953 '&lt;unused4051&gt;' is not marked as EOG\nload: control token: 256119 '&lt;unused217&gt;' is not marked as EOG\nload: control token: 261520 '&lt;unused5618&gt;' is not marked as EOG\nload: control token: 257628 '&lt;unused1726&gt;' is not marked as EOG\nload: control token: 257604 '&lt;unused1702&gt;' is not marked as EOG\nload: control token: 261813 '&lt;unused5911&gt;' is not marked as EOG\nload: control token: 260865 '&lt;unused4963&gt;' is not marked as EOG\nload: control token: 261939 '&lt;unused6037&gt;' is not marked as EOG\nload: control token: 257866 '&lt;unused1964&gt;' is not marked as EOG\nload: control token: 257501 '&lt;unused1599&gt;' is not marked as EOG\nload: control token: 261450 '&lt;unused5548&gt;' is not marked as EOG\nload: control token: 258779 '&lt;unused2877&gt;' is not marked as EOG\nload: control token: 262131 '&lt;unused6229&gt;' is not marked as EOG\nload: control token: 261020 '&lt;unused5118&gt;' is not marked as EOG\nload: control token: 256933 '&lt;unused1031&gt;' is not marked as EOG\nload: control token: 259318 '&lt;unused3416&gt;' is not marked as EOG\nload: control token: 261956 '&lt;unused6054&gt;' is not marked as EOG\nload: control token: 256912 '&lt;unused1010&gt;' is not marked as EOG\nload: control token: 257815 '&lt;unused1913&gt;' is not marked as EOG\nload: control token: 258724 '&lt;unused2822&gt;' is not marked as EOG\nload: control token: 260427 '&lt;unused4525&gt;' is not marked as EOG\nload: control token: 258975 '&lt;unused3073&gt;' is not marked as EOG\nload: control token: 259919 '&lt;unused4017&gt;' is not marked as EOG\nload: control token: 260081 '&lt;unused4179&gt;' is not marked as EOG\nload: control token: 259257 '&lt;unused3355&gt;' is not marked as EOG\nload: control token: 260947 '&lt;unused5045&gt;' is not marked as EOG\nload: control token: 259995 '&lt;unused4093&gt;' is not marked as EOG\nload: control token: 258407 '&lt;unused2505&gt;' is not marked as EOG\nload: control token: 259625 '&lt;unused3723&gt;' is not marked as EOG\nload: control token: 256674 '&lt;unused772&gt;' is not marked as EOG\nload: control token: 256228 '&lt;unused326&gt;' is not marked as EOG\nload: control token: 259567 '&lt;unused3665&gt;' is not marked as EOG\nload: control token: 260422 '&lt;unused4520&gt;' is not marked as EOG\nload: control token: 257590 '&lt;unused1688&gt;' is not marked as EOG\nload: control token: 260094 '&lt;unused4192&gt;' is not marked as EOG\nload: control token: 261751 '&lt;unused5849&gt;' is not marked as EOG\nload: control token: 259904 '&lt;unused4002&gt;' is not marked as EOG\nload: control token: 256127 '&lt;unused225&gt;' is not marked as EOG\nload: control token: 258988 '&lt;unused3086&gt;' is not marked as EOG\nload: control token: 257051 '&lt;unused1149&gt;' is not marked as EOG\nload: control token: 256657 '&lt;unused755&gt;' is not marked as EOG\nload: control token: 258207 '&lt;unused2305&gt;' is not marked as EOG\nload: control token: 260931 '&lt;unused5029&gt;' is not marked as EOG\nload: control token: 259435 '&lt;unused3533&gt;' is not marked as EOG\nload: control token: 256264 '&lt;unused362&gt;' is not marked as EOG\nload: control token: 258078 '&lt;unused2176&gt;' is not marked as EOG\nload: control token: 261041 '&lt;unused5139&gt;' is not marked as EOG\nload: control token: 258658 '&lt;unused2756&gt;' is not marked as EOG\nload: control token: 258566 '&lt;unused2664&gt;' is not marked as EOG\nload: control token: 260706 '&lt;unused4804&gt;' is not marked as EOG\nload: control token: 259277 '&lt;unused3375&gt;' is not marked as EOG\nload: control token: 257449 '&lt;unused1547&gt;' is not marked as EOG\nload: control token: 256728 '&lt;unused826&gt;' is not marked as EOG\nload: control token: 257135 '&lt;unused1233&gt;' is not marked as EOG\nload: control token: 258499 '&lt;unused2597&gt;' is not marked as EOG\nload: control token: 261004 '&lt;unused5102&gt;' is not marked as EOG\nload: control token: 258997 '&lt;unused3095&gt;' is not marked as EOG\nload: control token: 258260 '&lt;unused2358&gt;' is not marked as EOG\nload: control token: 260888 '&lt;unused4986&gt;' is not marked as EOG\nload: control token: 260926 '&lt;unused5024&gt;' is not marked as EOG\nload: control token: 260221 '&lt;unused4319&gt;' is not marked as EOG\nload: control token: 256477 '&lt;unused575&gt;' is not marked as EOG\nload: control token: 256704 '&lt;unused802&gt;' is not marked as EOG\nload: control token: 256288 '&lt;unused386&gt;' is not marked as EOG\nload: control token: 257680 '&lt;unused1778&gt;' is not marked as EOG\nload: control token: 260505 '&lt;unused4603&gt;' is not marked as EOG\nload: control token: 259516 '&lt;unused3614&gt;' is not marked as EOG\nload: control token: 259796 '&lt;unused3894&gt;' is not marked as EOG\nload: control token: 257020 '&lt;unused1118&gt;' is not marked as EOG\nload: control token: 256647 '&lt;unused745&gt;' is not marked as EOG\nload: control token: 257740 '&lt;unused1838&gt;' is not marked as EOG\nload: control token: 259287 '&lt;unused3385&gt;' is not marked as EOG\nload: control token: 257902 '&lt;unused2000&gt;' is not marked as EOG\nload: control token: 258889 '&lt;unused2987&gt;' is not marked as EOG\nload: control token: 256683 '&lt;unused781&gt;' is not marked as EOG\nload: control token: 257927 '&lt;unused2025&gt;' is not marked as EOG\nload: control token: 258455 '&lt;unused2553&gt;' is not marked as EOG\nload: control token: 261397 '&lt;unused5495&gt;' is not marked as EOG\nload: control token: 260088 '&lt;unused4186&gt;' is not marked as EOG\nload: control token: 259659 '&lt;unused3757&gt;' is not marked as EOG\nload: control token: 258641 '&lt;unused2739&gt;' is not marked as EOG\nload: control token: 260051 '&lt;unused4149&gt;' is not marked as EOG\nload: control token: 259214 '&lt;unused3312&gt;' is not marked as EOG\nload: control token: 259807 '&lt;unused3905&gt;' is not marked as EOG\nload: control token: 256730 '&lt;unused828&gt;' is not marked as EOG\nload: control token: 257828 '&lt;unused1926&gt;' is not marked as EOG\nload: control token: 259384 '&lt;unused3482&gt;' is not marked as EOG\nload: control token: 260049 '&lt;unused4147&gt;' is not marked as EOG\nload: control token: 256276 '&lt;unused374&gt;' is not marked as EOG\nload: control token: 260180 '&lt;unused4278&gt;' is not marked as EOG\nload: control token: 256284 '&lt;unused382&gt;' is not marked as EOG\nload: control token: 257225 '&lt;unused1323&gt;' is not marked as EOG\nload: control token: 259517 '&lt;unused3615&gt;' is not marked as EOG\nload: control token: 259603 '&lt;unused3701&gt;' is not marked as EOG\nload: control token: 262087 '&lt;unused6185&gt;' is not marked as EOG\nload: control token: 256869 '&lt;unused967&gt;' is not marked as EOG\nload: control token: 257559 '&lt;unused1657&gt;' is not marked as EOG\nload: control token: 256479 '&lt;unused577&gt;' is not marked as EOG\nload: control token: 259466 '&lt;unused3564&gt;' is not marked as EOG\nload: control token: 258423 '&lt;unused2521&gt;' is not marked as EOG\nload: control token: 257587 '&lt;unused1685&gt;' is not marked as EOG\nload: control token: 258740 '&lt;unused2838&gt;' is not marked as EOG\nload: control token: 259764 '&lt;unused3862&gt;' is not marked as EOG\nload: control token: 261948 '&lt;unused6046&gt;' is not marked as EOG\nload: control token: 261642 '&lt;unused5740&gt;' is not marked as EOG\nload: control token: 259968 '&lt;unused4066&gt;' is not marked as EOG\nload: control token: 257945 '&lt;unused2043&gt;' is not marked as EOG\nload: control token: 261933 '&lt;unused6031&gt;' is not marked as EOG\nload: control token: 262015 '&lt;unused6113&gt;' is not marked as EOG\nload: control token: 260988 '&lt;unused5086&gt;' is not marked as EOG\nload: control token: 257250 '&lt;unused1348&gt;' is not marked as EOG\nload: control token: 256727 '&lt;unused825&gt;' is not marked as EOG\nload: control token: 259884 '&lt;unused3982&gt;' is not marked as EOG\nload: control token: 256932 '&lt;unused1030&gt;' is not marked as EOG\nload: control token: 261121 '&lt;unused5219&gt;' is not marked as EOG\nload: control token: 257161 '&lt;unused1259&gt;' is not marked as EOG\nload: control token: 258836 '&lt;unused2934&gt;' is not marked as EOG\nload: control token: 261524 '&lt;unused5622&gt;' is not marked as EOG\nload: control token: 257220 '&lt;unused1318&gt;' is not marked as EOG\nload: control token: 256864 '&lt;unused962&gt;' is not marked as EOG\nload: control token: 257994 '&lt;unused2092&gt;' is not marked as EOG\nload: control token: 257765 '&lt;unused1863&gt;' is not marked as EOG\nload: control token: 256314 '&lt;unused412&gt;' is not marked as EOG\nload: control token: 259031 '&lt;unused3129&gt;' is not marked as EOG\nload: control token: 260418 '&lt;unused4516&gt;' is not marked as EOG\nload: control token: 259644 '&lt;unused3742&gt;' is not marked as EOG\nload: control token: 261436 '&lt;unused5534&gt;' is not marked as EOG\nload: control token: 256146 '&lt;unused244&gt;' is not marked as EOG\nload: control token: 259035 '&lt;unused3133&gt;' is not marked as EOG\nload: control token: 260850 '&lt;unused4948&gt;' is not marked as EOG\nload: control token: 257853 '&lt;unused1951&gt;' is not marked as EOG\nload: control token: 261157 '&lt;unused5255&gt;' is not marked as EOG\nload: control token: 257987 '&lt;unused2085&gt;' is not marked as EOG\nload: control token: 260910 '&lt;unused5008&gt;' is not marked as EOG\nload: control token: 258607 '&lt;unused2705&gt;' is not marked as EOG\nload: control token: 257764 '&lt;unused1862&gt;' is not marked as EOG\nload: control token: 257354 '&lt;unused1452&gt;' is not marked as EOG\nload: control token: 261910 '&lt;unused6008&gt;' is not marked as EOG\nload: control token: 259851 '&lt;unused3949&gt;' is not marked as EOG\nload: control token: 258449 '&lt;unused2547&gt;' is not marked as EOG\nload: control token: 257188 '&lt;unused1286&gt;' is not marked as EOG\nload: control token: 258580 '&lt;unused2678&gt;' is not marked as EOG\nload: control token: 256040 '&lt;unused138&gt;' is not marked as EOG\nload: control token: 262128 '&lt;unused6226&gt;' is not marked as EOG\nload: control token: 256684 '&lt;unused782&gt;' is not marked as EOG\nload: control token: 257437 '&lt;unused1535&gt;' is not marked as EOG\nload: control token: 258885 '&lt;unused2983&gt;' is not marked as EOG\nload: control token: 261341 '&lt;unused5439&gt;' is not marked as EOG\nload: control token: 261960 '&lt;unused6058&gt;' is not marked as EOG\nload: control token: 260616 '&lt;unused4714&gt;' is not marked as EOG\nload: control token: 257349 '&lt;unused1447&gt;' is not marked as EOG\nload: control token: 256587 '&lt;unused685&gt;' is not marked as EOG\nload: control token: 260241 '&lt;unused4339&gt;' is not marked as EOG\nload: control token: 260564 '&lt;unused4662&gt;' is not marked as EOG\nload: control token: 260095 '&lt;unused4193&gt;' is not marked as EOG\nload: control token: 260134 '&lt;unused4232&gt;' is not marked as EOG\nload: control token: 257102 '&lt;unused1200&gt;' is not marked as EOG\nload: control token: 262005 '&lt;unused6103&gt;' is not marked as EOG\nload: control token: 261872 '&lt;unused5970&gt;' is not marked as EOG\nload: control token: 259520 '&lt;unused3618&gt;' is not marked as EOG\nload: control token: 257544 '&lt;unused1642&gt;' is not marked as EOG\nload: control token: 257595 '&lt;unused1693&gt;' is not marked as EOG\nload: control token: 256580 '&lt;unused678&gt;' is not marked as EOG\nload: control token: 257304 '&lt;unused1402&gt;' is not marked as EOG\nload: control token: 262086 '&lt;unused6184&gt;' is not marked as EOG\nload: control token: 257254 '&lt;unused1352&gt;' is not marked as EOG\nload: control token: 256936 '&lt;unused1034&gt;' is not marked as EOG\nload: control token: 259136 '&lt;unused3234&gt;' is not marked as EOG\nload: control token: 257235 '&lt;unused1333&gt;' is not marked as EOG\nload: control token: 260813 '&lt;unused4911&gt;' is not marked as EOG\nload: control token: 256304 '&lt;unused402&gt;' is not marked as EOG\nload: control token: 259414 '&lt;unused3512&gt;' is not marked as EOG\nload: control token: 256317 '&lt;unused415&gt;' is not marked as EOG\nload: control token: 257578 '&lt;unused1676&gt;' is not marked as EOG\nload: control token: 256703 '&lt;unused801&gt;' is not marked as EOG\nload: control token: 259662 '&lt;unused3760&gt;' is not marked as EOG\nload: control token: 261787 '&lt;unused5885&gt;' is not marked as EOG\nload: control token: 261428 '&lt;unused5526&gt;' is not marked as EOG\nload: control token: 259961 '&lt;unused4059&gt;' is not marked as EOG\nload: control token: 260740 '&lt;unused4838&gt;' is not marked as EOG\nload: control token: 261952 '&lt;unused6050&gt;' is not marked as EOG\nload: control token: 259613 '&lt;unused3711&gt;' is not marked as EOG\nload: control token: 261591 '&lt;unused5689&gt;' is not marked as EOG\nload: control token: 259731 '&lt;unused3829&gt;' is not marked as EOG\nload: control token: 260697 '&lt;unused4795&gt;' is not marked as EOG\nload: control token: 256373 '&lt;unused471&gt;' is not marked as EOG\nload: control token: 257995 '&lt;unused2093&gt;' is not marked as EOG\nload: control token: 258493 '&lt;unused2591&gt;' is not marked as EOG\nload: control token: 259138 '&lt;unused3236&gt;' is not marked as EOG\nload: control token: 261947 '&lt;unused6045&gt;' is not marked as EOG\nload: control token: 261816 '&lt;unused5914&gt;' is not marked as EOG\nload: control token: 256608 '&lt;unused706&gt;' is not marked as EOG\nload: control token: 257473 '&lt;unused1571&gt;' is not marked as EOG\nload: control token: 256018 '&lt;unused116&gt;' is not marked as EOG\nload: control token: 261379 '&lt;unused5477&gt;' is not marked as EOG\nload: control token: 260173 '&lt;unused4271&gt;' is not marked as EOG\nload: control token: 261072 '&lt;unused5170&gt;' is not marked as EOG\nload: control token: 262049 '&lt;unused6147&gt;' is not marked as EOG\nload: control token: 258722 '&lt;unused2820&gt;' is not marked as EOG\nload: control token: 258008 '&lt;unused2106&gt;' is not marked as EOG\nload: control token: 261694 '&lt;unused5792&gt;' is not marked as EOG\nload: control token: 256571 '&lt;unused669&gt;' is not marked as EOG\nload: control token: 258453 '&lt;unused2551&gt;' is not marked as EOG\nload: control token: 258761 '&lt;unused2859&gt;' is not marked as EOG\nload: control token: 260712 '&lt;unused4810&gt;' is not marked as EOG\nload: control token: 256759 '&lt;unused857&gt;' is not marked as EOG\nload: control token: 259598 '&lt;unused3696&gt;' is not marked as EOG\nload: control token: 259391 '&lt;unused3489&gt;' is not marked as EOG\nload: control token: 257661 '&lt;unused1759&gt;' is not marked as EOG\nload: control token: 256673 '&lt;unused771&gt;' is not marked as EOG\nload: control token: 256536 '&lt;unused634&gt;' is not marked as EOG\nload: control token: 258962 '&lt;unused3060&gt;' is not marked as EOG\nload: control token: 256623 '&lt;unused721&gt;' is not marked as EOG\nload: control token: 261651 '&lt;unused5749&gt;' is not marked as EOG\nload: control token: 259737 '&lt;unused3835&gt;' is not marked as EOG\nload: control token: 258672 '&lt;unused2770&gt;' is not marked as EOG\nload: control token: 261569 '&lt;unused5667&gt;' is not marked as EOG\nload: control token: 260499 '&lt;unused4597&gt;' is not marked as EOG\nload: control token: 261002 '&lt;unused5100&gt;' is not marked as EOG\nload: control token: 261361 '&lt;unused5459&gt;' is not marked as EOG\nload: control token: 256355 '&lt;unused453&gt;' is not marked as EOG\nload: control token: 256320 '&lt;unused418&gt;' is not marked as EOG\nload: control token: 260033 '&lt;unused4131&gt;' is not marked as EOG\nload: control token: 260663 '&lt;unused4761&gt;' is not marked as EOG\nload: control token: 261517 '&lt;unused5615&gt;' is not marked as EOG\nload: control token: 258575 '&lt;unused2673&gt;' is not marked as EOG\nload: control token: 262057 '&lt;unused6155&gt;' is not marked as EOG\nload: control token: 257194 '&lt;unused1292&gt;' is not marked as EOG\nload: control token: 260255 '&lt;unused4353&gt;' is not marked as EOG\nload: control token: 259688 '&lt;unused3786&gt;' is not marked as EOG\nload: control token: 261319 '&lt;unused5417&gt;' is not marked as EOG\nload: control token: 261340 '&lt;unused5438&gt;' is not marked as EOG\nload: control token: 258966 '&lt;unused3064&gt;' is not marked as EOG\nload: control token: 258061 '&lt;unused2159&gt;' is not marked as EOG\nload: control token: 256782 '&lt;unused880&gt;' is not marked as EOG\nload: control token: 259870 '&lt;unused3968&gt;' is not marked as EOG\nload: control token: 256694 '&lt;unused792&gt;' is not marked as EOG\nload: control token: 260695 '&lt;unused4793&gt;' is not marked as EOG\nload: control token: 259725 '&lt;unused3823&gt;' is not marked as EOG\nload: control token: 259921 '&lt;unused4019&gt;' is not marked as EOG\nload: control token: 258450 '&lt;unused2548&gt;' is not marked as EOG\nload: control token: 257405 '&lt;unused1503&gt;' is not marked as EOG\nload: control token: 260438 '&lt;unused4536&gt;' is not marked as EOG\nload: control token: 257616 '&lt;unused1714&gt;' is not marked as EOG\nload: control token: 259246 '&lt;unused3344&gt;' is not marked as EOG\nload: control token: 261652 '&lt;unused5750&gt;' is not marked as EOG\nload: control token: 260576 '&lt;unused4674&gt;' is not marked as EOG\nload: control token: 261158 '&lt;unused5256&gt;' is not marked as EOG\nload: control token: 258176 '&lt;unused2274&gt;' is not marked as EOG\nload: control token: 261009 '&lt;unused5107&gt;' is not marked as EOG\nload: control token: 259244 '&lt;unused3342&gt;' is not marked as EOG\nload: control token: 262073 '&lt;unused6171&gt;' is not marked as EOG\nload: control token: 256518 '&lt;unused616&gt;' is not marked as EOG\nload: control token: 256293 '&lt;unused391&gt;' is not marked as EOG\nload: control token: 256960 '&lt;unused1058&gt;' is not marked as EOG\nload: control token: 261867 '&lt;unused5965&gt;' is not marked as EOG\nload: control token: 256131 '&lt;unused229&gt;' is not marked as EOG\nload: control token: 260930 '&lt;unused5028&gt;' is not marked as EOG\nload: control token: 261669 '&lt;unused5767&gt;' is not marked as EOG\nload: control token: 261898 '&lt;unused5996&gt;' is not marked as EOG\nload: control token: 258044 '&lt;unused2142&gt;' is not marked as EOG\nload: control token: 259067 '&lt;unused3165&gt;' is not marked as EOG\nload: control token: 259034 '&lt;unused3132&gt;' is not marked as EOG\nload: control token: 258840 '&lt;unused2938&gt;' is not marked as EOG\nload: control token: 257900 '&lt;unused1998&gt;' is not marked as EOG\nload: control token: 256344 '&lt;unused442&gt;' is not marked as EOG\nload: control token: 261457 '&lt;unused5555&gt;' is not marked as EOG\nload: control token: 256854 '&lt;unused952&gt;' is not marked as EOG\nload: control token: 256285 '&lt;unused383&gt;' is not marked as EOG\nload: control token: 256316 '&lt;unused414&gt;' is not marked as EOG\nload: control token: 256695 '&lt;unused793&gt;' is not marked as EOG\nload: control token: 261789 '&lt;unused5887&gt;' is not marked as EOG\nload: control token: 258744 '&lt;unused2842&gt;' is not marked as EOG\nload: control token: 258043 '&lt;unused2141&gt;' is not marked as EOG\nload: control token: 259678 '&lt;unused3776&gt;' is not marked as EOG\nload: control token: 257198 '&lt;unused1296&gt;' is not marked as EOG\nload: control token: 260827 '&lt;unused4925&gt;' is not marked as EOG\nload: control token: 256773 '&lt;unused871&gt;' is not marked as EOG\nload: control token: 256567 '&lt;unused665&gt;' is not marked as EOG\nload: control token: 260048 '&lt;unused4146&gt;' is not marked as EOG\nload: control token: 259142 '&lt;unused3240&gt;' is not marked as EOG\nload: control token: 257665 '&lt;unused1763&gt;' is not marked as EOG\nload: control token: 256856 '&lt;unused954&gt;' is not marked as EOG\nload: control token:     91 '&lt;unused85&gt;' is not marked as EOG\nload: control token: 256098 '&lt;unused196&gt;' is not marked as EOG\nload: control token: 257863 '&lt;unused1961&gt;' is not marked as EOG\nload: control token: 259562 '&lt;unused3660&gt;' is not marked as EOG\nload: control token: 258770 '&lt;unused2868&gt;' is not marked as EOG\nload: control token: 261365 '&lt;unused5463&gt;' is not marked as EOG\nload: control token: 260238 '&lt;unused4336&gt;' is not marked as EOG\nload: control token: 256214 '&lt;unused312&gt;' is not marked as EOG\nload: control token: 260745 '&lt;unused4843&gt;' is not marked as EOG\nload: control token: 258827 '&lt;unused2925&gt;' is not marked as EOG\nload: control token: 257843 '&lt;unused1941&gt;' is not marked as EOG\nload: control token: 257610 '&lt;unused1708&gt;' is not marked as EOG\nload: control token: 261599 '&lt;unused5697&gt;' is not marked as EOG\nload: control token: 261110 '&lt;unused5208&gt;' is not marked as EOG\nload: control token: 260812 '&lt;unused4910&gt;' is not marked as EOG\nload: control token: 258334 '&lt;unused2432&gt;' is not marked as EOG\nload: control token: 256812 '&lt;unused910&gt;' is not marked as EOG\nload: control token: 257066 '&lt;unused1164&gt;' is not marked as EOG\nload: control token: 260317 '&lt;unused4415&gt;' is not marked as EOG\nload: control token: 257655 '&lt;unused1753&gt;' is not marked as EOG\nload: control token:     81 '&lt;unused75&gt;' is not marked as EOG\nload: control token: 261258 '&lt;unused5356&gt;' is not marked as EOG\nload: control token: 260633 '&lt;unused4731&gt;' is not marked as EOG\nload: control token: 260885 '&lt;unused4983&gt;' is not marked as EOG\nload: control token: 257331 '&lt;unused1429&gt;' is not marked as EOG\nload: control token: 261768 '&lt;unused5866&gt;' is not marked as EOG\nload: control token: 259770 '&lt;unused3868&gt;' is not marked as EOG\nload: control token: 261589 '&lt;unused5687&gt;' is not marked as EOG\nload: control token: 256808 '&lt;unused906&gt;' is not marked as EOG\nload: control token: 260982 '&lt;unused5080&gt;' is not marked as EOG\nload: control token: 261214 '&lt;unused5312&gt;' is not marked as EOG\nload: control token: 260960 '&lt;unused5058&gt;' is not marked as EOG\nload: control token: 256901 '&lt;unused999&gt;' is not marked as EOG\nload: control token: 257862 '&lt;unused1960&gt;' is not marked as EOG\nload: control token: 259982 '&lt;unused4080&gt;' is not marked as EOG\nload: control token: 261911 '&lt;unused6009&gt;' is not marked as EOG\nload: control token: 259422 '&lt;unused3520&gt;' is not marked as EOG\nload: control token: 259103 '&lt;unused3201&gt;' is not marked as EOG\nload: control token: 258712 '&lt;unused2810&gt;' is not marked as EOG\nload: control token: 258203 '&lt;unused2301&gt;' is not marked as EOG\nload: control token: 258887 '&lt;unused2985&gt;' is not marked as EOG\nload: control token: 258883 '&lt;unused2981&gt;' is not marked as EOG\nload: control token: 256735 '&lt;unused833&gt;' is not marked as EOG\nload: control token: 260397 '&lt;unused4495&gt;' is not marked as EOG\nload: control token: 261684 '&lt;unused5782&gt;' is not marked as EOG\nload: control token: 259892 '&lt;unused3990&gt;' is not marked as EOG\nload: control token: 260860 '&lt;unused4958&gt;' is not marked as EOG\nload: control token: 261362 '&lt;unused5460&gt;' is not marked as EOG\nload: control token: 257025 '&lt;unused1123&gt;' is not marked as EOG\nload: control token: 258319 '&lt;unused2417&gt;' is not marked as EOG\nload: control token: 257109 '&lt;unused1207&gt;' is not marked as EOG\nload: control token: 261526 '&lt;unused5624&gt;' is not marked as EOG\nload: control token: 256340 '&lt;unused438&gt;' is not marked as EOG\nload: control token: 258930 '&lt;unused3028&gt;' is not marked as EOG\nload: control token: 260379 '&lt;unused4477&gt;' is not marked as EOG\nload: control token: 259115 '&lt;unused3213&gt;' is not marked as EOG\nload: control token: 257751 '&lt;unused1849&gt;' is not marked as EOG\nload: control token: 256660 '&lt;unused758&gt;' is not marked as EOG\nload: control token: 258953 '&lt;unused3051&gt;' is not marked as EOG\nload: control token: 258326 '&lt;unused2424&gt;' is not marked as EOG\nload: control token: 262047 '&lt;unused6145&gt;' is not marked as EOG\nload: control token: 261296 '&lt;unused5394&gt;' is not marked as EOG\nload: control token: 260456 '&lt;unused4554&gt;' is not marked as EOG\nload: control token: 256918 '&lt;unused1016&gt;' is not marked as EOG\nload: control token: 259522 '&lt;unused3620&gt;' is not marked as EOG\nload: control token: 261215 '&lt;unused5313&gt;' is not marked as EOG\nload: control token: 258454 '&lt;unused2552&gt;' is not marked as EOG\nload: control token: 256946 '&lt;unused1044&gt;' is not marked as EOG\nload: control token: 260684 '&lt;unused4782&gt;' is not marked as EOG\nload: control token: 258693 '&lt;unused2791&gt;' is not marked as EOG\nload: control token: 256875 '&lt;unused973&gt;' is not marked as EOG\nload: control token: 260371 '&lt;unused4469&gt;' is not marked as EOG\nload: control token: 260652 '&lt;unused4750&gt;' is not marked as EOG\nload: control token: 256029 '&lt;unused127&gt;' is not marked as EOG\nload: control token: 261969 '&lt;unused6067&gt;' is not marked as EOG\nload: control token: 260417 '&lt;unused4515&gt;' is not marked as EOG\nload: control token: 259187 '&lt;unused3285&gt;' is not marked as EOG\nload: control token: 260189 '&lt;unused4287&gt;' is not marked as EOG\nload: control token: 257513 '&lt;unused1611&gt;' is not marked as EOG\nload: control token: 257780 '&lt;unused1878&gt;' is not marked as EOG\nload: control token: 257100 '&lt;unused1198&gt;' is not marked as EOG\nload: control token: 261663 '&lt;unused5761&gt;' is not marked as EOG\nload: control token: 261395 '&lt;unused5493&gt;' is not marked as EOG\nload: control token: 259790 '&lt;unused3888&gt;' is not marked as EOG\nload: control token: 257620 '&lt;unused1718&gt;' is not marked as EOG\nload: control token: 260059 '&lt;unused4157&gt;' is not marked as EOG\nload: control token: 261744 '&lt;unused5842&gt;' is not marked as EOG\nload: control token: 261724 '&lt;unused5822&gt;' is not marked as EOG\nload: control token: 259862 '&lt;unused3960&gt;' is not marked as EOG\nload: control token: 260315 '&lt;unused4413&gt;' is not marked as EOG\nload: control token: 260805 '&lt;unused4903&gt;' is not marked as EOG\nload: control token: 261871 '&lt;unused5969&gt;' is not marked as EOG\nload: control token: 261536 '&lt;unused5634&gt;' is not marked as EOG\nload: control token: 260610 '&lt;unused4708&gt;' is not marked as EOG\nload: control token: 260545 '&lt;unused4643&gt;' is not marked as EOG\nload: control token: 256095 '&lt;unused193&gt;' is not marked as EOG\nload: control token: 262028 '&lt;unused6126&gt;' is not marked as EOG\nload: control token: 261778 '&lt;unused5876&gt;' is not marked as EOG\nload: control token: 256178 '&lt;unused276&gt;' is not marked as EOG\nload: control token: 258612 '&lt;unused2710&gt;' is not marked as EOG\nload: control token: 259514 '&lt;unused3612&gt;' is not marked as EOG\nload: control token: 260280 '&lt;unused4378&gt;' is not marked as EOG\nload: control token: 261178 '&lt;unused5276&gt;' is not marked as EOG\nload: control token: 258622 '&lt;unused2720&gt;' is not marked as EOG\nload: control token: 258153 '&lt;unused2251&gt;' is not marked as EOG\nload: control token: 256397 '&lt;unused495&gt;' is not marked as EOG\nload: control token: 257364 '&lt;unused1462&gt;' is not marked as EOG\nload: control token: 258731 '&lt;unused2829&gt;' is not marked as EOG\nload: control token: 259478 '&lt;unused3576&gt;' is not marked as EOG\nload: control token: 260722 '&lt;unused4820&gt;' is not marked as EOG\nload: control token: 260148 '&lt;unused4246&gt;' is not marked as EOG\nload: control token: 257769 '&lt;unused1867&gt;' is not marked as EOG\nload: control token: 256507 '&lt;unused605&gt;' is not marked as EOG\nload: control token: 261010 '&lt;unused5108&gt;' is not marked as EOG\nload: control token: 260711 '&lt;unused4809&gt;' is not marked as EOG\nload: control token: 260193 '&lt;unused4291&gt;' is not marked as EOG\nload: control token: 257306 '&lt;unused1404&gt;' is not marked as EOG\nload: control token: 261014 '&lt;unused5112&gt;' is not marked as EOG\nload: control token: 261311 '&lt;unused5409&gt;' is not marked as EOG\nload: control token: 259012 '&lt;unused3110&gt;' is not marked as EOG\nload: control token: 256504 '&lt;unused602&gt;' is not marked as EOG\nload: control token: 260948 '&lt;unused5046&gt;' is not marked as EOG\nload: control token: 257915 '&lt;unused2013&gt;' is not marked as EOG\nload: control token: 257489 '&lt;unused1587&gt;' is not marked as EOG\nload: control token: 259079 '&lt;unused3177&gt;' is not marked as EOG\nload: control token: 258917 '&lt;unused3015&gt;' is not marked as EOG\nload: control token: 257396 '&lt;unused1494&gt;' is not marked as EOG\nload: control token: 261298 '&lt;unused5396&gt;' is not marked as EOG\nload: control token: 257564 '&lt;unused1662&gt;' is not marked as EOG\nload: control token: 261312 '&lt;unused5410&gt;' is not marked as EOG\nload: control token: 259410 '&lt;unused3508&gt;' is not marked as EOG\nload: control token: 261434 '&lt;unused5532&gt;' is not marked as EOG\nload: control token: 260877 '&lt;unused4975&gt;' is not marked as EOG\nload: control token: 259279 '&lt;unused3377&gt;' is not marked as EOG\nload: control token: 257877 '&lt;unused1975&gt;' is not marked as EOG\nload: control token: 259977 '&lt;unused4075&gt;' is not marked as EOG\nload: control token: 258236 '&lt;unused2334&gt;' is not marked as EOG\nload: control token: 256020 '&lt;unused118&gt;' is not marked as EOG\nload: control token: 259425 '&lt;unused3523&gt;' is not marked as EOG\nload: control token: 257530 '&lt;unused1628&gt;' is not marked as EOG\nload: control token: 258134 '&lt;unused2232&gt;' is not marked as EOG\nload: control token: 260240 '&lt;unused4338&gt;' is not marked as EOG\nload: control token: 259767 '&lt;unused3865&gt;' is not marked as EOG\nload: control token:     70 '&lt;unused64&gt;' is not marked as EOG\nload: control token: 260659 '&lt;unused4757&gt;' is not marked as EOG\nload: control token: 261672 '&lt;unused5770&gt;' is not marked as EOG\nload: control token: 257692 '&lt;unused1790&gt;' is not marked as EOG\nload: control token: 256096 '&lt;unused194&gt;' is not marked as EOG\nload: control token: 259140 '&lt;unused3238&gt;' is not marked as EOG\nload: control token: 259728 '&lt;unused3826&gt;' is not marked as EOG\nload: control token: 259175 '&lt;unused3273&gt;' is not marked as EOG\nload: control token: 260352 '&lt;unused4450&gt;' is not marked as EOG\nload: control token: 260154 '&lt;unused4252&gt;' is not marked as EOG\nload: control token: 257920 '&lt;unused2018&gt;' is not marked as EOG\nload: control token: 258117 '&lt;unused2215&gt;' is not marked as EOG\nload: control token: 258898 '&lt;unused2996&gt;' is not marked as EOG\nload: control token: 256455 '&lt;unused553&gt;' is not marked as EOG\nload: control token: 260269 '&lt;unused4367&gt;' is not marked as EOG\nload: control token: 260750 '&lt;unused4848&gt;' is not marked as EOG\nload: control token: 259442 '&lt;unused3540&gt;' is not marked as EOG\nload: control token: 256996 '&lt;unused1094&gt;' is not marked as EOG\nload: control token: 258317 '&lt;unused2415&gt;' is not marked as EOG\nload: control token: 260755 '&lt;unused4853&gt;' is not marked as EOG\nload: control token: 256460 '&lt;unused558&gt;' is not marked as EOG\nload: control token: 261373 '&lt;unused5471&gt;' is not marked as EOG\nload: control token: 259254 '&lt;unused3352&gt;' is not marked as EOG\nload: control token: 258375 '&lt;unused2473&gt;' is not marked as EOG\nload: control token: 256450 '&lt;unused548&gt;' is not marked as EOG\nload: control token: 260552 '&lt;unused4650&gt;' is not marked as EOG\nload: control token: 256553 '&lt;unused651&gt;' is not marked as EOG\nload: control token: 257695 '&lt;unused1793&gt;' is not marked as EOG\nload: control token: 257584 '&lt;unused1682&gt;' is not marked as EOG\nload: control token: 260149 '&lt;unused4247&gt;' is not marked as EOG\nload: control token: 260509 '&lt;unused4607&gt;' is not marked as EOG\nload: control token: 260027 '&lt;unused4125&gt;' is not marked as EOG\nload: control token: 258314 '&lt;unused2412&gt;' is not marked as EOG\nload: control token: 257986 '&lt;unused2084&gt;' is not marked as EOG\nload: control token: 260541 '&lt;unused4639&gt;' is not marked as EOG\nload: control token: 256391 '&lt;unused489&gt;' is not marked as EOG\nload: control token: 259569 '&lt;unused3667&gt;' is not marked as EOG\nload: control token: 256715 '&lt;unused813&gt;' is not marked as EOG\nload: control token: 257999 '&lt;unused2097&gt;' is not marked as EOG\nload: control token: 261559 '&lt;unused5657&gt;' is not marked as EOG\nload: control token: 262045 '&lt;unused6143&gt;' is not marked as EOG\nload: control token: 260592 '&lt;unused4690&gt;' is not marked as EOG\nload: control token: 261543 '&lt;unused5641&gt;' is not marked as EOG\nload: control token: 260223 '&lt;unused4321&gt;' is not marked as EOG\nload: control token: 256217 '&lt;unused315&gt;' is not marked as EOG\nload: control token: 259631 '&lt;unused3729&gt;' is not marked as EOG\nload: control token: 258279 '&lt;unused2377&gt;' is not marked as EOG\nload: control token: 257688 '&lt;unused1786&gt;' is not marked as EOG\nload: control token: 262018 '&lt;unused6116&gt;' is not marked as EOG\nload: control token: 256678 '&lt;unused776&gt;' is not marked as EOG\nload: control token: 259747 '&lt;unused3845&gt;' is not marked as EOG\nload: control token: 258235 '&lt;unused2333&gt;' is not marked as EOG\nload: control token: 259570 '&lt;unused3668&gt;' is not marked as EOG\nload: control token: 257327 '&lt;unused1425&gt;' is not marked as EOG\nload: control token: 258470 '&lt;unused2568&gt;' is not marked as EOG\nload: control token: 257192 '&lt;unused1290&gt;' is not marked as EOG\nload: control token: 260991 '&lt;unused5089&gt;' is not marked as EOG\nload: control token: 256870 '&lt;unused968&gt;' is not marked as EOG\nload: control token: 258264 '&lt;unused2362&gt;' is not marked as EOG\nload: control token: 258834 '&lt;unused2932&gt;' is not marked as EOG\nload: control token: 256075 '&lt;unused173&gt;' is not marked as EOG\nload: control token: 256015 '&lt;unused113&gt;' is not marked as EOG\nload: control token: 257003 '&lt;unused1101&gt;' is not marked as EOG\nload: control token: 256389 '&lt;unused487&gt;' is not marked as EOG\nload: control token: 259272 '&lt;unused3370&gt;' is not marked as EOG\nload: control token: 261620 '&lt;unused5718&gt;' is not marked as EOG\nload: control token: 259932 '&lt;unused4030&gt;' is not marked as EOG\nload: control token: 259149 '&lt;unused3247&gt;' is not marked as EOG\nload: control token: 259960 '&lt;unused4058&gt;' is not marked as EOG\nload: control token: 259590 '&lt;unused3688&gt;' is not marked as EOG\nload: control token: 258890 '&lt;unused2988&gt;' is not marked as EOG\nload: control token: 261098 '&lt;unused5196&gt;' is not marked as EOG\nload: control token: 258664 '&lt;unused2762&gt;' is not marked as EOG\nload: control token: 261191 '&lt;unused5289&gt;' is not marked as EOG\nload: control token: 258509 '&lt;unused2607&gt;' is not marked as EOG\nload: control token: 257457 '&lt;unused1555&gt;' is not marked as EOG\nload: control token: 257054 '&lt;unused1152&gt;' is not marked as EOG\nload: control token: 256282 '&lt;unused380&gt;' is not marked as EOG\nload: control token: 260308 '&lt;unused4406&gt;' is not marked as EOG\nload: control token: 259118 '&lt;unused3216&gt;' is not marked as EOG\nload: control token: 257721 '&lt;unused1819&gt;' is not marked as EOG\nload: control token: 260518 '&lt;unused4616&gt;' is not marked as EOG\nload: control token: 259472 '&lt;unused3570&gt;' is not marked as EOG\nload: control token: 257218 '&lt;unused1316&gt;' is not marked as EOG\nload: control token: 260664 '&lt;unused4762&gt;' is not marked as EOG\nload: control token: 260916 '&lt;unused5014&gt;' is not marked as EOG\nload: control token:     25 '&lt;unused19&gt;' is not marked as EOG\nload: control token: 260971 '&lt;unused5069&gt;' is not marked as EOG\nload: control token: 258019 '&lt;unused2117&gt;' is not marked as EOG\nload: control token: 260345 '&lt;unused4443&gt;' is not marked as EOG\nload: control token: 259400 '&lt;unused3498&gt;' is not marked as EOG\nload: control token: 259920 '&lt;unused4018&gt;' is not marked as EOG\nload: control token: 258937 '&lt;unused3035&gt;' is not marked as EOG\nload: control token: 258252 '&lt;unused2350&gt;' is not marked as EOG\nload: control token: 259243 '&lt;unused3341&gt;' is not marked as EOG\nload: control token: 258007 '&lt;unused2105&gt;' is not marked as EOG\nload: control token: 256088 '&lt;unused186&gt;' is not marked as EOG\nload: control token: 259339 '&lt;unused3437&gt;' is not marked as EOG\nload: control token: 259026 '&lt;unused3124&gt;' is not marked as EOG\nload: control token: 260421 '&lt;unused4519&gt;' is not marked as EOG\nload: control token: 258412 '&lt;unused2510&gt;' is not marked as EOG\nload: control token: 260704 '&lt;unused4802&gt;' is not marked as EOG\nload: control token: 256839 '&lt;unused937&gt;' is not marked as EOG\nload: control token: 258944 '&lt;unused3042&gt;' is not marked as EOG\nload: control token: 257523 '&lt;unused1621&gt;' is not marked as EOG\nload: control token: 260344 '&lt;unused4442&gt;' is not marked as EOG\nload: control token: 259801 '&lt;unused3899&gt;' is not marked as EOG\nload: control token: 257261 '&lt;unused1359&gt;' is not marked as EOG\nload: control token: 262000 '&lt;unused6098&gt;' is not marked as EOG\nload: control token: 258943 '&lt;unused3041&gt;' is not marked as EOG\nload: control token: 257428 '&lt;unused1526&gt;' is not marked as EOG\nload: control token: 260142 '&lt;unused4240&gt;' is not marked as EOG\nload: control token: 257588 '&lt;unused1686&gt;' is not marked as EOG\nload: control token: 257633 '&lt;unused1731&gt;' is not marked as EOG\nload: control token: 256475 '&lt;unused573&gt;' is not marked as EOG\nload: control token: 261650 '&lt;unused5748&gt;' is not marked as EOG\nload: control token: 260967 '&lt;unused5065&gt;' is not marked as EOG\nload: control token: 256670 '&lt;unused768&gt;' is not marked as EOG\nload: control token: 259908 '&lt;unused4006&gt;' is not marked as EOG\nload: control token: 260334 '&lt;unused4432&gt;' is not marked as EOG\nload: control token: 256806 '&lt;unused904&gt;' is not marked as EOG\nload: control token: 256351 '&lt;unused449&gt;' is not marked as EOG\nload: control token: 260756 '&lt;unused4854&gt;' is not marked as EOG\nload: control token: 256488 '&lt;unused586&gt;' is not marked as EOG\nload: control token: 259525 '&lt;unused3623&gt;' is not marked as EOG\nload: control token: 258511 '&lt;unused2609&gt;' is not marked as EOG\nload: control token: 258859 '&lt;unused2957&gt;' is not marked as EOG\nload: control token: 259220 '&lt;unused3318&gt;' is not marked as EOG\nload: control token: 257840 '&lt;unused1938&gt;' is not marked as EOG\nload: control token: 261165 '&lt;unused5263&gt;' is not marked as EOG\nload: control token: 258409 '&lt;unused2507&gt;' is not marked as EOG\nload: control token: 260907 '&lt;unused5005&gt;' is not marked as EOG\nload: control token: 258092 '&lt;unused2190&gt;' is not marked as EOG\nload: control token: 259849 '&lt;unused3947&gt;' is not marked as EOG\nload: control token: 258383 '&lt;unused2481&gt;' is not marked as EOG\nload: control token: 258608 '&lt;unused2706&gt;' is not marked as EOG\nload: control token: 258223 '&lt;unused2321&gt;' is not marked as EOG\nload: control token: 257988 '&lt;unused2086&gt;' is not marked as EOG\nload: control token: 257341 '&lt;unused1439&gt;' is not marked as EOG\nload: control token: 256240 '&lt;unused338&gt;' is not marked as EOG\nload: control token: 261749 '&lt;unused5847&gt;' is not marked as EOG\nload: control token: 257822 '&lt;unused1920&gt;' is not marked as EOG\nload: control token: 259465 '&lt;unused3563&gt;' is not marked as EOG\nload: control token: 258167 '&lt;unused2265&gt;' is not marked as EOG\nload: control token: 257005 '&lt;unused1103&gt;' is not marked as EOG\nload: control token: 257106 '&lt;unused1204&gt;' is not marked as EOG\nload: control token: 257599 '&lt;unused1697&gt;' is not marked as EOG\nload: control token: 260358 '&lt;unused4456&gt;' is not marked as EOG\nload: control token: 256616 '&lt;unused714&gt;' is not marked as EOG\nload: control token: 256106 '&lt;unused204&gt;' is not marked as EOG\nload: control token: 258954 '&lt;unused3052&gt;' is not marked as EOG\nload: control token: 260609 '&lt;unused4707&gt;' is not marked as EOG\nload: control token: 257472 '&lt;unused1570&gt;' is not marked as EOG\nload: control token: 256448 '&lt;unused546&gt;' is not marked as EOG\nload: control token: 258855 '&lt;unused2953&gt;' is not marked as EOG\nload: control token: 259356 '&lt;unused3454&gt;' is not marked as EOG\nload: control token: 259521 '&lt;unused3619&gt;' is not marked as EOG\nload: control token: 262088 '&lt;unused6186&gt;' is not marked as EOG\nload: control token: 261243 '&lt;unused5341&gt;' is not marked as EOG\nload: control token: 258844 '&lt;unused2942&gt;' is not marked as EOG\nload: control token: 259447 '&lt;unused3545&gt;' is not marked as EOG\nload: control token: 257440 '&lt;unused1538&gt;' is not marked as EOG\nload: control token: 261547 '&lt;unused5645&gt;' is not marked as EOG\nload: control token: 258699 '&lt;unused2797&gt;' is not marked as EOG\nload: control token: 257318 '&lt;unused1416&gt;' is not marked as EOG\nload: control token: 256644 '&lt;unused742&gt;' is not marked as EOG\nload: control token: 256886 '&lt;unused984&gt;' is not marked as EOG\nload: control token: 257894 '&lt;unused1992&gt;' is not marked as EOG\nload: control token: 260700 '&lt;unused4798&gt;' is not marked as EOG\nload: control token: 256218 '&lt;unused316&gt;' is not marked as EOG\nload: control token: 262056 '&lt;unused6154&gt;' is not marked as EOG\nload: control token: 258225 '&lt;unused2323&gt;' is not marked as EOG\nload: control token: 261531 '&lt;unused5629&gt;' is not marked as EOG\nload: control token: 258897 '&lt;unused2995&gt;' is not marked as EOG\nload: control token: 260478 '&lt;unused4576&gt;' is not marked as EOG\nload: control token: 258056 '&lt;unused2154&gt;' is not marked as EOG\nload: control token: 261746 '&lt;unused5844&gt;' is not marked as EOG\nload: control token: 256756 '&lt;unused854&gt;' is not marked as EOG\nload: control token:     20 '&lt;unused14&gt;' is not marked as EOG\nload: control token: 256251 '&lt;unused349&gt;' is not marked as EOG\nload: control token: 259060 '&lt;unused3158&gt;' is not marked as EOG\nload: control token: 258282 '&lt;unused2380&gt;' is not marked as EOG\nload: control token: 258776 '&lt;unused2874&gt;' is not marked as EOG\nload: control token: 260702 '&lt;unused4800&gt;' is not marked as EOG\nload: control token: 260952 '&lt;unused5050&gt;' is not marked as EOG\nload: control token: 260114 '&lt;unused4212&gt;' is not marked as EOG\nload: control token: 261216 '&lt;unused5314&gt;' is not marked as EOG\nload: control token: 261344 '&lt;unused5442&gt;' is not marked as EOG\nload: control token: 259912 '&lt;unused4010&gt;' is not marked as EOG\nload: control token: 257314 '&lt;unused1412&gt;' is not marked as EOG\nload: control token: 257790 '&lt;unused1888&gt;' is not marked as EOG\nload: control token: 258018 '&lt;unused2116&gt;' is not marked as EOG\nload: control token: 260613 '&lt;unused4711&gt;' is not marked as EOG\nload: control token: 258913 '&lt;unused3011&gt;' is not marked as EOG\nload: control token: 256032 '&lt;unused130&gt;' is not marked as EOG\nload: control token: 256614 '&lt;unused712&gt;' is not marked as EOG\nload: control token: 258868 '&lt;unused2966&gt;' is not marked as EOG\nload: control token: 259288 '&lt;unused3386&gt;' is not marked as EOG\nload: control token: 257453 '&lt;unused1551&gt;' is not marked as EOG\nload: control token: 256252 '&lt;unused350&gt;' is not marked as EOG\nload: control token: 260354 '&lt;unused4452&gt;' is not marked as EOG\nload: control token: 257700 '&lt;unused1798&gt;' is not marked as EOG\nload: control token: 257601 '&lt;unused1699&gt;' is not marked as EOG\nload: control token: 256046 '&lt;unused144&gt;' is not marked as EOG\nload: control token: 259185 '&lt;unused3283&gt;' is not marked as EOG\nload: control token: 258431 '&lt;unused2529&gt;' is not marked as EOG\nload: control token: 259649 '&lt;unused3747&gt;' is not marked as EOG\nload: control token: 256706 '&lt;unused804&gt;' is not marked as EOG\nload: control token: 259663 '&lt;unused3761&gt;' is not marked as EOG\nload: control token: 256470 '&lt;unused568&gt;' is not marked as EOG\nload: control token: 258402 '&lt;unused2500&gt;' is not marked as EOG\nload: control token: 256126 '&lt;unused224&gt;' is not marked as EOG\nload: control token: 257773 '&lt;unused1871&gt;' is not marked as EOG\nload: control token: 256826 '&lt;unused924&gt;' is not marked as EOG\nload: control token: 259340 '&lt;unused3438&gt;' is not marked as EOG\nload: control token: 259074 '&lt;unused3172&gt;' is not marked as EOG\nload: control token: 258404 '&lt;unused2502&gt;' is not marked as EOG\nload: control token: 260202 '&lt;unused4300&gt;' is not marked as EOG\nload: control token: 261242 '&lt;unused5340&gt;' is not marked as EOG\nload: control token: 257332 '&lt;unused1430&gt;' is not marked as EOG\nload: control token: 260598 '&lt;unused4696&gt;' is not marked as EOG\nload: control token: 259346 '&lt;unused3444&gt;' is not marked as EOG\nload: control token: 261266 '&lt;unused5364&gt;' is not marked as EOG\nload: control token: 257509 '&lt;unused1607&gt;' is not marked as EOG\nload: control token: 256130 '&lt;unused228&gt;' is not marked as EOG\nload: control token: 259007 '&lt;unused3105&gt;' is not marked as EOG\nload: control token: 256557 '&lt;unused655&gt;' is not marked as EOG\nload: control token: 259133 '&lt;unused3231&gt;' is not marked as EOG\nload: control token: 261628 '&lt;unused5726&gt;' is not marked as EOG\nload: control token: 258046 '&lt;unused2144&gt;' is not marked as EOG\nload: control token: 256301 '&lt;unused399&gt;' is not marked as EOG\nload: control token: 258757 '&lt;unused2855&gt;' is not marked as EOG\nload: control token: 257030 '&lt;unused1128&gt;' is not marked as EOG\nload: control token: 257704 '&lt;unused1802&gt;' is not marked as EOG\nload: control token: 257462 '&lt;unused1560&gt;' is not marked as EOG\nload: control token: 258538 '&lt;unused2636&gt;' is not marked as EOG\nload: control token: 260390 '&lt;unused4488&gt;' is not marked as EOG\nload: control token: 256505 '&lt;unused603&gt;' is not marked as EOG\nload: control token: 258141 '&lt;unused2239&gt;' is not marked as EOG\nload: control token: 260876 '&lt;unused4974&gt;' is not marked as EOG\nload: control token: 259237 '&lt;unused3335&gt;' is not marked as EOG\nload: control token: 260163 '&lt;unused4261&gt;' is not marked as EOG\nload: control token: 262116 '&lt;unused6214&gt;' is not marked as EOG\nload: control token: 260539 '&lt;unused4637&gt;' is not marked as EOG\nload: control token: 257316 '&lt;unused1414&gt;' is not marked as EOG\nload: control token: 256338 '&lt;unused436&gt;' is not marked as EOG\nload: control token: 258681 '&lt;unused2779&gt;' is not marked as EOG\nload: control token: 256811 '&lt;unused909&gt;' is not marked as EOG\nload: control token: 256925 '&lt;unused1023&gt;' is not marked as EOG\nload: control token: 259147 '&lt;unused3245&gt;' is not marked as EOG\nload: control token: 257676 '&lt;unused1774&gt;' is not marked as EOG\nload: control token: 261743 '&lt;unused5841&gt;' is not marked as EOG\nload: control token: 259308 '&lt;unused3406&gt;' is not marked as EOG\nload: control token: 258432 '&lt;unused2530&gt;' is not marked as EOG\nload: control token: 256348 '&lt;unused446&gt;' is not marked as EOG\nload: control token: 258591 '&lt;unused2689&gt;' is not marked as EOG\nload: control token: 256440 '&lt;unused538&gt;' is not marked as EOG\nload: control token: 258347 '&lt;unused2445&gt;' is not marked as EOG\nload: control token: 261797 '&lt;unused5895&gt;' is not marked as EOG\nload: control token: 257619 '&lt;unused1717&gt;' is not marked as EOG\nload: control token: 257888 '&lt;unused1986&gt;' is not marked as EOG\nload: control token: 258195 '&lt;unused2293&gt;' is not marked as EOG\nload: control token: 259582 '&lt;unused3680&gt;' is not marked as EOG\nload: control token: 260483 '&lt;unused4581&gt;' is not marked as EOG\nload: control token: 256268 '&lt;unused366&gt;' is not marked as EOG\nload: control token: 256650 '&lt;unused748&gt;' is not marked as EOG\nload: control token: 259444 '&lt;unused3542&gt;' is not marked as EOG\nload: control token: 261412 '&lt;unused5510&gt;' is not marked as EOG\nload: control token: 256242 '&lt;unused340&gt;' is not marked as EOG\nload: control token: 257448 '&lt;unused1546&gt;' is not marked as EOG\nload: control token: 259314 '&lt;unused3412&gt;' is not marked as EOG\nload: control token: 257968 '&lt;unused2066&gt;' is not marked as EOG\nload: control token: 257427 '&lt;unused1525&gt;' is not marked as EOG\nload: control token: 262050 '&lt;unused6148&gt;' is not marked as EOG\nload: control token: 261891 '&lt;unused5989&gt;' is not marked as EOG\nload: control token: 256685 '&lt;unused783&gt;' is not marked as EOG\nload: control token: 256968 '&lt;unused1066&gt;' is not marked as EOG\nload: control token: 262023 '&lt;unused6121&gt;' is not marked as EOG\nload: control token: 256188 '&lt;unused286&gt;' is not marked as EOG\nload: control token: 258604 '&lt;unused2702&gt;' is not marked as EOG\nload: control token: 256890 '&lt;unused988&gt;' is not marked as EOG\nload: control token: 256042 '&lt;unused140&gt;' is not marked as EOG\nload: control token: 262012 '&lt;unused6110&gt;' is not marked as EOG\nload: control token: 259734 '&lt;unused3832&gt;' is not marked as EOG\nload: control token: 257140 '&lt;unused1238&gt;' is not marked as EOG\nload: control token: 260858 '&lt;unused4956&gt;' is not marked as EOG\nload: control token: 261423 '&lt;unused5521&gt;' is not marked as EOG\nload: control token:     22 '&lt;unused16&gt;' is not marked as EOG\nload: control token: 259397 '&lt;unused3495&gt;' is not marked as EOG\nload: control token: 258523 '&lt;unused2621&gt;' is not marked as EOG\nload: control token: 261661 '&lt;unused5759&gt;' is not marked as EOG\nload: control token: 261148 '&lt;unused5246&gt;' is not marked as EOG\nload: control token: 256135 '&lt;unused233&gt;' is not marked as EOG\nload: control token: 261941 '&lt;unused6039&gt;' is not marked as EOG\nload: control token: 258899 '&lt;unused2997&gt;' is not marked as EOG\nload: control token: 256636 '&lt;unused734&gt;' is not marked as EOG\nload: control token: 260401 '&lt;unused4499&gt;' is not marked as EOG\nload: control token: 258001 '&lt;unused2099&gt;' is not marked as EOG\nload: control token: 261223 '&lt;unused5321&gt;' is not marked as EOG\nload: control token: 260428 '&lt;unused4526&gt;' is not marked as EOG\nload: control token: 260832 '&lt;unused4930&gt;' is not marked as EOG\nload: control token: 260435 '&lt;unused4533&gt;' is not marked as EOG\nload: control token: 258990 '&lt;unused3088&gt;' is not marked as EOG\nload: control token: 260366 '&lt;unused4464&gt;' is not marked as EOG\nload: control token: 258284 '&lt;unused2382&gt;' is not marked as EOG\nload: control token: 261521 '&lt;unused5619&gt;' is not marked as EOG\nload: control token: 258972 '&lt;unused3070&gt;' is not marked as EOG\nload: control token: 259223 '&lt;unused3321&gt;' is not marked as EOG\nload: control token: 258986 '&lt;unused3084&gt;' is not marked as EOG\nload: control token: 256889 '&lt;unused987&gt;' is not marked as EOG\nload: control token: 258132 '&lt;unused2230&gt;' is not marked as EOG\nload: control token: 256664 '&lt;unused762&gt;' is not marked as EOG\nload: control token: 258233 '&lt;unused2331&gt;' is not marked as EOG\nload: control token: 260445 '&lt;unused4543&gt;' is not marked as EOG\nload: control token: 259468 '&lt;unused3566&gt;' is not marked as EOG\nload: control token: 259228 '&lt;unused3326&gt;' is not marked as EOG\nload: control token: 258309 '&lt;unused2407&gt;' is not marked as EOG\nload: control token: 261187 '&lt;unused5285&gt;' is not marked as EOG\nload: control token:     47 '&lt;unused41&gt;' is not marked as EOG\nload: control token: 260903 '&lt;unused5001&gt;' is not marked as EOG\nload: control token: 261912 '&lt;unused6010&gt;' is not marked as EOG\nload: control token: 258071 '&lt;unused2169&gt;' is not marked as EOG\nload: control token: 260780 '&lt;unused4878&gt;' is not marked as EOG\nload: control token: 258275 '&lt;unused2373&gt;' is not marked as EOG\nload: control token: 258969 '&lt;unused3067&gt;' is not marked as EOG\nload: control token: 258097 '&lt;unused2195&gt;' is not marked as EOG\nload: control token: 258171 '&lt;unused2269&gt;' is not marked as EOG\nload: control token: 257642 '&lt;unused1740&gt;' is not marked as EOG\nload: control token: 259983 '&lt;unused4081&gt;' is not marked as EOG\nload: control token: 256416 '&lt;unused514&gt;' is not marked as EOG\nload: control token: 257357 '&lt;unused1455&gt;' is not marked as EOG\nload: control token: 261108 '&lt;unused5206&gt;' is not marked as EOG\nload: control token: 260451 '&lt;unused4549&gt;' is not marked as EOG\nload: control token: 261059 '&lt;unused5157&gt;' is not marked as EOG\nload: control token: 261554 '&lt;unused5652&gt;' is not marked as EOG\nload: control token: 258939 '&lt;unused3037&gt;' is not marked as EOG\nload: control token: 261410 '&lt;unused5508&gt;' is not marked as EOG\nload: control token: 261615 '&lt;unused5713&gt;' is not marked as EOG\nload: control token: 256104 '&lt;unused202&gt;' is not marked as EOG\nload: control token: 259199 '&lt;unused3297&gt;' is not marked as EOG\nload: control token: 262061 '&lt;unused6159&gt;' is not marked as EOG\nload: control token: 258870 '&lt;unused2968&gt;' is not marked as EOG\nload: control token: 256337 '&lt;unused435&gt;' is not marked as EOG\nload: control token: 258238 '&lt;unused2336&gt;' is not marked as EOG\nload: control token: 261859 '&lt;unused5957&gt;' is not marked as EOG\nload: control token: 258598 '&lt;unused2696&gt;' is not marked as EOG\nload: control token: 259556 '&lt;unused3654&gt;' is not marked as EOG\nload: control token: 257409 '&lt;unused1507&gt;' is not marked as EOG\nload: control token: 257712 '&lt;unused1810&gt;' is not marked as EOG\nload: control token: 261519 '&lt;unused5617&gt;' is not marked as EOG\nload: control token: 257213 '&lt;unused1311&gt;' is not marked as EOG\nload: control token: 256732 '&lt;unused830&gt;' is not marked as EOG\nload: control token: 259114 '&lt;unused3212&gt;' is not marked as EOG\nload: control token: 260554 '&lt;unused4652&gt;' is not marked as EOG\nload: control token: 259626 '&lt;unused3724&gt;' is not marked as EOG\nload: control token: 259019 '&lt;unused3117&gt;' is not marked as EOG\nload: control token: 260886 '&lt;unused4984&gt;' is not marked as EOG\nload: control token: 258305 '&lt;unused2403&gt;' is not marked as EOG\nload: control token: 258766 '&lt;unused2864&gt;' is not marked as EOG\nload: control token: 261913 '&lt;unused6011&gt;' is not marked as EOG\nload: control token: 259875 '&lt;unused3973&gt;' is not marked as EOG\nload: control token: 259979 '&lt;unused4077&gt;' is not marked as EOG\nload: control token: 256542 '&lt;unused640&gt;' is not marked as EOG\nload: control token: 256702 '&lt;unused800&gt;' is not marked as EOG\nload: control token: 259377 '&lt;unused3475&gt;' is not marked as EOG\nload: control token: 260425 '&lt;unused4523&gt;' is not marked as EOG\nload: control token: 256445 '&lt;unused543&gt;' is not marked as EOG\nload: control token: 260470 '&lt;unused4568&gt;' is not marked as EOG\nload: control token: 258532 '&lt;unused2630&gt;' is not marked as EOG\nload: control token: 257385 '&lt;unused1483&gt;' is not marked as EOG\nload: control token: 257487 '&lt;unused1585&gt;' is not marked as EOG\nload: control token: 259811 '&lt;unused3909&gt;' is not marked as EOG\nload: control token: 260128 '&lt;unused4226&gt;' is not marked as EOG\nload: control token: 257607 '&lt;unused1705&gt;' is not marked as EOG\nload: control token: 257812 '&lt;unused1910&gt;' is not marked as EOG\nload: control token: 256051 '&lt;unused149&gt;' is not marked as EOG\nload: control token: 261646 '&lt;unused5744&gt;' is not marked as EOG\nload: control token: 258590 '&lt;unused2688&gt;' is not marked as EOG\nload: control token: 256620 '&lt;unused718&gt;' is not marked as EOG\nload: control token: 257015 '&lt;unused1113&gt;' is not marked as EOG\nload: control token: 259838 '&lt;unused3936&gt;' is not marked as EOG\nload: control token: 256583 '&lt;unused681&gt;' is not marked as EOG\nload: control token:     12 '&lt;unused6&gt;' is not marked as EOG\nload: control token: 256471 '&lt;unused569&gt;' is not marked as EOG\nload: control token: 258242 '&lt;unused2340&gt;' is not marked as EOG\nload: control token: 256646 '&lt;unused744&gt;' is not marked as EOG\nload: control token: 257236 '&lt;unused1334&gt;' is not marked as EOG\nload: control token: 259404 '&lt;unused3502&gt;' is not marked as EOG\nload: control token: 258654 '&lt;unused2752&gt;' is not marked as EOG\nload: control token: 256427 '&lt;unused525&gt;' is not marked as EOG\nload: control token: 257775 '&lt;unused1873&gt;' is not marked as EOG\nload: control token: 256630 '&lt;unused728&gt;' is not marked as EOG\nload: control token: 256930 '&lt;unused1028&gt;' is not marked as EOG\nload: control token: 258799 '&lt;unused2897&gt;' is not marked as EOG\nload: control token: 259040 '&lt;unused3138&gt;' is not marked as EOG\nload: control token: 260292 '&lt;unused4390&gt;' is not marked as EOG\nload: control token: 260935 '&lt;unused5033&gt;' is not marked as EOG\nload: control token: 259973 '&lt;unused4071&gt;' is not marked as EOG\nload: control token: 261942 '&lt;unused6040&gt;' is not marked as EOG\nload: control token: 257237 '&lt;unused1335&gt;' is not marked as EOG\nload: control token: 259037 '&lt;unused3135&gt;' is not marked as EOG\nload: control token: 258253 '&lt;unused2351&gt;' is not marked as EOG\nload: control token: 257944 '&lt;unused2042&gt;' is not marked as EOG\nload: control token: 257279 '&lt;unused1377&gt;' is not marked as EOG\nload: control token: 256194 '&lt;unused292&gt;' is not marked as EOG\nload: control token: 259773 '&lt;unused3871&gt;' is not marked as EOG\nload: control token: 257916 '&lt;unused2014&gt;' is not marked as EOG\nload: control token: 256596 '&lt;unused694&gt;' is not marked as EOG\nload: control token: 260940 '&lt;unused5038&gt;' is not marked as EOG\nload: control token: 257746 '&lt;unused1844&gt;' is not marked as EOG\nload: control token: 257012 '&lt;unused1110&gt;' is not marked as EOG\nload: control token: 258400 '&lt;unused2498&gt;' is not marked as EOG\nload: control token: 262081 '&lt;unused6179&gt;' is not marked as EOG\nload: control token: 257917 '&lt;unused2015&gt;' is not marked as EOG\nload: control token: 257418 '&lt;unused1516&gt;' is not marked as EOG\nload: control token: 259996 '&lt;unused4094&gt;' is not marked as EOG\nload: control token: 257163 '&lt;unused1261&gt;' is not marked as EOG\nload: control token: 256116 '&lt;unused214&gt;' is not marked as EOG\nload: control token: 258318 '&lt;unused2416&gt;' is not marked as EOG\nload: control token: 257879 '&lt;unused1977&gt;' is not marked as EOG\nload: control token: 258732 '&lt;unused2830&gt;' is not marked as EOG\nload: control token: 259456 '&lt;unused3554&gt;' is not marked as EOG\nload: control token: 260724 '&lt;unused4822&gt;' is not marked as EOG\nload: control token: 260629 '&lt;unused4727&gt;' is not marked as EOG\nload: control token: 259869 '&lt;unused3967&gt;' is not marked as EOG\nload: control token: 259741 '&lt;unused3839&gt;' is not marked as EOG\nload: control token: 259441 '&lt;unused3539&gt;' is not marked as EOG\nload: control token: 256137 '&lt;unused235&gt;' is not marked as EOG\nload: control token: 259275 '&lt;unused3373&gt;' is not marked as EOG\nload: control token: 261756 '&lt;unused5854&gt;' is not marked as EOG\nload: control token: 256155 '&lt;unused253&gt;' is not marked as EOG\nload: control token: 260359 '&lt;unused4457&gt;' is not marked as EOG\nload: control token: 259652 '&lt;unused3750&gt;' is not marked as EOG\nload: control token: 257989 '&lt;unused2087&gt;' is not marked as EOG\nload: control token: 258350 '&lt;unused2448&gt;' is not marked as EOG\nload: control token: 259406 '&lt;unused3504&gt;' is not marked as EOG\nload: control token: 260491 '&lt;unused4589&gt;' is not marked as EOG\nload: control token: 256193 '&lt;unused291&gt;' is not marked as EOG\nload: control token: 256872 '&lt;unused970&gt;' is not marked as EOG\nload: control token: 258701 '&lt;unused2799&gt;' is not marked as EOG\nload: control token: 257419 '&lt;unused1517&gt;' is not marked as EOG\nload: control token: 257045 '&lt;unused1143&gt;' is not marked as EOG\nload: control token: 256794 '&lt;unused892&gt;' is not marked as EOG\nload: control token: 260050 '&lt;unused4148&gt;' is not marked as EOG\nload: control token: 256687 '&lt;unused785&gt;' is not marked as EOG\nload: control token: 260301 '&lt;unused4399&gt;' is not marked as EOG\nload: control token: 256412 '&lt;unused510&gt;' is not marked as EOG\nload: control token: 261404 '&lt;unused5502&gt;' is not marked as EOG\nload: control token: 261270 '&lt;unused5368&gt;' is not marked as EOG\nload: control token: 257274 '&lt;unused1372&gt;' is not marked as EOG\nload: control token: 260137 '&lt;unused4235&gt;' is not marked as EOG\nload: control token: 262121 '&lt;unused6219&gt;' is not marked as EOG\nload: control token: 258530 '&lt;unused2628&gt;' is not marked as EOG\nload: control token: 258058 '&lt;unused2156&gt;' is not marked as EOG\nload: control token: 262138 '&lt;unused6236&gt;' is not marked as EOG\nload: control token: 261012 '&lt;unused5110&gt;' is not marked as EOG\nload: control token: 257892 '&lt;unused1990&gt;' is not marked as EOG\nload: control token: 260803 '&lt;unused4901&gt;' is not marked as EOG\nload: control token: 260293 '&lt;unused4391&gt;' is not marked as EOG\nload: control token: 260627 '&lt;unused4725&gt;' is not marked as EOG\nload: control token:     94 '&lt;unused88&gt;' is not marked as EOG\nload: control token: 260305 '&lt;unused4403&gt;' is not marked as EOG\nload: control token: 257290 '&lt;unused1388&gt;' is not marked as EOG\nload: control token: 257685 '&lt;unused1783&gt;' is not marked as EOG\nload: control token: 257533 '&lt;unused1631&gt;' is not marked as EOG\nload: control token: 257350 '&lt;unused1448&gt;' is not marked as EOG\nload: control token: 258743 '&lt;unused2841&gt;' is not marked as EOG\nload: control token: 260475 '&lt;unused4573&gt;' is not marked as EOG\nload: control token: 261067 '&lt;unused5165&gt;' is not marked as EOG\nload: control token: 259010 '&lt;unused3108&gt;' is not marked as EOG\nload: control token: 260837 '&lt;unused4935&gt;' is not marked as EOG\nload: control token: 259658 '&lt;unused3756&gt;' is not marked as EOG\nload: control token: 257939 '&lt;unused2037&gt;' is not marked as EOG\nload: control token: 261078 '&lt;unused5176&gt;' is not marked as EOG\nload: control token: 258586 '&lt;unused2684&gt;' is not marked as EOG\nload: control token: 257818 '&lt;unused1916&gt;' is not marked as EOG\nload: control token: 257177 '&lt;unused1275&gt;' is not marked as EOG\nload: control token: 256105 '&lt;unused203&gt;' is not marked as EOG\nload: control token: 256619 '&lt;unused717&gt;' is not marked as EOG\nload: control token: 259206 '&lt;unused3304&gt;' is not marked as EOG\nload: control token: 260121 '&lt;unused4219&gt;' is not marked as EOG\nload: control token: 256277 '&lt;unused375&gt;' is not marked as EOG\nload: control token: 257206 '&lt;unused1304&gt;' is not marked as EOG\nload: control token: 256762 '&lt;unused860&gt;' is not marked as EOG\nload: control token: 260879 '&lt;unused4977&gt;' is not marked as EOG\nload: control token: 256610 '&lt;unused708&gt;' is not marked as EOG\nload: control token: 261950 '&lt;unused6048&gt;' is not marked as EOG\nload: control token: 256648 '&lt;unused746&gt;' is not marked as EOG\nload: control token: 257103 '&lt;unused1201&gt;' is not marked as EOG\nload: control token: 259329 '&lt;unused3427&gt;' is not marked as EOG\nload: control token: 256222 '&lt;unused320&gt;' is not marked as EOG\nload: control token: 258055 '&lt;unused2153&gt;' is not marked as EOG\nload: control token: 261103 '&lt;unused5201&gt;' is not marked as EOG\nload: control token: 259857 '&lt;unused3955&gt;' is not marked as EOG\nload: control token: 256828 '&lt;unused926&gt;' is not marked as EOG\nload: control token: 259191 '&lt;unused3289&gt;' is not marked as EOG\nload: control token: 258384 '&lt;unused2482&gt;' is not marked as EOG\nload: control token: 260751 '&lt;unused4849&gt;' is not marked as EOG\nload: control token: 256517 '&lt;unused615&gt;' is not marked as EOG\nload: control token: 256983 '&lt;unused1081&gt;' is not marked as EOG\nload: control token: 258251 '&lt;unused2349&gt;' is not marked as EOG\nload: control token: 258488 '&lt;unused2586&gt;' is not marked as EOG\nload: control token: 257656 '&lt;unused1754&gt;' is not marked as EOG\nload: control token: 257459 '&lt;unused1557&gt;' is not marked as EOG\nload: control token: 259830 '&lt;unused3928&gt;' is not marked as EOG\nload: control token: 261896 '&lt;unused5994&gt;' is not marked as EOG\nload: control token: 259639 '&lt;unused3737&gt;' is not marked as EOG\nload: control token: 258751 '&lt;unused2849&gt;' is not marked as EOG\nload: control token: 261408 '&lt;unused5506&gt;' is not marked as EOG\nload: control token: 256628 '&lt;unused726&gt;' is not marked as EOG\nload: control token: 258635 '&lt;unused2733&gt;' is not marked as EOG\nload: control token: 258016 '&lt;unused2114&gt;' is not marked as EOG\nload: control token: 261972 '&lt;unused6070&gt;' is not marked as EOG\nload: control token: 260353 '&lt;unused4451&gt;' is not marked as EOG\nload: control token: 261601 '&lt;unused5699&gt;' is not marked as EOG\nload: control token: 261788 '&lt;unused5886&gt;' is not marked as EOG\nload: control token: 258797 '&lt;unused2895&gt;' is not marked as EOG\nload: control token: 258705 '&lt;unused2803&gt;' is not marked as EOG\nload: control token: 258045 '&lt;unused2143&gt;' is not marked as EOG\nload: control token: 257494 '&lt;unused1592&gt;' is not marked as EOG\nload: control token: 261997 '&lt;unused6095&gt;' is not marked as EOG\nload: control token: 260580 '&lt;unused4678&gt;' is not marked as EOG\nload: control token: 257783 '&lt;unused1881&gt;' is not marked as EOG\nload: control token: 258846 '&lt;unused2944&gt;' is not marked as EOG\nload: control token: 260409 '&lt;unused4507&gt;' is not marked as EOG\nload: control token: 257521 '&lt;unused1619&gt;' is not marked as EOG\nload: control token: 261989 '&lt;unused6087&gt;' is not marked as EOG\nload: control token: 256671 '&lt;unused769&gt;' is not marked as EOG\nload: control token: 256296 '&lt;unused394&gt;' is not marked as EOG\nload: control token: 261844 '&lt;unused5942&gt;' is not marked as EOG\nload: control token: 256822 '&lt;unused920&gt;' is not marked as EOG\nload: control token: 260725 '&lt;unused4823&gt;' is not marked as EOG\nload: control token: 260384 '&lt;unused4482&gt;' is not marked as EOG\nload: control token: 256689 '&lt;unused787&gt;' is not marked as EOG\nload: control token: 258981 '&lt;unused3079&gt;' is not marked as EOG\nload: control token: 260370 '&lt;unused4468&gt;' is not marked as EOG\nload: control token: 256079 '&lt;unused177&gt;' is not marked as EOG\nload: control token: 257539 '&lt;unused1637&gt;' is not marked as EOG\nload: control token: 260302 '&lt;unused4400&gt;' is not marked as EOG\nload: control token: 261618 '&lt;unused5716&gt;' is not marked as EOG\nload: control token: 258091 '&lt;unused2189&gt;' is not marked as EOG\nload: control token: 260687 '&lt;unused4785&gt;' is not marked as EOG\nload: control token: 259768 '&lt;unused3866&gt;' is not marked as EOG\nload: control token: 259891 '&lt;unused3989&gt;' is not marked as EOG\nload: control token: 258385 '&lt;unused2483&gt;' is not marked as EOG\nload: control token: 260339 '&lt;unused4437&gt;' is not marked as EOG\nload: control token: 257007 '&lt;unused1105&gt;' is not marked as EOG\nload: control token: 257016 '&lt;unused1114&gt;' is not marked as EOG\nload: control token: 260981 '&lt;unused5079&gt;' is not marked as EOG\nload: control token: 256094 '&lt;unused192&gt;' is not marked as EOG\nload: control token: 261820 '&lt;unused5918&gt;' is not marked as EOG\nload: control token: 256156 '&lt;unused254&gt;' is not marked as EOG\nload: control token: 257303 '&lt;unused1401&gt;' is not marked as EOG\nload: control token: 261957 '&lt;unused6055&gt;' is not marked as EOG\nload: control token: 258211 '&lt;unused2309&gt;' is not marked as EOG\nload: control token: 256238 '&lt;unused336&gt;' is not marked as EOG\nload: control token:     43 '&lt;unused37&gt;' is not marked as EOG\nload: control token: 260747 '&lt;unused4845&gt;' is not marked as EOG\nload: control token: 257515 '&lt;unused1613&gt;' is not marked as EOG\nload: control token: 261862 '&lt;unused5960&gt;' is not marked as EOG\nload: control token: 257809 '&lt;unused1907&gt;' is not marked as EOG\nload: control token: 257087 '&lt;unused1185&gt;' is not marked as EOG\nload: control token: 256013 '&lt;unused111&gt;' is not marked as EOG\nload: control token: 257265 '&lt;unused1363&gt;' is not marked as EOG\nload: control token: 257284 '&lt;unused1382&gt;' is not marked as EOG\nload: control token: 256682 '&lt;unused780&gt;' is not marked as EOG\nload: control token: 256598 '&lt;unused696&gt;' is not marked as EOG\nload: control token: 256287 '&lt;unused385&gt;' is not marked as EOG\nload: control token: 259305 '&lt;unused3403&gt;' is not marked as EOG\nload: control token:     42 '&lt;unused36&gt;' is not marked as EOG\nload: control token: 257911 '&lt;unused2009&gt;' is not marked as EOG\nload: control token:     53 '&lt;unused47&gt;' is not marked as EOG\nload: control token: 256143 '&lt;unused241&gt;' is not marked as EOG\nload: control token: 261442 '&lt;unused5540&gt;' is not marked as EOG\nload: control token: 256226 '&lt;unused324&gt;' is not marked as EOG\nload: control token: 259493 '&lt;unused3591&gt;' is not marked as EOG\nload: control token: 261465 '&lt;unused5563&gt;' is not marked as EOG\nload: control token: 257948 '&lt;unused2046&gt;' is not marked as EOG\nload: control token: 257823 '&lt;unused1921&gt;' is not marked as EOG\nload: control token: 262074 '&lt;unused6172&gt;' is not marked as EOG\nload: control token: 257836 '&lt;unused1934&gt;' is not marked as EOG\nload: control token: 261225 '&lt;unused5323&gt;' is not marked as EOG\nload: control token: 256697 '&lt;unused795&gt;' is not marked as EOG\nload: control token: 258978 '&lt;unused3076&gt;' is not marked as EOG\nload: control token: 256910 '&lt;unused1008&gt;' is not marked as EOG\nload: control token: 257567 '&lt;unused1665&gt;' is not marked as EOG\nload: control token: 256495 '&lt;unused593&gt;' is not marked as EOG\nload: control token: 258564 '&lt;unused2662&gt;' is not marked as EOG\nload: control token: 258679 '&lt;unused2777&gt;' is not marked as EOG\nload: control token: 259427 '&lt;unused3525&gt;' is not marked as EOG\nload: control token: 259420 '&lt;unused3518&gt;' is not marked as EOG\nload: control token: 256161 '&lt;unused259&gt;' is not marked as EOG\nload: control token: 261881 '&lt;unused5979&gt;' is not marked as EOG\nload: control token: 256459 '&lt;unused557&gt;' is not marked as EOG\nload: control token: 259104 '&lt;unused3202&gt;' is not marked as EOG\nload: control token: 261464 '&lt;unused5562&gt;' is not marked as EOG\nload: control token: 258914 '&lt;unused3012&gt;' is not marked as EOG\nload: control token: 261983 '&lt;unused6081&gt;' is not marked as EOG\nload: control token: 260139 '&lt;unused4237&gt;' is not marked as EOG\nload: control token: 258241 '&lt;unused2339&gt;' is not marked as EOG\nload: control token: 260013 '&lt;unused4111&gt;' is not marked as EOG\nload: control token: 259833 '&lt;unused3931&gt;' is not marked as EOG\nload: control token: 262025 '&lt;unused6123&gt;' is not marked as EOG\nload: control token: 259350 '&lt;unused3448&gt;' is not marked as EOG\nload: control token:     54 '&lt;unused48&gt;' is not marked as EOG\nload: control token: 256769 '&lt;unused867&gt;' is not marked as EOG\nload: control token: 258257 '&lt;unused2355&gt;' is not marked as EOG\nload: control token: 260559 '&lt;unused4657&gt;' is not marked as EOG\nload: control token: 260495 '&lt;unused4593&gt;' is not marked as EOG\nload: control token: 257282 '&lt;unused1380&gt;' is not marked as EOG\nload: control token: 261656 '&lt;unused5754&gt;' is not marked as EOG\nload: control token: 256286 '&lt;unused384&gt;' is not marked as EOG\nload: control token: 258818 '&lt;unused2916&gt;' is not marked as EOG\nload: control token: 259155 '&lt;unused3253&gt;' is not marked as EOG\nload: control token: 261304 '&lt;unused5402&gt;' is not marked as EOG\nload: control token: 257723 '&lt;unused1821&gt;' is not marked as EOG\nload: control token: 256182 '&lt;unused280&gt;' is not marked as EOG\nload: control token: 256907 '&lt;unused1005&gt;' is not marked as EOG\nload: control token: 258638 '&lt;unused2736&gt;' is not marked as EOG\nload: control token: 258424 '&lt;unused2522&gt;' is not marked as EOG\nload: control token: 258154 '&lt;unused2252&gt;' is not marked as EOG\nload: control token: 261048 '&lt;unused5146&gt;' is not marked as EOG\nload: control token: 261616 '&lt;unused5714&gt;' is not marked as EOG\nload: control token: 257262 '&lt;unused1360&gt;' is not marked as EOG\nload: control token: 261125 '&lt;unused5223&gt;' is not marked as EOG\nload: control token: 257484 '&lt;unused1582&gt;' is not marked as EOG\nload: control token: 261496 '&lt;unused5594&gt;' is not marked as EOG\nload: control token: 259268 '&lt;unused3366&gt;' is not marked as EOG\nload: control token: 259490 '&lt;unused3588&gt;' is not marked as EOG\nload: control token: 262066 '&lt;unused6164&gt;' is not marked as EOG\nload: control token: 256602 '&lt;unused700&gt;' is not marked as EOG\nload: control token: 259324 '&lt;unused3422&gt;' is not marked as EOG\nload: control token: 261571 '&lt;unused5669&gt;' is not marked as EOG\nload: control token: 259322 '&lt;unused3420&gt;' is not marked as EOG\nload: control token: 260216 '&lt;unused4314&gt;' is not marked as EOG\nload: control token: 260557 '&lt;unused4655&gt;' is not marked as EOG\nload: control token: 257011 '&lt;unused1109&gt;' is not marked as EOG\nload: control token: 259709 '&lt;unused3807&gt;' is not marked as EOG\nload: control token: 258138 '&lt;unused2236&gt;' is not marked as EOG\nload: control token: 257407 '&lt;unused1505&gt;' is not marked as EOG\nload: control token: 258893 '&lt;unused2991&gt;' is not marked as EOG\nload: control token: 256083 '&lt;unused181&gt;' is not marked as EOG\nload: control token: 260274 '&lt;unused4372&gt;' is not marked as EOG\nload: control token: 259013 '&lt;unused3111&gt;' is not marked as EOG\nload: control token: 260414 '&lt;unused4512&gt;' is not marked as EOG\nload: control token: 256540 '&lt;unused638&gt;' is not marked as EOG\nload: control token: 259677 '&lt;unused3775&gt;' is not marked as EOG\nload: control token: 259507 '&lt;unused3605&gt;' is not marked as EOG\nload: control token: 258458 '&lt;unused2556&gt;' is not marked as EOG\nload: control token: 258808 '&lt;unused2906&gt;' is not marked as EOG\nload: control token: 261640 '&lt;unused5738&gt;' is not marked as EOG\nload: control token: 262076 '&lt;unused6174&gt;' is not marked as EOG\nload: control token: 256509 '&lt;unused607&gt;' is not marked as EOG\nload: control token: 258759 '&lt;unused2857&gt;' is not marked as EOG\nload: control token: 258084 '&lt;unused2182&gt;' is not marked as EOG\nload: control token: 259090 '&lt;unused3188&gt;' is not marked as EOG\nload: control token: 257660 '&lt;unused1758&gt;' is not marked as EOG\nload: control token: 256774 '&lt;unused872&gt;' is not marked as EOG\nload: control token: 256125 '&lt;unused223&gt;' is not marked as EOG\nload: control token: 259487 '&lt;unused3585&gt;' is not marked as EOG\nload: control token: 261641 '&lt;unused5739&gt;' is not marked as EOG\nload: control token: 257967 '&lt;unused2065&gt;' is not marked as EOG\nload: control token: 257615 '&lt;unused1713&gt;' is not marked as EOG\nload: control token: 259697 '&lt;unused3795&gt;' is not marked as EOG\nload: control token: 257654 '&lt;unused1752&gt;' is not marked as EOG\nload: control token: 260522 '&lt;unused4620&gt;' is not marked as EOG\nload: control token: 256937 '&lt;unused1035&gt;' is not marked as EOG\nload: control token: 259969 '&lt;unused4067&gt;' is not marked as EOG\nload: control token: 259083 '&lt;unused3181&gt;' is not marked as EOG\nload: control token: 261902 '&lt;unused6000&gt;' is not marked as EOG\nload: control token: 258434 '&lt;unused2532&gt;' is not marked as EOG\nload: control token: 256809 '&lt;unused907&gt;' is not marked as EOG\nload: control token: 259528 '&lt;unused3626&gt;' is not marked as EOG\nload: control token: 261021 '&lt;unused5119&gt;' is not marked as EOG\nload: control token: 257876 '&lt;unused1974&gt;' is not marked as EOG\nload: control token: 257309 '&lt;unused1407&gt;' is not marked as EOG\nload: control token: 259714 '&lt;unused3812&gt;' is not marked as EOG\nload: control token: 257505 '&lt;unused1603&gt;' is not marked as EOG\nload: control token: 257706 '&lt;unused1804&gt;' is not marked as EOG\nload: control token: 259559 '&lt;unused3657&gt;' is not marked as EOG\nload: control token: 261381 '&lt;unused5479&gt;' is not marked as EOG\nload: control token: 260732 '&lt;unused4830&gt;' is not marked as EOG\nload: control token: 258037 '&lt;unused2135&gt;' is not marked as EOG\nload: control token:     15 '&lt;unused9&gt;' is not marked as EOG\nload: control token: 260046 '&lt;unused4144&gt;' is not marked as EOG\nload: control token: 256865 '&lt;unused963&gt;' is not marked as EOG\nload: control token: 262019 '&lt;unused6117&gt;' is not marked as EOG\nload: control token: 259028 '&lt;unused3126&gt;' is not marked as EOG\nload: control token: 256005 '&lt;unused103&gt;' is not marked as EOG\nload: control token: 257110 '&lt;unused1208&gt;' is not marked as EOG\nload: control token: 257807 '&lt;unused1905&gt;' is not marked as EOG\nload: control token: 261053 '&lt;unused5151&gt;' is not marked as EOG\nload: control token: 261584 '&lt;unused5682&gt;' is not marked as EOG\nload: control token: 261278 '&lt;unused5376&gt;' is not marked as EOG\nload: control token: 256631 '&lt;unused729&gt;' is not marked as EOG\nload: control token: 258895 '&lt;unused2993&gt;' is not marked as EOG\nload: control token: 258596 '&lt;unused2694&gt;' is not marked as EOG\nload: control token: 260515 '&lt;unused4613&gt;' is not marked as EOG\nload: control token: 259900 '&lt;unused3998&gt;' is not marked as EOG\nload: control token: 256698 '&lt;unused796&gt;' is not marked as EOG\nload: control token: 257380 '&lt;unused1478&gt;' is not marked as EOG\nload: control token: 260953 '&lt;unused5051&gt;' is not marked as EOG\nload: control token: 256528 '&lt;unused626&gt;' is not marked as EOG\nload: control token: 257526 '&lt;unused1624&gt;' is not marked as EOG\nload: control token: 261966 '&lt;unused6064&gt;' is not marked as EOG\nload: control token: 258125 '&lt;unused2223&gt;' is not marked as EOG\nload: control token: 258628 '&lt;unused2726&gt;' is not marked as EOG\nload: control token: 257172 '&lt;unused1270&gt;' is not marked as EOG\nload: control token: 258199 '&lt;unused2297&gt;' is not marked as EOG\nload: control token: 258000 '&lt;unused2098&gt;' is not marked as EOG\nload: control token: 256149 '&lt;unused247&gt;' is not marked as EOG\nload: control token: 257707 '&lt;unused1805&gt;' is not marked as EOG\nload: control token: 256031 '&lt;unused129&gt;' is not marked as EOG\nload: control token: 257372 '&lt;unused1470&gt;' is not marked as EOG\nload: control token: 260685 '&lt;unused4783&gt;' is not marked as EOG\nload: control token: 257777 '&lt;unused1875&gt;' is not marked as EOG\nload: control token: 257508 '&lt;unused1606&gt;' is not marked as EOG\nload: control token: 257178 '&lt;unused1276&gt;' is not marked as EOG\nload: control token: 261953 '&lt;unused6051&gt;' is not marked as EOG\nload: control token: 257154 '&lt;unused1252&gt;' is not marked as EOG\nload: control token: 258149 '&lt;unused2247&gt;' is not marked as EOG\nload: control token: 256729 '&lt;unused827&gt;' is not marked as EOG\nload: control token: 258281 '&lt;unused2379&gt;' is not marked as EOG\nload: control token: 256108 '&lt;unused206&gt;' is not marked as EOG\nload: control token: 261574 '&lt;unused5672&gt;' is not marked as EOG\nload: control token: 257458 '&lt;unused1556&gt;' is not marked as EOG\nload: control token: 259739 '&lt;unused3837&gt;' is not marked as EOG\nload: control token: 261093 '&lt;unused5191&gt;' is not marked as EOG\nload: control token:     19 '&lt;unused13&gt;' is not marked as EOG\nload: control token:     80 '&lt;unused74&gt;' is not marked as EOG\nload: control token: 257247 '&lt;unused1345&gt;' is not marked as EOG\nload: control token: 262118 '&lt;unused6216&gt;' is not marked as EOG\nload: control token: 259385 '&lt;unused3483&gt;' is not marked as EOG\nload: control token: 256224 '&lt;unused322&gt;' is not marked as EOG\nload: control token: 257854 '&lt;unused1952&gt;' is not marked as EOG\nload: control token: 261697 '&lt;unused5795&gt;' is not marked as EOG\nload: control token: 256172 '&lt;unused270&gt;' is not marked as EOG\nload: control token: 260979 '&lt;unused5077&gt;' is not marked as EOG\nload: control token: 257047 '&lt;unused1145&gt;' is not marked as EOG\nload: control token: 258783 '&lt;unused2881&gt;' is not marked as EOG\nload: control token: 260184 '&lt;unused4282&gt;' is not marked as EOG\nload: control token: 259896 '&lt;unused3994&gt;' is not marked as EOG\nload: control token: 257694 '&lt;unused1792&gt;' is not marked as EOG\nload: control token: 258659 '&lt;unused2757&gt;' is not marked as EOG\nload: control token: 258778 '&lt;unused2876&gt;' is not marked as EOG\nload: control token: 256640 '&lt;unused738&gt;' is not marked as EOG\nload: control token: 259044 '&lt;unused3142&gt;' is not marked as EOG\nload: control token: 261204 '&lt;unused5302&gt;' is not marked as EOG\nload: control token: 261980 '&lt;unused6078&gt;' is not marked as EOG\nload: control token: 257919 '&lt;unused2017&gt;' is not marked as EOG\nload: control token: 258804 '&lt;unused2902&gt;' is not marked as EOG\nload: control token: 262003 '&lt;unused6101&gt;' is not marked as EOG\nload: control token: 258287 '&lt;unused2385&gt;' is not marked as EOG\nload: control token: 257334 '&lt;unused1432&gt;' is not marked as EOG\nload: control token: 259839 '&lt;unused3937&gt;' is not marked as EOG\nload: control token: 257824 '&lt;unused1922&gt;' is not marked as EOG\nload: control token: 260000 '&lt;unused4098&gt;' is not marked as EOG\nload: control token: 258286 '&lt;unused2384&gt;' is not marked as EOG\nload: control token: 260104 '&lt;unused4202&gt;' is not marked as EOG\nload: control token: 259592 '&lt;unused3690&gt;' is not marked as EOG\nload: control token: 260052 '&lt;unused4150&gt;' is not marked as EOG\nload: control token: 257574 '&lt;unused1672&gt;' is not marked as EOG\nload: control token: 261050 '&lt;unused5148&gt;' is not marked as EOG\nload: control token: 256653 '&lt;unused751&gt;' is not marked as EOG\nload: control token: 260731 '&lt;unused4829&gt;' is not marked as EOG\nload: control token: 259252 '&lt;unused3350&gt;' is not marked as EOG\nload: control token: 258781 '&lt;unused2879&gt;' is not marked as EOG\nload: control token:     92 '&lt;unused86&gt;' is not marked as EOG\nload: control token: 261439 '&lt;unused5537&gt;' is not marked as EOG\nload: control token: 257232 '&lt;unused1330&gt;' is not marked as EOG\nload: control token:     45 '&lt;unused39&gt;' is not marked as EOG\nload: control token: 261508 '&lt;unused5606&gt;' is not marked as EOG\nload: control token: 259479 '&lt;unused3577&gt;' is not marked as EOG\nload: control token: 260993 '&lt;unused5091&gt;' is not marked as EOG\nload: control token: 257591 '&lt;unused1689&gt;' is not marked as EOG\nload: control token: 256863 '&lt;unused961&gt;' is not marked as EOG\nload: control token: 256438 '&lt;unused536&gt;' is not marked as EOG\nload: control token: 260583 '&lt;unused4681&gt;' is not marked as EOG\nload: control token: 259957 '&lt;unused4055&gt;' is not marked as EOG\nload: control token: 261143 '&lt;unused5241&gt;' is not marked as EOG\nload: control token: 259106 '&lt;unused3204&gt;' is not marked as EOG\nload: control token: 260290 '&lt;unused4388&gt;' is not marked as EOG\nload: control token: 261955 '&lt;unused6053&gt;' is not marked as EOG\nload: control token: 257732 '&lt;unused1830&gt;' is not marked as EOG\nload: control token: 258248 '&lt;unused2346&gt;' is not marked as EOG\nload: control token: 260343 '&lt;unused4441&gt;' is not marked as EOG\nload: control token: 259925 '&lt;unused4023&gt;' is not marked as EOG\nload: control token: 258329 '&lt;unused2427&gt;' is not marked as EOG\nload: control token: 260101 '&lt;unused4199&gt;' is not marked as EOG\nload: control token: 261700 '&lt;unused5798&gt;' is not marked as EOG\nload: control token: 260973 '&lt;unused5071&gt;' is not marked as EOG\nload: control token: 261575 '&lt;unused5673&gt;' is not marked as EOG\nload: control token: 256437 '&lt;unused535&gt;' is not marked as EOG\nload: control token: 259749 '&lt;unused3847&gt;' is not marked as EOG\nload: control token: 258360 '&lt;unused2458&gt;' is not marked as EOG\nload: control token: 257298 '&lt;unused1396&gt;' is not marked as EOG\nload: control token: 256496 '&lt;unused594&gt;' is not marked as EOG\nload: control token: 259827 '&lt;unused3925&gt;' is not marked as EOG\nload: control token: 257805 '&lt;unused1903&gt;' is not marked as EOG\nload: control token: 261484 '&lt;unused5582&gt;' is not marked as EOG\nload: control token: 256667 '&lt;unused765&gt;' is not marked as EOG\nload: control token: 261309 '&lt;unused5407&gt;' is not marked as EOG\nload: control token: 259722 '&lt;unused3820&gt;' is not marked as EOG\nload: control token: 258436 '&lt;unused2534&gt;' is not marked as EOG\nload: control token: 260118 '&lt;unused4216&gt;' is not marked as EOG\nload: control token: 261838 '&lt;unused5936&gt;' is not marked as EOG\nload: control token: 256810 '&lt;unused908&gt;' is not marked as EOG\nload: control token: 259868 '&lt;unused3966&gt;' is not marked as EOG\nload: control token: 259394 '&lt;unused3492&gt;' is not marked as EOG\nload: control token: 257703 '&lt;unused1801&gt;' is not marked as EOG\nload: control token: 258028 '&lt;unused2126&gt;' is not marked as EOG\nload: control token: 258034 '&lt;unused2132&gt;' is not marked as EOG\nload: control token: 257326 '&lt;unused1424&gt;' is not marked as EOG\nload: control token: 260972 '&lt;unused5070&gt;' is not marked as EOG\nload: control token: 256948 '&lt;unused1046&gt;' is not marked as EOG\nload: control token: 261426 '&lt;unused5524&gt;' is not marked as EOG\nload: control token: 258574 '&lt;unused2672&gt;' is not marked as EOG\nload: control token: 259249 '&lt;unused3347&gt;' is not marked as EOG\nload: control token: 257181 '&lt;unused1279&gt;' is not marked as EOG\nload: control token: 260570 '&lt;unused4668&gt;' is not marked as EOG\nload: control token: 259594 '&lt;unused3692&gt;' is not marked as EOG\nload: control token: 260869 '&lt;unused4967&gt;' is not marked as EOG\nload: control token: 258926 '&lt;unused3024&gt;' is not marked as EOG\nload: control token: 256362 '&lt;unused460&gt;' is not marked as EOG\nload: control token: 258690 '&lt;unused2788&gt;' is not marked as EOG\nload: control token: 262017 '&lt;unused6115&gt;' is not marked as EOG\nload: control token: 258617 '&lt;unused2715&gt;' is not marked as EOG\nload: control token: 258525 '&lt;unused2623&gt;' is not marked as EOG\nload: control token: 258219 '&lt;unused2317&gt;' is not marked as EOG\nload: control token: 258506 '&lt;unused2604&gt;' is not marked as EOG\nload: control token: 257132 '&lt;unused1230&gt;' is not marked as EOG\nload: control token: 258292 '&lt;unused2390&gt;' is not marked as EOG\nload: control token: 256038 '&lt;unused136&gt;' is not marked as EOG\nload: control token: 257174 '&lt;unused1272&gt;' is not marked as EOG\nload: control token: 261676 '&lt;unused5774&gt;' is not marked as EOG\nload: control token: 261626 '&lt;unused5724&gt;' is not marked as EOG\nload: control token: 261288 '&lt;unused5386&gt;' is not marked as EOG\nload: control token: 261730 '&lt;unused5828&gt;' is not marked as EOG\nload: control token: 256111 '&lt;unused209&gt;' is not marked as EOG\nload: control token: 257319 '&lt;unused1417&gt;' is not marked as EOG\nload: control token: 256726 '&lt;unused824&gt;' is not marked as EOG\nload: control token: 258642 '&lt;unused2740&gt;' is not marked as EOG\nload: control token: 257647 '&lt;unused1745&gt;' is not marked as EOG\nload: control token: 260833 '&lt;unused4931&gt;' is not marked as EOG\nload: control token: 258448 '&lt;unused2546&gt;' is not marked as EOG\nload: control token: 257966 '&lt;unused2064&gt;' is not marked as EOG\nload: control token: 259866 '&lt;unused3964&gt;' is not marked as EOG\nload: control token: 256345 '&lt;unused443&gt;' is not marked as EOG\nload: control token: 257786 '&lt;unused1884&gt;' is not marked as EOG\nload: control token: 258212 '&lt;unused2310&gt;' is not marked as EOG\nload: control token: 261239 '&lt;unused5337&gt;' is not marked as EOG\nload: control token: 256334 '&lt;unused432&gt;' is not marked as EOG\nload: control token: 256569 '&lt;unused667&gt;' is not marked as EOG\nload: control token: 261489 '&lt;unused5587&gt;' is not marked as EOG\nload: control token: 258380 '&lt;unused2478&gt;' is not marked as EOG\nload: control token: 256354 '&lt;unused452&gt;' is not marked as EOG\nload: control token: 259984 '&lt;unused4082&gt;' is not marked as EOG\nload: control token: 256768 '&lt;unused866&gt;' is not marked as EOG\nload: control token: 261421 '&lt;unused5519&gt;' is not marked as EOG\nload: control token: 260162 '&lt;unused4260&gt;' is not marked as EOG\nload: control token: 261037 '&lt;unused5135&gt;' is not marked as EOG\nload: control token: 259585 '&lt;unused3683&gt;' is not marked as EOG\nload: control token: 256011 '&lt;unused109&gt;' is not marked as EOG\nload: control token: 258747 '&lt;unused2845&gt;' is not marked as EOG\nload: control token: 261233 '&lt;unused5331&gt;' is not marked as EOG\nload: control token: 259656 '&lt;unused3754&gt;' is not marked as EOG\nload: control token: 261949 '&lt;unused6047&gt;' is not marked as EOG\nload: control token: 260618 '&lt;unused4716&gt;' is not marked as EOG\nload: control token: 260247 '&lt;unused4345&gt;' is not marked as EOG\nload: control token: 257096 '&lt;unused1194&gt;' is not marked as EOG\nload: control token: 259077 '&lt;unused3175&gt;' is not marked as EOG\nload: control token: 261858 '&lt;unused5956&gt;' is not marked as EOG\nload: control token: 258785 '&lt;unused2883&gt;' is not marked as EOG\nload: control token: 256853 '&lt;unused951&gt;' is not marked as EOG\nload: control token: 261357 '&lt;unused5455&gt;' is not marked as EOG\nload: control token: 259299 '&lt;unused3397&gt;' is not marked as EOG\nload: control token: 257880 '&lt;unused1978&gt;' is not marked as EOG\nload: control token: 259641 '&lt;unused3739&gt;' is not marked as EOG\nload: control token: 260787 '&lt;unused4885&gt;' is not marked as EOG\nload: control token: 261645 '&lt;unused5743&gt;' is not marked as EOG\nload: control token: 260699 '&lt;unused4797&gt;' is not marked as EOG\nload: control token: 260480 '&lt;unused4578&gt;' is not marked as EOG\nload: control token: 260717 '&lt;unused4815&gt;' is not marked as EOG\nload: control token: 259091 '&lt;unused3189&gt;' is not marked as EOG\nload: control token: 261370 '&lt;unused5468&gt;' is not marked as EOG\nload: control token: 258933 '&lt;unused3031&gt;' is not marked as EOG\nload: control token: 259321 '&lt;unused3419&gt;' is not marked as EOG\nload: control token: 260548 '&lt;unused4646&gt;' is not marked as EOG\nload: control token: 258561 '&lt;unused2659&gt;' is not marked as EOG\nload: control token: 259209 '&lt;unused3307&gt;' is not marked as EOG\nload: control token: 259431 '&lt;unused3529&gt;' is not marked as EOG\nload: control token: 259069 '&lt;unused3167&gt;' is not marked as EOG\nload: control token: 256904 '&lt;unused1002&gt;' is not marked as EOG\nload: control token: 260263 '&lt;unused4361&gt;' is not marked as EOG\nload: control token: 257170 '&lt;unused1268&gt;' is not marked as EOG\nload: control token: 258715 '&lt;unused2813&gt;' is not marked as EOG\nload: control token: 259342 '&lt;unused3440&gt;' is not marked as EOG\nload: control token: 259210 '&lt;unused3308&gt;' is not marked as EOG\nload: control token: 258355 '&lt;unused2453&gt;' is not marked as EOG\nload: control token: 256367 '&lt;unused465&gt;' is not marked as EOG\nload: control token: 257725 '&lt;unused1823&gt;' is not marked as EOG\nload: control token: 259753 '&lt;unused3851&gt;' is not marked as EOG\nload: control token: 257813 '&lt;unused1911&gt;' is not marked as EOG\nload: control token: 256570 '&lt;unused668&gt;' is not marked as EOG\nload: control token: 258269 '&lt;unused2367&gt;' is not marked as EOG\nload: control token: 258114 '&lt;unused2212&gt;' is not marked as EOG\nload: control token: 257413 '&lt;unused1511&gt;' is not marked as EOG\nload: control token: 260990 '&lt;unused5088&gt;' is not marked as EOG\nload: control token: 260668 '&lt;unused4766&gt;' is not marked as EOG\nload: control token: 259023 '&lt;unused3121&gt;' is not marked as EOG\nload: control token: 256552 '&lt;unused650&gt;' is not marked as EOG\nload: control token: 261000 '&lt;unused5098&gt;' is not marked as EOG\nload: control token: 261199 '&lt;unused5297&gt;' is not marked as EOG\nload: control token: 256474 '&lt;unused572&gt;' is not marked as EOG\nload: control token: 259785 '&lt;unused3883&gt;' is not marked as EOG\nload: control token: 258102 '&lt;unused2200&gt;' is not marked as EOG\nload: control token: 261301 '&lt;unused5399&gt;' is not marked as EOG\nload: control token: 258809 '&lt;unused2907&gt;' is not marked as EOG\nload: control token: 258060 '&lt;unused2158&gt;' is not marked as EOG\nload: control token:     75 '&lt;unused69&gt;' is not marked as EOG\nload: control token: 256858 '&lt;unused956&gt;' is not marked as EOG\nload: control token: 259050 '&lt;unused3148&gt;' is not marked as EOG\nload: control token: 260484 '&lt;unused4582&gt;' is not marked as EOG\nload: control token: 257631 '&lt;unused1729&gt;' is not marked as EOG\nload: control token: 260974 '&lt;unused5072&gt;' is not marked as EOG\nload: control token: 258209 '&lt;unused2307&gt;' is not marked as EOG\nload: control token: 258316 '&lt;unused2414&gt;' is not marked as EOG\nload: control token: 257223 '&lt;unused1321&gt;' is not marked as EOG\nload: control token: 260642 '&lt;unused4740&gt;' is not marked as EOG\nload: control token: 257634 '&lt;unused1732&gt;' is not marked as EOG\nload: control token: 259956 '&lt;unused4054&gt;' is not marked as EOG\nload: control token: 260265 '&lt;unused4363&gt;' is not marked as EOG\nload: control token: 258504 '&lt;unused2602&gt;' is not marked as EOG\nload: control token: 258359 '&lt;unused2457&gt;' is not marked as EOG\nload: control token: 261189 '&lt;unused5287&gt;' is not marked as EOG\nload: control token: 256173 '&lt;unused271&gt;' is not marked as EOG\nload: control token: 261542 '&lt;unused5640&gt;' is not marked as EOG\nload: control token: 257208 '&lt;unused1306&gt;' is not marked as EOG\nload: control token: 256805 '&lt;unused903&gt;' is not marked as EOG\nload: control token: 260089 '&lt;unused4187&gt;' is not marked as EOG\nload: control token: 256271 '&lt;unused369&gt;' is not marked as EOG\nload: control token:     36 '&lt;unused30&gt;' is not marked as EOG\nload: control token: 258096 '&lt;unused2194&gt;' is not marked as EOG\nload: control token: 260900 '&lt;unused4998&gt;' is not marked as EOG\nload: control token: 257222 '&lt;unused1320&gt;' is not marked as EOG\nload: control token: 261040 '&lt;unused5138&gt;' is not marked as EOG\nload: control token: 261463 '&lt;unused5561&gt;' is not marked as EOG\nload: control token: 257242 '&lt;unused1340&gt;' is not marked as EOG\nload: control token: 259783 '&lt;unused3881&gt;' is not marked as EOG\nload: control token: 258462 '&lt;unused2560&gt;' is not marked as EOG\nload: control token: 258169 '&lt;unused2267&gt;' is not marked as EOG\nload: control token: 258515 '&lt;unused2613&gt;' is not marked as EOG\nload: control token: 259867 '&lt;unused3965&gt;' is not marked as EOG\nload: control token: 257283 '&lt;unused1381&gt;' is not marked as EOG\nload: control token: 258142 '&lt;unused2240&gt;' is not marked as EOG\nload: control token: 261005 '&lt;unused5103&gt;' is not marked as EOG\nload: control token: 258512 '&lt;unused2610&gt;' is not marked as EOG\nload: control token: 259382 '&lt;unused3480&gt;' is not marked as EOG\nload: control token: 259395 '&lt;unused3493&gt;' is not marked as EOG\nload: control token: 258378 '&lt;unused2476&gt;' is not marked as EOG\nload: control token: 261899 '&lt;unused5997&gt;' is not marked as EOG\nload: control token: 257249 '&lt;unused1347&gt;' is not marked as EOG\nload: control token: 256724 '&lt;unused822&gt;' is not marked as EOG\nload: control token:     61 '&lt;unused55&gt;' is not marked as EOG\nload: control token: 261372 '&lt;unused5470&gt;' is not marked as EOG\nload: control token: 256986 '&lt;unused1084&gt;' is not marked as EOG\nload: control token: 258537 '&lt;unused2635&gt;' is not marked as EOG\nload: control token: 258813 '&lt;unused2911&gt;' is not marked as EOG\nload: control token: 257856 '&lt;unused1954&gt;' is not marked as EOG\nload: control token: 261469 '&lt;unused5567&gt;' is not marked as EOG\nload: control token: 257668 '&lt;unused1766&gt;' is not marked as EOG\nload: control token: 261895 '&lt;unused5993&gt;' is not marked as EOG\nload: control token: 260596 '&lt;unused4694&gt;' is not marked as EOG\nload: control token: 261001 '&lt;unused5099&gt;' is not marked as EOG\nload: control token: 256546 '&lt;unused644&gt;' is not marked as EOG\nload: control token: 258593 '&lt;unused2691&gt;' is not marked as EOG\nload: control token: 262042 '&lt;unused6140&gt;' is not marked as EOG\nload: control token: 256275 '&lt;unused373&gt;' is not marked as EOG\nload: control token: 256499 '&lt;unused597&gt;' is not marked as EOG\nload: control token: 257982 '&lt;unused2080&gt;' is not marked as EOG\nload: control token: 259965 '&lt;unused4063&gt;' is not marked as EOG\nload: control token: 261627 '&lt;unused5725&gt;' is not marked as EOG\nload: control token: 259548 '&lt;unused3646&gt;' is not marked as EOG\nload: control token: 257486 '&lt;unused1584&gt;' is not marked as EOG\nload: control token: 260862 '&lt;unused4960&gt;' is not marked as EOG\nload: control token: 261445 '&lt;unused5543&gt;' is not marked as EOG\nload: control token: 258520 '&lt;unused2618&gt;' is not marked as EOG\nload: control token: 262004 '&lt;unused6102&gt;' is not marked as EOG\nload: control token: 259269 '&lt;unused3367&gt;' is not marked as EOG\nload: control token: 258082 '&lt;unused2180&gt;' is not marked as EOG\nload: control token: 258026 '&lt;unused2124&gt;' is not marked as EOG\nload: control token: 257965 '&lt;unused2063&gt;' is not marked as EOG\nload: control token: 257415 '&lt;unused1513&gt;' is not marked as EOG\nload: control token: 260060 '&lt;unused4158&gt;' is not marked as EOG\nload: control token: 260611 '&lt;unused4709&gt;' is not marked as EOG\nload: control token: 257810 '&lt;unused1908&gt;' is not marked as EOG\nload: control token: 259388 '&lt;unused3486&gt;' is not marked as EOG\nload: control token: 257755 '&lt;unused1853&gt;' is not marked as EOG\nload: control token: 261183 '&lt;unused5281&gt;' is not marked as EOG\nload: control token: 260294 '&lt;unused4392&gt;' is not marked as EOG\nload: control token: 259782 '&lt;unused3880&gt;' is not marked as EOG\nload: control token: 257292 '&lt;unused1390&gt;' is not marked as EOG\nload: control token: 259733 '&lt;unused3831&gt;' is not marked as EOG\nload: control token: 257608 '&lt;unused1706&gt;' is not marked as EOG\nload: control token: 260969 '&lt;unused5067&gt;' is not marked as EOG\nload: control token: 257621 '&lt;unused1719&gt;' is not marked as EOG\nload: control token: 258325 '&lt;unused2423&gt;' is not marked as EOG\nload: control token: 258581 '&lt;unused2679&gt;' is not marked as EOG\nload: control token: 260980 '&lt;unused5078&gt;' is not marked as EOG\nload: control token: 257464 '&lt;unused1562&gt;' is not marked as EOG\nload: control token: 259686 '&lt;unused3784&gt;' is not marked as EOG\nload: control token: 261295 '&lt;unused5393&gt;' is not marked as EOG\nload: control token: 256798 '&lt;unused896&gt;' is not marked as EOG\nload: control token: 257896 '&lt;unused1994&gt;' is not marked as EOG\nload: control token: 256272 '&lt;unused370&gt;' is not marked as EOG\nload: control token: 257629 '&lt;unused1727&gt;' is not marked as EOG\nload: control token: 261823 '&lt;unused5921&gt;' is not marked as EOG\nload: control token: 257602 '&lt;unused1700&gt;' is not marked as EOG\nload: control token: 261766 '&lt;unused5864&gt;' is not marked as EOG\nload: control token: 256273 '&lt;unused371&gt;' is not marked as EOG\nload: control token: 260230 '&lt;unused4328&gt;' is not marked as EOG\nload: control token: 260680 '&lt;unused4778&gt;' is not marked as EOG\nload: control token: 256003 '&lt;unused101&gt;' is not marked as EOG\nload: control token: 259668 '&lt;unused3766&gt;' is not marked as EOG\nload: control token: 257300 '&lt;unused1398&gt;' is not marked as EOG\nload: control token: 259000 '&lt;unused3098&gt;' is not marked as EOG\nload: control token: 257779 '&lt;unused1877&gt;' is not marked as EOG\nload: control token: 260742 '&lt;unused4840&gt;' is not marked as EOG\nload: control token: 256707 '&lt;unused805&gt;' is not marked as EOG\nload: control token:     86 '&lt;unused80&gt;' is not marked as EOG\nload: control token: 259230 '&lt;unused3328&gt;' is not marked as EOG\nload: control token: 257941 '&lt;unused2039&gt;' is not marked as EOG\nload: control token: 258472 '&lt;unused2570&gt;' is not marked as EOG\nload: control token: 261308 '&lt;unused5406&gt;' is not marked as EOG\nload: control token: 260096 '&lt;unused4194&gt;' is not marked as EOG\nload: control token: 256873 '&lt;unused971&gt;' is not marked as EOG\nload: control token: 259198 '&lt;unused3296&gt;' is not marked as EOG\nload: control token: 260203 '&lt;unused4301&gt;' is not marked as EOG\nload: control token: 260431 '&lt;unused4529&gt;' is not marked as EOG\nload: control token: 259445 '&lt;unused3543&gt;' is not marked as EOG\nload: control token: 258737 '&lt;unused2835&gt;' is not marked as EOG\nload: control token: 259670 '&lt;unused3768&gt;' is not marked as EOG\nload: control token: 256533 '&lt;unused631&gt;' is not marked as EOG\nload: control token: 256212 '&lt;unused310&gt;' is not marked as EOG\nload: control token: 259621 '&lt;unused3719&gt;' is not marked as EOG\nload: control token: 256749 '&lt;unused847&gt;' is not marked as EOG\nload: control token: 256748 '&lt;unused846&gt;' is not marked as EOG\nload: control token: 260205 '&lt;unused4303&gt;' is not marked as EOG\nload: control token: 261400 '&lt;unused5498&gt;' is not marked as EOG\nload: control token: 257098 '&lt;unused1196&gt;' is not marked as EOG\nload: control token: 258992 '&lt;unused3090&gt;' is not marked as EOG\nload: control token: 261655 '&lt;unused5753&gt;' is not marked as EOG\nload: control token: 256752 '&lt;unused850&gt;' is not marked as EOG\nload: control token: 260117 '&lt;unused4215&gt;' is not marked as EOG\nload: control token: 259194 '&lt;unused3292&gt;' is not marked as EOG\nload: control token: 257942 '&lt;unused2040&gt;' is not marked as EOG\nload: control token: 261906 '&lt;unused6004&gt;' is not marked as EOG\nload: control token: 258112 '&lt;unused2210&gt;' is not marked as EOG\nload: control token: 257241 '&lt;unused1339&gt;' is not marked as EOG\nload: control token: 257299 '&lt;unused1397&gt;' is not marked as EOG\nload: control token: 259353 '&lt;unused3451&gt;' is not marked as EOG\nload: control token: 259599 '&lt;unused3697&gt;' is not marked as EOG\nload: control token: 260594 '&lt;unused4692&gt;' is not marked as EOG\nload: control token: 259111 '&lt;unused3209&gt;' is not marked as EOG\nload: control token:     89 '&lt;unused83&gt;' is not marked as EOG\nload: control token: 262139 '&lt;unused6237&gt;' is not marked as EOG\nload: control token: 256745 '&lt;unused843&gt;' is not marked as EOG\nload: control token: 259124 '&lt;unused3222&gt;' is not marked as EOG\nload: control token: 259876 '&lt;unused3974&gt;' is not marked as EOG\nload: control token: 257702 '&lt;unused1800&gt;' is not marked as EOG\nload: control token: 259222 '&lt;unused3320&gt;' is not marked as EOG\nload: control token: 259577 '&lt;unused3675&gt;' is not marked as EOG\nload: control token: 256979 '&lt;unused1077&gt;' is not marked as EOG\nload: control token: 256383 '&lt;unused481&gt;' is not marked as EOG\nload: control token: 260152 '&lt;unused4250&gt;' is not marked as EOG\nload: control token: 259227 '&lt;unused3325&gt;' is not marked as EOG\nload: control token: 259418 '&lt;unused3516&gt;' is not marked as EOG\nload: control token: 260661 '&lt;unused4759&gt;' is not marked as EOG\nload: control token: 261761 '&lt;unused5859&gt;' is not marked as EOG\nload: control token: 261962 '&lt;unused6060&gt;' is not marked as EOG\nload: control token: 261033 '&lt;unused5131&gt;' is not marked as EOG\nload: control token: 257625 '&lt;unused1723&gt;' is not marked as EOG\nload: control token: 257089 '&lt;unused1187&gt;' is not marked as EOG\nload: control token: 261658 '&lt;unused5756&gt;' is not marked as EOG\nload: control token: 259707 '&lt;unused3805&gt;' is not marked as EOG\nload: control token: 260577 '&lt;unused4675&gt;' is not marked as EOG\nload: control token: 259380 '&lt;unused3478&gt;' is not marked as EOG\nload: control token: 258669 '&lt;unused2767&gt;' is not marked as EOG\nload: control token: 257229 '&lt;unused1327&gt;' is not marked as EOG\nload: control token: 258942 '&lt;unused3040&gt;' is not marked as EOG\nload: control token: 261688 '&lt;unused5786&gt;' is not marked as EOG\nload: control token: 256894 '&lt;unused992&gt;' is not marked as EOG\nload: control token: 256860 '&lt;unused958&gt;' is not marked as EOG\nload: control token: 256237 '&lt;unused335&gt;' is not marked as EOG\nload: control token: 256435 '&lt;unused533&gt;' is not marked as EOG\nload: control token: 257439 '&lt;unused1537&gt;' is not marked as EOG\nload: control token: 256708 '&lt;unused806&gt;' is not marked as EOG\nload: control token: 257226 '&lt;unused1324&gt;' is not marked as EOG\nload: control token: 260970 '&lt;unused5068&gt;' is not marked as EOG\nload: control token: 260287 '&lt;unused4385&gt;' is not marked as EOG\nload: control token: 260734 '&lt;unused4832&gt;' is not marked as EOG\nload: control token: 261047 '&lt;unused5145&gt;' is not marked as EOG\nload: control token: 259436 '&lt;unused3534&gt;' is not marked as EOG\nload: control token: 256327 '&lt;unused425&gt;' is not marked as EOG\nload: control token: 256974 '&lt;unused1072&gt;' is not marked as EOG\nload: control token: 259087 '&lt;unused3185&gt;' is not marked as EOG\nload: control token: 262065 '&lt;unused6163&gt;' is not marked as EOG\nload: control token: 256299 '&lt;unused397&gt;' is not marked as EOG\nload: control token: 261809 '&lt;unused5907&gt;' is not marked as EOG\nload: control token: 260372 '&lt;unused4470&gt;' is not marked as EOG\nload: control token: 256609 '&lt;unused707&gt;' is not marked as EOG\nload: control token: 260282 '&lt;unused4380&gt;' is not marked as EOG\nload: control token: 261082 '&lt;unused5180&gt;' is not marked as EOG\nload: control token: 261530 '&lt;unused5628&gt;' is not marked as EOG\nload: control token: 257058 '&lt;unused1156&gt;' is not marked as EOG\nload: control token: 260442 '&lt;unused4540&gt;' is not marked as EOG\nload: control token: 258381 '&lt;unused2479&gt;' is not marked as EOG\nload: control token: 260412 '&lt;unused4510&gt;' is not marked as EOG\nload: control token: 256654 '&lt;unused752&gt;' is not marked as EOG\nload: control token: 258075 '&lt;unused2173&gt;' is not marked as EOG\nload: control token: 257727 '&lt;unused1825&gt;' is not marked as EOG\nload: control token: 257129 '&lt;unused1227&gt;' is not marked as EOG\nload: control token: 261507 '&lt;unused5605&gt;' is not marked as EOG\nload: control token: 260681 '&lt;unused4779&gt;' is not marked as EOG\nload: control token: 256372 '&lt;unused470&gt;' is not marked as EOG\nload: control token: 258041 '&lt;unused2139&gt;' is not marked as EOG\nload: control token: 257081 '&lt;unused1179&gt;' is not marked as EOG\nload: control token: 258478 '&lt;unused2576&gt;' is not marked as EOG\nload: control token: 259043 '&lt;unused3141&gt;' is not marked as EOG\nload: control token: 257553 '&lt;unused1651&gt;' is not marked as EOG\nload: control token: 256547 '&lt;unused645&gt;' is not marked as EOG\nload: control token: 260179 '&lt;unused4277&gt;' is not marked as EOG\nload: control token: 258065 '&lt;unused2163&gt;' is not marked as EOG\nload: control token: 259713 '&lt;unused3811&gt;' is not marked as EOG\nload: control token: 258070 '&lt;unused2168&gt;' is not marked as EOG\nload: control token: 256360 '&lt;unused458&gt;' is not marked as EOG\nload: control token: 258106 '&lt;unused2204&gt;' is not marked as EOG\nload: control token: 261634 '&lt;unused5732&gt;' is not marked as EOG\nload: control token: 256491 '&lt;unused589&gt;' is not marked as EOG\nload: control token: 256559 '&lt;unused657&gt;' is not marked as EOG\nload: control token: 256895 '&lt;unused993&gt;' is not marked as EOG\nload: control token: 257176 '&lt;unused1274&gt;' is not marked as EOG\nload: control token: 257794 '&lt;unused1892&gt;' is not marked as EOG\nload: control token: 258647 '&lt;unused2745&gt;' is not marked as EOG\nload: control token: 260961 '&lt;unused5059&gt;' is not marked as EOG\nload: control token: 261889 '&lt;unused5987&gt;' is not marked as EOG\nload: control token: 260614 '&lt;unused4712&gt;' is not marked as EOG\nload: control token: 261167 '&lt;unused5265&gt;' is not marked as EOG\nload: control token: 262001 '&lt;unused6099&gt;' is not marked as EOG\nload: control token: 261003 '&lt;unused5101&gt;' is not marked as EOG\nload: control token: 257403 '&lt;unused1501&gt;' is not marked as EOG\nload: control token: 260469 '&lt;unused4567&gt;' is not marked as EOG\nload: control token: 257328 '&lt;unused1426&gt;' is not marked as EOG\nload: control token: 261732 '&lt;unused5830&gt;' is not marked as EOG\nload: control token: 257841 '&lt;unused1939&gt;' is not marked as EOG\nload: control token: 257589 '&lt;unused1687&gt;' is not marked as EOG\nload: control token: 261466 '&lt;unused5564&gt;' is not marked as EOG\nload: control token: 261624 '&lt;unused5722&gt;' is not marked as EOG\nload: control token: 256281 '&lt;unused379&gt;' is not marked as EOG\nload: control token: 256454 '&lt;unused552&gt;' is not marked as EOG\nload: control token: 256456 '&lt;unused554&gt;' is not marked as EOG\nload: control token: 256174 '&lt;unused272&gt;' is not marked as EOG\nload: control token: 261282 '&lt;unused5380&gt;' is not marked as EOG\nload: control token: 256714 '&lt;unused812&gt;' is not marked as EOG\nload: control token: 260191 '&lt;unused4289&gt;' is not marked as EOG\nload: control token: 259765 '&lt;unused3863&gt;' is not marked as EOG\nload: control token: 257635 '&lt;unused1733&gt;' is not marked as EOG\nload: control token: 259337 '&lt;unused3435&gt;' is not marked as EOG\nload: control token: 261525 '&lt;unused5623&gt;' is not marked as EOG\nload: control token: 260405 '&lt;unused4503&gt;' is not marked as EOG\nload: control token: 257670 '&lt;unused1768&gt;' is not marked as EOG\nload: control token: 259492 '&lt;unused3590&gt;' is not marked as EOG\nload: control token: 259387 '&lt;unused3485&gt;' is not marked as EOG\nload: control token: 261358 '&lt;unused5456&gt;' is not marked as EOG\nload: control token: 258585 '&lt;unused2683&gt;' is not marked as EOG\nload: control token: 258006 '&lt;unused2104&gt;' is not marked as EOG\nload: control token: 261681 '&lt;unused5779&gt;' is not marked as EOG\nload: control token: 257562 '&lt;unused1660&gt;' is not marked as EOG\nload: control token: 261297 '&lt;unused5395&gt;' is not marked as EOG\nload: control token: 258673 '&lt;unused2771&gt;' is not marked as EOG\nload: control token: 260220 '&lt;unused4318&gt;' is not marked as EOG\nload: control token: 257750 '&lt;unused1848&gt;' is not marked as EOG\nload: control token: 259989 '&lt;unused4087&gt;' is not marked as EOG\nload: control token: 260944 '&lt;unused5042&gt;' is not marked as EOG\nload: control token: 258464 '&lt;unused2562&gt;' is not marked as EOG\nload: control token: 261534 '&lt;unused5632&gt;' is not marked as EOG\nload: control token:     39 '&lt;unused33&gt;' is not marked as EOG\nload: control token: 256772 '&lt;unused870&gt;' is not marked as EOG\nload: control token: 259752 '&lt;unused3850&gt;' is not marked as EOG\nload: control token: 256967 '&lt;unused1065&gt;' is not marked as EOG\nload: control token: 256611 '&lt;unused709&gt;' is not marked as EOG\nload: control token: 260984 '&lt;unused5082&gt;' is not marked as EOG\nload: control token: 260260 '&lt;unused4358&gt;' is not marked as EOG\nload: control token: 257971 '&lt;unused2069&gt;' is not marked as EOG\nload: control token: 259161 '&lt;unused3259&gt;' is not marked as EOG\nload: control token: 256658 '&lt;unused756&gt;' is not marked as EOG\nload: control token: 256295 '&lt;unused393&gt;' is not marked as EOG\nload: control token: 258542 '&lt;unused2640&gt;' is not marked as EOG\nload: control token: 260630 '&lt;unused4728&gt;' is not marked as EOG\nload: control token: 257037 '&lt;unused1135&gt;' is not marked as EOG\nload: control token: 258105 '&lt;unused2203&gt;' is not marked as EOG\nload: control token: 259251 '&lt;unused3349&gt;' is not marked as EOG\nload: control token: 259903 '&lt;unused4001&gt;' is not marked as EOG\nload: control token: 260419 '&lt;unused4517&gt;' is not marked as EOG\nload: control token: 258553 '&lt;unused2651&gt;' is not marked as EOG\nload: control token: 260368 '&lt;unused4466&gt;' is not marked as EOG\nload: control token:     64 '&lt;unused58&gt;' is not marked as EOG\nload: control token: 260487 '&lt;unused4585&gt;' is not marked as EOG\nload: control token: 260836 '&lt;unused4934&gt;' is not marked as EOG\nload: control token: 257592 '&lt;unused1690&gt;' is not marked as EOG\nload: control token: 256802 '&lt;unused900&gt;' is not marked as EOG\nload: control token: 258405 '&lt;unused2503&gt;' is not marked as EOG\nload: control token: 256561 '&lt;unused659&gt;' is not marked as EOG\nload: control token: 259042 '&lt;unused3140&gt;' is not marked as EOG\nload: control token: 259033 '&lt;unused3131&gt;' is not marked as EOG\nload: control token: 259218 '&lt;unused3316&gt;' is not marked as EOG\nload: control token: 259365 '&lt;unused3463&gt;' is not marked as EOG\nload: control token: 260956 '&lt;unused5054&gt;' is not marked as EOG\nload: control token: 261971 '&lt;unused6069&gt;' is not marked as EOG\nload: control token: 259270 '&lt;unused3368&gt;' is not marked as EOG\nload: control token: 261784 '&lt;unused5882&gt;' is not marked as EOG\nload: control token: 257077 '&lt;unused1175&gt;' is not marked as EOG\nload: control token: 257245 '&lt;unused1343&gt;' is not marked as EOG\nload: control token: 256386 '&lt;unused484&gt;' is not marked as EOG\nload: control token: 260015 '&lt;unused4113&gt;' is not marked as EOG\nload: control token: 261480 '&lt;unused5578&gt;' is not marked as EOG\nload: control token: 258611 '&lt;unused2709&gt;' is not marked as EOG\nload: control token: 258545 '&lt;unused2643&gt;' is not marked as EOG\nload: control token: 257113 '&lt;unused1211&gt;' is not marked as EOG\nload: control token: 256595 '&lt;unused693&gt;' is not marked as EOG\nload: control token: 261245 '&lt;unused5343&gt;' is not marked as EOG\nload: control token: 261235 '&lt;unused5333&gt;' is not marked as EOG\nload: control token: 261996 '&lt;unused6094&gt;' is not marked as EOG\nload: control token: 259129 '&lt;unused3227&gt;' is not marked as EOG\nload: control token: 258218 '&lt;unused2316&gt;' is not marked as EOG\nload: control token: 258086 '&lt;unused2184&gt;' is not marked as EOG\nload: control token: 261750 '&lt;unused5848&gt;' is not marked as EOG\nload: control token: 261259 '&lt;unused5357&gt;' is not marked as EOG\nload: control token: 256784 '&lt;unused882&gt;' is not marked as EOG\nload: control token: 261763 '&lt;unused5861&gt;' is not marked as EOG\nload: control token: 256736 '&lt;unused834&gt;' is not marked as EOG\nload: control token: 260676 '&lt;unused4774&gt;' is not marked as EOG\nload: control token: 256158 '&lt;unused256&gt;' is not marked as EOG\nload: control token: 256765 '&lt;unused863&gt;' is not marked as EOG\nload: control token: 257285 '&lt;unused1383&gt;' is not marked as EOG\nload: control token: 260444 '&lt;unused4542&gt;' is not marked as EOG\nload: control token: 256686 '&lt;unused784&gt;' is not marked as EOG\nload: control token: 260679 '&lt;unused4777&gt;' is not marked as EOG\nload: control token: 257529 '&lt;unused1627&gt;' is not marked as EOG\nload: control token: 261598 '&lt;unused5696&gt;' is not marked as EOG\nload: control token: 261914 '&lt;unused6012&gt;' is not marked as EOG\nload: control token: 260367 '&lt;unused4465&gt;' is not marked as EOG\nload: control token: 261055 '&lt;unused5153&gt;' is not marked as EOG\nload: control token: 256680 '&lt;unused778&gt;' is not marked as EOG\nload: control token: 260396 '&lt;unused4494&gt;' is not marked as EOG\nload: control token: 259944 '&lt;unused4042&gt;' is not marked as EOG\nload: control token: 256302 '&lt;unused400&gt;' is not marked as EOG\nload: control token: 262106 '&lt;unused6204&gt;' is not marked as EOG\nload: control token: 262142 '&lt;unused6240&gt;' is not marked as EOG\nload: control token: 260067 '&lt;unused4165&gt;' is not marked as EOG\nload: control token: 260100 '&lt;unused4198&gt;' is not marked as EOG\nload: control token: 261498 '&lt;unused5596&gt;' is not marked as EOG\nload: control token: 259460 '&lt;unused3558&gt;' is not marked as EOG\nload: control token: 258227 '&lt;unused2325&gt;' is not marked as EOG\nload: control token: 256004 '&lt;unused102&gt;' is not marked as EOG\nload: control token: 262008 '&lt;unused6106&gt;' is not marked as EOG\nload: control token: 256358 '&lt;unused456&gt;' is not marked as EOG\nload: control token: 261777 '&lt;unused5875&gt;' is not marked as EOG\nload: control token: 260073 '&lt;unused4171&gt;' is not marked as EOG\nload: control token: 256357 '&lt;unused455&gt;' is not marked as EOG\nload: control token: 259564 '&lt;unused3662&gt;' is not marked as EOG\nload: control token: 258560 '&lt;unused2658&gt;' is not marked as EOG\nload: control token: 260852 '&lt;unused4950&gt;' is not marked as EOG\nload: control token: 257302 '&lt;unused1400&gt;' is not marked as EOG\nload: control token: 261007 '&lt;unused5105&gt;' is not marked as EOG\nload: control token: 262100 '&lt;unused6198&gt;' is not marked as EOG\nload: control token: 259116 '&lt;unused3214&gt;' is not marked as EOG\nload: control token: 260927 '&lt;unused5025&gt;' is not marked as EOG\nload: control token: 259864 '&lt;unused3962&gt;' is not marked as EOG\nload: control token: 261921 '&lt;unused6019&gt;' is not marked as EOG\nload: control token: 257196 '&lt;unused1294&gt;' is not marked as EOG\nload: control token: 261325 '&lt;unused5423&gt;' is not marked as EOG\nload: control token: 257646 '&lt;unused1744&gt;' is not marked as EOG\nload: control token:     16 '&lt;unused10&gt;' is not marked as EOG\nload: control token: 260187 '&lt;unused4285&gt;' is not marked as EOG\nload: control token: 256903 '&lt;unused1001&gt;' is not marked as EOG\nload: control token: 257970 '&lt;unused2068&gt;' is not marked as EOG\nload: control token: 261987 '&lt;unused6085&gt;' is not marked as EOG\nload: control token: 261378 '&lt;unused5476&gt;' is not marked as EOG\nload: control token: 258805 '&lt;unused2903&gt;' is not marked as EOG\nload: control token: 262054 '&lt;unused6152&gt;' is not marked as EOG\nload: control token: 256529 '&lt;unused627&gt;' is not marked as EOG\nload: control token: 261596 '&lt;unused5694&gt;' is not marked as EOG\nload: control token: 261275 '&lt;unused5373&gt;' is not marked as EOG\nload: control token: 260738 '&lt;unused4836&gt;' is not marked as EOG\nload: control token: 261068 '&lt;unused5166&gt;' is not marked as EOG\nload: control token: 259110 '&lt;unused3208&gt;' is not marked as EOG\nload: control token: 259803 '&lt;unused3901&gt;' is not marked as EOG\nload: control token: 261174 '&lt;unused5272&gt;' is not marked as EOG\nload: control token:     34 '&lt;unused28&gt;' is not marked as EOG\nload: control token: 261293 '&lt;unused5391&gt;' is not marked as EOG\nload: control token: 260047 '&lt;unused4145&gt;' is not marked as EOG\nload: control token: 256090 '&lt;unused188&gt;' is not marked as EOG\nload: control token:     55 '&lt;unused49&gt;' is not marked as EOG\nload: control token: 258356 '&lt;unused2454&gt;' is not marked as EOG\nload: control token: 257032 '&lt;unused1130&gt;' is not marked as EOG\nload: control token: 259808 '&lt;unused3906&gt;' is not marked as EOG\nload: control token: 257073 '&lt;unused1171&gt;' is not marked as EOG\nload: control token: 256377 '&lt;unused475&gt;' is not marked as EOG\nload: control token: 261034 '&lt;unused5132&gt;' is not marked as EOG\nload: control token: 260898 '&lt;unused4996&gt;' is not marked as EOG\nload: control token: 259046 '&lt;unused3144&gt;' is not marked as EOG\nload: control token: 256641 '&lt;unused739&gt;' is not marked as EOG\nload: control token: 261735 '&lt;unused5833&gt;' is not marked as EOG\nload: control token: 257195 '&lt;unused1293&gt;' is not marked as EOG\nload: control token: 256461 '&lt;unused559&gt;' is not marked as EOG\nload: control token: 258925 '&lt;unused3023&gt;' is not marked as EOG\nload: control token: 256857 '&lt;unused955&gt;' is not marked as EOG\nload: control token: 257963 '&lt;unused2061&gt;' is not marked as EOG\nload: control token: 260884 '&lt;unused4982&gt;' is not marked as EOG\nload: control token: 257152 '&lt;unused1250&gt;' is not marked as EOG\nload: control token: 258989 '&lt;unused3087&gt;' is not marked as EOG\nload: control token: 256312 '&lt;unused410&gt;' is not marked as EOG\nload: control token: 260875 '&lt;unused4973&gt;' is not marked as EOG\nload: control token: 259112 '&lt;unused3210&gt;' is not marked as EOG\nload: control token: 256102 '&lt;unused200&gt;' is not marked as EOG\nload: control token: 262021 '&lt;unused6119&gt;' is not marked as EOG\nload: control token: 257737 '&lt;unused1835&gt;' is not marked as EOG\nload: control token: 261994 '&lt;unused6092&gt;' is not marked as EOG\nload: control token: 261459 '&lt;unused5557&gt;' is not marked as EOG\nload: control token: 257189 '&lt;unused1287&gt;' is not marked as EOG\nload: control token: 256767 '&lt;unused865&gt;' is not marked as EOG\nload: control token: 261113 '&lt;unused5211&gt;' is not marked as EOG\nload: control token: 257507 '&lt;unused1605&gt;' is not marked as EOG\nload: control token: 257355 '&lt;unused1453&gt;' is not marked as EOG\nload: control token: 256403 '&lt;unused501&gt;' is not marked as EOG\nload: control token: 257739 '&lt;unused1837&gt;' is not marked as EOG\nload: control token: 261893 '&lt;unused5991&gt;' is not marked as EOG\nload: control token: 259276 '&lt;unused3374&gt;' is not marked as EOG\nload: control token: 259018 '&lt;unused3116&gt;' is not marked as EOG\nload: control token: 258621 '&lt;unused2719&gt;' is not marked as EOG\nload: control token: 256564 '&lt;unused662&gt;' is not marked as EOG\nload: control token: 260749 '&lt;unused4847&gt;' is not marked as EOG\nload: control token: 258938 '&lt;unused3036&gt;' is not marked as EOG\nload: control token: 256034 '&lt;unused132&gt;' is not marked as EOG\nload: control token: 260434 '&lt;unused4532&gt;' is not marked as EOG\nload: control token: 259059 '&lt;unused3157&gt;' is not marked as EOG\nload: control token: 260277 '&lt;unused4375&gt;' is not marked as EOG\nload: control token: 261717 '&lt;unused5815&gt;' is not marked as EOG\nload: control token: 260323 '&lt;unused4421&gt;' is not marked as EOG\nload: control token: 258160 '&lt;unused2258&gt;' is not marked as EOG\nload: control token: 258993 '&lt;unused3091&gt;' is not marked as EOG\nload: control token: 259250 '&lt;unused3348&gt;' is not marked as EOG\nload: control token: 261238 '&lt;unused5336&gt;' is not marked as EOG\nload: control token: 260638 '&lt;unused4736&gt;' is not marked as EOG\nload: control token: 259307 '&lt;unused3405&gt;' is not marked as EOG\nload: control token: 260058 '&lt;unused4156&gt;' is not marked as EOG\nload: control token: 259952 '&lt;unused4050&gt;' is not marked as EOG\nload: control token: 261411 '&lt;unused5509&gt;' is not marked as EOG\nload: control token: 260983 '&lt;unused5081&gt;' is not marked as EOG\nload: control token: 259695 '&lt;unused3793&gt;' is not marked as EOG\nload: control token: 257752 '&lt;unused1850&gt;' is not marked as EOG\nload: control token: 256843 '&lt;unused941&gt;' is not marked as EOG\nload: control token: 261561 '&lt;unused5659&gt;' is not marked as EOG\nload: control token: 259962 '&lt;unused4060&gt;' is not marked as EOG\nload: control token:    103 '&lt;unused97&gt;' is not marked as EOG\nload: control token: 260063 '&lt;unused4161&gt;' is not marked as EOG\nload: control token: 261567 '&lt;unused5665&gt;' is not marked as EOG\nload: control token: 256750 '&lt;unused848&gt;' is not marked as EOG\nload: control token: 256266 '&lt;unused364&gt;' is not marked as EOG\nload: control token: 257033 '&lt;unused1131&gt;' is not marked as EOG\nload: control token: 258907 '&lt;unused3005&gt;' is not marked as EOG\nload: control token: 256050 '&lt;unused148&gt;' is not marked as EOG\nload: control token: 261808 '&lt;unused5906&gt;' is not marked as EOG\nload: control token: 259139 '&lt;unused3237&gt;' is not marked as EOG\nload: control token: 257937 '&lt;unused2035&gt;' is not marked as EOG\nload: control token: 261096 '&lt;unused5194&gt;' is not marked as EOG\nload: control token: 259914 '&lt;unused4012&gt;' is not marked as EOG\nload: control token: 256346 '&lt;unused444&gt;' is not marked as EOG\nload: control token: 260891 '&lt;unused4989&gt;' is not marked as EOG\nload: control token: 260389 '&lt;unused4487&gt;' is not marked as EOG\nload: control token: 257638 '&lt;unused1736&gt;' is not marked as EOG\nload: control token: 258867 '&lt;unused2965&gt;' is not marked as EOG\nload: control token: 256258 '&lt;unused356&gt;' is not marked as EOG\nload: control token: 261334 '&lt;unused5432&gt;' is not marked as EOG\nload: control token: 257342 '&lt;unused1440&gt;' is not marked as EOG\nload: control token: 259729 '&lt;unused3827&gt;' is not marked as EOG\nload: control token: 259916 '&lt;unused4014&gt;' is not marked as EOG\nload: control token:     27 '&lt;unused21&gt;' is not marked as EOG\nload: control token: 259242 '&lt;unused3340&gt;' is not marked as EOG\nload: control token: 256884 '&lt;unused982&gt;' is not marked as EOG\nload: control token: 256705 '&lt;unused803&gt;' is not marked as EOG\nload: control token: 261454 '&lt;unused5552&gt;' is not marked as EOG\nload: control token: 256721 '&lt;unused819&gt;' is not marked as EOG\nload: control token: 262036 '&lt;unused6134&gt;' is not marked as EOG\nload: control token: 261031 '&lt;unused5129&gt;' is not marked as EOG\nload: control token: 258374 '&lt;unused2472&gt;' is not marked as EOG\nload: control token: 258927 '&lt;unused3025&gt;' is not marked as EOG\nload: control token: 258101 '&lt;unused2199&gt;' is not marked as EOG\nload: control token: 258578 '&lt;unused2676&gt;' is not marked as EOG\nload: control token: 261699 '&lt;unused5797&gt;' is not marked as EOG\nload: control token: 257244 '&lt;unused1342&gt;' is not marked as EOG\nload: control token: 259841 '&lt;unused3939&gt;' is not marked as EOG\nload: control token: 256141 '&lt;unused239&gt;' is not marked as EOG\nload: control token: 261169 '&lt;unused5267&gt;' is not marked as EOG\nload: control token: 259167 '&lt;unused3265&gt;' is not marked as EOG\nload: control token: 257065 '&lt;unused1163&gt;' is not marked as EOG\nload: control token: 260340 '&lt;unused4438&gt;' is not marked as EOG\nload: control token: 261114 '&lt;unused5212&gt;' is not marked as EOG\nload: control token: 257040 '&lt;unused1138&gt;' is not marked as EOG\nload: control token: 258159 '&lt;unused2257&gt;' is not marked as EOG\nload: control token: 261332 '&lt;unused5430&gt;' is not marked as EOG\nload: control token: 259588 '&lt;unused3686&gt;' is not marked as EOG\nload: control token: 259735 '&lt;unused3833&gt;' is not marked as EOG\nload: control token: 259145 '&lt;unused3243&gt;' is not marked as EOG\nload: control token: 258163 '&lt;unused2261&gt;' is not marked as EOG\nload: control token: 256975 '&lt;unused1073&gt;' is not marked as EOG\nload: control token: 259760 '&lt;unused3858&gt;' is not marked as EOG\nload: control token: 259705 '&lt;unused3803&gt;' is not marked as EOG\nload: control token: 259424 '&lt;unused3522&gt;' is not marked as EOG\nload: control token:     69 '&lt;unused63&gt;' is not marked as EOG\nload: control token: 259405 '&lt;unused3503&gt;' is not marked as EOG\nload: control token: 259476 '&lt;unused3574&gt;' is not marked as EOG\nload: control token:     13 '&lt;unused7&gt;' is not marked as EOG\nload: control token: 259794 '&lt;unused3892&gt;' is not marked as EOG\nload: control token: 260056 '&lt;unused4154&gt;' is not marked as EOG\nload: control token: 260939 '&lt;unused5037&gt;' is not marked as EOG\nload: control token: 256300 '&lt;unused398&gt;' is not marked as EOG\nload: control token: 258505 '&lt;unused2603&gt;' is not marked as EOG\nload: control token: 260790 '&lt;unused4888&gt;' is not marked as EOG\nload: control token: 256821 '&lt;unused919&gt;' is not marked as EOG\nload: control token: 260775 '&lt;unused4873&gt;' is not marked as EOG\nload: control token: 259213 '&lt;unused3311&gt;' is not marked as EOG\nload: control token: 261147 '&lt;unused5245&gt;' is not marked as EOG\nload: control token: 260061 '&lt;unused4159&gt;' is not marked as EOG\nload: control token: 260113 '&lt;unused4211&gt;' is not marked as EOG\nload: control token: 259845 '&lt;unused3943&gt;' is not marked as EOG\nload: control token: 259371 '&lt;unused3469&gt;' is not marked as EOG\nload: control token: 260037 '&lt;unused4135&gt;' is not marked as EOG\nload: control token: 257882 '&lt;unused1980&gt;' is not marked as EOG\nload: control token:      3 '&lt;unk&gt;' is not marked as EOG\nload: control token: 256600 '&lt;unused698&gt;' is not marked as EOG\nload: control token: 256722 '&lt;unused820&gt;' is not marked as EOG\nload: control token: 258924 '&lt;unused3022&gt;' is not marked as EOG\nload: control token: 259024 '&lt;unused3122&gt;' is not marked as EOG\nload: control token: 261433 '&lt;unused5531&gt;' is not marked as EOG\nload: control token: 257699 '&lt;unused1797&gt;' is not marked as EOG\nload: control token:     93 '&lt;unused87&gt;' is not marked as EOG\nload: control token: 261458 '&lt;unused5556&gt;' is not marked as EOG\nload: control token: 259937 '&lt;unused4035&gt;' is not marked as EOG\nload: control token: 260906 '&lt;unused5004&gt;' is not marked as EOG\nload: control token: 261783 '&lt;unused5881&gt;' is not marked as EOG\nload: control token: 262143 '&lt;unused6241&gt;' is not marked as EOG\nload: control token: 257614 '&lt;unused1712&gt;' is not marked as EOG\nload: control token: 260924 '&lt;unused5022&gt;' is not marked as EOG\nload: control token:     17 '&lt;unused11&gt;' is not marked as EOG\nload: control token: 256487 '&lt;unused585&gt;' is not marked as EOG\nload: control token: 256652 '&lt;unused750&gt;' is not marked as EOG\nload: control token: 261184 '&lt;unused5282&gt;' is not marked as EOG\nload: control token: 257842 '&lt;unused1940&gt;' is not marked as EOG\nload: control token: 258324 '&lt;unused2422&gt;' is not marked as EOG\nload: control token: 258234 '&lt;unused2332&gt;' is not marked as EOG\nload: control token: 256248 '&lt;unused346&gt;' is not marked as EOG\nload: control token: 261648 '&lt;unused5746&gt;' is not marked as EOG\nload: control token: 259694 '&lt;unused3792&gt;' is not marked as EOG\nload: control token: 257934 '&lt;unused2032&gt;' is not marked as EOG\nload: control token: 261863 '&lt;unused5961&gt;' is not marked as EOG\nload: control token: 256789 '&lt;unused887&gt;' is not marked as EOG\nload: control token:     48 '&lt;unused42&gt;' is not marked as EOG\nload: control token: 257950 '&lt;unused2048&gt;' is not marked as EOG\nload: control token: 261728 '&lt;unused5826&gt;' is not marked as EOG\nload: control token: 260313 '&lt;unused4411&gt;' is not marked as EOG\nload: control token: 258711 '&lt;unused2809&gt;' is not marked as EOG\nload: control token: 258896 '&lt;unused2994&gt;' is not marked as EOG\nload: control token: 261707 '&lt;unused5805&gt;' is not marked as EOG\nload: control token: 258191 '&lt;unused2289&gt;' is not marked as EOG\nload: control token: 259179 '&lt;unused3277&gt;' is not marked as EOG\nload: control token: 261254 '&lt;unused5352&gt;' is not marked as EOG\nload: control token: 259408 '&lt;unused3506&gt;' is not marked as EOG\nload: control token: 257055 '&lt;unused1153&gt;' is not marked as EOG\nload: control token: 259757 '&lt;unused3855&gt;' is not marked as EOG\nload: control token: 256195 '&lt;unused293&gt;' is not marked as EOG\nload: control token: 259745 '&lt;unused3843&gt;' is not marked as EOG\nload: control token: 258584 '&lt;unused2682&gt;' is not marked as EOG\nload: control token: 257180 '&lt;unused1278&gt;' is not marked as EOG\nload: control token: 259310 '&lt;unused3408&gt;' is not marked as EOG\nload: control token: 261149 '&lt;unused5247&gt;' is not marked as EOG\nload: control token: 256335 '&lt;unused433&gt;' is not marked as EOG\nload: control token: 257239 '&lt;unused1337&gt;' is not marked as EOG\nload: control token: 261389 '&lt;unused5487&gt;' is not marked as EOG\nload: control token: 256279 '&lt;unused377&gt;' is not marked as EOG\nload: control token: 258769 '&lt;unused2867&gt;' is not marked as EOG\nload: control token: 258335 '&lt;unused2433&gt;' is not marked as EOG\nload: control token: 257535 '&lt;unused1633&gt;' is not marked as EOG\nload: control token: 260816 '&lt;unused4914&gt;' is not marked as EOG\nload: control token: 257463 '&lt;unused1561&gt;' is not marked as EOG\nload: control token: 258971 '&lt;unused3069&gt;' is not marked as EOG\nload: control token: 257121 '&lt;unused1219&gt;' is not marked as EOG\nload: control token: 256478 '&lt;unused576&gt;' is not marked as EOG\nload: control token: 261713 '&lt;unused5811&gt;' is not marked as EOG\nload: control token: 258140 '&lt;unused2238&gt;' is not marked as EOG\nload: control token: 259204 '&lt;unused3302&gt;' is not marked as EOG\nload: control token: 261709 '&lt;unused5807&gt;' is not marked as EOG\nload: control token: 257395 '&lt;unused1493&gt;' is not marked as EOG\nload: control token: 258735 '&lt;unused2833&gt;' is not marked as EOG\nload: control token: 260776 '&lt;unused4874&gt;' is not marked as EOG\nload: control token: 262010 '&lt;unused6108&gt;' is not marked as EOG\nload: control token: 258421 '&lt;unused2519&gt;' is not marked as EOG\nload: control token: 259450 '&lt;unused3548&gt;' is not marked as EOG\nload: control token: 259843 '&lt;unused3941&gt;' is not marked as EOG\nload: control token: 260765 '&lt;unused4863&gt;' is not marked as EOG\nload: control token: 256543 '&lt;unused641&gt;' is not marked as EOG\nload: control token: 262024 '&lt;unused6122&gt;' is not marked as EOG\nload: control token: 261695 '&lt;unused5793&gt;' is not marked as EOG\nload: control token: 256676 '&lt;unused774&gt;' is not marked as EOG\nload: control token: 256462 '&lt;unused560&gt;' is not marked as EOG\nload: control token:     77 '&lt;unused71&gt;' is not marked as EOG\nload: control token: 257268 '&lt;unused1366&gt;' is not marked as EOG\nload: control token: 257510 '&lt;unused1608&gt;' is not marked as EOG\nload: control token: 256758 '&lt;unused856&gt;' is not marked as EOG\nload: control token: 256747 '&lt;unused845&gt;' is not marked as EOG\nload: control token: 257762 '&lt;unused1860&gt;' is not marked as EOG\nload: control token: 260793 '&lt;unused4891&gt;' is not marked as EOG\nload: control token: 258285 '&lt;unused2383&gt;' is not marked as EOG\nload: control token: 257930 '&lt;unused2028&gt;' is not marked as EOG\nload: control token: 259671 '&lt;unused3769&gt;' is not marked as EOG\nload: control token: 261289 '&lt;unused5387&gt;' is not marked as EOG\nload: control token: 257778 '&lt;unused1876&gt;' is not marked as EOG\nload: control token: 261538 '&lt;unused5636&gt;' is not marked as EOG\nload: control token: 257784 '&lt;unused1882&gt;' is not marked as EOG\nload: control token: 259105 '&lt;unused3203&gt;' is not marked as EOG\nload: control token: 261734 '&lt;unused5832&gt;' is not marked as EOG\nload: control token: 259485 '&lt;unused3583&gt;' is not marked as EOG\nload: control token: 258666 '&lt;unused2764&gt;' is not marked as EOG\nload: control token: 258863 '&lt;unused2961&gt;' is not marked as EOG\nload: control token: 260103 '&lt;unused4201&gt;' is not marked as EOG\nload: control token: 259580 '&lt;unused3678&gt;' is not marked as EOG\nload: control token: 259718 '&lt;unused3816&gt;' is not marked as EOG\nload: control token: 260273 '&lt;unused4371&gt;' is not marked as EOG\nload: control token: 256718 '&lt;unused816&gt;' is not marked as EOG\nload: control token: 257577 '&lt;unused1675&gt;' is not marked as EOG\nload: control token: 258268 '&lt;unused2366&gt;' is not marked as EOG\nload: control token: 260493 '&lt;unused4591&gt;' is not marked as EOG\nload: control token: 259778 '&lt;unused3876&gt;' is not marked as EOG\nload: control token: 256008 '&lt;unused106&gt;' is not marked as EOG\nload: control token: 261129 '&lt;unused5227&gt;' is not marked as EOG\nload: control token: 256402 '&lt;unused500&gt;' is not marked as EOG\nload: control token: 260426 '&lt;unused4524&gt;' is not marked as EOG\nload: control token: 260062 '&lt;unused4160&gt;' is not marked as EOG\nload: control token: 258397 '&lt;unused2495&gt;' is not marked as EOG\nload: control token:     11 '&lt;unused5&gt;' is not marked as EOG\nload: control token: 260549 '&lt;unused4647&gt;' is not marked as EOG\nload: control token: 256433 '&lt;unused531&gt;' is not marked as EOG\nload: control token: 260237 '&lt;unused4335&gt;' is not marked as EOG\nload: control token: 256743 '&lt;unused841&gt;' is not marked as EOG\nload: control token: 259032 '&lt;unused3130&gt;' is not marked as EOG\nload: control token: 259132 '&lt;unused3230&gt;' is not marked as EOG\nload: control token: 261923 '&lt;unused6021&gt;' is not marked as EOG\nload: control token: 258534 '&lt;unused2632&gt;' is not marked as EOG\nload: control token: 261200 '&lt;unused5298&gt;' is not marked as EOG\nload: control token: 258170 '&lt;unused2268&gt;' is not marked as EOG\nload: control token: 257852 '&lt;unused1950&gt;' is not marked as EOG\nload: control token: 259601 '&lt;unused3699&gt;' is not marked as EOG\nload: control token: 260782 '&lt;unused4880&gt;' is not marked as EOG\nload: control token: 260268 '&lt;unused4366&gt;' is not marked as EOG\nload: control token: 258210 '&lt;unused2308&gt;' is not marked as EOG\nload: control token: 259877 '&lt;unused3975&gt;' is not marked as EOG\nload: control token: 260122 '&lt;unused4220&gt;' is not marked as EOG\nload: control token: 261880 '&lt;unused5978&gt;' is not marked as EOG\nload: control token: 256236 '&lt;unused334&gt;' is not marked as EOG\nload: control token: 256783 '&lt;unused881&gt;' is not marked as EOG\nload: control token: 259886 '&lt;unused3984&gt;' is not marked as EOG\nload: control token: 259113 '&lt;unused3211&gt;' is not marked as EOG\nload: control token: 257044 '&lt;unused1142&gt;' is not marked as EOG\nload: control token: 258637 '&lt;unused2735&gt;' is not marked as EOG\nload: control token: 256120 '&lt;unused218&gt;' is not marked as EOG\nload: control token: 259148 '&lt;unused3246&gt;' is not marked as EOG\nload: control token: 261213 '&lt;unused5311&gt;' is not marked as EOG\nload: control token:    102 '&lt;unused96&gt;' is not marked as EOG\nload: control token: 258379 '&lt;unused2477&gt;' is not marked as EOG\nload: control token: 258957 '&lt;unused3055&gt;' is not marked as EOG\nload: control token: 256425 '&lt;unused523&gt;' is not marked as EOG\nload: control token: 258107 '&lt;unused2205&gt;' is not marked as EOG\nload: control token: 259959 '&lt;unused4057&gt;' is not marked as EOG\nload: control token: 257305 '&lt;unused1403&gt;' is not marked as EOG\nload: control token: 256972 '&lt;unused1070&gt;' is not marked as EOG\nload: control token: 262006 '&lt;unused6104&gt;' is not marked as EOG\nload: control token: 256492 '&lt;unused590&gt;' is not marked as EOG\nload: control token: 259798 '&lt;unused3896&gt;' is not marked as EOG\nload: control token: 259605 '&lt;unused3703&gt;' is not marked as EOG\nload: control token: 261754 '&lt;unused5852&gt;' is not marked as EOG\nload: control token: 256679 '&lt;unused777&gt;' is not marked as EOG\nload: control token: 256059 '&lt;unused157&gt;' is not marked as EOG\nload: control token: 260908 '&lt;unused5006&gt;' is not marked as EOG\nload: control token: 257998 '&lt;unused2096&gt;' is not marked as EOG\nload: control token: 261870 '&lt;unused5968&gt;' is not marked as EOG\nload: control token: 258174 '&lt;unused2272&gt;' is not marked as EOG\nload: control token: 257667 '&lt;unused1765&gt;' is not marked as EOG\nload: control token: 256267 '&lt;unused365&gt;' is not marked as EOG\nload: control token: 261291 '&lt;unused5389&gt;' is not marked as EOG\nload: control token: 258959 '&lt;unused3057&gt;' is not marked as EOG\nload: control token: 260595 '&lt;unused4693&gt;' is not marked as EOG\nload: control token: 261185 '&lt;unused5283&gt;' is not marked as EOG\nload: control token: 260503 '&lt;unused4601&gt;' is not marked as EOG\nload: control token: 257709 '&lt;unused1807&gt;' is not marked as EOG\nload: control token: 257375 '&lt;unused1473&gt;' is not marked as EOG\nload: control token: 259812 '&lt;unused3910&gt;' is not marked as EOG\nload: control token: 260169 '&lt;unused4267&gt;' is not marked as EOG\nload: control token: 260794 '&lt;unused4892&gt;' is not marked as EOG\nload: control token: 260997 '&lt;unused5095&gt;' is not marked as EOG\nload: control token: 260848 '&lt;unused4946&gt;' is not marked as EOG\nload: control token: 261668 '&lt;unused5766&gt;' is not marked as EOG\nload: control token: 261842 '&lt;unused5940&gt;' is not marked as EOG\nload: control token: 256328 '&lt;unused426&gt;' is not marked as EOG\nload: control token: 260612 '&lt;unused4710&gt;' is not marked as EOG\nload: control token: 259740 '&lt;unused3838&gt;' is not marked as EOG\nload: control token: 256124 '&lt;unused222&gt;' is not marked as EOG\nload: control token: 260507 '&lt;unused4605&gt;' is not marked as EOG\nload: control token: 257219 '&lt;unused1317&gt;' is not marked as EOG\nload: control token: 258259 '&lt;unused2357&gt;' is not marked as EOG\nload: control token: 258923 '&lt;unused3021&gt;' is not marked as EOG\nload: control token: 256054 '&lt;unused152&gt;' is not marked as EOG\nload: control token:     37 '&lt;unused31&gt;' is not marked as EOG\nload: control token: 261285 '&lt;unused5383&gt;' is not marked as EOG\nload: control token: 256339 '&lt;unused437&gt;' is not marked as EOG\nload: control token: 256940 '&lt;unused1038&gt;' is not marked as EOG\nload: control token: 258495 '&lt;unused2593&gt;' is not marked as EOG\nload: control token: 259761 '&lt;unused3859&gt;' is not marked as EOG\nload: control token: 261355 '&lt;unused5453&gt;' is not marked as EOG\nload: control token: 259165 '&lt;unused3263&gt;' is not marked as EOG\nload: control token: 257397 '&lt;unused1495&gt;' is not marked as EOG\nload: control token: 259002 '&lt;unused3100&gt;' is not marked as EOG\nload: control token: 257323 '&lt;unused1421&gt;' is not marked as EOG\nload: control token: 259917 '&lt;unused4015&gt;' is not marked as EOG\nload: control token: 260381 '&lt;unused4479&gt;' is not marked as EOG\nload: control token: 259703 '&lt;unused3801&gt;' is not marked as EOG\nload: control token: 259719 '&lt;unused3817&gt;' is not marked as EOG\nload: control token: 261196 '&lt;unused5294&gt;' is not marked as EOG\nload: control token: 259188 '&lt;unused3286&gt;' is not marked as EOG\nload: control token: 257763 '&lt;unused1861&gt;' is not marked as EOG\nload: control token: 259178 '&lt;unused3276&gt;' is not marked as EOG\nload: control token: 261120 '&lt;unused5218&gt;' is not marked as EOG\nload: control token: 258172 '&lt;unused2270&gt;' is not marked as EOG\nload: control token: 258438 '&lt;unused2536&gt;' is not marked as EOG\nload: control token: 256612 '&lt;unused710&gt;' is not marked as EOG\nload: control token: 257370 '&lt;unused1468&gt;' is not marked as EOG\nload: control token: 259300 '&lt;unused3398&gt;' is not marked as EOG\nload: control token: 261509 '&lt;unused5607&gt;' is not marked as EOG\nload: control token: 260296 '&lt;unused4394&gt;' is not marked as EOG\nload: control token: 260689 '&lt;unused4787&gt;' is not marked as EOG\nload: control token: 256481 '&lt;unused579&gt;' is not marked as EOG\nload: control token: 259101 '&lt;unused3199&gt;' is not marked as EOG\nload: control token: 256494 '&lt;unused592&gt;' is not marked as EOG\nload: control token: 256963 '&lt;unused1061&gt;' is not marked as EOG\nload: control token: 260281 '&lt;unused4379&gt;' is not marked as EOG\nload: control token: 257127 '&lt;unused1225&gt;' is not marked as EOG\nload: control token: 259818 '&lt;unused3916&gt;' is not marked as EOG\nload: control token: 256318 '&lt;unused416&gt;' is not marked as EOG\nload: control token: 258057 '&lt;unused2155&gt;' is not marked as EOG\nload: control token:     10 '&lt;unused4&gt;' is not marked as EOG\nload: control token: 257421 '&lt;unused1519&gt;' is not marked as EOG\nload: control token: 257925 '&lt;unused2023&gt;' is not marked as EOG\nload: control token: 260516 '&lt;unused4614&gt;' is not marked as EOG\nload: control token: 257991 '&lt;unused2089&gt;' is not marked as EOG\nload: control token: 258619 '&lt;unused2717&gt;' is not marked as EOG\nload: control token: 261261 '&lt;unused5359&gt;' is not marked as EOG\nload: control token: 256871 '&lt;unused969&gt;' is not marked as EOG\nload: control token: 256799 '&lt;unused897&gt;' is not marked as EOG\nload: control token: 260310 '&lt;unused4408&gt;' is not marked as EOG\nload: control token: 260183 '&lt;unused4281&gt;' is not marked as EOG\nload: control token: 257443 '&lt;unused1541&gt;' is not marked as EOG\nload: control token: 257321 '&lt;unused1419&gt;' is not marked as EOG\nload: control token: 259094 '&lt;unused3192&gt;' is not marked as EOG\nload: control token: 260506 '&lt;unused4604&gt;' is not marked as EOG\nload: control token: 257729 '&lt;unused1827&gt;' is not marked as EOG\nload: control token: 257865 '&lt;unused1963&gt;' is not marked as EOG\nload: control token: 256934 '&lt;unused1032&gt;' is not marked as EOG\nload: control token: 257046 '&lt;unused1144&gt;' is not marked as EOG\nload: control token:     68 '&lt;unused62&gt;' is not marked as EOG\nload: control token: 256365 '&lt;unused463&gt;' is not marked as EOG\nload: control token: 259144 '&lt;unused3242&gt;' is not marked as EOG\nload: control token: 260457 '&lt;unused4555&gt;' is not marked as EOG\nload: control token: 261826 '&lt;unused5924&gt;' is not marked as EOG\nload: control token: 261097 '&lt;unused5195&gt;' is not marked as EOG\nload: control token: 258354 '&lt;unused2452&gt;' is not marked as EOG\nload: control token: 261051 '&lt;unused5149&gt;' is not marked as EOG\nload: control token: 257870 '&lt;unused1968&gt;' is not marked as EOG\nload: control token: 261240 '&lt;unused5338&gt;' is not marked as EOG\nload: control token: 260123 '&lt;unused4221&gt;' is not marked as EOG\nload: control token: 259716 '&lt;unused3814&gt;' is not marked as EOG\nload: control token: 258361 '&lt;unused2459&gt;' is not marked as EOG\nload: control token: 258618 '&lt;unused2716&gt;' is not marked as EOG\nload: control token: 261013 '&lt;unused5111&gt;' is not marked as EOG\nload: control token: 259779 '&lt;unused3877&gt;' is not marked as EOG\nload: control token: 260231 '&lt;unused4329&gt;' is not marked as EOG\nload: control token: 261671 '&lt;unused5769&gt;' is not marked as EOG\nload: control token: 259617 '&lt;unused3715&gt;' is not marked as EOG\nload: control token: 260710 '&lt;unused4808&gt;' is not marked as EOG\nload: control token: 258340 '&lt;unused2438&gt;' is not marked as EOG\nload: control token: 258845 '&lt;unused2943&gt;' is not marked as EOG\nload: control token: 256601 '&lt;unused699&gt;' is not marked as EOG\nload: control token: 260737 '&lt;unused4835&gt;' is not marked as EOG\nload: control token: 259549 '&lt;unused3647&gt;' is not marked as EOG\nload: control token: 261280 '&lt;unused5378&gt;' is not marked as EOG\nload: control token: 256562 '&lt;unused660&gt;' is not marked as EOG\nload: control token: 260586 '&lt;unused4684&gt;' is not marked as EOG\nload: control token: 257871 '&lt;unused1969&gt;' is not marked as EOG\nload: control token: 260309 '&lt;unused4407&gt;' is not marked as EOG\nload: control token: 256560 '&lt;unused658&gt;' is not marked as EOG\nload: control token: 256057 '&lt;unused155&gt;' is not marked as EOG\nload: control token: 261318 '&lt;unused5416&gt;' is not marked as EOG\nload: control token: 258636 '&lt;unused2734&gt;' is not marked as EOG\nload: control token: 258871 '&lt;unused2969&gt;' is not marked as EOG\nload: control token: 261556 '&lt;unused5654&gt;' is not marked as EOG\nload: control token: 260950 '&lt;unused5048&gt;' is not marked as EOG\nload: control token: 256167 '&lt;unused265&gt;' is not marked as EOG\nload: control token: 262071 '&lt;unused6169&gt;' is not marked as EOG\nload: control token: 258891 '&lt;unused2989&gt;' is not marked as EOG\nload: control token: 258151 '&lt;unused2249&gt;' is not marked as EOG\nload: control token: 258036 '&lt;unused2134&gt;' is not marked as EOG\nload: control token: 261644 '&lt;unused5742&gt;' is not marked as EOG\nload: control token: 256002 '&lt;unused100&gt;' is not marked as EOG\nload: control token: 260245 '&lt;unused4343&gt;' is not marked as EOG\nload: control token: 260703 '&lt;unused4801&gt;' is not marked as EOG\nload: control token: 257961 '&lt;unused2059&gt;' is not marked as EOG\nload: control token: 257217 '&lt;unused1315&gt;' is not marked as EOG\nload: control token: 260918 '&lt;unused5016&gt;' is not marked as EOG\nload: control token: 258507 '&lt;unused2605&gt;' is not marked as EOG\nload: control token: 256298 '&lt;unused396&gt;' is not marked as EOG\nload: control token:     90 '&lt;unused84&gt;' is not marked as EOG\nload: control token: 256329 '&lt;unused427&gt;' is not marked as EOG\nload: control token: 256725 '&lt;unused823&gt;' is not marked as EOG\nload: control token: 256091 '&lt;unused189&gt;' is not marked as EOG\nload: control token: 260208 '&lt;unused4306&gt;' is not marked as EOG\nload: control token: 257454 '&lt;unused1552&gt;' is not marked as EOG\nload: control token: 260899 '&lt;unused4997&gt;' is not marked as EOG\nload: control token: 256558 '&lt;unused656&gt;' is not marked as EOG\nload: control token: 261907 '&lt;unused6005&gt;' is not marked as EOG\nload: control token: 259020 '&lt;unused3118&gt;' is not marked as EOG\nload: control token: 259573 '&lt;unused3671&gt;' is not marked as EOG\nload: control token: 261473 '&lt;unused5571&gt;' is not marked as EOG\nload: control token:     26 '&lt;unused20&gt;' is not marked as EOG\nload: control token: 256430 '&lt;unused528&gt;' is not marked as EOG\nload: control token: 257204 '&lt;unused1302&gt;' is not marked as EOG\nload: control token: 256017 '&lt;unused115&gt;' is not marked as EOG\nload: control token: 258696 '&lt;unused2794&gt;' is not marked as EOG\nload: control token: 257312 '&lt;unused1410&gt;' is not marked as EOG\nload: control token: 261075 '&lt;unused5173&gt;' is not marked as EOG\nload: control token: 258976 '&lt;unused3074&gt;' is not marked as EOG\nload: control token: 256423 '&lt;unused521&gt;' is not marked as EOG\nload: control token: 257019 '&lt;unused1117&gt;' is not marked as EOG\nload: control token:      8 '&lt;unused2&gt;' is not marked as EOG\nload: control token: 258539 '&lt;unused2637&gt;' is not marked as EOG\nload: control token: 259673 '&lt;unused3771&gt;' is not marked as EOG\nload: control token: 257839 '&lt;unused1937&gt;' is not marked as EOG\nload: control token: 260822 '&lt;unused4920&gt;' is not marked as EOG\nload: control token: 259163 '&lt;unused3261&gt;' is not marked as EOG\nload: control token: 256241 '&lt;unused339&gt;' is not marked as EOG\nload: control token: 261180 '&lt;unused5278&gt;' is not marked as EOG\nload: control token: 261604 '&lt;unused5702&gt;' is not marked as EOG\nload: control token: 261118 '&lt;unused5216&gt;' is not marked as EOG\nload: control token: 261132 '&lt;unused5230&gt;' is not marked as EOG\nload: control token: 258193 '&lt;unused2291&gt;' is not marked as EOG\nload: control token: 257480 '&lt;unused1578&gt;' is not marked as EOG\nload: control token: 256777 '&lt;unused875&gt;' is not marked as EOG\nload: control token: 259070 '&lt;unused3168&gt;' is not marked as EOG\nload: control token:     32 '&lt;unused26&gt;' is not marked as EOG\nload: control token: 257630 '&lt;unused1728&gt;' is not marked as EOG\nload: control token: 258120 '&lt;unused2218&gt;' is not marked as EOG\nload: control token: 258685 '&lt;unused2783&gt;' is not marked as EOG\nload: control token: 262016 '&lt;unused6114&gt;' is not marked as EOG\nload: control token: 258510 '&lt;unused2608&gt;' is not marked as EOG\nload: control token: 260356 '&lt;unused4454&gt;' is not marked as EOG\nload: control token: 258271 '&lt;unused2369&gt;' is not marked as EOG\nload: control token: 257924 '&lt;unused2022&gt;' is not marked as EOG\nload: control token: 261776 '&lt;unused5874&gt;' is not marked as EOG\nload: control token: 258202 '&lt;unused2300&gt;' is not marked as EOG\nload: control token: 260526 '&lt;unused4624&gt;' is not marked as EOG\nload: control token: 256245 '&lt;unused343&gt;' is not marked as EOG\nload: control token: 256315 '&lt;unused413&gt;' is not marked as EOG\nload: control token: 261479 '&lt;unused5577&gt;' is not marked as EOG\nload: control token: 259271 '&lt;unused3369&gt;' is not marked as EOG\nload: control token: 257696 '&lt;unused1794&gt;' is not marked as EOG\nload: control token: 256274 '&lt;unused372&gt;' is not marked as EOG\nload: control token: 260160 '&lt;unused4258&gt;' is not marked as EOG\nload: control token: 260413 '&lt;unused4511&gt;' is not marked as EOG\nload: control token: 262129 '&lt;unused6227&gt;' is not marked as EOG\nload: control token: 258531 '&lt;unused2629&gt;' is not marked as EOG\nload: control token: 260744 '&lt;unused4842&gt;' is not marked as EOG\nload: control token: 259141 '&lt;unused3239&gt;' is not marked as EOG\nload: control token: 261022 '&lt;unused5120&gt;' is not marked as EOG\nload: control token: 258005 '&lt;unused2103&gt;' is not marked as EOG\nload: control token:    100 '&lt;unused94&gt;' is not marked as EOG\nload: control token: 258650 '&lt;unused2748&gt;' is not marked as EOG\nload: control token: 256541 '&lt;unused639&gt;' is not marked as EOG\nload: control token: 256923 '&lt;unused1021&gt;' is not marked as EOG\nload: control token: 261586 '&lt;unused5684&gt;' is not marked as EOG\nload: control token: 258496 '&lt;unused2594&gt;' is not marked as EOG\nload: control token: 258535 '&lt;unused2633&gt;' is not marked as EOG\nload: control token:     88 '&lt;unused82&gt;' is not marked as EOG\nload: control token: 258691 '&lt;unused2789&gt;' is not marked as EOG\nload: control token: 259647 '&lt;unused3745&gt;' is not marked as EOG\nload: control token: 258351 '&lt;unused2449&gt;' is not marked as EOG\nload: control token: 259056 '&lt;unused3154&gt;' is not marked as EOG\nload: control token: 259657 '&lt;unused3755&gt;' is not marked as EOG\nload: control token: 257874 '&lt;unused1972&gt;' is not marked as EOG\nload: control token: 258626 '&lt;unused2724&gt;' is not marked as EOG\nload: control token: 256093 '&lt;unused191&gt;' is not marked as EOG\nload: control token: 256516 '&lt;unused614&gt;' is not marked as EOG\nload: control token: 257141 '&lt;unused1239&gt;' is not marked as EOG\nload: control token: 258173 '&lt;unused2271&gt;' is not marked as EOG\nload: control token: 259531 '&lt;unused3629&gt;' is not marked as EOG\nload: control token: 258466 '&lt;unused2564&gt;' is not marked as EOG\nload: control token: 260857 '&lt;unused4955&gt;' is not marked as EOG\nload: control token: 260845 '&lt;unused4943&gt;' is not marked as EOG\nload: control token: 256982 '&lt;unused1080&gt;' is not marked as EOG\nload: control token: 259750 '&lt;unused3848&gt;' is not marked as EOG\nload: control token: 260672 '&lt;unused4770&gt;' is not marked as EOG\nload: control token: 258023 '&lt;unused2121&gt;' is not marked as EOG\nload: control token: 260306 '&lt;unused4404&gt;' is not marked as EOG\nload: control token: 259543 '&lt;unused3641&gt;' is not marked as EOG\nload: control token:     62 '&lt;unused56&gt;' is not marked as EOG\nload: control token: 260144 '&lt;unused4242&gt;' is not marked as EOG\nload: control token: 259974 '&lt;unused4072&gt;' is not marked as EOG\nload: control token: 260523 '&lt;unused4621&gt;' is not marked as EOG\nload: control token: 260003 '&lt;unused4101&gt;' is not marked as EOG\nload: control token: 256428 '&lt;unused526&gt;' is not marked as EOG\nload: control token:     49 '&lt;unused43&gt;' is not marked as EOG\nload: control token: 259232 '&lt;unused3330&gt;' is not marked as EOG\nload: control token: 258911 '&lt;unused3009&gt;' is not marked as EOG\nload: control token: 261271 '&lt;unused5369&gt;' is not marked as EOG\nload: control token: 261430 '&lt;unused5528&gt;' is not marked as EOG\nload: control token: 260382 '&lt;unused4480&gt;' is not marked as EOG\nload: control token: 258587 '&lt;unused2685&gt;' is not marked as EOG\nload: control token: 258550 '&lt;unused2648&gt;' is not marked as EOG\nload: control token: 259469 '&lt;unused3567&gt;' is not marked as EOG\nload: control token:     24 '&lt;unused18&gt;' is not marked as EOG\nload: control token: 260322 '&lt;unused4420&gt;' is not marked as EOG\nload: control token: 260198 '&lt;unused4296&gt;' is not marked as EOG\nload: control token: 258963 '&lt;unused3061&gt;' is not marked as EOG\nload: control token: 259795 '&lt;unused3893&gt;' is not marked as EOG\nload: control token: 261250 '&lt;unused5348&gt;' is not marked as EOG\nload: control token: 257543 '&lt;unused1641&gt;' is not marked as EOG\nload: control token: 261771 '&lt;unused5869&gt;' is not marked as EOG\nload: control token: 259880 '&lt;unused3978&gt;' is not marked as EOG\nload: control token: 256711 '&lt;unused809&gt;' is not marked as EOG\nload: control token: 261958 '&lt;unused6056&gt;' is not marked as EOG\nload: control token: 260488 '&lt;unused4586&gt;' is not marked as EOG\nload: control token: 256405 '&lt;unused503&gt;' is not marked as EOG\nload: control token: 261905 '&lt;unused6003&gt;' is not marked as EOG\nload: control token: 261847 '&lt;unused5945&gt;' is not marked as EOG\nload: control token: 259438 '&lt;unused3536&gt;' is not marked as EOG\nload: control token: 260105 '&lt;unused4203&gt;' is not marked as EOG\nload: control token: 257551 '&lt;unused1649&gt;' is not marked as EOG\nload: control token: 256062 '&lt;unused160&gt;' is not marked as EOG\nload: control token: 260090 '&lt;unused4188&gt;' is not marked as EOG\nload: control token: 260796 '&lt;unused4894&gt;' is not marked as EOG\nload: control token: 260082 '&lt;unused4180&gt;' is not marked as EOG\nload: control token: 260195 '&lt;unused4293&gt;' is not marked as EOG\nload: control token: 260363 '&lt;unused4461&gt;' is not marked as EOG\nload: control token: 256577 '&lt;unused675&gt;' is not marked as EOG\nload: control token: 257414 '&lt;unused1512&gt;' is not marked as EOG\nload: control token: 261218 '&lt;unused5316&gt;' is not marked as EOG\nload: control token: 256970 '&lt;unused1068&gt;' is not marked as EOG\nload: control token:     97 '&lt;unused91&gt;' is not marked as EOG\nload: control token: 258429 '&lt;unused2527&gt;' is not marked as EOG\nload: control token: 259915 '&lt;unused4013&gt;' is not marked as EOG\nload: control token: 261080 '&lt;unused5178&gt;' is not marked as EOG\nload: control token: 261929 '&lt;unused6027&gt;' is not marked as EOG\nload: control token: 258353 '&lt;unused2451&gt;' is not marked as EOG\nload: control token: 260132 '&lt;unused4230&gt;' is not marked as EOG\nload: control token: 257307 '&lt;unused1405&gt;' is not marked as EOG\nload: control token: 256326 '&lt;unused424&gt;' is not marked as EOG\nload: control token: 258369 '&lt;unused2467&gt;' is not marked as EOG\nload: control token: 258418 '&lt;unused2516&gt;' is not marked as EOG\nload: control token: 256944 '&lt;unused1042&gt;' is not marked as EOG\nload: control token: 258089 '&lt;unused2187&gt;' is not marked as EOG\nload: control token: 260707 '&lt;unused4805&gt;' is not marked as EOG\nload: control token: 262029 '&lt;unused6127&gt;' is not marked as EOG\nload: control token: 257682 '&lt;unused1780&gt;' is not marked as EOG\nload: control token:      7 '&lt;unused1&gt;' is not marked as EOG\nload: control token: 259988 '&lt;unused4086&gt;' is not marked as EOG\nload: control token: 262130 '&lt;unused6228&gt;' is not marked as EOG\nload: control token:     51 '&lt;unused45&gt;' is not marked as EOG\nload: control token: 257612 '&lt;unused1710&gt;' is not marked as EOG\nload: control token: 257346 '&lt;unused1444&gt;' is not marked as EOG\nload: control token: 261764 '&lt;unused5862&gt;' is not marked as EOG\nload: control token: 260324 '&lt;unused4422&gt;' is not marked as EOG\nload: control token: 257799 '&lt;unused1897&gt;' is not marked as EOG\nload: control token: 258073 '&lt;unused2171&gt;' is not marked as EOG\nload: control token: 259509 '&lt;unused3607&gt;' is not marked as EOG\nload: control token: 259614 '&lt;unused3712&gt;' is not marked as EOG\nload: control token: 257678 '&lt;unused1776&gt;' is not marked as EOG\nload: control token: 260404 '&lt;unused4502&gt;' is not marked as EOG\nload: control token: 257731 '&lt;unused1829&gt;' is not marked as EOG\nload: control token:     33 '&lt;unused27&gt;' is not marked as EOG\nload: control token: 260086 '&lt;unused4184&gt;' is not marked as EOG\nload: control token: 256607 '&lt;unused705&gt;' is not marked as EOG\nload: control token: 256246 '&lt;unused344&gt;' is not marked as EOG\nload: control token: 257861 '&lt;unused1959&gt;' is not marked as EOG\nload: control token: 260797 '&lt;unused4895&gt;' is not marked as EOG\nload: control token:     21 '&lt;unused15&gt;' is not marked as EOG\nload: control token: 260071 '&lt;unused4169&gt;' is not marked as EOG\nload: control token: 258128 '&lt;unused2226&gt;' is not marked as EOG\nload: control token: 256168 '&lt;unused266&gt;' is not marked as EOG\nload: control token: 261708 '&lt;unused5806&gt;' is not marked as EOG\nload: control token: 258307 '&lt;unused2405&gt;' is not marked as EOG\nload: control token:     59 '&lt;unused53&gt;' is not marked as EOG\nload: control token: 257390 '&lt;unused1488&gt;' is not marked as EOG\nload: control token: 256927 '&lt;unused1025&gt;' is not marked as EOG\nload: control token: 259687 '&lt;unused3785&gt;' is not marked as EOG\nload: control token: 261931 '&lt;unused6029&gt;' is not marked as EOG\nload: control token: 256814 '&lt;unused912&gt;' is not marked as EOG\nload: control token: 261396 '&lt;unused5494&gt;' is not marked as EOG\nload: control token: 256629 '&lt;unused727&gt;' is not marked as EOG\nload: control token: 260574 '&lt;unused4672&gt;' is not marked as EOG\nload: control token: 262096 '&lt;unused6194&gt;' is not marked as EOG\nload: control token: 259878 '&lt;unused3976&gt;' is not marked as EOG\nload: control token: 259001 '&lt;unused3099&gt;' is not marked as EOG\nload: control token: 258702 '&lt;unused2800&gt;' is not marked as EOG\nload: control token: 259972 '&lt;unused4070&gt;' is not marked as EOG\nload: control token: 257394 '&lt;unused1492&gt;' is not marked as EOG\nload: control token: 259591 '&lt;unused3689&gt;' is not marked as EOG\nload: control token: 261814 '&lt;unused5912&gt;' is not marked as EOG\nload: control token: 257131 '&lt;unused1229&gt;' is not marked as EOG\nload: control token: 257076 '&lt;unused1174&gt;' is not marked as EOG\nload: control token: 260080 '&lt;unused4178&gt;' is not marked as EOG\nload: control token: 260170 '&lt;unused4268&gt;' is not marked as EOG\nload: control token:    101 '&lt;unused95&gt;' is not marked as EOG\nload: control token:     67 '&lt;unused61&gt;' is not marked as EOG\nload: control token: 256211 '&lt;unused309&gt;' is not marked as EOG\nload: control token: 258443 '&lt;unused2541&gt;' is not marked as EOG\nload: control token: 256740 '&lt;unused838&gt;' is not marked as EOG\nload: control token:     14 '&lt;unused8&gt;' is not marked as EOG\nload: control token: 259443 '&lt;unused3541&gt;' is not marked as EOG\nload: control token: 256132 '&lt;unused230&gt;' is not marked as EOG\nload: control token: 258414 '&lt;unused2512&gt;' is not marked as EOG\nload: control token: 259015 '&lt;unused3113&gt;' is not marked as EOG\nload: control token: 258229 '&lt;unused2327&gt;' is not marked as EOG\nload: control token:      4 '&lt;mask&gt;' is not marked as EOG\nload: control token: 260314 '&lt;unused4412&gt;' is not marked as EOG\nload: control token: 261993 '&lt;unused6091&gt;' is not marked as EOG\nload: control token: 261128 '&lt;unused5226&gt;' is not marked as EOG\nload: control token: 262123 '&lt;unused6221&gt;' is not marked as EOG\nload: control token: 261975 '&lt;unused6073&gt;' is not marked as EOG\nload: control token: 256260 '&lt;unused358&gt;' is not marked as EOG\nload: control token: 256087 '&lt;unused185&gt;' is not marked as EOG\nload: control token: 257972 '&lt;unused2070&gt;' is not marked as EOG\nload: control token: 257735 '&lt;unused1833&gt;' is not marked as EOG\nload: control token: 257434 '&lt;unused1532&gt;' is not marked as EOG\nload: control token:     84 '&lt;unused78&gt;' is not marked as EOG\nload: control token: 256615 '&lt;unused713&gt;' is not marked as EOG\nload: control token: 260786 '&lt;unused4884&gt;' is not marked as EOG\nload: control token: 261540 '&lt;unused5638&gt;' is not marked as EOG\nload: control token: 257803 '&lt;unused1901&gt;' is not marked as EOG\nload: control token: 260361 '&lt;unused4459&gt;' is not marked as EOG\nload: control token: 259373 '&lt;unused3471&gt;' is not marked as EOG\nload: control token: 258144 '&lt;unused2242&gt;' is not marked as EOG\nload: control token:    105 '&lt;start_of_turn&gt;' is not marked as EOG\nload: control token: 256882 '&lt;unused980&gt;' is not marked as EOG\nload: control token: 258295 '&lt;unused2393&gt;' is not marked as EOG\nload: control token: 259553 '&lt;unused3651&gt;' is not marked as EOG\nload: control token: 260746 '&lt;unused4844&gt;' is not marked as EOG\nload: control token: 260225 '&lt;unused4323&gt;' is not marked as EOG\nload: control token: 257240 '&lt;unused1338&gt;' is not marked as EOG\nload: control token: 258773 '&lt;unused2871&gt;' is not marked as EOG\nload: control token: 259195 '&lt;unused3293&gt;' is not marked as EOG\nload: control token: 259667 '&lt;unused3765&gt;' is not marked as EOG\nload: control token: 261515 '&lt;unused5613&gt;' is not marked as EOG\nload: control token: 259027 '&lt;unused3125&gt;' is not marked as EOG\nload: control token:     95 '&lt;unused89&gt;' is not marked as EOG\nload: control token: 257041 '&lt;unused1139&gt;' is not marked as EOG\nload: control token: 258674 '&lt;unused2772&gt;' is not marked as EOG\nload: control token: 259100 '&lt;unused3198&gt;' is not marked as EOG\nload: control token: 256114 '&lt;unused212&gt;' is not marked as EOG\nload: control token: 261837 '&lt;unused5935&gt;' is not marked as EOG\nload: control token: 256696 '&lt;unused794&gt;' is not marked as EOG\nload: control token: 260009 '&lt;unused4107&gt;' is not marked as EOG\nload: control token: 257215 '&lt;unused1313&gt;' is not marked as EOG\nload: control token:     74 '&lt;unused68&gt;' is not marked as EOG\nload: control token: 259863 '&lt;unused3961&gt;' is not marked as EOG\nload: control token: 258733 '&lt;unused2831&gt;' is not marked as EOG\nload: control token: 258857 '&lt;unused2955&gt;' is not marked as EOG\nload: control token: 260741 '&lt;unused4839&gt;' is not marked as EOG\nload: control token: 258814 '&lt;unused2912&gt;' is not marked as EOG\nload: control token: 257537 '&lt;unused1635&gt;' is not marked as EOG\nload: control token: 261758 '&lt;unused5856&gt;' is not marked as EOG\nload: control token: 256639 '&lt;unused737&gt;' is not marked as EOG\nload: control token: 261739 '&lt;unused5837&gt;' is not marked as EOG\nload: control token: 258676 '&lt;unused2774&gt;' is not marked as EOG\nload: control token:     83 '&lt;unused77&gt;' is not marked as EOG\nload: control token: 260312 '&lt;unused4410&gt;' is not marked as EOG\nload: control token: 257519 '&lt;unused1617&gt;' is not marked as EOG\nload: control token: 260733 '&lt;unused4831&gt;' is not marked as EOG\nload: control token: 259080 '&lt;unused3178&gt;' is not marked as EOG\nload: control token: 260279 '&lt;unused4377&gt;' is not marked as EOG\nload: control token: 261390 '&lt;unused5488&gt;' is not marked as EOG\nload: control token: 257885 '&lt;unused1983&gt;' is not marked as EOG\nload: control token: 259712 '&lt;unused3810&gt;' is not marked as EOG\nload: control token: 258985 '&lt;unused3083&gt;' is not marked as EOG\nload: control token:     96 '&lt;unused90&gt;' is not marked as EOG\nload: control token: 260349 '&lt;unused4447&gt;' is not marked as EOG\nload: control token: 259123 '&lt;unused3221&gt;' is not marked as EOG\nload: control token: 256498 '&lt;unused596&gt;' is not marked as EOG\nload: control token: 256480 '&lt;unused578&gt;' is not marked as EOG\nload: control token: 262075 '&lt;unused6173&gt;' is not marked as EOG\nload: control token: 256356 '&lt;unused454&gt;' is not marked as EOG\nload: control token: 260694 '&lt;unused4792&gt;' is not marked as EOG\nload: control token: 261460 '&lt;unused5558&gt;' is not marked as EOG\nload: control token: 257964 '&lt;unused2062&gt;' is not marked as EOG\nload: control token: 262055 '&lt;unused6153&gt;' is not marked as EOG\nload: control token: 261613 '&lt;unused5711&gt;' is not marked as EOG\nload: control token: 261483 '&lt;unused5581&gt;' is not marked as EOG\nload: control token: 261822 '&lt;unused5920&gt;' is not marked as EOG\nload: control token: 261679 '&lt;unused5777&gt;' is not marked as EOG\nload: control token: 261714 '&lt;unused5812&gt;' is not marked as EOG\nload: control token: 256014 '&lt;unused112&gt;' is not marked as EOG\nload: control token:     41 '&lt;unused35&gt;' is not marked as EOG\nload: control token: 259544 '&lt;unused3642&gt;' is not marked as EOG\nload: control token: 259430 '&lt;unused3528&gt;' is not marked as EOG\nload: control token: 259177 '&lt;unused3275&gt;' is not marked as EOG\nload: control token:     85 '&lt;unused79&gt;' is not marked as EOG\nload: control token: 259967 '&lt;unused4065&gt;' is not marked as EOG\nload: control token: 261675 '&lt;unused5773&gt;' is not marked as EOG\nload: control token: 259508 '&lt;unused3606&gt;' is not marked as EOG\nload: control token: 259482 '&lt;unused3580&gt;' is not marked as EOG\nload: control token: 261500 '&lt;unused5598&gt;' is not marked as EOG\nload: control token: 260878 '&lt;unused4976&gt;' is not marked as EOG\nload: control token: 257155 '&lt;unused1253&gt;' is not marked as EOG\nload: control token: 261083 '&lt;unused5181&gt;' is not marked as EOG\nload: control token: 260072 '&lt;unused4170&gt;' is not marked as EOG\nload: control token: 258991 '&lt;unused3089&gt;' is not marked as EOG\nload: control token: 258620 '&lt;unused2718&gt;' is not marked as EOG\nload: control token: 260450 '&lt;unused4548&gt;' is not marked as EOG\nload: control token: 261452 '&lt;unused5550&gt;' is not marked as EOG\nload: control token: 257166 '&lt;unused1264&gt;' is not marked as EOG\nload: control token: 261805 '&lt;unused5903&gt;' is not marked as EOG\nload: control token: 257713 '&lt;unused1811&gt;' is not marked as EOG\nload: control token: 258473 '&lt;unused2571&gt;' is not marked as EOG\nload: control token: 261246 '&lt;unused5344&gt;' is not marked as EOG\nload: control token: 257115 '&lt;unused1213&gt;' is not marked as EOG\nload: control token: 262101 '&lt;unused6199&gt;' is not marked as EOG\nload: control token: 258024 '&lt;unused2122&gt;' is not marked as EOG\nload: control token: 260267 '&lt;unused4365&gt;' is not marked as EOG\nload: control token: 260391 '&lt;unused4489&gt;' is not marked as EOG\nload: control token: 259881 '&lt;unused3979&gt;' is not marked as EOG\nload: control token: 259062 '&lt;unused3160&gt;' is not marked as EOG\nload: control token: 258800 '&lt;unused2898&gt;' is not marked as EOG\nload: control token: 261851 '&lt;unused5949&gt;' is not marked as EOG\nload: control token: 258703 '&lt;unused2801&gt;' is not marked as EOG\nload: control token: 259211 '&lt;unused3309&gt;' is not marked as EOG\nload: control token: 258430 '&lt;unused2528&gt;' is not marked as EOG\nload: control token: 256981 '&lt;unused1079&gt;' is not marked as EOG\nload: control token: 261195 '&lt;unused5293&gt;' is not marked as EOG\nload: control token: 259372 '&lt;unused3470&gt;' is not marked as EOG\nload: control token: 257348 '&lt;unused1446&gt;' is not marked as EOG\nload: control token: 259942 '&lt;unused4040&gt;' is not marked as EOG\nload: control token: 260528 '&lt;unused4626&gt;' is not marked as EOG\nload: control token: 256113 '&lt;unused211&gt;' is not marked as EOG\nload: control token: 260182 '&lt;unused4280&gt;' is not marked as EOG\nload: control token: 259715 '&lt;unused3813&gt;' is not marked as EOG\nload: control token: 261999 '&lt;unused6097&gt;' is not marked as EOG\nload: control token: 257605 '&lt;unused1703&gt;' is not marked as EOG\nload: control token:     46 '&lt;unused40&gt;' is not marked as EOG\nload: control token: 258648 '&lt;unused2746&gt;' is not marked as EOG\nload: control token: 257009 '&lt;unused1107&gt;' is not marked as EOG\nload: control token: 260057 '&lt;unused4155&gt;' is not marked as EOG\nload: control token: 259698 '&lt;unused3796&gt;' is not marked as EOG\nload: control token: 257325 '&lt;unused1423&gt;' is not marked as EOG\nload: control token: 258459 '&lt;unused2557&gt;' is not marked as EOG\nload: control token: 259512 '&lt;unused3610&gt;' is not marked as EOG\nload: control token: 259475 '&lt;unused3573&gt;' is not marked as EOG\nload: control token: 256009 '&lt;unused107&gt;' is not marked as EOG\nload: control token: 260276 '&lt;unused4374&gt;' is not marked as EOG\nload: control token: 261084 '&lt;unused5182&gt;' is not marked as EOG\nload: control token: 256215 '&lt;unused313&gt;' is not marked as EOG\nload: control token: 260138 '&lt;unused4236&gt;' is not marked as EOG\nload: control token: 260896 '&lt;unused4994&gt;' is not marked as EOG\nload: control token: 260604 '&lt;unused4702&gt;' is not marked as EOG\nload: control token: 259480 '&lt;unused3578&gt;' is not marked as EOG\nload: control token: 256140 '&lt;unused238&gt;' is not marked as EOG\nload: control token: 261368 '&lt;unused5466&gt;' is not marked as EOG\nload: control token: 256209 '&lt;unused307&gt;' is not marked as EOG\nload: control token: 259683 '&lt;unused3781&gt;' is not marked as EOG\nload: control token: 256515 '&lt;unused613&gt;' is not marked as EOG\nload: control token: 261682 '&lt;unused5780&gt;' is not marked as EOG\nload: control token:     76 '&lt;unused70&gt;' is not marked as EOG\nload: control token: 261329 '&lt;unused5427&gt;' is not marked as EOG\nload: control token: 256780 '&lt;unused878&gt;' is not marked as EOG\nload: control token:     56 '&lt;unused50&gt;' is not marked as EOG\nload: control token: 257748 '&lt;unused1846&gt;' is not marked as EOG\nload: control token: 256956 '&lt;unused1054&gt;' is not marked as EOG\nload: control token: 257691 '&lt;unused1789&gt;' is not marked as EOG\nload: control token: 261623 '&lt;unused5721&gt;' is not marked as EOG\nload: control token: 256359 '&lt;unused457&gt;' is not marked as EOG\nload: control token: 260229 '&lt;unused4327&gt;' is not marked as EOG\nload: control token: 260720 '&lt;unused4818&gt;' is not marked as EOG\nload: control token: 259375 '&lt;unused3473&gt;' is not marked as EOG\nload: control token: 261840 '&lt;unused5938&gt;' is not marked as EOG\nload: control token:     72 '&lt;unused66&gt;' is not marked as EOG\nload: control token: 261491 '&lt;unused5589&gt;' is not marked as EOG\nload: control token:     73 '&lt;unused67&gt;' is not marked as EOG\nload: control token: 261677 '&lt;unused5775&gt;' is not marked as EOG\nload: control token: 261493 '&lt;unused5591&gt;' is not marked as EOG\nload: control token: 259255 '&lt;unused3353&gt;' is not marked as EOG\nload: control token: 259945 '&lt;unused4043&gt;' is not marked as EOG\nload: control token: 260437 '&lt;unused4535&gt;' is not marked as EOG\nload: control token:     78 '&lt;unused72&gt;' is not marked as EOG\nload: control token: 258572 '&lt;unused2670&gt;' is not marked as EOG\nload: control token: 261025 '&lt;unused5123&gt;' is not marked as EOG\nload: control token: 258839 '&lt;unused2937&gt;' is not marked as EOG\nload: control token: 261391 '&lt;unused5489&gt;' is not marked as EOG\nload: control token: 257126 '&lt;unused1224&gt;' is not marked as EOG\nload: control token: 260632 '&lt;unused4730&gt;' is not marked as EOG\nload: control token: 258677 '&lt;unused2775&gt;' is not marked as EOG\nload: control token: 261188 '&lt;unused5286&gt;' is not marked as EOG\nload: control token: 260582 '&lt;unused4680&gt;' is not marked as EOG\nload: control token: 256151 '&lt;unused249&gt;' is not marked as EOG\nload: control token: 257531 '&lt;unused1629&gt;' is not marked as EOG\nload: control token: 257002 '&lt;unused1100&gt;' is not marked as EOG\nload: control token: 257271 '&lt;unused1369&gt;' is not marked as EOG\nload: control token: 258639 '&lt;unused2737&gt;' is not marked as EOG\nload: control token: 261719 '&lt;unused5817&gt;' is not marked as EOG\nload: control token: 256618 '&lt;unused716&gt;' is not marked as EOG\nload: control token: 257187 '&lt;unused1285&gt;' is not marked as EOG\nload: control token: 258069 '&lt;unused2167&gt;' is not marked as EOG\nload: control token: 261565 '&lt;unused5663&gt;' is not marked as EOG\nload: control token: 256908 '&lt;unused1006&gt;' is not marked as EOG\nload: control token: 256443 '&lt;unused541&gt;' is not marked as EOG\nload: control token: 256324 '&lt;unused422&gt;' is not marked as EOG\nload: control token:     57 '&lt;unused51&gt;' is not marked as EOG\nload: control token: 257175 '&lt;unused1273&gt;' is not marked as EOG\nload: control token: 256045 '&lt;unused143&gt;' is not marked as EOG\nload: control token: 260914 '&lt;unused5012&gt;' is not marked as EOG\nload: control token: 259152 '&lt;unused3250&gt;' is not marked as EOG\nload: control token: 260166 '&lt;unused4264&gt;' is not marked as EOG\nload: control token: 259557 '&lt;unused3655&gt;' is not marked as EOG\nload: control token: 259799 '&lt;unused3897&gt;' is not marked as EOG\nload: control token: 258131 '&lt;unused2229&gt;' is not marked as EOG\nload: control token: 261553 '&lt;unused5651&gt;' is not marked as EOG\nload: control token: 258847 '&lt;unused2945&gt;' is not marked as EOG\nload: control token: 259190 '&lt;unused3288&gt;' is not marked as EOG\nload: control token:     40 '&lt;unused34&gt;' is not marked as EOG\nload: control token: 256978 '&lt;unused1076&gt;' is not marked as EOG\nload: control token: 256551 '&lt;unused649&gt;' is not marked as EOG\nload: control token: 257820 '&lt;unused1918&gt;' is not marked as EOG\nload: control token: 259341 '&lt;unused3439&gt;' is not marked as EOG\nload: control token:    104 '&lt;unused98&gt;' is not marked as EOG\nload: control token: 256833 '&lt;unused931&gt;' is not marked as EOG\nload: control token: 261773 '&lt;unused5871&gt;' is not marked as EOG\nload: control token:      2 '&lt;bos&gt;' is not marked as EOG\nload: control token: 261683 '&lt;unused5781&gt;' is not marked as EOG\nload: control token: 257563 '&lt;unused1661&gt;' is not marked as EOG\nload: control token: 261181 '&lt;unused5279&gt;' is not marked as EOG\nload: control token: 258415 '&lt;unused2513&gt;' is not marked as EOG\nload: control token: 259873 '&lt;unused3971&gt;' is not marked as EOG\nload: control token: 259181 '&lt;unused3279&gt;' is not marked as EOG\nload: control token: 260575 '&lt;unused4673&gt;' is not marked as EOG\nload: control token: 262120 '&lt;unused6218&gt;' is not marked as EOG\nload: control token: 259899 '&lt;unused3997&gt;' is not marked as EOG\nload: control token: 260753 '&lt;unused4851&gt;' is not marked as EOG\nload: control token: 261696 '&lt;unused5794&gt;' is not marked as EOG\nload: control token: 261965 '&lt;unused6063&gt;' is not marked as EOG\nload: control token: 256775 '&lt;unused873&gt;' is not marked as EOG\nload: control token: 257947 '&lt;unused2045&gt;' is not marked as EOG\nload: control token: 258753 '&lt;unused2851&gt;' is not marked as EOG\nload: control token: 259602 '&lt;unused3700&gt;' is not marked as EOG\nload: control token: 260854 '&lt;unused4952&gt;' is not marked as EOG\nload: control token: 258108 '&lt;unused2206&gt;' is not marked as EOG\nload: control token:     52 '&lt;unused46&gt;' is not marked as EOG\nload: control token: 256709 '&lt;unused807&gt;' is not marked as EOG\nload: control token: 258748 '&lt;unused2846&gt;' is not marked as EOG\nload: control token: 258908 '&lt;unused3006&gt;' is not marked as EOG\nload: control token:     87 '&lt;unused81&gt;' is not marked as EOG\nload: control token:     31 '&lt;unused25&gt;' is not marked as EOG\nload: control token: 259991 '&lt;unused4089&gt;' is not marked as EOG\nload: control token: 261044 '&lt;unused5142&gt;' is not marked as EOG\nload: control token: 260524 '&lt;unused4622&gt;' is not marked as EOG\nload: control token: 261173 '&lt;unused5271&gt;' is not marked as EOG\nload: control token: 260224 '&lt;unused4322&gt;' is not marked as EOG\nload: control token: 258974 '&lt;unused3072&gt;' is not marked as EOG\nload: control token: 256000 '&lt;end_of_image&gt;' is not marked as EOG\nload: control token: 259464 '&lt;unused3562&gt;' is not marked as EOG\nload: control token: 256763 '&lt;unused861&gt;' is not marked as EOG\nload: control token: 258720 '&lt;unused2818&gt;' is not marked as EOG\nload: control token: 258630 '&lt;unused2728&gt;' is not marked as EOG\nload: control token: 261177 '&lt;unused5275&gt;' is not marked as EOG\nload: control token: 261333 '&lt;unused5431&gt;' is not marked as EOG\nload: control token:     63 '&lt;unused57&gt;' is not marked as EOG\nload: control token:     23 '&lt;unused17&gt;' is not marked as EOG\nload: control token: 257905 '&lt;unused2003&gt;' is not marked as EOG\nload: control token: 261876 '&lt;unused5974&gt;' is not marked as EOG\nload: control token: 259895 '&lt;unused3993&gt;' is not marked as EOG\nload: control token: 258541 '&lt;unused2639&gt;' is not marked as EOG\nload: control token: 261557 '&lt;unused5655&gt;' is not marked as EOG\nload: control token:      9 '&lt;unused3&gt;' is not marked as EOG\nload: control token: 256442 '&lt;unused540&gt;' is not marked as EOG\nload: control token: 259066 '&lt;unused3164&gt;' is not marked as EOG\nload: control token: 261548 '&lt;unused5646&gt;' is not marked as EOG\nload: control token: 256100 '&lt;unused198&gt;' is not marked as EOG\nload: control token: 261848 '&lt;unused5946&gt;' is not marked as EOG\nload: control token: 258940 '&lt;unused3038&gt;' is not marked as EOG\nload: control token: 258671 '&lt;unused2769&gt;' is not marked as EOG\nload: control token: 260369 '&lt;unused4467&gt;' is not marked as EOG\nload: control token: 258605 '&lt;unused2703&gt;' is not marked as EOG\nload: control token: 259295 '&lt;unused3393&gt;' is not marked as EOG\nload: control token:     66 '&lt;unused60&gt;' is not marked as EOG\nload: control token: 260211 '&lt;unused4309&gt;' is not marked as EOG\nload: control token: 259949 '&lt;unused4047&gt;' is not marked as EOG\nload: control token: 256951 '&lt;unused1049&gt;' is not marked as EOG\nload: control token: 260757 '&lt;unused4855&gt;' is not marked as EOG\nload: control token: 260064 '&lt;unused4162&gt;' is not marked as EOG\nload: control token: 259622 '&lt;unused3720&gt;' is not marked as EOG\nload: control token: 256184 '&lt;unused282&gt;' is not marked as EOG\nload: control token: 260760 '&lt;unused4858&gt;' is not marked as EOG\nload: control token: 256235 '&lt;unused333&gt;' is not marked as EOG\nload: control token: 256514 '&lt;unused612&gt;' is not marked as EOG\nload: control token: 258881 '&lt;unused2979&gt;' is not marked as EOG\nload: control token: 256061 '&lt;unused159&gt;' is not marked as EOG\nload: control token: 257347 '&lt;unused1445&gt;' is not marked as EOG\nload: control token:     50 '&lt;unused44&gt;' is not marked as EOG\nload: control token: 260769 '&lt;unused4867&gt;' is not marked as EOG\nload: control token: 260536 '&lt;unused4634&gt;' is not marked as EOG\nload: control token: 257624 '&lt;unused1722&gt;' is not marked as EOG\nload: control token: 261294 '&lt;unused5392&gt;' is not marked as EOG\nload: control token:     30 '&lt;unused24&gt;' is not marked as EOG\nload: control token: 258088 '&lt;unused2186&gt;' is not marked as EOG\nload: control token: 256128 '&lt;unused226&gt;' is not marked as EOG\nload: control token: 260116 '&lt;unused4214&gt;' is not marked as EOG\nload: control token: 259836 '&lt;unused3934&gt;' is not marked as EOG\nload: control token: 260209 '&lt;unused4307&gt;' is not marked as EOG\nload: control token: 261892 '&lt;unused5990&gt;' is not marked as EOG\nload: control token:      1 '&lt;eos&gt;' is not marked as EOG\nload: control token:     98 '&lt;unused92&gt;' is not marked as EOG\nload: control token: 256388 '&lt;unused486&gt;' is not marked as EOG\nload: control token: 256259 '&lt;unused357&gt;' is not marked as EOG\nload: control token:     99 '&lt;unused93&gt;' is not marked as EOG\nload: control token:     58 '&lt;unused52&gt;' is not marked as EOG\nload: control token:     44 '&lt;unused38&gt;' is not marked as EOG\nload: control token: 256896 '&lt;unused994&gt;' is not marked as EOG\nload: control token: 261160 '&lt;unused5258&gt;' is not marked as EOG\nload: control token: 260894 '&lt;unused4992&gt;' is not marked as EOG\nload: control token: 260347 '&lt;unused4445&gt;' is not marked as EOG\nload: control token:      6 '&lt;unused0&gt;' is not marked as EOG\nload: control token: 258640 '&lt;unused2738&gt;' is not marked as EOG\nload: control token: 259971 '&lt;unused4069&gt;' is not marked as EOG\nload: control token: 260376 '&lt;unused4474&gt;' is not marked as EOG\nload: control token: 260055 '&lt;unused4153&gt;' is not marked as EOG\nload: control token: 259510 '&lt;unused3608&gt;' is not marked as EOG\nload: control token: 259383 '&lt;unused3481&gt;' is not marked as EOG\nload: control token: 257932 '&lt;unused2030&gt;' is not marked as EOG\nload: control token: 258435 '&lt;unused2533&gt;' is not marked as EOG\nload: control token: 259759 '&lt;unused3857&gt;' is not marked as EOG\nload: control token: 259224 '&lt;unused3322&gt;' is not marked as EOG\nload: control token: 257039 '&lt;unused1137&gt;' is not marked as EOG\nload: control token: 259455 '&lt;unused3553&gt;' is not marked as EOG\nload: control token: 260321 '&lt;unused4419&gt;' is not marked as EOG\nload: control token: 259685 '&lt;unused3783&gt;' is not marked as EOG\nload: control token: 261742 '&lt;unused5840&gt;' is not marked as EOG\nload: control token:      0 '&lt;pad&gt;' is not marked as EOG\nload: control token:     28 '&lt;unused22&gt;' is not marked as EOG\nload: control token: 258052 '&lt;unused2150&gt;' is not marked as EOG\nload: control token: 256129 '&lt;unused227&gt;' is not marked as EOG\nload: control token: 259368 '&lt;unused3466&gt;' is not marked as EOG\nload: control token: 260234 '&lt;unused4332&gt;' is not marked as EOG\nload: control token: 259413 '&lt;unused3511&gt;' is not marked as EOG\nload: control token: 258801 '&lt;unused2899&gt;' is not marked as EOG\nload: control token: 261286 '&lt;unused5384&gt;' is not marked as EOG\nload: control token: 259360 '&lt;unused3458&gt;' is not marked as EOG\nload: control token: 258393 '&lt;unused2491&gt;' is not marked as EOG\nload: control token: 258629 '&lt;unused2727&gt;' is not marked as EOG\nload: control token: 261302 '&lt;unused5400&gt;' is not marked as EOG\nload: control token: 260036 '&lt;unused4134&gt;' is not marked as EOG\nload: control token: 257554 '&lt;unused1652&gt;' is not marked as EOG\nload: control token: 259235 '&lt;unused3333&gt;' is not marked as EOG\nload: control token:     35 '&lt;unused29&gt;' is not marked as EOG\nload: control token: 260529 '&lt;unused4627&gt;' is not marked as EOG\nload: control token:     79 '&lt;unused73&gt;' is not marked as EOG\nload: control token: 258758 '&lt;unused2856&gt;' is not marked as EOG\nload: control token: 259216 '&lt;unused3314&gt;' is not marked as EOG\nload: control token: 259068 '&lt;unused3166&gt;' is not marked as EOG\nload: control token: 259565 '&lt;unused3663&gt;' is not marked as EOG\nload: control token: 258706 '&lt;unused2804&gt;' is not marked as EOG\nload: control token: 256622 '&lt;unused720&gt;' is not marked as EOG\nload: control token: 261864 '&lt;unused5962&gt;' is not marked as EOG\nload: control token: 258388 '&lt;unused2486&gt;' is not marked as EOG\nload: control token: 260859 '&lt;unused4957&gt;' is not marked as EOG\nload: control token: 257095 '&lt;unused1193&gt;' is not marked as EOG\nload: control token:     60 '&lt;unused54&gt;' is not marked as EOG\nload: control token: 258880 '&lt;unused2978&gt;' is not marked as EOG\nload: control token: 261154 '&lt;unused5252&gt;' is not marked as EOG\nload: control token: 256776 '&lt;unused874&gt;' is not marked as EOG\nload: control token: 257575 '&lt;unused1673&gt;' is not marked as EOG\nload: control token: 258116 '&lt;unused2214&gt;' is not marked as EOG\nload: control token: 256229 '&lt;unused327&gt;' is not marked as EOG\nload: control token: 261253 '&lt;unused5351&gt;' is not marked as EOG\nload: control token: 260258 '&lt;unused4356&gt;' is not marked as EOG\nload: control token: 257558 '&lt;unused1656&gt;' is not marked as EOG\nload: control token: 257540 '&lt;unused1638&gt;' is not marked as EOG\nload: control token: 261201 '&lt;unused5299&gt;' is not marked as EOG\nload: control token: 259126 '&lt;unused3224&gt;' is not marked as EOG\nload: control token: 258372 '&lt;unused2470&gt;' is not marked as EOG\nload: control token: 256817 '&lt;unused915&gt;' is not marked as EOG\nload: control token: 260849 '&lt;unused4947&gt;' is not marked as EOG\nload: control token: 261106 '&lt;unused5204&gt;' is not marked as EOG\nload: control token:     65 '&lt;unused59&gt;' is not marked as EOG\nload: control token: 257339 '&lt;unused1437&gt;' is not marked as EOG\nload: control token: 256791 '&lt;unused889&gt;' is not marked as EOG\nload: control token: 262132 '&lt;unused6230&gt;' is not marked as EOG\nload: control token:     71 '&lt;unused65&gt;' is not marked as EOG\nload: control token: 256263 '&lt;unused361&gt;' is not marked as EOG\nload: control token: 256964 '&lt;unused1062&gt;' is not marked as EOG\nload: control token: 260266 '&lt;unused4364&gt;' is not marked as EOG\nload: control token: 259412 '&lt;unused3510&gt;' is not marked as EOG\nload: control token: 259358 '&lt;unused3456&gt;' is not marked as EOG\nload: control token: 261414 '&lt;unused5512&gt;' is not marked as EOG\nload: control token: 260227 '&lt;unused4325&gt;' is not marked as EOG\nload: control token:     18 '&lt;unused12&gt;' is not marked as EOG\nload: control token: 259403 '&lt;unused3501&gt;' is not marked as EOG\nload: control token: 258548 '&lt;unused2646&gt;' is not marked as EOG\nload: control token: 258482 '&lt;unused2580&gt;' is not marked as EOG\nload: control token: 260620 '&lt;unused4718&gt;' is not marked as EOG\nload: control token: 256118 '&lt;unused216&gt;' is not marked as EOG\nload: control token: 259172 '&lt;unused3270&gt;' is not marked as EOG\nload: control token: 261855 '&lt;unused5953&gt;' is not marked as EOG\nload: control token:     38 '&lt;unused32&gt;' is not marked as EOG\nload: control token: 258188 '&lt;unused2286&gt;' is not marked as EOG\nload: control token: 258416 '&lt;unused2514&gt;' is not marked as EOG\nload: control token: 256352 '&lt;unused450&gt;' is not marked as EOG\nload: control token: 261176 '&lt;unused5274&gt;' is not marked as EOG\nload: control token:     82 '&lt;unused76&gt;' is not marked as EOG\nload: control token: 261314 '&lt;unused5412&gt;' is not marked as EOG\nload: control token: 257626 '&lt;unused1724&gt;' is not marked as EOG\nload: control token: 260748 '&lt;unused4846&gt;' is not marked as EOG\nload: control token:     29 '&lt;unused23&gt;' is not marked as EOG\nload: control token: 257497 '&lt;unused1595&gt;' is not marked as EOG\nload: printing all EOG tokens:\nload:   - 106 ('&lt;end_of_turn&gt;')\nload: special tokens cache size = 6414\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 1152\nprint_info: n_layer          = 26\nprint_info: n_head           = 4\nprint_info: n_head_kv        = 1\nprint_info: n_rot            = 256\nprint_info: n_swa            = 512\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 256\nprint_info: n_embd_v_gqa     = 256\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 6912\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 1B\nprint_info: model params     = 999.89 M\nprint_info: general.name     = Gemma-3-1B-It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262144\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '&lt;bos&gt;'\nprint_info: EOS token        = 106 '&lt;end_of_turn&gt;'\nprint_info: EOT token        = 106 '&lt;end_of_turn&gt;'\nprint_info: UNK token        = 3 '&lt;unk&gt;'\nprint_info: PAD token        = 0 '&lt;pad&gt;'\nprint_info: LF token         = 248 '&lt;0x0A&gt;'\nprint_info: EOG token        = 106 '&lt;end_of_turn&gt;'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CPU, is_swa = 1\nload_tensors: layer   1 assigned to device CPU, is_swa = 1\nload_tensors: layer   2 assigned to device CPU, is_swa = 1\nload_tensors: layer   3 assigned to device CPU, is_swa = 1\nload_tensors: layer   4 assigned to device CPU, is_swa = 1\nload_tensors: layer   5 assigned to device CPU, is_swa = 0\nload_tensors: layer   6 assigned to device CPU, is_swa = 1\nload_tensors: layer   7 assigned to device CPU, is_swa = 1\nload_tensors: layer   8 assigned to device CPU, is_swa = 1\nload_tensors: layer   9 assigned to device CPU, is_swa = 1\nload_tensors: layer  10 assigned to device CPU, is_swa = 1\nload_tensors: layer  11 assigned to device CPU, is_swa = 0\nload_tensors: layer  12 assigned to device CPU, is_swa = 1\nload_tensors: layer  13 assigned to device CPU, is_swa = 1\nload_tensors: layer  14 assigned to device CPU, is_swa = 1\nload_tensors: layer  15 assigned to device CPU, is_swa = 1\nload_tensors: layer  16 assigned to device CPU, is_swa = 1\nload_tensors: layer  17 assigned to device CPU, is_swa = 0\nload_tensors: layer  18 assigned to device CPU, is_swa = 1\nload_tensors: layer  19 assigned to device CPU, is_swa = 1\nload_tensors: layer  20 assigned to device CPU, is_swa = 1\nload_tensors: layer  21 assigned to device CPU, is_swa = 1\nload_tensors: layer  22 assigned to device CPU, is_swa = 1\nload_tensors: layer  23 assigned to device CPU, is_swa = 0\nload_tensors: layer  24 assigned to device CPU, is_swa = 1\nload_tensors: layer  25 assigned to device CPU, is_swa = 1\nload_tensors: layer  26 assigned to device CPU, is_swa = 0\nload_tensors: tensor 'token_embd.weight' (q8_0) (and 340 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\nload_tensors: offloading 0 repeating layers to GPU\nload_tensors: offloaded 0/27 layers to GPU\nload_tensors:   CPU_Mapped model buffer size =   762.49 MiB\n.............................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 512\nllama_context: n_ctx_per_seq = 512\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (512) &lt; n_ctx_train (32768) -- the full capacity of the model will not be utilized\nggml_metal_init: allocating\nggml_metal_init: found device: Apple M2 Max\nggml_metal_init: picking default device: Apple M2 Max\nggml_metal_init: GPU name:   Apple M2 Max\nggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\nggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\nggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)\nggml_metal_init: simdgroup reduction   = true\nggml_metal_init: simdgroup matrix mul. = true\nggml_metal_init: has residency sets    = true\nggml_metal_init: has bfloat            = true\nggml_metal_init: use bfloat            = false\nggml_metal_init: hasUnifiedMemory      = true\nggml_metal_init: recommendedMaxWorkingSetSize  = 83494.17 MB\nggml_metal_init: loaded kernel_add                                    0xbc9f29500 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_fuse_2                             0xbc9f29800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_fuse_3                             0xbc9f29b00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_fuse_4                             0xbc9f29e00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_fuse_5                             0xbc9f2a100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_fuse_6                             0xbc9f2a400 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_fuse_7                             0xbc9f2a700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_fuse_8                             0xbc9f2aa00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_row_c4                             0xbc9f2ad00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0xbc9f2b000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0xbc9f2b300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0xbc9f2b600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0xbc9f2b900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0xbc9f2bc00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0xbc7c10000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0xbc7c10300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_sub                                    0xbc7c10600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_sub_row_c4                             0xbc7c10900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul                                    0xbc7c10c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_row_c4                             0xbc7c10f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_div                                    0xbc7c11200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_div_row_c4                             0xbc7c11500 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_add_id                                 0xbc7c11800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_repeat_f32                             0xbc7c11b00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_repeat_f16                             0xbc7c11e00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_repeat_i32                             0xbc7c12100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_repeat_i16                             0xbc7c12400 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_scale                                  0xbc7c12700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_scale_4                                0xbc7c12a00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_clamp                                  0xbc7c12d00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_tanh                                   0xbc7c13000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_relu                                   0xbc7c13300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_sigmoid                                0xbc7c13600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_gelu                                   0xbc7c13900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_gelu_4                                 0xbc7c13c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_gelu_erf                               0xbc7c14000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_gelu_erf_4                             0xbc7c14300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_gelu_quick                             0xbc7c14600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_gelu_quick_4                           0xbc7c14900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_silu                                   0xbc7c14c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_silu_4                                 0xbc7c14f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_elu                                    0xbc7c15200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_abs                                    0xbc7c15500 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_sgn                                    0xbc7c15800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_step                                   0xbc7c15b00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_hardswish                              0xbc7c15e00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_hardsigmoid                            0xbc7c16100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_exp                                    0xbc7c16400 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_soft_max_f16                           0xbc7c16700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_soft_max_f16_4                         0xbc7c16a00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_soft_max_f32                           0xbc7c16d00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_soft_max_f32_4                         0xbc7c17000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_diag_mask_inf                          0xbc7c17300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_diag_mask_inf_8                        0xbc7c17600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_f32                           0xbc7c17900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_f16                           0xbc7c17c00 | th_max = 1024 | th_width =   32\nggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\nggml_metal_init: loaded kernel_get_rows_q4_0                          0xbc7c1c000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_q4_1                          0xbc7c1c300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_q5_0                          0xbc7c1c600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_q5_1                          0xbc7c1c900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_q8_0                          0xbc7c1cc00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_mxfp4                         0xbc7c1cf00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_q2_K                          0xbc7c1d200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_q3_K                          0xbc7c1d500 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_q4_K                          0xbc7c1d800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_q5_K                          0xbc7c1db00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_q6_K                          0xbc7c1de00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0xbc7c1e100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_iq2_xs                        0xbc7c1e400 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0xbc7c1e700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_iq3_s                         0xbc7c1ea00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_iq2_s                         0xbc7c1ed00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_iq1_s                         0xbc7c1f000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_iq1_m                         0xbc7c1f300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_iq4_nl                        0xbc7c1f600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_iq4_xs                        0xbc7c1f900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_get_rows_i32                           0xbc7c1fc00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_set_rows_f32                           0xbc7c24000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_set_rows_f16                           0xbc7c24300 | th_max = 1024 | th_width =   32\nggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\nggml_metal_init: loaded kernel_set_rows_q8_0                          0xbc7c24600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_set_rows_q4_0                          0xbc7c24900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_set_rows_q4_1                          0xbc7c24c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_set_rows_q5_0                          0xbc7c24f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_set_rows_q5_1                          0xbc7c25200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_set_rows_iq4_nl                        0xbc7c25500 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rms_norm                               0xbc7c25800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rms_norm_mul                           0xbc7c25b00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rms_norm_mul_add                       0xbc7c25e00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_l2_norm                                0xbc7c26100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_group_norm                             0xbc7c26400 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_norm                                   0xbc7c26700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_ssm_conv_f32                           0xbc7c26a00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_ssm_scan_f32                           0xbc7c26d00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_ssm_scan_f32_group                     0xbc7c27000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0xbc7c27300 | th_max =  384 | th_width =   32\nggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0xbc7c27600 | th_max =  448 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_f32_f32                         0xbc7c27900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0xbc7c27c00 | th_max = 1024 | th_width =   32\nggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\nggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\nggml_metal_init: loaded kernel_mul_mv_f16_f32                         0xbc9a20000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0xbc9a20300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0xbc9a20600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0xbc9a20900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_f16_f16                         0xbc9a20c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0xbc9a20f00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0xbc9a21200 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0xbc9a21500 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0xbc9a21800 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0xbc9a21b00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0xbc9a21e00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0xbc9a22100 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0xbc9a22400 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0xbc9a22700 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0xbc9a22a00 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0xbc9a22d00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0xbc9a23000 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0xbc9a23300 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0xbc9a23600 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0xbc9a23900 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0xbc9a23c00 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0xbc9a24000 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0xbc9a24300 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0xbc9a24600 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0xbc9a24900 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0xbc9a24c00 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0xbc9a24f00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0xbc9a25200 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0xbc9a25500 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0xbc9a25800 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0xbc9a25b00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0xbc9a25e00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0xbc9a26100 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0xbc9a26400 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0xbc9a26700 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0xbc9a26a00 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0xbc9a26d00 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0xbc9a27000 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0xbc9a27300 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0xbc9a27600 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0xbc9a27900 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0xbc9a27c00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0xbc9a2c000 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0xbc9a2c300 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0xbc9a2c600 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0xbc9a2c900 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0xbc9a2cc00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0xbc9a2cf00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0xbc9a2d200 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0xbc9a2d500 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0xbc9a2d800 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0xbc9a2db00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0xbc9a2de00 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0xbc9a2e100 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0xbc9a2e400 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0xbc9a2e700 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0xbc9a2ea00 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0xbc9a2ed00 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0xbc9a2f000 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0xbc9a2f300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0xbc9a2f600 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0xbc9a2f900 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0xbc9a2fc00 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0xbc9a34000 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0xbc9a34300 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0xbc9a34600 | th_max =  448 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0xbc9a34900 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0xbc9a34c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0xbc9a34f00 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0xbc9a35200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0xbc9a35500 | th_max = 1024 | th_width =   32\nggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\nggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0xbc9a35800 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0xbc9a35b00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0xbc9a35e00 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0xbc9a36100 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0xbc9a36400 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0xbc9a36700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0xbc9a36a00 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0xbc9a36d00 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0xbc9a37000 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0xbc9a37300 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0xbc9a37600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0xbc9a37900 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0xbc9a37c00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0xbc9a38000 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0xbc9a38300 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0xbc9a38600 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0xbc9a38900 | th_max =  448 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0xbc9a38c00 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0xbc9a38f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0xbc9a39200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_f32_f32                         0xbc9a39500 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_f16_f32                         0xbc9a39800 | th_max =  832 | th_width =   32\nggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\nggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0xbc9a39b00 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0xbc9a39e00 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0xbc9a3a100 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0xbc9a3a400 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0xbc9a3a700 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0xbc9a3aa00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0xbc9a3ad00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0xbc9a3b000 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0xbc9a3b300 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0xbc9a3b600 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0xbc9a3b900 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0xbc9a3bc00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0xbc9a3c000 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0xbc9a3c300 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0xbc9a3c600 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0xbc9a3c900 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0xbc9a3cc00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0xbc9a3cf00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0xbc9a3d200 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0xbc9a3d500 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0xbc9a3d800 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0xbc9a3db00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0xbc9a3de00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0xbc9a3e100 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0xbc9a3e400 | th_max =  896 | th_width =   32\nggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\nggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0xbc9a3e700 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0xbc9a3ea00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0xbc9a3ed00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0xbc9a3f000 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0xbc9a3f300 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0xbc9a3f600 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0xbc9a3f900 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0xbc9a3fc00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0xbc9a40000 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0xbc9a40300 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0xbc9a40600 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0xbc9a40900 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0xbc9a40c00 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0xbc9a40f00 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0xbc9a41200 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0xbc9a41500 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0xbc9a41800 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0xbc9a41b00 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0xbc9a41e00 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0xbc9a42100 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_rope_norm_f32                          0xbc9a42400 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_norm_f16                          0xbc9a42700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_multi_f32                         0xbc9a42a00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_multi_f16                         0xbc9a42d00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_vision_f32                        0xbc9a43000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_vision_f16                        0xbc9a43300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_neox_f32                          0xbc9a43600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_rope_neox_f16                          0xbc9a43900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_f16                             0xbc9a43c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_f32                             0xbc9a44000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_ext_f16                         0xbc9a44300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_im2col_ext_f32                         0xbc9a44600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0xbc9a44900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0xbc9a44c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_upscale_f32                            0xbc9a44f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_pad_f32                                0xbc9a45200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0xbc9a45500 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_timestep_embedding_f32                 0xbc9a45800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_arange_f32                             0xbc9a45b00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0xbc9a45e00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0xbc9a46100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_leaky_relu_f32                         0xbc9a46400 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0xbc9a46700 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0xbc9a46a00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0xbc9a46d00 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0xbc9a47000 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0xbc9a47300 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0xbc9a47600 | th_max =  512 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0xbc9a47900 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0xbc9a47c00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0xbc9a4c000 | th_max =  384 | th_width =   32\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\nggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0xbc9a4c300 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0xbc9a4c600 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0xbc9a4c900 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0xbc9a4cc00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0xbc9a4cf00 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0xbc9a4d200 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0xbc9a4d500 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0xbc9a4d800 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0xbc9a4db00 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0xbc9a4de00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0xbc9a4e100 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0xbc9a4e400 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0xbc9a4e700 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0xbc9a4ea00 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0xbc9a4ed00 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0xbc9a4f000 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0xbc9a4f300 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0xbc9a4f600 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0xbc9a4f900 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0xbc9a4fc00 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0xbc9a50000 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0xbc9a50300 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0xbc9a50600 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0xbc9a50900 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0xbc9a50c00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0xbc9a50f00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0xbc9a51200 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0xbc9a51500 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0xbc9a51800 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0xbc9a51b00 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0xbc9a51e00 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0xbc9a52100 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0xbc9a52400 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0xbc9a52700 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0xbc9a52a00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0xbc9a52d00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0xbc9a53000 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0xbc9a53300 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0xbc9a53600 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0xbc9a53900 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0xbc9a53c00 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0xbc9a54000 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0xbc9a54300 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0xbc9a54600 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0xbc9a54900 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0xbc9a54c00 | th_max =  896 | th_width =   32\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0xbc9a54f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0xbc9a55200 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0xbc9a55500 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0xbc9a55800 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0xbc9a55b00 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0xbc9a55e00 | th_max = 1024 | th_width =   32\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0xbc9a56100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0xbc9a56400 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0xbc9a56700 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0xbc9a56a00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0xbc9a56d00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0xbc9a57000 | th_max =  896 | th_width =   32\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0xbc9a57300 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0xbc9a57600 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0xbc9a57900 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0xbc9a57c00 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0xbc9a5c000 | th_max =  896 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0xbc9a5c300 | th_max =  640 | th_width =   32\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0xbc9a5c600 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0xbc9a5c900 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0xbc9a5cc00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0xbc9a5cf00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0xbc9a5d200 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0xbc9a5d500 | th_max =  768 | th_width =   32\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0xbc9a5d800 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0xbc9a5db00 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0xbc9a5de00 | th_max =  768 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0xbc9a5e100 | th_max =  704 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0xbc9a5e400 | th_max =  832 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0xbc9a5e700 | th_max =  576 | th_width =   32\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0xbc9a5ea00 | th_max =  640 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0xbc9a5ed00 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0xbc9a5f000 | th_max =  512 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0xbc9a5f300 | th_max =  512 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0xbc9a5f600 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0xbc9a5f900 | th_max =  576 | th_width =   32\nggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0xbc9a5fc00 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0xbc7c28000 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0xbc7c28300 | th_max =  512 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0xbc7c28600 | th_max =  512 | th_width =   32\nggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0xbc7c28900 | th_max =  576 | th_width =   32\nggml_metal_init: loaded kernel_set_f32                                0xbc7c28c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_set_i32                                0xbc7c28f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_f32_f32                            0xbc7c29200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_f32_f16                            0xbc7c29500 | th_max = 1024 | th_width =   32\nggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\nggml_metal_init: loaded kernel_cpy_f16_f32                            0xbc7c29800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_f16_f16                            0xbc7c29b00 | th_max = 1024 | th_width =   32\nggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\nggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\nggml_metal_init: loaded kernel_cpy_f32_q8_0                           0xbc7c29e00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_f32_q4_0                           0xbc7c2a100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_f32_q4_1                           0xbc7c2a400 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_f32_q5_0                           0xbc7c2a700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_f32_q5_1                           0xbc7c2aa00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0xbc7c2ad00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_q4_0_f32                           0xbc7c2b000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_q4_0_f16                           0xbc7c2b300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_q4_1_f32                           0xbc7c2b600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_q4_1_f16                           0xbc7c2b900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_q5_0_f32                           0xbc7c2bc00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_q5_0_f16                           0xbc9a64000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_q5_1_f32                           0xbc9a64300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_q5_1_f16                           0xbc9a64600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_q8_0_f32                           0xbc9a64900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cpy_q8_0_f16                           0xbc9a64c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_concat                                 0xbc9a64f00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_sqr                                    0xbc9a65200 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_sqrt                                   0xbc9a65500 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_sin                                    0xbc9a65800 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_cos                                    0xbc9a65b00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_neg                                    0xbc9a65e00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_reglu                                  0xbc9a66100 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_geglu                                  0xbc9a66400 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_swiglu                                 0xbc9a66700 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_swiglu_oai                             0xbc9a66a00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_geglu_erf                              0xbc9a66d00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_geglu_quick                            0xbc9a67000 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_sum_rows                               0xbc9a67300 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_mean                                   0xbc9a67600 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_argmax                                 0xbc9a67900 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_pool_2d_avg_f32                        0xbc9a67c00 | th_max = 1024 | th_width =   32\nggml_metal_init: loaded kernel_pool_2d_max_f32                        0xbc9a68000 | th_max = 1024 | th_width =   32\nset_abort_callback: call\nllama_context:        CPU  output buffer size =     1.00 MiB\ncreate_memory: n_ctx = 512 (padded)\nllama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\nllama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 512 cells\nllama_kv_cache_unified: layer   0: skipped\nllama_kv_cache_unified: layer   1: skipped\nllama_kv_cache_unified: layer   2: skipped\nllama_kv_cache_unified: layer   3: skipped\nllama_kv_cache_unified: layer   4: skipped\nllama_kv_cache_unified: layer   5: dev = CPU\nllama_kv_cache_unified: layer   6: skipped\nllama_kv_cache_unified: layer   7: skipped\nllama_kv_cache_unified: layer   8: skipped\nllama_kv_cache_unified: layer   9: skipped\nllama_kv_cache_unified: layer  10: skipped\nllama_kv_cache_unified: layer  11: dev = CPU\nllama_kv_cache_unified: layer  12: skipped\nllama_kv_cache_unified: layer  13: skipped\nllama_kv_cache_unified: layer  14: skipped\nllama_kv_cache_unified: layer  15: skipped\nllama_kv_cache_unified: layer  16: skipped\nllama_kv_cache_unified: layer  17: dev = CPU\nllama_kv_cache_unified: layer  18: skipped\nllama_kv_cache_unified: layer  19: skipped\nllama_kv_cache_unified: layer  20: skipped\nllama_kv_cache_unified: layer  21: skipped\nllama_kv_cache_unified: layer  22: skipped\nllama_kv_cache_unified: layer  23: dev = CPU\nllama_kv_cache_unified: layer  24: skipped\nllama_kv_cache_unified: layer  25: skipped\nllama_kv_cache_unified:        CPU KV buffer size =     2.00 MiB\nllama_kv_cache_unified: size =    2.00 MiB (   512 cells,   4 layers,  1/1 seqs), K (f16):    1.00 MiB, V (f16):    1.00 MiB\nllama_kv_cache_unified_iswa: creating     SWA KV cache, size = 512 cells\nllama_kv_cache_unified: layer   0: dev = CPU\nllama_kv_cache_unified: layer   1: dev = CPU\nllama_kv_cache_unified: layer   2: dev = CPU\nllama_kv_cache_unified: layer   3: dev = CPU\nllama_kv_cache_unified: layer   4: dev = CPU\nllama_kv_cache_unified: layer   5: skipped\nllama_kv_cache_unified: layer   6: dev = CPU\nllama_kv_cache_unified: layer   7: dev = CPU\nllama_kv_cache_unified: layer   8: dev = CPU\nllama_kv_cache_unified: layer   9: dev = CPU\nllama_kv_cache_unified: layer  10: dev = CPU\nllama_kv_cache_unified: layer  11: skipped\nllama_kv_cache_unified: layer  12: dev = CPU\nllama_kv_cache_unified: layer  13: dev = CPU\nllama_kv_cache_unified: layer  14: dev = CPU\nllama_kv_cache_unified: layer  15: dev = CPU\nllama_kv_cache_unified: layer  16: dev = CPU\nllama_kv_cache_unified: layer  17: skipped\nllama_kv_cache_unified: layer  18: dev = CPU\nllama_kv_cache_unified: layer  19: dev = CPU\nllama_kv_cache_unified: layer  20: dev = CPU\nllama_kv_cache_unified: layer  21: dev = CPU\nllama_kv_cache_unified: layer  22: dev = CPU\nllama_kv_cache_unified: layer  23: skipped\nllama_kv_cache_unified: layer  24: dev = CPU\nllama_kv_cache_unified: layer  25: dev = CPU\nllama_kv_cache_unified:        CPU KV buffer size =    11.00 MiB\nllama_kv_cache_unified: size =   11.00 MiB (   512 cells,  22 layers,  1/1 seqs), K (f16):    5.50 MiB, V (f16):    5.50 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 3\nllama_context: max_nodes = 2720\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\ngraph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\nllama_context:        CPU compute buffer size =   516.50 MiB\nllama_context: graph nodes  = 1151\nllama_context: graph splits = 366 (with bs=512), 1 (with bs=1)\nMetal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \nModel metadata: {'quantize.imatrix.entries_count': '182', 'quantize.imatrix.file': 'gemma-3-1b-it-GGUF/imatrix_unsloth.dat', 'general.quantization_version': '2', 'quantize.imatrix.chunks_count': '663', 'tokenizer.chat_template': '{{ bos_token }}\\n{%- if messages[0][\\'role\\'] == \\'system\\' -%}\\n    {%- if messages[0][\\'content\\'] is string -%}\\n        {%- set first_user_prefix = messages[0][\\'content\\'] + \\'\\n\\n\\' -%}\\n    {%- else -%}\\n        {%- set first_user_prefix = messages[0][\\'content\\'][0][\\'text\\'] + \\'\\n\\n\\' -%}\\n    {%- endif -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \"\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\\n    {%- endif -%}\\n    {%- if (message[\\'role\\'] == \\'assistant\\') -%}\\n        {%- set role = \"model\" -%}\\n    {%- else -%}\\n        {%- set role = message[\\'role\\'] -%}\\n    {%- endif -%}\\n    {{ \\'&lt;start_of_turn&gt;\\' + role + \\'\\n\\' + (first_user_prefix if loop.first else \"\") }}\\n    {%- if message[\\'content\\'] is string -%}\\n        {{ message[\\'content\\'] | trim }}\\n    {%- elif message[\\'content\\'] is iterable -%}\\n        {%- for item in message[\\'content\\'] -%}\\n            {%- if item[\\'type\\'] == \\'image\\' -%}\\n                {{ \\'&lt;start_of_image&gt;\\' }}\\n            {%- elif item[\\'type\\'] == \\'text\\' -%}\\n                {{ item[\\'text\\'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\"Invalid content type\") }}\\n    {%- endif -%}\\n    {{ \\'&lt;end_of_turn&gt;\\n\\' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{\\'&lt;start_of_turn&gt;model\\n\\'}}\\n{%- endif -%}\\n', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.pre': 'default', 'tokenizer.ggml.padding_token_id': '0', 'gemma3.attention.head_count_kv': '1', 'gemma3.attention.sliding_window': '512', 'gemma3.rope.freq_base': '1000000.000000', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '2', 'gemma3.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '106', 'general.repo_url': 'https://huggingface.co/unsloth', 'gemma3.feed_forward_length': '6912', 'quantize.imatrix.dataset': 'unsloth_calibration_gemma-3-1b-it.txt', 'general.name': 'Gemma-3-1B-It', 'general.quantized_by': 'Unsloth', 'gemma3.attention.key_length': '256', 'gemma3.attention.head_count': '4', 'general.size_label': '1B', 'gemma3.block_count': '26', 'tokenizer.ggml.model': 'llama', 'gemma3.context_length': '32768', 'gemma3.attention.value_length': '256', 'gemma3.embedding_length': '1152', 'general.file_type': '15', 'general.finetune': 'it', 'tokenizer.ggml.add_space_prefix': 'false', 'general.architecture': 'gemma3', 'general.basename': 'Gemma-3-1B-It'}\nAvailable chat formats from metadata: chat_template.default"
  },
  {
    "objectID": "src/05/notebooks/python-llamacpp-binding.html#chat-with-the-model-using-chat-completion-api",
    "href": "src/05/notebooks/python-llamacpp-binding.html#chat-with-the-model-using-chat-completion-api",
    "title": "Using llama.cpp Binding for Python",
    "section": "Chat with the model using chat completion API",
    "text": "Chat with the model using chat completion API\n\nllm.create_chat_completion(\n      messages = [\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\n              \"role\": \"user\",\n              \"content\": \"Hello\"\n          }\n      ]\n)\n\nllama_perf_context_print:        load time =     205.62 ms\nllama_perf_context_print: prompt eval time =     205.11 ms /    10 tokens (   20.51 ms per token,    48.76 tokens per second)\nllama_perf_context_print:        eval time =     394.64 ms /    32 runs   (   12.33 ms per token,    81.09 tokens per second)\nllama_perf_context_print:       total time =     611.16 ms /    42 tokens\nllama_perf_context_print:    graphs reused =         30\n\n\n{'id': 'chatcmpl-2887bb2b-b1a5-428d-a42b-46c83a97fb6b',\n 'object': 'chat.completion',\n 'created': 1770147671,\n 'model': '../code/gguf/gemma-3-1b-it-Q4_K_M.gguf',\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant',\n    'content': \"Hello there! How's your day going so far?  \\n\\nIs there anything you'd like to chat about, or need any help with?\"},\n   'logprobs': None,\n   'finish_reason': 'stop'}],\n 'usage': {'prompt_tokens': 10, 'completion_tokens': 32, 'total_tokens': 42}}"
  }
]