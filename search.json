[
  {
    "objectID": "src/04/notebooks/vlm-gemma-3-4b.html",
    "href": "src/04/notebooks/vlm-gemma-3-4b.html",
    "title": "Using Gemma 3 (4B) to identify images",
    "section": "",
    "text": "from transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-4b-it\",\n    device=\"cuda\",\n    dtype=torch.bfloat16\n)\n\n\nfrom IPython.display import Image\n\nIMAGE_URL = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"\n\nImage(url=IMAGE_URL, width=500)\n\n\n\n\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": IMAGE_URL},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"},\n        ],\n    },\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n\nOkay, let's take a look!\n\nBased on the image, the animal on the candy is a **turtle**. You can see the shell pattern clearly. \n\nWould you like to know anything else about these candies?",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "vlm-gemma-3-4b.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-replicate.html#install-dependencies",
    "href": "src/04/notebooks/text-to-image-replicate.html#install-dependencies",
    "title": "Text-to-Image using Replicate",
    "section": "Install Dependencies",
    "text": "Install Dependencies\n\n!uv pip install replicate -q",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-replicate.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-replicate.html#set-replicate-api-token",
    "href": "src/04/notebooks/text-to-image-replicate.html#set-replicate-api-token",
    "title": "Text-to-Image using Replicate",
    "section": "Set Replicate API Token",
    "text": "Set Replicate API Token\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  REPLICATE_API_TOKEN = userdata.get('REPLICATE_API_TOKEN')\nelse:\n  load_dotenv()\n  REPLICATE_API_TOKEN = os.getenv('REPLICATE_API_TOKEN')",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-replicate.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-replicate.html#text-to-image",
    "href": "src/04/notebooks/text-to-image-replicate.html#text-to-image",
    "title": "Text-to-Image using Replicate",
    "section": "Text-to-Image",
    "text": "Text-to-Image\n\nimport replicate\noutput = replicate.run(\n  \"black-forest-labs/flux-pro\",\n  input={\n      \"steps\": 28,\n      \"prompt\": \"lemon cupcake spelling out the words 'DigiPen' with sparklers, tasty, food photography, dynamic shot\",\n      \"seed\": 1564435,\n      \"output_format\": \"png\",\n      \"safety_tolerance\": 2,\n      \"prompt_upsampling\": False\n  },\n)\n\n\nimport io\nfrom PIL import Image\n\noutput_bytes = io.BytesIO(output.read())\noutput_image = Image.open(output_bytes)\noutput_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-replicate.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/outpainting.html",
    "href": "src/04/notebooks/outpainting.html",
    "title": "Outpainting using black-forest-labs/flux-fill-pro",
    "section": "",
    "text": "!uv pip install replicate",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "outpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/outpainting.html#input-image",
    "href": "src/04/notebooks/outpainting.html#input-image",
    "title": "Outpainting using black-forest-labs/flux-fill-pro",
    "section": "Input Image",
    "text": "Input Image\n\nfrom IPython.display import Image\n\nINPUT_IMAGE = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/campus.png\"\n\nImage(url=INPUT_IMAGE)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "outpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/outpainting.html#call-black-forest-labsflux-fill-pro-with-outpainting-parameters",
    "href": "src/04/notebooks/outpainting.html#call-black-forest-labsflux-fill-pro-with-outpainting-parameters",
    "title": "Outpainting using black-forest-labs/flux-fill-pro",
    "section": "Call black-forest-labs/flux-fill-pro with outpainting parameters",
    "text": "Call black-forest-labs/flux-fill-pro with outpainting parameters\n\nimport replicate\n\n# Call the Replicate API\noutput = replicate.run(\n    \"black-forest-labs/flux-fill-pro\",\n    input={\n        \"image\": INPUT_IMAGE,\n        \"prompt\": \"The main building of a technical college, no text\",\n        \"seed\": 123456,\n        \"steps\": 50,\n        \"guidance\": 60,\n        \"outpaint\": \"Zoom out 2x\",\n        \"output_format\": \"jpg\",\n        \"safety_tolerance\": 2,\n        \"prompt_upsampling\": False\n    }\n)\n\n\nimport io\nfrom PIL import Image\n\noutput_bytes = io.BytesIO(output.read())\noutput_image = Image.open(output_bytes)\noutput_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "outpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "",
    "text": "# @title Install dependencies\n!uv pip install -q diffusers[\"torch\"]==0.35.1 transformers==4.56.2 accelerate==1.10.1",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#load-sd-1.5-model-using-pipeline",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#load-sd-1.5-model-using-pipeline",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Load SD 1.5 model using pipeline",
    "text": "Load SD 1.5 model using pipeline\n\nimport torch\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\n# Load the img2img pipeline\nprint(\"Loading Stable Diffusion img2img pipeline...\")\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n    dtype=torch.float16\n)\npipe = pipe.to(\"cuda\")\n\nLoading Stable Diffusion img2img pipeline...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeyword arguments {'dtype': torch.float16} are not expected by StableDiffusionImg2ImgPipeline and will be ignored.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#generate-intermediate-images",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#generate-intermediate-images",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Generate Intermediate Images",
    "text": "Generate Intermediate Images\n\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nIMAGE_URL = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/luna.jpg\"\nSEED = 128763\nPROMPT = \"a goldendoodle wearing sunglasses, high quality, detailed\"\nNEGATIVE_PROMPT = \"blurry, low quality, distorted\"\n\nresponse = requests.get(IMAGE_URL)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n# Resize to standard size for faster processing\ninit_image = init_image.resize((512, 712))\n\ndisplay(init_image)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#function-to-generate-using-strength-param",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#function-to-generate-using-strength-param",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Function to generate using strength param",
    "text": "Function to generate using strength param\n\ndef generate_image(strength):\n  return pipe(\n      prompt=PROMPT,\n      negative_prompt=NEGATIVE_PROMPT,\n      image=init_image,\n      strength=strength,\n      guidance_scale=7.5,\n      num_inference_steps=30,\n      generator=torch.Generator().manual_seed(SEED) if SEED != -1 else None,\n  ).images[0]",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.3",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.3",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Strength 0.3",
    "text": "Strength 0.3\n\ndisplay(generate_image(0.3))",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.5",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.5",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Strength 0.5",
    "text": "Strength 0.5\n\ndisplay(generate_image(0.5))",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.7",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.7",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Strength 0.7",
    "text": "Strength 0.7\n\ndisplay(generate_image(0.7))",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.9",
    "href": "src/04/notebooks/image-to-image-sd-1.5.html#strength-0.9",
    "title": "Image-to-Image using Stable Diffusion 1.5",
    "section": "Strength 0.9",
    "text": "Strength 0.9\n\ndisplay(generate_image(0.9))",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "image-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/depth-map-as-context.html#install-dependencies",
    "href": "src/04/notebooks/depth-map-as-context.html#install-dependencies",
    "title": "Depth Map as Context",
    "section": "Install Dependencies",
    "text": "Install Dependencies\n\n!uv pip install replicate -q",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "depth-map-as-context.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/depth-map-as-context.html#set-replicate-api-token",
    "href": "src/04/notebooks/depth-map-as-context.html#set-replicate-api-token",
    "title": "Depth Map as Context",
    "section": "Set Replicate API Token",
    "text": "Set Replicate API Token\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  REPLICATE_API_TOKEN = userdata.get('REPLICATE_API_TOKEN')\nelse:\n  load_dotenv()\n  REPLICATE_API_TOKEN = os.getenv('REPLICATE_API_TOKEN')",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "depth-map-as-context.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/depth-map-as-context.html#control-image",
    "href": "src/04/notebooks/depth-map-as-context.html#control-image",
    "title": "Depth Map as Context",
    "section": "Control Image",
    "text": "Control Image\n\nfrom IPython.display import Image\n\nCONTROL_IMAGE = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/campus-depth.png\"\n\nImage(url=CONTROL_IMAGE)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "depth-map-as-context.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/depth-map-as-context.html#image-to-image-with-guiding-prompt-using-flux-depth-pro",
    "href": "src/04/notebooks/depth-map-as-context.html#image-to-image-with-guiding-prompt-using-flux-depth-pro",
    "title": "Depth Map as Context",
    "section": "Image-to-Image with guiding prompt using Flux Depth Pro",
    "text": "Image-to-Image with guiding prompt using Flux Depth Pro\n\nimport replicate\n\nMODEL = \"black-forest-labs/flux-depth-pro\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"control_image\": CONTROL_IMAGE,\n      \"output_format\": \"png\",\n      \"seed\": 12345,\n      \"prompt\": \"A futuristic building set in 2050, neon lighting, night shot, dynamic\"\n  },\n)\n\n\nimport io\nfrom PIL import Image\n\noutput_bytes = io.BytesIO(output.read())\noutput_image = Image.open(output_bytes)\noutput_image\n\n\n\n\n\n\n\n\n\nimport replicate\n\nMODEL = \"black-forest-labs/flux-depth-pro\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"control_image\": CONTROL_IMAGE,\n      \"output_format\": \"png\",\n      \"seed\": 12345,\n      \"prompt\": \"A historical castle set in medieval England, clear day, partially cloudy sky\"\n  },\n)\n\n\nimport io\nfrom PIL import Image\n\noutput_bytes = io.BytesIO(output.read())\noutput_image = Image.open(output_bytes)\noutput_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "depth-map-as-context.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/open-meteo-mcp.html#note-to-run-this-notebook-on-colab",
    "href": "src/03/notebooks/open-meteo-mcp.html#note-to-run-this-notebook-on-colab",
    "title": "OpenAI Agents SDK with MCP Server",
    "section": "NOTE: To run this notebook on Colab…",
    "text": "NOTE: To run this notebook on Colab…\n\nRun the pre-requisites cell below to install libraries and NodeJS\nOpen up the Terminal (bottom left of window in Colab)\nRun the following command: TRANSPORT=http npx -y open-meteo-mcp-server - this will start the MCP Server\nRun the rest of the cells",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "open-meteo-mcp.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/open-meteo-mcp.html#install-pre-requisites-updated-openai-agents-sdk-and-nodejs",
    "href": "src/03/notebooks/open-meteo-mcp.html#install-pre-requisites-updated-openai-agents-sdk-and-nodejs",
    "title": "OpenAI Agents SDK with MCP Server",
    "section": "Install pre-requisites (Updated OpenAI Agents SDK and NodeJS)",
    "text": "Install pre-requisites (Updated OpenAI Agents SDK and NodeJS)\n\n!uv pip install -q openai-agents==0.7.0\n!curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "open-meteo-mcp.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/open-meteo-mcp.html#set-the-openai-api-key-environment-variable",
    "href": "src/03/notebooks/open-meteo-mcp.html#set-the-openai-api-key-environment-variable",
    "title": "OpenAI Agents SDK with MCP Server",
    "section": "Set the OpenAI API Key environment variable",
    "text": "Set the OpenAI API Key environment variable\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nelse:\n  load_dotenv()",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "open-meteo-mcp.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/open-meteo-mcp.html#mcp-server-setup",
    "href": "src/03/notebooks/open-meteo-mcp.html#mcp-server-setup",
    "title": "OpenAI Agents SDK with MCP Server",
    "section": "MCP Server Setup",
    "text": "MCP Server Setup\n\nimport logging\n\n# Suppress MCP client warnings about non-JSON messages from the server\nlogging.getLogger(\"mcp\").setLevel(logging.CRITICAL)\n\nfrom agents import Agent, Runner\nfrom agents.mcp import MCPServerStdio, MCPServerStreamableHttp\n\n\nList Available Tools\n\ntry:\n  async with MCPServerStreamableHttp(\n      params = {\"url\": \"http://localhost:3000/mcp\"}\n      ) as server:\n    tools = await server.list_tools()\n    print(f\"Available tools: {[tool.name for tool in tools]}\")\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nAvailable tools: ['weather_forecast', 'weather_archive', 'air_quality', 'marine_weather', 'elevation', 'flood_forecast', 'seasonal_forecast', 'climate_projection', 'ensemble_forecast', 'geocoding', 'dwd_icon_forecast', 'gfs_forecast', 'meteofrance_forecast', 'ecmwf_forecast', 'jma_forecast', 'metno_forecast', 'gem_forecast']\n\n\n\n\nSimple Weather Query\n\ntry:\n  async with MCPServerStreamableHttp(\n    params = {\"url\": \"http://localhost:3000/mcp\"}\n    ) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n    result = await Runner.run(agent, \"What's the weather forecast for Minneapolis–St. Paul this week?\")\n    print(result.final_output)\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nMinneapolis–St. Paul (Twin Cities) forecast for the next 7 days (America/Chicago):\n\n- **Fri Jan 23:** Partly cloudy. **High -8°F / Low -20°F**. Precip **0**. Wind up to **12 mph**.  \n- **Sat Jan 24:** Partly cloudy. **High 0°F / Low -16°F**. Precip **0**. Wind up to **6 mph**.  \n- **Sun Jan 25:** Partly cloudy. **High 8°F / Low -7°F**. Precip **0**. Wind up to **9 mph**.  \n- **Mon Jan 26:** Partly cloudy. **High 13°F / Low -7°F**. Precip **0**. Wind up to **9 mph**.  \n- **Tue Jan 27:** Partly cloudy. **High 12°F / Low 3°F**. Precip **0**. Wind up to **12 mph**.  \n- **Wed Jan 28:** Partly cloudy. **High 8°F / Low 0°F**. Precip **0**. Wind up to **8 mph**.  \n- **Thu Jan 29:** Partly cloudy. **High 10°F / Low 2°F**. Precip **0**. Wind up to **5 mph**.\n\nOverall: **cold, mostly partly cloudy, and dry all week** (no measurable precipitation in the forecast).\n\n\n\n\nAir Quality Query\n\ntry:\n  async with MCPServerStreamableHttp(\n    params = {\"url\": \"http://localhost:3000/mcp\"}\n    ) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n\n    result = await Runner.run(agent, \"What's the current air quality in Los Angeles?\")\n    print(result.final_output)\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nLos Angeles air quality (latest available hour in the model: **2026-01-23 00:00**, America/Los_Angeles):\n\n- **PM2.5:** 48.4 µg/m³  \n- **PM10:** 50.1 µg/m³  \n- **Ozone (O₃):** 0 µg/m³  \n- **Nitrogen dioxide (NO₂):** 71.5 µg/m³  \n- **Carbon monoxide (CO):** 896 µg/m³  \n- **Sulphur dioxide (SO₂):** 18.8 µg/m³  \n\nIf you tell me your nearest neighborhood (e.g., Downtown, Hollywood, Santa Monica), I can check a more location-specific point.\n\n\n\n\nHistorical Weather Data\n\ntry:\n  async with MCPServerStreamableHttp(\n    params = {\"url\": \"http://localhost:3000/mcp\"}\n    ) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n\n    result = await Runner.run(agent, \"What was the average temperature in Tokyo during January 2024?\")\n    print(result.final_output)\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nUsing ERA5 reanalysis for **Tokyo (35.6895, 139.6917)**, the **average temperature in January 2024** (computed as the mean of each day’s \\((T_\\max + T_\\min)/2\\) over Jan 1–31) was:\n\n**≈ 6.1 °C**.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "open-meteo-mcp.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#install-pre-requisites",
    "href": "src/03/notebooks/campus-agent.html#install-pre-requisites",
    "title": "DigiPen Campus Agent",
    "section": "Install pre-requisites",
    "text": "Install pre-requisites\n\n!uv pip install openai-agents==0.4.2 gradio==5.49.1\n\n\nResolved 190 packages in 1ms\n\nInstalled 10 packages in 33ms12.0                           \n\n + colorama==0.4.6\n\n + cryptography==46.0.3\n\n + griffe==1.15.0\n\n + httpx-sse==0.4.3\n\n + mcp==1.25.0\n\n + openai-agents==0.4.2\n\n + pydantic-settings==2.12.0\n\n + pyjwt==2.10.1\n\n + sse-starlette==3.1.2\n\n + types-requests==2.32.4.20260107",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#set-the-openai-api-key-environment-variable",
    "href": "src/03/notebooks/campus-agent.html#set-the-openai-api-key-environment-variable",
    "title": "DigiPen Campus Agent",
    "section": "Set the OpenAI API Key Environment Variable",
    "text": "Set the OpenAI API Key Environment Variable\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nelse:\n  load_dotenv()",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#agents",
    "href": "src/03/notebooks/campus-agent.html#agents",
    "title": "DigiPen Campus Agent",
    "section": "Agents",
    "text": "Agents\n\nfrom agents import Agent, Runner, FileSearchTool\n\n\nCafe Agent\n\nfrom agents import function_tool\n\n@function_tool\ndef get_bytes_cafe_menu(date: str) -&gt; any:\n    return {\n        f\"{date}\": {\n            \"daily byte\": {\n                \"name\": \"Steak Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Flank steak, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"vegetarian\": {\n                \"name\": \"Impossible Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Impossible plant based product, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"international\": {\n                \"name\": \"Chicken Curry\",\n                \"price\": 12,\n                \"description\": \"Chicken thighs, onion, carrot, potato, curry sauce served over rice\",\n            },\n        }\n    }\n\n\ncafe_agent = Agent(\n    name=\"Cafe Agent\",\n    instructions=\"You help students locate and provide information about the Bytes Cafe.\",\n    tools=[\n        get_bytes_cafe_menu,\n    ],\n)\n\n\n\nSet Vector Store\n\nVECTOR_STORE_ID = \"vs_6896d8c959008191981d645850b42313\"\n\n\n\nBuilding Agent\n\nbuilding_agent = Agent(\n    name=\"Building Agent\",\n    instructions=\"You help students locate and provide information about buildings and rooms on campus. Be descriptive when giving locations.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=3,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)\n\n\n\nCourse Agent\n\ncourse_agent = Agent(\n    name=\"Course Agent\",\n    instructions=\"You help students find out information about courses held at DigiPen.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)\n\n\n\nHandbook Agent\n\nhandbook_agent = Agent(\n    name=\"Handbook Agent\",\n    instructions=\"You help students navigate the school handbook, providing information about campus policies and student conduct.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)\n\n\n\nCampus Agent\n\nagent = Agent(\n    name=\"DigiPen Campus Agent\",\n    instructions=\"You are a helpful campus agent that can plan and execute tasks for students at DigiPen. Please be concise and accurate in handing off tasks to other agents as needed.\",\n    handoffs=[building_agent, course_agent, handbook_agent, cafe_agent],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#gradio-interface",
    "href": "src/03/notebooks/campus-agent.html#gradio-interface",
    "title": "DigiPen Campus Agent",
    "section": "Gradio Interface",
    "text": "Gradio Interface\n\nChat function and display of tool calls\n\nfrom gradio import ChatMessage\n\nasync def chat_with_agent(user_msg: str, history: list):\n    messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in history]\n    messages.append({\"role\": \"user\", \"content\": user_msg})\n    responses = []\n    reply_created = False\n    active_agent = None\n\n    result = Runner.run_streamed(agent, messages)\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\":\n            if event.data.type == \"response.output_text.delta\":\n                if not reply_created:\n                    responses.append(ChatMessage(role=\"assistant\", content=\"\"))\n                    reply_created = True\n                responses[-1].content += event.data.delta\n        elif event.type == \"agent_updated_stream_event\":\n            active_agent = event.new_agent.name\n            responses.append(\n                ChatMessage(\n                    content=event.new_agent.name,\n                    metadata={\"title\": \"Agent Now Running\", \"id\": active_agent},\n                )\n            )\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                if event.item.raw_item.type == \"file_search_call\":\n                    responses.append(\n                        ChatMessage(\n                            content=f\"Query used: {event.item.raw_item.queries}\",\n                            metadata={\n                                \"title\": \"File Search Completed\",\n                                \"parent_id\": active_agent,\n                            },\n                        )\n                    )\n                else:\n                    tool_name = getattr(event.item.raw_item, \"name\", \"unknown_tool\")\n                    tool_args = getattr(event.item.raw_item, \"arguments\", {})\n                    responses.append(\n                        ChatMessage(\n                            content=f\"Calling tool {tool_name} with arguments {tool_args}\",\n                            metadata={\"title\": \"Tool Call\", \"parent_id\": active_agent},\n                        )\n                    )\n            if event.item.type == \"tool_call_output_item\":\n                responses.append(\n                    ChatMessage(\n                        content=f\"Tool output: '{event.item.raw_item['output']}'\",\n                        metadata={\"title\": \"Tool Output\", \"parent_id\": active_agent},\n                    )\n                )\n            if event.item.type == \"handoff_call_item\":\n                responses.append(\n                    ChatMessage(\n                        content=f\"Name: {event.item.raw_item.name}\",\n                        metadata={\n                            \"title\": \"Handing Off Request\",\n                            \"parent_id\": active_agent,\n                        },\n                    )\n                )\n        yield responses\n\n\n\nLaunch Gradio\n\nimport gradio as gr\n\ndemo = gr.ChatInterface(\n    chat_with_agent,\n    title=\"DigiPen Campus Agent\",\n    theme=gr.themes.Soft(\n        primary_hue=\"red\", secondary_hue=\"slate\", font=[gr.themes.GoogleFont(\"Inter\")]\n    ),\n    examples=[\n        \"I'm trying to find the WANIC classrooms. Can you help?\",\n        \"What's the policy for eating in auditoriums?\",\n        \"What's today's vegetarian dish at the Bytes Cafe?\",\n        \"What are the prerequisites for FLM201?\"\n    ],\n    submit_btn=True,\n    flagging_mode=\"manual\",\n    flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n    type=\"messages\",\n    save_history=False,\n)\n\ndemo.launch(share=False, debug=True)\n\n* Running on local URL:  http://127.0.0.1:7863\n* To create a public link, set `share=True` in `launch()`.\n\n\n\n\n\nKeyboard interruption in main thread... closing server.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/structured-outputs.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Structured Outputs",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/structured-outputs.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Structured Outputs",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#create-the-openai-client-with-openrouter-url",
    "href": "src/02/notebooks/structured-outputs.html#create-the-openai-client-with-openrouter-url",
    "title": "Structured Outputs",
    "section": "Create the OpenAI client with OpenRouter URL",
    "text": "Create the OpenAI client with OpenRouter URL\n\nimport openai\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#create-the-location-type",
    "href": "src/02/notebooks/structured-outputs.html#create-the-location-type",
    "title": "Structured Outputs",
    "section": "Create the “Location” type",
    "text": "Create the “Location” type\n\nfrom pydantic import BaseModel\n\n# Define the model for a geographic location\nclass Location(BaseModel):\n  name: str\n  country: str\n  latitude: float\n  longitude: float",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#send-request-with-response_format-set",
    "href": "src/02/notebooks/structured-outputs.html#send-request-with-response_format-set",
    "title": "Structured Outputs",
    "section": "Send request with response_format set",
    "text": "Send request with response_format set\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.parse(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"What are the GPS coordinates for Paris?\"},\n    ],\n    response_format=Location\n)\n\ncompletion = response.choices[0].message\nprint(completion)\n\nParsedChatCompletionMessage[Location](content='{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=Location(name='Paris', country='France', latitude=48.8566, longitude=2.3522), reasoning=None)\n\n\n\n# Display the JSON repesentation\nprint(completion.content)\n\n# Display the parsed type\nprint(completion.parsed)\n\n# Pretty-print\nif completion.parsed:\n  location: Location = completion.parsed\n  print(f\"{location.name}, {location.country} has GPS coordinates of {location.latitude}, {location.longitude}\")\n\n{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}\nname='Paris' country='France' latitude=48.8566 longitude=2.3522\nParis, France has GPS coordinates of 48.8566, 2.3522",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#install-required-packages",
    "href": "src/02/notebooks/gradio.html#install-required-packages",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Install required packages",
    "text": "Install required packages\n\n!uv pip install gradio==5.49.1 openai\n\n\nUsing Python 3.13.1 environment at: /Users/simon/Dev/CS-394/.venv\n\nAudited 2 packages in 14ms",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/gradio.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/gradio.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#initialize-the-openai-client",
    "href": "src/02/notebooks/gradio.html#initialize-the-openai-client",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Initialize the OpenAI client",
    "text": "Initialize the OpenAI client\n\nimport openai\n\n# Initialize OpenAI client\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#example-1-basic-gradio-interface",
    "href": "src/02/notebooks/gradio.html#example-1-basic-gradio-interface",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Example 1: Basic Gradio interface",
    "text": "Example 1: Basic Gradio interface\n\nimport gradio as gr\n\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\n\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\")\ndemo.launch()\n\n* Running on local URL:  http://127.0.0.1:7862\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#example-2-basic-chat-interface-with-conversation-history",
    "href": "src/02/notebooks/gradio.html#example-2-basic-chat-interface-with-conversation-history",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Example 2: Basic chat interface with conversation history",
    "text": "Example 2: Basic chat interface with conversation history\n\nimport gradio as gr\n\ndef chat_with_history(message, history):\n    # Add current message\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Get response from API\n    response = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n    )\n    \n    return response.choices[0].message.content\n\n# Create a chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_history,\n    title=\"Basic Chat with Conversation History\",\n    type=\"messages\"\n)\n\ndemo.launch()\n\n* Running on local URL:  http://127.0.0.1:7863\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#example-3-streaming-chat-interface",
    "href": "src/02/notebooks/gradio.html#example-3-streaming-chat-interface",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Example 3: Streaming chat interface",
    "text": "Example 3: Streaming chat interface\n\nimport gradio as gr\n\ndef chat_with_streaming(message, history):\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Stream the response\n    stream = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n        stream=True,\n    )\n    \n    response_text = \"\"\n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            token = chunk.choices[0].delta.content\n            response_text += token\n            yield response_text\n\n# Create streaming chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_streaming,\n    title=\"AI Chat with Streaming\",\n    type=\"messages\"\n)\n\ndemo.launch()\n\n* Running on local URL:  http://127.0.0.1:7864\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openai.html#or-grab-the-openai-api-key-if-running-locally",
    "href": "src/02/notebooks/chat-completion-openai.html#or-grab-the-openai-api-key-if-running-locally",
    "title": "Chat Completion API (via OpenAI)",
    "section": "(Or grab the OpenAI API key if running locally)",
    "text": "(Or grab the OpenAI API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openai.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openai.html#logging-function-to-print-the-api-request-to-the-console",
    "href": "src/02/notebooks/chat-completion-openai.html#logging-function-to-print-the-api-request-to-the-console",
    "title": "Chat Completion API (via OpenAI)",
    "section": "Logging function to print the API request to the console",
    "text": "Logging function to print the API request to the console\n\nimport json\n\ndef log_request(request):\n  print(\"\\n=== REQUEST ===\")\n  print(f\"URL: {request.url}\")\n  print(f\"Method: {request.method}\")\n\n  if request.content:\n    try:\n      body = json.loads(request.content.decode('utf-8'))\n      print(\"\\nBody:\")\n      print(json.dumps(body, indent=2))\n    except:\n      print(\"\\nBody:\")\n      print(request.content.decode('utf-8'))\n  print(\"=\" * 50)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openai.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openai.html#call-openai-via-the-sdk",
    "href": "src/02/notebooks/chat-completion-openai.html#call-openai-via-the-sdk",
    "title": "Chat Completion API (via OpenAI)",
    "section": "Call OpenAI via the SDK",
    "text": "Call OpenAI via the SDK\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    api_key=OPENAI_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-5\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://api.openai.com/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"gpt-5\"\n}\n==================================================\n\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"chatcmpl-CuVn7EYuGJUEUEQ18Cl0SM2nNz9Mj\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Awesome! I can tailor a plan, but a few quick questions help:\\n- When are you going and for how many days?\\n- First time in Paris?\\n- Main interests (art, food, fashion, history, photography, nightlife, kid-friendly, etc.) and preferred pace (relaxed vs. packed)?\\n- Any must-sees or hard no’s?\\n- Rough budget and food needs (vegetarian, kosher/halal, allergies)?\\n- Where are you staying (neighborhood) and are day trips okay (Versailles, Champagne, Giverny, Disneyland)?\\n\\nIf you want a quick starter plan, here’s a flexible 4-day outline you can reshuffle by weather and museum closures:\\n\\nDay 1 – Islands + Latin Quarter\\n- Île de la Cité: Notre-Dame exterior, Sainte-Chapelle (timed ticket), Conciergerie.\\n- Stroll the Latin Quarter: Shakespeare & Company, Sorbonne, Luxembourg Gardens.\\n- Evening: Seine cruise or sunset along the river.\\n\\nDay 2 – Louvre to Arc de Triomphe\\n- Morning: Louvre (timed entry). Tuileries and Palais-Royal gardens.\\n- Covered passages (Véronique/Grand Cerf/Jouffroy) and Opéra Garnier.\\n- Sunset view: Arc de Triomphe rooftop or Galeries Lafayette/Printemps terrace.\\n\\nDay 3 – Montmartre + Left Bank art\\n- Montmartre: Sacré-Cœur, Place du Tertre, quieter backstreets (Rue de l’Abreuvoir).\\n- Afternoon: Musée d’Orsay and/or Orangerie.\\n- Evening: Saint-Germain wine bar or jazz.\\n\\nDay 4 – Le Marais or Day Trip\\n- Marais walk: Place des Vosges, Musée Carnavalet, Picasso Museum (check hours), Jewish quarter, trendy boutiques.\\n- Optional day trip: Versailles (palace + gardens; get the timed passport ticket).\\n- Night: Eiffel Tower area (view from Trocadéro or Champ de Mars; book tower tickets if going up).\\n\\nOther great adds by interest\\n- Art/architecture: Rodin Museum; Bourse de Commerce; Fondation Louis Vuitton. Note: check Centre Pompidou’s renovation status.\\n- Food: Morning market (Aligre or Rue Cler), cheese/wine tasting, pastry crawl, bistro lunch, cooking class.\\n- Unique: Catacombs (book ahead), Père Lachaise Cemetery, Canal Saint-Martin, covered markets (Le Marché des Enfants Rouges).\\n- With kids: Jardin des Plantes (zoo + galleries), Cité des Sciences, Jardin d’Acclimatation, Parc de la Villette.\\n- Day trips: Giverny (Apr–Oct), Reims/Epernay for Champagne, Fontainebleau, Auvers-sur-Oise, Disneyland Paris.\\n\\nBook these in advance\\n- Eiffel Tower, Louvre, Sainte-Chapelle, Catacombs, Versailles, Palais Garnier tours, popular restaurants.\\n- Consider the Paris Museum Pass (2/4/6 days) if you’ll visit several museums; the Louvre still needs a timed reservation even with the pass.\\n\\nPractical tips\\n- Closures: Many museums close one day/week (e.g., Orsay Mon, some Tue). Check hours.\\n- Getting around: The Métro is fastest. Use a contactless bank card to tap in, or get a reloadable Navigo Easy. For a Monday–Sunday stay with lots of rides, a Navigo Découverte weekly pass can be good value.\\n- Dining: Reserve for dinner, especially weekends. Tipping is minimal (service included); round up or leave 5–10% for great service.\\n- Safety: Watch for pickpockets in crowded areas and on the Metro.\\n\\nShare your dates, length of stay, and interests, and I’ll turn this into a detailed day-by-day plan with mapped routes and restaurant picks near each stop.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": [],\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1767584609,\n  \"model\": \"gpt-5-2025-08-07\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": \"default\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 2224,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 2268,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"reasoning_tokens\": 1408,\n      \"rejected_prediction_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0\n    }\n  }\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openai.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html",
    "href": "src/01/notebooks/translation-transformer.html",
    "title": "Translation Transformer",
    "section": "",
    "text": "In this notebook, we use a small transformer (Helsinki-NLP/opus-mt-fr-en) to translate from French to English.",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#load-model",
    "href": "src/01/notebooks/translation-transformer.html#load-model",
    "title": "Translation Transformer",
    "section": "Load model",
    "text": "Load model\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"Helsinki-NLP/opus-mt-fr-en\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n\n\n\n\n\n\n\n\n\n/Users/simon/Dev/CS-394/.venv/lib/python3.13/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#tokenize",
    "href": "src/01/notebooks/translation-transformer.html#tokenize",
    "title": "Translation Transformer",
    "section": "Tokenize",
    "text": "Tokenize\n\nfrench_text = \"Bonjour, comment allez-vous?\"\ninput_ids = tokenizer.encode(french_text, return_tensors=\"pt\")\nprint(input_ids[0])\nprint(\"Tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n\ntensor([8703,    2, 1027, 5682,   21,  682,   54,    0])\nTokens: ['▁Bonjour', ',', '▁comment', '▁allez', '-', 'vous', '?', '&lt;/s&gt;']\n\n\n\n# @title Demonstrate contextual vectors using the encoder\n\n# French: \"Bonjour , comment allez  - vous  ?\"\n#          ↓       ↓    ↓      ↓    ↓  ↓    ↓\n# Encoder: [v1]   [v2] [v3]  [v4] [v5][v6][v7]  ← 7 vectors, each 512-dim\n#          └─────────────────────────────────┘\n\nencoder = model.get_encoder()\nencoder_output = encoder(input_ids)\nprint(\"Encoder output shape:\", encoder_output.last_hidden_state.shape)\nprint(\"Encoder output:\", encoder_output)\n\nEncoder output shape: torch.Size([1, 8, 512])\nEncoder output: BaseModelOutput(last_hidden_state=tensor([[[-0.3943,  0.4660,  0.0190,  ..., -0.5069,  0.2120, -0.3190],\n         [ 0.0957,  0.0780,  0.1918,  ..., -0.0854,  0.2138,  0.1528],\n         [-0.6160,  0.0295,  0.1918,  ..., -0.3886,  0.0770,  0.2311],\n         ...,\n         [-0.1839, -0.3798,  0.1832,  ..., -0.0041, -0.3633, -0.5455],\n         [ 0.0153,  0.0264,  0.1122,  ...,  0.1966, -0.3027, -0.3659],\n         [-0.0484,  0.0147,  0.0078,  ..., -0.1359, -0.0295, -0.0799]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), hidden_states=None, attentions=None)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#run-through-tokenizer",
    "href": "src/01/notebooks/translation-transformer.html#run-through-tokenizer",
    "title": "Translation Transformer",
    "section": "Run through tokenizer",
    "text": "Run through tokenizer\n\noutput_ids = model.generate(input_ids)\nprint(output_ids)\n\ntensor([[59513, 10537,     2,   541,    52,    55,    54,     0]])",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#decode-back-to-tokens-to-complete-the-translation",
    "href": "src/01/notebooks/translation-transformer.html#decode-back-to-tokens-to-complete-the-translation",
    "title": "Translation Transformer",
    "section": "Decode back to tokens to complete the translation",
    "text": "Decode back to tokens to complete the translation\n\nenglish_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(\"Translation:\", english_text)\n\nTranslation: Hello, how are you?",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/GPT-2.html",
    "href": "src/01/notebooks/GPT-2.html",
    "title": "Pre-trained GPT-2 Notebook",
    "section": "",
    "text": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load pre-trained GPT-2 model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n# Set pad token\ntokenizer.pad_token = tokenizer.eos_token\n\n\nimport torch\n\ndef autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n    # Encode the prompt with attention mask\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    # Generate continuation\n    with torch.no_grad():\n        output = model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n\n\nprompt = \"Mary had a little lamb\"\ncompletion = autocomplete(prompt, max_length=80)\nprint(completion)\n\nMary had a little lamb, and the young woman asked her for a little lamb, and they gave it to her.\n\n\"Oh, my child, it is good to have a little lamb,\" said he, \"but it is not to be bought, for it is hard to make, and it is much more difficult to make.\n\n\"When you have a little lamb, it\n\n\n\nprompts = [\n    \"Mary had a little lamb\",\n    \"The future of artificial intelligence\",\n    \"In a galaxy far, far away\",\n    \"DigiPen is a place where\",\n    \"def calculate_fibonacci(n):\"\n]\n\nfor prompt in prompts:\n    print(f\"\\nPrompt: {prompt}\")\n    print(\"-\" * 50)\n    completion = autocomplete(prompt, max_length=80)\n    print(f\"Output: {completion}\\n\")\n\n\nPrompt: Mary had a little lamb\n--------------------------------------------------\nOutput: Mary had a little lamb, and the child was very hungry, and so he took a small lamb and brought it to her, and she and the child were very merry. So the child went home and the lamb was brought to her. So she and the child went to the priest and he gave her a piece of bread and said to her, \"This is good bread for you, but what\n\n\nPrompt: The future of artificial intelligence\n--------------------------------------------------\nOutput: The future of artificial intelligence is uncertain, but its future is bright.\n\nAnd so, we are all waiting for a breakthrough.\n\nAnd that's why I think that it's important to understand how AI is coming to the table.\n\nOne of the big questions we have right now is how AI will be able to take over a world, and how it will be able to take\n\n\nPrompt: In a galaxy far, far away\n--------------------------------------------------\nOutput: In a galaxy far, far away, there is only one thing that matters. The fate of our galaxy.\n\nAnd it matters only to you.\n\nA New Frontier for Space\n\nIt's been almost two years since I first wrote a post about this book. And that's because I've been busy.\n\nIn the last month or so, I've been working on an\n\n\nPrompt: DigiPen is a place where\n--------------------------------------------------\nOutput: DigiPen is a place where you can share your creations.\n\nDon't let the name fool you. This is the place to share your creations and to share your creativity.\n\nDon't let the name fool you. This is the place to share your creations and to share your creativity.\n\nDon't let the name fool you. This is the place to share your creations and\n\n\nPrompt: def calculate_fibonacci(n):\n--------------------------------------------------\nOutput: def calculate_fibonacci(n):\n\nfibonacci(n) = 0.01\n\nreturn f(n)\n\ndef calculate_fibonacci(n):\n\nfibonacci(n) = 0.01\n\nreturn f(n)\n\ndef calculate_fibonacci(n):\n\nfibonacci",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "GPT-2.ipynb"
    ]
  },
  {
    "objectID": "src/08/resources.html",
    "href": "src/08/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Resources"
    ]
  },
  {
    "objectID": "src/08/resources.html#citations",
    "href": "src/08/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Resources"
    ]
  },
  {
    "objectID": "src/07/slides.html#recap",
    "href": "src/07/slides.html#recap",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "Recap",
    "text": "Recap\n\nUnderstood model training, dataset curation, what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy\nExplored use cases, advantages, and disadvantages of prompt engineering\nIntroduced and implemented RAG (Retrieval Augmented Generation) to increase the accuracy of a limited SLM\nStarted the exploration of how to fine-tune models using LoRA (Low Ranked Adaptation)\nUsed a foundational model to generate synthetic data for fine-tuning a 1B parameter model",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/slides.html#lesson-objectives",
    "href": "src/07/slides.html#lesson-objectives",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUse generated synthetic data to fine-tune a 1B parameter model\nUse W&B (Weights and Biases) to observe parameters during the training run\nPost-training, use W&B to use cosine similarity and LLM-as-a-Judge to evaluate the accuracy of our trained model\nTrain smaller models (270M parameters) and compare the results\nUnderstand and create a model card, upload the model to Hugging Face and share",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/slides.html#looking-ahead-1",
    "href": "src/07/slides.html#looking-ahead-1",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nTBD",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/slides.html#references-1",
    "href": "src/07/slides.html#references-1",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/assignment.html",
    "href": "src/07/assignment.html",
    "title": "Module 7 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/07/assignment.html#assignment",
    "href": "src/07/assignment.html#assignment",
    "title": "Module 7 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/06/resources.html",
    "href": "src/06/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Resources"
    ]
  },
  {
    "objectID": "src/06/resources.html#citations",
    "href": "src/06/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Resources"
    ]
  },
  {
    "objectID": "src/05/slides.html#recap",
    "href": "src/05/slides.html#recap",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Recap",
    "text": "Recap\n\nUnderstood the fundamentals and history of diffuser models\nExplored and used models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet\nSetup and used Replicate to create a custom pipeline of production-grade models\nUnderstood the fundamentals and history of Vision Encoders and VLMs\nImplemented/tested a local VLM model for on-device inference",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#lesson-objectives",
    "href": "src/05/slides.html#lesson-objectives",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile\nUnderstand hardware requirements and architectures for model inference - e.g., CUDA vs. ONNX vs. MLX vs. WebGPU\nExplore how quantization works and understand techniques and formats for quantizing existing models\nUse llama.cpp to quantize and run an SLM on local hardware/gaming PC\nIntegrate a quantized model within Unity/Unreal/WebAssembly",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#why-local-models-1",
    "href": "src/05/slides.html#why-local-models-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Why Local Models?",
    "text": "Why Local Models?\n\nPrivacy\n\nEvery call you make to OpenAI/Claude/OpenRouter may (or may not) get logged and/or be used for training purposes\nMany organizations don’t want their customer/financial data logged with an AI vendor\nThere may also be legal regulations/restrictions controlling this",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#why-local-models-2",
    "href": "src/05/slides.html#why-local-models-2",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Why Local Models?",
    "text": "Why Local Models?\n\nOffline\n\nEvery call you make to OpenAI/Claude/OpenRouter needs an Internet connection\nThat’s not always guaranteed!\nEducation is a good example - remote school in India and/or rural districts here in the US",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#why-local-models-3",
    "href": "src/05/slides.html#why-local-models-3",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Why Local Models?",
    "text": "Why Local Models?\n\nLatency\n\nEven with a network connection, calls can suffer from increased latency\nCan be a challenge if your application needs frequent, quick responses\ne.g., using a VLM to determine the contents of a video stream for a user with vision impairments",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#why-local-models-4",
    "href": "src/05/slides.html#why-local-models-4",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Why Local Models?",
    "text": "Why Local Models?\n\nCost\n\nWhile per-API costs are fractions of a cent, these can grow out of control with exponential growth\nMore pronounced for long conversation threads (think call center)\nOr agents with verbose tool call JSON requests/responses",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#whats-your-hardware-1",
    "href": "src/05/slides.html#whats-your-hardware-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "What’s Your Hardware?",
    "text": "What’s Your Hardware?\n\nNVIDIA (CUDA)\nAMD (ROCm)\nApple Silicon\nVarious NPU (Neural Processing Unit) vendors",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#nvidia-cuda",
    "href": "src/05/slides.html#nvidia-cuda",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "NVIDIA CUDA",
    "text": "NVIDIA CUDA\n\nCUDA (Compute Unified Device Architecture)\n\nLaunched in 2006 to introduce programming on GPUs (GPGPUs or General Purpose GPUs)\nA C-like programming interface\nPerfectly timed for the deep learning revolution of the 2010s\nAdditional libraries (e.g., cuBLAS, cuDNN) make CUDA the defacto standard today",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#how-cuda-works",
    "href": "src/05/slides.html#how-cuda-works",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "How CUDA Works",
    "text": "How CUDA Works\n\nMassive parallelism: CUDA exploits thousands of GPU cores simultaneously, making it ideal for matrix operations.\nMemory hierarchy: A tiered memory system with global, shared, and registers.\nKernel execution: Programs can launch kernels - same function that can operate on different data.",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#nvidia-cuda---hardware-support",
    "href": "src/05/slides.html#nvidia-cuda---hardware-support",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "NVIDIA CUDA - Hardware Support",
    "text": "NVIDIA CUDA - Hardware Support\n\nConsumer: RTX 40- and 50- cards with various VRAM options (8Gb - 24Gb) for local inference and small fine-tuning tasks. RTX 30- series still popular for education.\nLaptop: RTX 40- and 50- series also available on laptops (although less performant than discrete cards)\nWorkstation: DGX Spark launch in 2025, with GB10 and 128Gb of unified memory for medium fine-tuning tasks\nDatacenter GPUs: A/H series and GB-series for datacenters. NVLink for multi-GPU interconnectivity.",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#sidebar-tops",
    "href": "src/05/slides.html#sidebar-tops",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Sidebar: TOPS",
    "text": "Sidebar: TOPS\n\nTOPS (Terra Operations Per Second)\n\nHow many trillion operations a processor can perform per second\nOften qualified with the data type\n64 INT8 TOPS == 64 trillion 8-bit operations per second\n\nTFLOPS (Terra Floating-Point Operations Per Second)\n\n1 TFLOPS == 1 FP32 (32-bit floating point) TOPS",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#sidebar-tops-1",
    "href": "src/05/slides.html#sidebar-tops-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Sidebar: TOPS",
    "text": "Sidebar: TOPS\n\nRough Throughput Calculations\n\nYou have an NVIDIA 3090 (advertized at 35 TFLOPS)\nAssume a 7B param model with FP32 weights\nEach token generation requires ~2 FLOPs per parameter\nEach token generation ~= 14B FLOPs (7B params × 2)\nTheoretical max = 35T FLOPs/sec ÷ 14B FLOPs/token ≈ 2,500 tokens/sec\nReality: 10-100 tokens/sec typical due to memory bandwidth bottlenecks",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#amd-rocm",
    "href": "src/05/slides.html#amd-rocm",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "AMD ROCm",
    "text": "AMD ROCm\n\nROCm (Radeon Open Compute)\n\nLaunched in 2016 as an open-source alternative to CUDA\nEmbraced open standards (e.g., OpenCL), positioning as avoiding vendor lock-in, although this fragmentation initially hurt adoption\nHas evolved significantly since (e.g., rocBLAS) although ecosystem gaps compared to CUDA persist",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#amd-rocm---hardware-support",
    "href": "src/05/slides.html#amd-rocm---hardware-support",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "AMD ROCm - Hardware Support",
    "text": "AMD ROCm - Hardware Support\n\nConsumer: RX7000 series offer sustantial VRAM (up to 24Gb) at competitive prices compared to NVIDIA RTX\nLaptop: Some laptop options for AMD-based machines\nWorkstation: Strix Halo, competitor to DGX Spark, with RX8060S and 128Gb unified memory\nSoftware support: Linux only with no Windows support (some via WSL)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#apple-silicon",
    "href": "src/05/slides.html#apple-silicon",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Apple Silicon",
    "text": "Apple Silicon\n\nApple Silicon\n\nMetal, a low-level graphics and compute API, launched in 2014 and later expanded for general GPU compute tasks\nMPS (Metal Performance Shaders) introduced in 2017 and optimized primitives for neural networks. PyTorch added MPS device support in 2022.\nMLX released in 2023, providing NumPy-like API for Apple Silicon hardware",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#apple-silicon---hardware-support",
    "href": "src/05/slides.html#apple-silicon---hardware-support",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Apple Silicon - Hardware Support",
    "text": "Apple Silicon - Hardware Support\n\nAvailable on all M-series hardware\nUnified memory by default, upto 128Gb on laptops and 512Gb for the Mac Studio with M3 Ultra\nNon portable models. (MLX uses safetensors/npz format and MLX-specific code for Metal.)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#sidebar-unified-memory",
    "href": "src/05/slides.html#sidebar-unified-memory",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Sidebar: Unified Memory",
    "text": "Sidebar: Unified Memory\n\nGPUs have historically had separate memory (VRAM)\nUnified memory is a process to share memory between CPU and GPU\n\nFor Apple/MLX, it’s a true SoC (System on a Chip); NVIDIA DGX Spark, two physical components connected via NVLink-C2C\n\nHigher memory availability, but lower memory bandwidth\n\n~275Gb/s for Spark/MLX; ~1TB/s for 5080; ~1.7TB/s for 5090; ~3TBs for H100",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#npus",
    "href": "src/05/slides.html#npus",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "NPUs",
    "text": "NPUs\n\nNPUs (Neural Processing Units) are specialized AI accelerators, designed for lower power consumption\nOptimized specifically for NN operations (e.g., matmul, convolutions, activations)\nCommonly found in edge devices (smartphones, IoT, embedded systems)\n15-80 TOPS common for NPUs (~10x less that desktop PCI-based GPUs)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#npu-vendors",
    "href": "src/05/slides.html#npu-vendors",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "NPU Vendors",
    "text": "NPU Vendors\n\nIntel: Acquired Movidius in 2016; released Myriad X in 2017 and a neural compute stick. Superceded by NPUs in Core Ultra processors.\nQualcomm: Snapdragon 865 range in 2019; now Snapdragon X and 8 ranges. Used in Windows/ARM devices. Popular with Android, although Google recently moved to their own TPUs.\nAMD: XDNA formerly Xilinx; Windows Copilot PC range competing with SnapDragon and Intel Core Ultra.\nApple: ANE (Apple Neural Engine) to support CoreML workloads on iPhone, iPad devices",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#overcoming-compute-and-memory-bottlenecks-1",
    "href": "src/05/slides.html#overcoming-compute-and-memory-bottlenecks-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Overcoming Compute and Memory Bottlenecks",
    "text": "Overcoming Compute and Memory Bottlenecks\n\nTBD",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#out-of-vram",
    "href": "src/05/slides.html#out-of-vram",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "“Out of VRAM”",
    "text": "“Out of VRAM”\nOne challenge of running models on your own hardware is VRAM\n\nRoughly speaking, the size of the model will determine how much VRAM you need\nGemma 3 models\n\ngemma-3-1b-it = 2Gb\ngemma-3-4b-it = 8.6Gb\ngemma-3-12b-it = 23.37Gb\nQwen3-VL-235B-A22B-Thinking = ~475Gb",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#out-of-vram-1",
    "href": "src/05/slides.html#out-of-vram-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "“Out of VRAM”",
    "text": "“Out of VRAM”\n\nGoogle Colab Tiers\n\nColab Free T4 = 16Gb VRAM (15Gb usable)\nColab Pro V100 = 16Gb VRAM\nColab Pro A100 = 40Gb VRAM\n\nYour Gaming PC\n\nProbably 8Gb VRAM\n\nYour Phone\n\nV-what? :)",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#quantization",
    "href": "src/05/slides.html#quantization",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Quantization",
    "text": "Quantization\nProcess of reducing the precision of a model’s weights and activations. For example, 16-bit numbers to 4-bit.\n\nParameter count matters more than precision\n\nA 70B parameter model at 4-bit often beats a 13B model at b16\nThe models knowledge remains largely intact\nOften the extra precision doesn’t meaningfully improve outputs",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#quantization-formats",
    "href": "src/05/slides.html#quantization-formats",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Quantization Formats",
    "text": "Quantization Formats\nThe llama.cpp project (implementing LLMs in pure C/C++) has driven advancements in quantization\n\nGGUF (GPT-Generated Unified Format)\n\nSingle file architecture\nModel format supporting multiple quantization levels (2-bit through 8-bit) with CPU and GPU handoff\n\nMLX (Apple’s ML framework and format for Apple Silicon)\n\nDebuted in late 2023\nSupports 4 and 8 bit quantization schemes",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#running-quantized-models",
    "href": "src/05/slides.html#running-quantized-models",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Running Quantized Models",
    "text": "Running Quantized Models\n\nTools built upon llama.cpp\n\nOllama, LM Studio, koboldcpp",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#demo-c-client---gemma-3-27b-local",
    "href": "src/05/slides.html#demo-c-client---gemma-3-27b-local",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Demo: C# Client <-> Gemma 3 27B Local",
    "text": "Demo: C# Client &lt;-&gt; Gemma 3 27B Local\ndemos/01/lmstudio-client/LMStudioClient.csproj",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#hosting-models-in-unity",
    "href": "src/05/slides.html#hosting-models-in-unity",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Hosting Models in Unity",
    "text": "Hosting Models in Unity\n\nDownload the GGUF model locally to Assets/StreamingAssets folder\nUse llama.cpp bindings for C# to host\n\nLLAMASharp: https://github.com/SciSharp/LLamaSharp\n\nUse OpenAI SDK (or similar) as client\nUnity Demo\n\nhttps://github.com/eublefar/LLAMASharpUnityDemo",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#looking-ahead-1",
    "href": "src/05/slides.html#looking-ahead-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nTBD",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#references-1",
    "href": "src/05/slides.html#references-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/assignment.html",
    "href": "src/05/assignment.html",
    "title": "Module 5 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Assignment"
    ]
  },
  {
    "objectID": "src/05/assignment.html#assignment",
    "href": "src/05/assignment.html#assignment",
    "title": "Module 5 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/resources.html",
    "href": "src/04/resources.html",
    "title": "Resources",
    "section": "",
    "text": "The Illustrated Stable Diffusion - Jay Alammar’s visual guide to how diffusion models work\nStable Diffusion Paper - “High-Resolution Image Synthesis with Latent Diffusion Models” (2022)\nWhat are Diffusion Models? - Lilian Weng’s comprehensive overview\nHugging Face Diffusers Library - Official documentation for the diffusers library",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#diffusion-models",
    "href": "src/04/resources.html#diffusion-models",
    "title": "Resources",
    "section": "",
    "text": "The Illustrated Stable Diffusion - Jay Alammar’s visual guide to how diffusion models work\nStable Diffusion Paper - “High-Resolution Image Synthesis with Latent Diffusion Models” (2022)\nWhat are Diffusion Models? - Lilian Weng’s comprehensive overview\nHugging Face Diffusers Library - Official documentation for the diffusers library",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#stable-diffusion",
    "href": "src/04/resources.html#stable-diffusion",
    "title": "Resources",
    "section": "Stable Diffusion",
    "text": "Stable Diffusion\n\nStable Diffusion 1.5 on Hugging Face - Model card and weights\nSDXL Paper - “SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis”\nStability AI - Company behind Stable Diffusion",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#flux-models",
    "href": "src/04/resources.html#flux-models",
    "title": "Resources",
    "section": "FLUX Models",
    "text": "FLUX Models\n\nBlack Forest Labs - Creators of FLUX models\nFLUX.1 on Hugging Face - Official model repository\nFLUX.1 Technical Report - Overview of FLUX capabilities and tools",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#controlnet",
    "href": "src/04/resources.html#controlnet",
    "title": "Resources",
    "section": "ControlNet",
    "text": "ControlNet\n\nControlNet Paper - “Adding Conditional Control to Text-to-Image Diffusion Models” (2023)\nControlNet on Hugging Face - Original ControlNet models by Lvmin Zhang\nControlNet Guide - How to use ControlNet with diffusers",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#replicate",
    "href": "src/04/resources.html#replicate",
    "title": "Resources",
    "section": "Replicate",
    "text": "Replicate\n\nReplicate Home Page - Platform for running ML models via API\nReplicate Documentation - API reference and guides\nReplicate Python Client - Official Python library\nReplicate Collections - Curated model collections including free-to-try models",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#depth-estimation",
    "href": "src/04/resources.html#depth-estimation",
    "title": "Resources",
    "section": "Depth Estimation",
    "text": "Depth Estimation\n\nDepth Anything - State-of-the-art monocular depth estimation\nMiDaS - Intel’s robust monocular depth estimation model\nZoeDepth Paper - “ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth”",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#inpainting-and-outpainting",
    "href": "src/04/resources.html#inpainting-and-outpainting",
    "title": "Resources",
    "section": "Inpainting and Outpainting",
    "text": "Inpainting and Outpainting\n\nLaMa: Large Mask Inpainting - Resolution-robust large mask inpainting\nFlux Fill on Replicate - FLUX-based inpainting model",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#vision-transformers",
    "href": "src/04/resources.html#vision-transformers",
    "title": "Resources",
    "section": "Vision Transformers",
    "text": "Vision Transformers\n\nViT Paper - “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” (2020)\nCLIP Paper - “Learning Transferable Visual Models From Natural Language Supervision”\nDINOv2 - Meta’s self-supervised vision transformer",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#vision-language-models-vlms",
    "href": "src/04/resources.html#vision-language-models-vlms",
    "title": "Resources",
    "section": "Vision Language Models (VLMs)",
    "text": "Vision Language Models (VLMs)\n\nLLaVA Project Page - Large Language and Vision Assistant\nLLaVA Paper - “Visual Instruction Tuning”\nGemma 3 on Hugging Face - Google’s multimodal Gemma model\nFastVLM Paper - “FastVLM: Efficient Vision Encoding for Vision Language Models”\nFastVLM on Hugging Face - Apple’s efficient on-device VLM\nFastVLM WebGPU Demo - Run FastVLM in your browser",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#gradio-for-image-applications",
    "href": "src/04/resources.html#gradio-for-image-applications",
    "title": "Resources",
    "section": "Gradio for Image Applications",
    "text": "Gradio for Image Applications\n\nGradio Image Components - Image input/output documentation\nGradio ImageEditor - Component for drawing and editing images\nGradio Sketchpad - Simple drawing canvas component",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#prompt-engineering-for-image-models",
    "href": "src/04/resources.html#prompt-engineering-for-image-models",
    "title": "Resources",
    "section": "Prompt Engineering for Image Models",
    "text": "Prompt Engineering for Image Models\n\nDALL-E 3 Prompt Guide - OpenAI’s guide to image prompting\nStable Diffusion Prompt Guide - Community guide to effective prompts\nLexica - Search engine for Stable Diffusion prompts and images",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#citations",
    "href": "src/04/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/slides.html#recap",
    "href": "src/03/slides.html#recap",
    "title": "Module 3: Agents and Tools",
    "section": "Recap",
    "text": "Recap\n\nUnderstood the evolution and licensing of models from GPT-2 through to modern day\nUnderstood instruction-tuned models, how they work, and how to configure\nSetup and used OpenRouter for accessing hosted models\nUnderstood the OpenAI API specification, the request/response payload, parameters, streaming, and structured output\nCreated and shared a chatbot using a Gradio-based UI",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#lesson-objectives",
    "href": "src/03/slides.html#lesson-objectives",
    "title": "Module 3: Agents and Tools",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nDescribe the fundamental concepts behind Agents/Agentic AI\nExplore and provide feedback on an existing multi-agent setup\nUnderstand available agent SDKs, how they differ, and advantages/disadvantages\nUse the OpenAI Agents SDK to build a multi-agent system from scratch, including document indexing and retrieval\nUnderstand and implement tool calls and implement using OpenAI’s function calling and via MCP",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-agents-1",
    "href": "src/03/slides.html#why-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Why Agents?",
    "text": "Why Agents?\n\nLimitations of our prior chatbots\n\nNeeds constant human input every turn; No ability to plan beyond a single interaction\nSingle model with single context (conversation)\nNo ability to interact with external systems",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent",
    "href": "src/03/slides.html#what-is-an-agent",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.youtube.com/watch?v=bwXaJXgezf4",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-1",
    "href": "src/03/slides.html#what-is-an-agent-1",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.weforum.org/stories/2025/06/cognitive-enterprise-agentic-business-revolution/",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-2",
    "href": "src/03/slides.html#what-is-an-agent-2",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.crn.com/news/ai/2025/10-hottest-agentic-ai-tools-and-agents-of-2025-so-far",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-3",
    "href": "src/03/slides.html#what-is-an-agent-3",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-4",
    "href": "src/03/slides.html#what-is-an-agent-4",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\nImagine a DigiPen Campus Assistant: An AI agent that can help you navigate anything and everything at DigiPen!\n\n“Where can I find the ‘Hopper’ room?”\n“Can you tell me more about FLM201?”\n“Oh, and what’s today’s vegetarian option at the Bytes Cafe?”",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents",
    "href": "src/03/slides.html#five-characteristics-of-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Planners\n\n\nAgents are driven by goals\nAnd they can put together a plan for the steps to complete that goal.\n\n“First, I will discover where course information is located”\n“Then I will search for any courses that reference FLM201”\n“Then I summarize all of the key points for the student”",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-1",
    "href": "src/03/slides.html#five-characteristics-of-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Autonomous\n\n\nAgents can then go off and execute the plan, independent of human input\nThe concept of “human in the loop” still applies for confirmation\n\ne.g. “Do you really want to place this order at the Bytes Cafe?”",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-2",
    "href": "src/03/slides.html#five-characteristics-of-agents-2",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Reactive\n\n\nAgents can change mid-course depending on what they find and/or the environment.\n\ne.g. “I couldn’t find any course information on FLM201. I’m going to check if there are other 200-level FLM courses before responding to the student.”",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-3",
    "href": "src/03/slides.html#five-characteristics-of-agents-3",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents have Persistence\n\n\nAgents often have memory systems beyond the current conversation\nBroadly classified as short and long-term memory\n\nShort-term memory could be your order request at the Bytes cafe\nLong-term memory could be your food preferences",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-4",
    "href": "src/03/slides.html#five-characteristics-of-agents-4",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents can Interact with external systems\n\n\nAgents can delegate to other agents for complex tasks\n\n(Or for tasks where other agents are better suited for.)\ne.g., Campus Agent -&gt; delegating to a Course Agent\n\nAgents can also be given access to external tools\n\ne.g., File search, Web search, access to the Bytes Cafe API",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-1",
    "href": "src/03/slides.html#openai-agents-sdk-1",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK",
    "text": "OpenAI Agents SDK\n\nAnnounced in Mar 2025\n\nTogether with web search, file search, and computer use\nAnd a new Responses API (formerly Assistants API)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-2",
    "href": "src/03/slides.html#openai-agents-sdk-2",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK",
    "text": "OpenAI Agents SDK\n\nCreated to address the gap between chat completions (what we were using last week) and multi-step systems\n\nvs. building your own, which a lot of developers were doing at the time\n\nIntegrates function calling, handoffs, and session management in the same package\nSupports Python and TypeScript; MIT licensed",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#not-the-only-agent-sdk-in-town",
    "href": "src/03/slides.html#not-the-only-agent-sdk-in-town",
    "title": "Module 3: Agents and Tools",
    "section": "Not the only Agent SDK in town!",
    "text": "Not the only Agent SDK in town!\n\nSource: https://e2b.dev",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#langgraph",
    "href": "src/03/slides.html#langgraph",
    "title": "Module 3: Agents and Tools",
    "section": "LangGraph",
    "text": "LangGraph\n\nhttps://langchain-ai.github.io/langgraph/\nPython only\nMIT License\nOne of the first agent frameworks, building on LangChain\n\nIMO, too abstract/complex/bloated",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#crew.ai",
    "href": "src/03/slides.html#crew.ai",
    "title": "Module 3: Agents and Tools",
    "section": "Crew.ai",
    "text": "Crew.ai\n\nhttps://github.com/crewaiinc/crewai\nPython only\nOne of the more popular commercial offerings\n\n(Although they do have an MIT License/freemium model)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#microsoft",
    "href": "src/03/slides.html#microsoft",
    "title": "Module 3: Agents and Tools",
    "section": "Microsoft",
    "text": "Microsoft\n\nAutoGen\n\nhttps://microsoft.github.io/autogen/stable/\nPython (.NET coming soon)\nMIT License\n\nMicrosoft Semantic Kernel\n\nhttps://github.com/microsoft/semantic-kernel\nPython, .NET, Java\nMIT License",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#microsoft-1",
    "href": "src/03/slides.html#microsoft-1",
    "title": "Module 3: Agents and Tools",
    "section": "Microsoft",
    "text": "Microsoft\n\nNow converging into the Microsoft Agent Framework\nOne of the few agent SDKs to support .NET\nSpeaking of which, why are most SDKs in Python?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#agent-structure",
    "href": "src/03/slides.html#agent-structure",
    "title": "Module 3: Agents and Tools",
    "section": "Agent Structure",
    "text": "Agent Structure\n\nAgents and tools for the DigiPen Campus Agent",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#campus-agent",
    "href": "src/03/slides.html#campus-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Campus Agent",
    "text": "Campus Agent\n\n\nagent = Agent(\n    name=\"DigiPen Campus Agent\",\n    instructions=\"You are a helpful campus agent that can plan and execute tasks for students at DigiPen. Please be concise and accurate in handing off tasks to other agents as needed.\",\n    handoffs=[building_agent, course_agent, handbook_agent, cafe_agent],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#building-agent",
    "href": "src/03/slides.html#building-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Building Agent",
    "text": "Building Agent\n\n\nbuilding_agent = Agent(\n    name=\"Building Agent\",\n    instructions=\"You help students locate and provide information about buildings and rooms on campus. Be descriptive when giving locations.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=3,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#course-agent",
    "href": "src/03/slides.html#course-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Course Agent",
    "text": "Course Agent\n\n\ncourse_agent = Agent(\n    name=\"Course Agent\",\n    instructions=\"You help students find out information about courses held at DigiPen.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#handbook-agent",
    "href": "src/03/slides.html#handbook-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Handbook Agent",
    "text": "Handbook Agent\n\n\nhandbook_agent = Agent(\n    name=\"Handbook Agent\",\n    instructions=\"You help students navigate the school handbook, providing information about campus policies and student conduct.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store",
    "href": "src/03/slides.html#whats-a-vector-store",
    "title": "Module 3: Agents and Tools",
    "section": "What’s a Vector Store?",
    "text": "What’s a Vector Store?\n\nA way to provide domain-specific knowledge beyond training data\n\ne.g., current semester course information that is newer than GPT-5.2’s cutoff date\n\nWe could just insert these into the context window\n\nBut doesn’t scale to more than a few documents",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store-1",
    "href": "src/03/slides.html#whats-a-vector-store-1",
    "title": "Module 3: Agents and Tools",
    "section": "What’s a Vector Store?",
    "text": "What’s a Vector Store?\n\nInstead, we use a vector store\n\nConverts documents, paragraphs, or sentences into vector embeddings\n(Remember these from module 1? :)\n\nSimilar concepts are close to each other in vector space\n\nWhich makes it efficient to query\n\nQueries return the document (or pages within a document) that match\n\ne.g., “Michelangelo” returns “Page 1 of the floor map”",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store-2",
    "href": "src/03/slides.html#whats-a-vector-store-2",
    "title": "Module 3: Agents and Tools",
    "section": "What’s a Vector Store?",
    "text": "What’s a Vector Store?\n\nFoundation for RAG (Retrieval Augmented Generation)\n\n(Which we will cover in module 6)\n\nMany different types of vectors stores/databases\nFor now, we will be using OpenAI’s storage",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store-3",
    "href": "src/03/slides.html#whats-a-vector-store-3",
    "title": "Module 3: Agents and Tools",
    "section": "What’s a Vector Store?",
    "text": "What’s a Vector Store?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#agent-structure-1",
    "href": "src/03/slides.html#agent-structure-1",
    "title": "Module 3: Agents and Tools",
    "section": "Agent Structure",
    "text": "Agent Structure\n\nAgents and tools for the DigiPen Campus Agent",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-do-agents-need-tools",
    "href": "src/03/slides.html#why-do-agents-need-tools",
    "title": "Module 3: Agents and Tools",
    "section": "Why Do Agents Need Tools?",
    "text": "Why Do Agents Need Tools?\n\nThe scope of the agent’s ability is contained within the model\nTools enable the agent to reach out to systems beyond the model\nExamples\n\nRead a file from disk or search the web (built in)\nCalculator (because LLMs aren’t great at math)\nCode interpreter (running code on the fly)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-tool-calling",
    "href": "src/03/slides.html#openai-tool-calling",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Tool Calling",
    "text": "OpenAI Tool Calling\n\nIntroduced by OpenAI in June 2023\nOriginally called Function Calling\nModels are fine-tuned to return a structured function_call JSON object, specifying which function to call and with what arguments.\nTools are provided as functions\nOption for the LLM to decide when to call the tool (always, never, auto)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#cafe-agent",
    "href": "src/03/slides.html#cafe-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Cafe Agent",
    "text": "Cafe Agent\n\n\ncafe_agent = Agent(\n    name=\"Cafe Agent\",\n    instructions=\"You help students locate and provide information about the Bytes Cafe.\",\n    tools=[\n        get_bytes_cafe_menu,\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#cafe-agent-tool",
    "href": "src/03/slides.html#cafe-agent-tool",
    "title": "Module 3: Agents and Tools",
    "section": "Cafe Agent Tool",
    "text": "Cafe Agent Tool\n\n\nfrom agents import function_tool\n\n@function_tool\ndef get_bytes_cafe_menu(date: str) -&gt; any:\n    return {\n        f\"{date}\": {\n            \"daily byte\": {\n                \"name\": \"Steak Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Flank steak, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"vegetarian\": {\n                \"name\": \"Impossible Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Impossible plant based product, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"international\": {\n                \"name\": \"Chicken Curry\",\n                \"price\": 12,\n                \"description\": \"Chicken thighs, onion, carrot, potato, curry sauce served over rice\",\n            },\n        }\n    }",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n\nHow do models know when they should call a tool?\n\nModels are fine-tuned on conversations with tool call examples\nThe model learns patterns like “when the user asks about the weather, call the get_weather tool”\nThe request to call the tool is returned as a JSON payload",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-1",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-1",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n{\n  \"role\": \"assistant\",\n  \"content\": null,\n  \"tool_calls\": [\n    {\n      \"id\": \"call_abc123\",\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_weather\",\n        \"arguments\": \"{\\\"location\\\": \\\"San Francisco\\\", \\\"unit\\\": \\\"celsius\\\"}\"\n      }\n    }\n  ]\n}",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-2",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-2",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n\nThe client then calls the tool with the required parameters\nAnd returns the result back to the model as a “tool” role message",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-3",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-3",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n{\n  \"role\": \"tool\",\n  \"tool_call_id\": \"call_abc123\",\n  \"content\": \"{\\\"temperature\\\": 18, \\\"condition\\\": \\\"partly cloudy\\\", \\\"humidity\\\": 65, \\\"wind_speed\\\": 12}\"\n}",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-4",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-4",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n\nRLHF is used to improve the accuracy for tool selection\n\nRewards are given for correctly choosing the right tool for a task\nOr penalized for hallucinating tools and methods that don’t exist",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#lets-run-the-campus-agent-1",
    "href": "src/03/slides.html#lets-run-the-campus-agent-1",
    "title": "Module 3: Agents and Tools",
    "section": "Let’s Run the Campus Agent",
    "text": "Let’s Run the Campus Agent\n\nDoes the OpenAI Agents SDK work with OpenRouter?\n\nYes and No :)\n\nYes to core functionality\n\nCreating an agent, handoffs, calling custom tools\n\nNo to calling built-in OpenAI tools\n\nFile search, Web search, Code interpreter",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#lets-run-the-campus-agent-2",
    "href": "src/03/slides.html#lets-run-the-campus-agent-2",
    "title": "Module 3: Agents and Tools",
    "section": "Let’s Run the Campus Agent",
    "text": "Let’s Run the Campus Agent\n\nWe’ll need to create an OpenAI developer account\nPotentially add some credits to it",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-multiple-agents",
    "href": "src/03/slides.html#why-multiple-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Why Multiple Agents?",
    "text": "Why Multiple Agents?\n\nContext window limitations\nEach agent can have a different system prompt (instructions)\nMakes tool separation cleaner and more accurate\nEach agent can have a different underlying model\n\nSpecialized models (e.g., a vision encoder)\nOr to blend cost",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-multiple-agents-1",
    "href": "src/03/slides.html#why-multiple-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Why Multiple Agents?",
    "text": "Why Multiple Agents?\n\nCost considerations are really important with agents\n\nLarge number of tokens for reasoning\nHandoffs with multiple API calls\nEven more tokens with verbose tool calls\n\nMitigations\n\nDoes every agent need full GPT / frontier model capability?\nAgents doing primarily function calling (e.g., file search) can be much smaller/cheaper",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#example-of-multiple-agents",
    "href": "src/03/slides.html#example-of-multiple-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Example of Multiple Agents",
    "text": "Example of Multiple Agents\n\nCode generation\n\nAgents for ‘architect’, code writer, tester, debugger, etc.\n\nContent generation\n\nAgent to create content, other agents to generate images, translate content, etc.\n\nTravel booking\n\nAgent to book flights, hotels, cars, etc. for packages",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#patterns-for-agents",
    "href": "src/03/slides.html#patterns-for-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Patterns for Agents",
    "text": "Patterns for Agents\n\nAs you get deeper into building agents, patterns start to emerge\n\nRouter (which is what we used in our demo) - hand off of tasks\nOrchestrator (using other agents as tools)\nParallel agents (calling other agents in parallel and aggregating results)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#patterns-for-agents-1",
    "href": "src/03/slides.html#patterns-for-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Patterns for Agents",
    "text": "Patterns for Agents\n\nSource: https://www.anthropic.com/engineering/building-effective-agents",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#when-things-go-wrong-1",
    "href": "src/03/slides.html#when-things-go-wrong-1",
    "title": "Module 3: Agents and Tools",
    "section": "When Things Go Wrong",
    "text": "When Things Go Wrong\n\nAgents can be difficult to debug\n\nIncorrect handoffs\nInfinite loops can be common\nFailed to call the right tool at the right time",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#when-things-go-wrong-2",
    "href": "src/03/slides.html#when-things-go-wrong-2",
    "title": "Module 3: Agents and Tools",
    "section": "When Things Go Wrong",
    "text": "When Things Go Wrong\n\nOpenAI Agents SDK includes built-in tracing\n\nActually enabled by default!\n\nComprehensive record of:\n\ngenerations\ntool calls\nhandoffs\nguardrails\ncustom events",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#the-need-for-memory",
    "href": "src/03/slides.html#the-need-for-memory",
    "title": "Module 3: Agents and Tools",
    "section": "The Need for Memory",
    "text": "The Need for Memory\n\nJust like API calls, agents need the conversation/context every call\nThis can be challenging with agents working on long-running tasks\n\nAnd/or agents working on multiple threads with other agents\n\nShort-term and long-term memory",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory",
    "href": "src/03/slides.html#short-term-memory",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\nUsed to store/retreive the current conversation thread\nBuilt-in to most SDKs\nIn OpenAI Agents SDK called a session",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-1",
    "href": "src/03/slides.html#short-term-memory-1",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nfrom agents import Agent, Runner, SQLiteSession\n\nagent = Agent(name=\"Assistant\", instructions=\"Reply very concisely\")\nsession = SQLiteSession(\"conv_123\", db_path=SQLITE_DB)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-2",
    "href": "src/03/slides.html#short-term-memory-2",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nresult = await Runner.run(agent, \"My name is Simon\", session=session)\nprint(result.final_output)\n\nNice to meet you, Simon!",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-3",
    "href": "src/03/slides.html#short-term-memory-3",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nresult = await Runner.run(agent, \"What is my name?\", session=session)\nprint(result.final_output)\n\nYour name is Simon.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-4",
    "href": "src/03/slides.html#short-term-memory-4",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nresult = await Runner.run(agent, \"What is my name?\")\nprint(result.final_output)\n\nYou haven’t shared your name yet.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#long-term-memory",
    "href": "src/03/slides.html#long-term-memory",
    "title": "Module 3: Agents and Tools",
    "section": "Long-term Memory",
    "text": "Long-term Memory\n\nMore challenging\nYou don’t want to store/retrieve the entire conversation\nLong-term memory types\n\nFactual: General facts (e.g., name, address, seating preferences)\nEpisodic: Past conversations (e.g., user booked a trip to Paris)\nProcedural: Learnings (e.g., the best hotel site to book accommodation)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#implementing-long-term-memory",
    "href": "src/03/slides.html#implementing-long-term-memory",
    "title": "Module 3: Agents and Tools",
    "section": "Implementing Long-term Memory",
    "text": "Implementing Long-term Memory\n\nLots of startup options!\n\nSupermemory\nLetta\nmem0\n…and lots more",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#implementing-long-term-memory-mem0",
    "href": "src/03/slides.html#implementing-long-term-memory-mem0",
    "title": "Module 3: Agents and Tools",
    "section": "Implementing Long-term Memory (mem0)",
    "text": "Implementing Long-term Memory (mem0)\nfrom mem0 import Memory\nmemory = Memory()\n\n# Create new memories from the conversation\nmessages.append({\"role\": \"assistant\", \"content\": assistant_response})\nmemory.add(messages, user_id=user_id)\n\n# Retrieve relevant memories\nrelevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n# (append these to the system prompt)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#implementing-long-term-memory-1",
    "href": "src/03/slides.html#implementing-long-term-memory-1",
    "title": "Module 3: Agents and Tools",
    "section": "Implementing Long-term Memory",
    "text": "Implementing Long-term Memory\n\nOr “roll your own”\nLong-term memory types\n\nFactual, Episodic, Procedural\n\nCreate tools for factual storage\n\ne.g., a profile tool with set/get options\n\nUse LLM to summarize short-term session conversations to store episodic and procedural learnings.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#beyond-tool-calling-1",
    "href": "src/03/slides.html#beyond-tool-calling-1",
    "title": "Module 3: Agents and Tools",
    "section": "Beyond Tool Calling",
    "text": "Beyond Tool Calling\n\nTool calling is super useful, but…\n\nYou need to write the function(s) yourself\nAnd then expose them to OpenAI using the @function_tool method\n\nWhat if there was a way to standardize this?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol",
    "href": "src/03/slides.html#mcp-model-context-protocol",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)\n\nReleased by Anthropic in Nov 2024\nProvides a standard interface for tools - akin to a USB standard for peripherals\nImplementations are known as “MCP servers”\n\nA server exposes one or more tools (functions)\nUses JSON-RPC 2.0 as underlying RPC protocol\nServers can run remotely over HTTP (supports SSE)\nOr can be hosted locally and accessed via stdio\nMany servers hosted using Node.js",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol-1",
    "href": "src/03/slides.html#mcp-model-context-protocol-1",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol-2",
    "href": "src/03/slides.html#mcp-model-context-protocol-2",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol-3",
    "href": "src/03/slides.html#mcp-model-context-protocol-3",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-and-mcp-1",
    "href": "src/03/slides.html#openai-agents-sdk-and-mcp-1",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK and MCP",
    "text": "OpenAI Agents SDK and MCP\n\nMCP supported in OpenAI Agents SDK (as of Sep 2025)\nExposes MCPServerStdio and MCPServerSse to connect to local and remote servers",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-and-mcp-2",
    "href": "src/03/slides.html#openai-agents-sdk-and-mcp-2",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK and MCP",
    "text": "OpenAI Agents SDK and MCP\n\n\ntry:\n  async with MCPServerStreamableHttp(\n      params = {\"url\": \"http://localhost:3000/mcp\"}\n      ) as server:\n    tools = await server.list_tools()\n    print(f\"Available tools: {[tool.name for tool in tools]}\")\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nAvailable tools: ['weather_forecast', 'weather_archive', 'air_quality', 'marine_weather', 'elevation', 'flood_forecast', 'seasonal_forecast', 'climate_projection', 'ensemble_forecast', 'geocoding', 'dwd_icon_forecast', 'gfs_forecast', 'meteofrance_forecast', 'ecmwf_forecast', 'jma_forecast', 'metno_forecast', 'gem_forecast']",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-and-mcp-3",
    "href": "src/03/slides.html#openai-agents-sdk-and-mcp-3",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK and MCP",
    "text": "OpenAI Agents SDK and MCP\n\n\ntry:\n  async with MCPServerStreamableHttp(\n    params = {\"url\": \"http://localhost:3000/mcp\"}\n    ) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n    result = await Runner.run(agent, \"What's the weather forecast for Minneapolis–St. Paul this week?\")\n    print(result.final_output)\nexcept:\n  print(\"Is the MCP server running? Check at the top of this notebook for instructions.\")\n\nMinneapolis–St. Paul (Twin Cities) forecast for the next 7 days (America/Chicago):\n\n- **Fri Jan 23:** Partly cloudy. **High -8°F / Low -20°F**. Precip **0**. Wind up to **12 mph**.  \n- **Sat Jan 24:** Partly cloudy. **High 0°F / Low -16°F**. Precip **0**. Wind up to **6 mph**.  \n- **Sun Jan 25:** Partly cloudy. **High 8°F / Low -7°F**. Precip **0**. Wind up to **9 mph**.  \n- **Mon Jan 26:** Partly cloudy. **High 13°F / Low -7°F**. Precip **0**. Wind up to **9 mph**.  \n- **Tue Jan 27:** Partly cloudy. **High 12°F / Low 3°F**. Precip **0**. Wind up to **12 mph**.  \n- **Wed Jan 28:** Partly cloudy. **High 8°F / Low 0°F**. Precip **0**. Wind up to **8 mph**.  \n- **Thu Jan 29:** Partly cloudy. **High 10°F / Low 2°F**. Precip **0**. Wind up to **5 mph**.\n\nOverall: **cold, mostly partly cloudy, and dry all week** (no measurable precipitation in the forecast).",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#creating-your-own-mcp-server",
    "href": "src/03/slides.html#creating-your-own-mcp-server",
    "title": "Module 3: Agents and Tools",
    "section": "Creating Your Own MCP Server",
    "text": "Creating Your Own MCP Server\n\nMultiple SDKs on https://modelcontextprotocol.io/docs/sdk\n\nPython, TypeScript, Go, Rust, C#, and more\n\nVery similar to tool calling\n\nDefine your MCP server\nAnnotate your functions with @mcp.tool()\nAdd descriptions to the tool methods to help the LLM select which tool to call",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#example-microbit-mcp-server",
    "href": "src/03/slides.html#example-microbit-mcp-server",
    "title": "Module 3: Agents and Tools",
    "section": "Example: micro:bit MCP Server",
    "text": "Example: micro:bit MCP Server\n\n\nhttps://simonguest.com/p/microbit-mcp/\nhttps://simonguest.com/p/microbit-mcp/",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#hugging-face-spaces-1",
    "href": "src/03/slides.html#hugging-face-spaces-1",
    "title": "Module 3: Agents and Tools",
    "section": "Hugging Face Spaces",
    "text": "Hugging Face Spaces\n\nWe’ve been using Gradio, but hosting via notebooks isn’t ideal\n\nEven with share=True you have to keep the notebook running\n\nWouldn’t it be nice if we could easily host our Gradio app?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#hugging-face-spaces-2",
    "href": "src/03/slides.html#hugging-face-spaces-2",
    "title": "Module 3: Agents and Tools",
    "section": "Hugging Face Spaces",
    "text": "Hugging Face Spaces",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#hugging-face-spaces-3",
    "href": "src/03/slides.html#hugging-face-spaces-3",
    "title": "Module 3: Agents and Tools",
    "section": "Hugging Face Spaces",
    "text": "Hugging Face Spaces\n\nFree cloud hosting for ML demos and applications\n\nSupports Gradio, Streamlit, and static HTML/JS\n\nFor Gradio, either upload a main.py or a Docker configuration file\nHugging Face handles resource allocation\n\nSleeps the space if it’s inactive\nIntegrates with the queuing mechanism of Gradio to batch requests\nSupports multiple GPU types (if signed up for Pro account)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#looking-ahead-1",
    "href": "src/03/slides.html#looking-ahead-1",
    "title": "Module 3: Agents and Tools",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nLeave text behind and explore image-based models!\nIntroduce the diffuser\nAnd go other way with vision encoders",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#references-1",
    "href": "src/03/slides.html#references-1",
    "title": "Module 3: Agents and Tools",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/assignment.html",
    "href": "src/03/assignment.html",
    "title": "Module 3 Assignment: Extend the Campus Agent",
    "section": "",
    "text": "Objective: Extend the DigiPen Campus Agent by adding a new specialized agent that handles a specific domain.\nBackground:\nThe Campus Agent currently has four specialized agents: Building Agent, Course Agent, Handbook Agent, and Cafe Agent. Your task is to add a fifth agent that serves a new purpose on campus.\nSuggested Agent Ideas (pick one, or create your own):\n\nEvents Agent - Helps students find information about campus events, club meetings, and activities\nIT Agent - Assists with common IT issues, lab software, and printing\nLibrary Agent - Helps students find resources, reserve study rooms, or check hours\nCareer Services Agent - Provides information about internships, resume reviews, and career fairs\nTransportation Agent - Helps with parking, shuttle schedules, and commute options\n\nRequirements:\n\nCreate a new agent with appropriate instructions (system prompt)\nImplement knowledge retrieval using one (or more!) of these approaches:\n\nAdd documents to the vector store and use FileSearchTool, OR\nUse the WebSearchTool to give your agent access to the web, OR\nCreate a custom tool using @function_tool that returns relevant data (should be more complex than the cafe example) OR\nCreate a new (or select an existing) MCP server\n\nIntegrate with the main Campus Agent via handoff\nTest your agent with at least 3 different queries and document the results\nUse the OpenAI Traces Dashboard to debug at least one interaction and include a screenshot or description of what you observed\n\nDeliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation (building on the campus-agent.ipynb starter)\nUses OPENAI_API_KEY for the API token. (Please do not include your API key in your notebook!)\nMarkdown cells explaining:\n\nWhat agent you chose and why\nHow you implemented knowledge retrieval (vector store vs. custom tool)\nYour test queries and the agent’s responses\nWhat you learned from the Traces Dashboard (what worked, what didn’t, any debugging insights)\n\n\nBonus:\nDeploy your extended Campus Agent to Hugging Face Spaces and include the URL in your submission. Remember to add your OPENAI_API_KEY as a secret in your Space settings (not in the code!).\nHints:\n\nStart by copying the campus-agent.ipynb notebook and modifying it\nKeep your agent’s scope focused. A narrower domain with good data works better than a broad domain with sparse data\nIf using the vector store, you can upload files via the OpenAI Platform Storage\nIf using a custom tool, remember that the function’s docstring helps the LLM understand when to call it",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Assignment"
    ]
  },
  {
    "objectID": "src/02/resources.html",
    "href": "src/02/resources.html",
    "title": "Resources",
    "section": "",
    "text": "GPT-2 Release Blog Post - OpenAI’s original GPT-2 announcement\nGPT-3 Paper - “Language Models are Few-Shot Learners”\nInstructGPT Paper - “Training language models to follow instructions with human feedback”\nMeta’s Llama Models - Official Llama model family page",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#model-history-and-background",
    "href": "src/02/resources.html#model-history-and-background",
    "title": "Resources",
    "section": "",
    "text": "GPT-2 Release Blog Post - OpenAI’s original GPT-2 announcement\nGPT-3 Paper - “Language Models are Few-Shot Learners”\nInstructGPT Paper - “Training language models to follow instructions with human feedback”\nMeta’s Llama Models - Official Llama model family page",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#chat-templates-and-model-formats",
    "href": "src/02/resources.html#chat-templates-and-model-formats",
    "title": "Resources",
    "section": "Chat Templates and Model Formats",
    "text": "Chat Templates and Model Formats\n\nHugging Face Chat Templates Guide - Comprehensive guide to chat templates",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#openai-api",
    "href": "src/02/resources.html#openai-api",
    "title": "Resources",
    "section": "OpenAI API",
    "text": "OpenAI API\n\nOpenAI Platform - Create an account and get API keys\nOpenAI API Documentation - Official API reference\nChat Completions Guide - How to use the chat completions endpoint\nStructured Outputs Guide - Guide to structured outputs",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#openrouter",
    "href": "src/02/resources.html#openrouter",
    "title": "Resources",
    "section": "OpenRouter",
    "text": "OpenRouter\n\nOpenRouter Home Page - Unified API for hundreds of models\nOpenRouter Documentation - API docs and model listings\nOpenRouter Models - Browse available models and pricing",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#server-sent-events-sse-and-streaming",
    "href": "src/02/resources.html#server-sent-events-sse-and-streaming",
    "title": "Resources",
    "section": "Server-Sent Events (SSE) and Streaming",
    "text": "Server-Sent Events (SSE) and Streaming\n\nMDN: Server-Sent Events - Technical overview of SSE\nEventSource API - Browser API for SSE\nOpenAI Streaming Guide - How to implement streaming with OpenAI API",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#gradio",
    "href": "src/02/resources.html#gradio",
    "title": "Resources",
    "section": "Gradio",
    "text": "Gradio\n\nGradio Home Page\nGradio Documentation - Official documentation\nGradio ChatInterface - Chat interface component documentation\nGradio Guides - Tutorials and examples\nHugging Face Spaces - Platform for deploying Gradio apps",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#rlhf-and-model-training",
    "href": "src/02/resources.html#rlhf-and-model-training",
    "title": "Resources",
    "section": "RLHF and Model Training",
    "text": "RLHF and Model Training\n\nRLHF Explainer - Hugging Face’s comprehensive guide to RLHF\nReinforcement Learning from Human Feedback Paper - Original RLHF research",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#context-windows-and-token-management",
    "href": "src/02/resources.html#context-windows-and-token-management",
    "title": "Resources",
    "section": "Context Windows and Token Management",
    "text": "Context Windows and Token Management\n\nUnderstanding Context Windows - Anthropic’s post on long context\nToken Counting Best Practices - OpenAI cookbook example\nTiktoken - Token counting library for estimating costs",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#citations",
    "href": "src/02/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/slides.html#module-objectives",
    "href": "src/01/slides.html#module-objectives",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nExplore the history of vector embeddings and tokenization\nUnderstand the transformer architecture at a high level\nUse our first transformer to translate language\nCover a brief history of early generative transformers\nSetup and use Colab, and become familiar with the basics of notebooks and Python (if you haven’t used them already)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#rewind-to-2013",
    "href": "src/01/slides.html#rewind-to-2013",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Rewind To 2013",
    "text": "Rewind To 2013\n\nNLP (Natural Language Processing) was the thing!\n\nSentiment analysis, named entity recognition, parsing, etc.\n\nBut, you had limited options…\n\nOne-hot encoding\nHand crafted features\nNeural language models",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#word2vec-released",
    "href": "src/01/slides.html#word2vec-released",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2013: Word2Vec Released",
    "text": "2013: Word2Vec Released\n\nWord2Vec introduced by Mikolov and colleagues at Google Research in two papers\n\nSkip-gram and Continuous Bag-of-Words (CBOW) (Mikolov, Chen, et al. 2013)\nNegative sampling and subsampling techniques (Mikolov, Sutskever, et al. 2013)\n\nParadigm shift from count-based methods\n\nUsed Neural Networks (NNs) to predict words vs. large matrices\n\nFoundation for modern NLP tasks",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work",
    "href": "src/01/slides.html#how-does-word2vec-work",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\nWord Embeddings are meaningful numerical representations of words\n\nRepresentations where words are encoded into multi-dimensional space\nLarge number of dimensions (200-500 is typical)\nSimilar words have similar numbers",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-1",
    "href": "src/01/slides.html#how-does-word2vec-work-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"cat\"\nvector = model[word]\nvector[:10]\n\narray([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656],\n      dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-2",
    "href": "src/01/slides.html#how-does-word2vec-work-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"dog\"\nvector = model[word]\nvector[:10]\n\narray([ 0.05126953, -0.02233887, -0.17285156,  0.16113281, -0.08447266,\n        0.05737305,  0.05859375, -0.08251953, -0.01538086, -0.06347656],\n      dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-3",
    "href": "src/01/slides.html#how-does-word2vec-work-3",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"pizza\"\nvector = model[word]\nvector[:10]\n\narray([-1.2597656e-01,  2.5390625e-02,  1.6699219e-01,  5.5078125e-01,\n       -7.6660156e-02,  1.2890625e-01,  1.0253906e-01, -3.9482117e-04,\n        1.2158203e-01,  4.3212891e-02], dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#why-do-this",
    "href": "src/01/slides.html#why-do-this",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Why Do This?",
    "text": "Why Do This?\n\nMapping words to multi-dimensional vectors enables\n\nTest for similarity\nCompute similarity\nPerform vector arithmetic\nExplore sets of words through visualizations",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-4",
    "href": "src/01/slides.html#how-does-word2vec-work-4",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nfind_similar_words(\"cat\")\nfind_similar_words(\"dog\")\nfind_similar_words(\"pizza\")\n\n\nWords most similar to 'cat':\n----------------------------------------\ncats                 | similarity: 0.8099\ndog                  | similarity: 0.7609\nkitten               | similarity: 0.7465\nfeline               | similarity: 0.7326\nbeagle               | similarity: 0.7151\npuppy                | similarity: 0.7075\npup                  | similarity: 0.6934\npet                  | similarity: 0.6892\nfelines              | similarity: 0.6756\nchihuahua            | similarity: 0.6710\n\nWords most similar to 'dog':\n----------------------------------------\ndogs                 | similarity: 0.8680\npuppy                | similarity: 0.8106\npit_bull             | similarity: 0.7804\npooch                | similarity: 0.7627\ncat                  | similarity: 0.7609\ngolden_retriever     | similarity: 0.7501\nGerman_shepherd      | similarity: 0.7465\nRottweiler           | similarity: 0.7438\nbeagle               | similarity: 0.7419\npup                  | similarity: 0.7407\n\nWords most similar to 'pizza':\n----------------------------------------\npizzas               | similarity: 0.7863\nDomino_pizza         | similarity: 0.7343\nPizza                | similarity: 0.6988\npepperoni_pizza      | similarity: 0.6903\nsandwich             | similarity: 0.6840\nburger               | similarity: 0.6570\nsandwiches           | similarity: 0.6495\ntakeout_pizza        | similarity: 0.6492\ngourmet_pizza        | similarity: 0.6401\nmeatball_sandwich    | similarity: 0.6377",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-5",
    "href": "src/01/slides.html#how-does-word2vec-work-5",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\ncompute_similarity('cat', 'dog')\ncompute_similarity('cat', 'kitten')\ncompute_similarity('cat', 'car')\ncompute_similarity('doctor', 'hospital')\ncompute_similarity('king', 'queen')\n\nSimilarity between 'cat' and 'dog': 0.7609\nSimilarity between 'cat' and 'kitten': 0.7465\nSimilarity between 'cat' and 'car': 0.2153\nSimilarity between 'doctor' and 'hospital': 0.5143\nSimilarity between 'king' and 'queen': 0.6511",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-6",
    "href": "src/01/slides.html#how-does-word2vec-work-6",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nvector_arithmetic(['king', 'woman'], ['man'])\nvector_arithmetic(['Paris', 'Italy'], ['France'])\nvector_arithmetic(['walking', 'swim'], ['walk'])\n\n\nking + woman - man:\n--------------------------------------------------\nqueen                | similarity: 0.7118\nmonarch              | similarity: 0.6190\nprincess             | similarity: 0.5902\ncrown_prince         | similarity: 0.5499\nprince               | similarity: 0.5377\n\nParis + Italy - France:\n--------------------------------------------------\nMilan                | similarity: 0.7222\nRome                 | similarity: 0.7028\nPalermo_Sicily       | similarity: 0.5968\nItalian              | similarity: 0.5911\nTuscany              | similarity: 0.5633\n\nwalking + swim - walk:\n--------------------------------------------------\nswimming             | similarity: 0.8246\nswam                 | similarity: 0.6807\nswims                | similarity: 0.6538\nswimmers             | similarity: 0.6495\npaddling             | similarity: 0.6424",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-7",
    "href": "src/01/slides.html#how-does-word2vec-work-7",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-python",
    "href": "src/01/slides.html#what-is-python",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is Python?",
    "text": "What is Python?\n\nInterpreted language (vs. compiled like C++ or C#)\n\nNo compilation step - code runs directly\nInteractive and flexible, great for experimentation\n\nCreated by Guido van Rossum in 1991\n\nPython 2 (2000-2020), Python 3 (2008-present)\nWe’ll use Python 3.13\n\nCross-platform: Runs on Windows, macOS, Linux\nDynamically typed: No need to declare variable types\nThe language of AI/ML: Vast ecosystem of libraries (NumPy, TensorFlow, PyTorch, Transformers)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#variables-and-data-types",
    "href": "src/01/slides.html#variables-and-data-types",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\n\nVariables store data (no type declaration needed)\n\nx = 42 (integer)\nname = \"Alice\" (string)\npi = 3.14 (float)\n\nLists hold multiple values\n\nnumbers = [1, 2, 3, 4, 5]\nwords = [\"cat\", \"dog\", \"bird\"]\n\nAccess with square brackets: numbers[0] returns 1",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#functions",
    "href": "src/01/slides.html#functions",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Functions",
    "text": "Functions\n\nFunctions perform actions\n\nBuilt-in: print(\"Hello\"), len([1, 2, 3])\nDefine your own: def greet(name): return f\"Hello {name}\"\nIndentation vs. braces\nSupport for classes (although used rarely in AI/ML)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#libraries-and-packages",
    "href": "src/01/slides.html#libraries-and-packages",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Libraries and Packages",
    "text": "Libraries and Packages\n\nLibraries extend Python’s capabilities\n\nimport math - mathematical functions\nfrom transformers import AutoModel - import specific components\n\nUse dot notation to access: math.sqrt(16)\nPackage management\n\npip - standard package installer (similar to NuGet for C#)\nuv - modern, faster alternative to pip\nPyPI (Python Package Index) - central repository with 500K+ packages",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-notebook",
    "href": "src/01/slides.html#what-is-a-notebook",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is a Notebook?",
    "text": "What is a Notebook?\n\nAn interactive document that combines:\n\nLive code that can be executed\nRich text explanations (markdown)\nVisualizations and outputs\n\nThink of it as a computational narrative\n\nTell a story with code, data, and explanations\n\nOriginally designed for data science and research\nAlso used for learning, experimenting, and sharing results",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#a-brief-history-of-notebooks",
    "href": "src/01/slides.html#a-brief-history-of-notebooks",
    "title": "Module 1: Foundations of Generative AI",
    "section": "A Brief History of Notebooks",
    "text": "A Brief History of Notebooks\n\n2011: IPython Notebook project begins\n\nInteractive Python shell → web-based notebook\n\n2014: Renamed to Jupyter (Julia, Python, R)\n\nNow supports 40+ programming languages\nPython is most popular by far\n\n2017: Google launches Colab\n\nFree cloud-based Jupyter notebooks\nFree access to GPUs and TPUs\n\nToday: Industry standard for ML/AI development",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#anatomy-of-a-python-notebook",
    "href": "src/01/slides.html#anatomy-of-a-python-notebook",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Anatomy of a Python Notebook",
    "text": "Anatomy of a Python Notebook\n\nFormat: Extension is .ipynb\n\nJSON format, using Jupyter Document Schema\n\nCells: Building blocks of notebooks\n\nCode cells: Executable Python code\nMarkdown cells: Text, headings, images, equations\n\nKernel: The computational engine running your code\n\nMaintains state between cell executions\n\nOutputs: Results appear directly below code cells\n\nText, tables, plots, interactive widgets",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-to-run-notebooks",
    "href": "src/01/slides.html#how-to-run-notebooks",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How to Run Notebooks",
    "text": "How to Run Notebooks\n\nJupyter Notebook Server (Classic approach)\n\nWeb interface on localhost\n\nVS Code (Local development)\n\nJupyter extension for VS Code\nRun on your own machine\n\nGoogle Colab (Recommended)\n\nBrowser-based, no installation needed\nFree(-ish) GPU access\nCan also access local GPU",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#why-recommend-google-colab",
    "href": "src/01/slides.html#why-recommend-google-colab",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Why Recommend Google Colab?",
    "text": "Why Recommend Google Colab?\n\nAccess to GPUs and TPUs for AI-based tasks\n\ne.g., A100 and H100 with 40Gb/80Gb VRAM\n\nModel downloaded between cloud vendors\n\nvs. downloading large models via the DigiPen network\n\nMany libraries pre-installed\nEasy to share notebooks with others\nGenerous (free) GPU limits for students!",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#challenges-with-word-embeddings-1",
    "href": "src/01/slides.html#challenges-with-word-embeddings-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Challenges with Word Embeddings",
    "text": "Challenges with Word Embeddings\n\nLarge vocabularies\n\n100K+ words\nAnd not particularly friendly to non-English vocabularies\n\nLittle representation between certain words\n\n“Run” and “Running” should be related\n\nLack of context\n\nEmbedding for the word “bank” is the same, regardless of context\nRiver bank != Savings bank",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#challenges-with-word-embeddings-2",
    "href": "src/01/slides.html#challenges-with-word-embeddings-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Challenges with Word Embeddings",
    "text": "Challenges with Word Embeddings\n\nSome researchers tried character-level models\n\nSmall vocabulary (26 letters + punctuation for English)\nBut very long sequences\nAnd hard to extract meaning",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#byte-pair-encoding-bpe",
    "href": "src/01/slides.html#byte-pair-encoding-bpe",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2016: Byte Pair Encoding (BPE)",
    "text": "2016: Byte Pair Encoding (BPE)\n\nOriginally developed in 1994 as a simple compression algorithm (Gage 1994)\n\nFrequent pairs of adjacent bytes represented as a single byte\n\nIn 2016, adapted to neural machine translation (Sennrich, Haddow, and Birch 2016)\n\nApplied BPE to break words into subword units for better handling of rare words",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#byte-pair-encoding-bpe-1",
    "href": "src/01/slides.html#byte-pair-encoding-bpe-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2016: Byte Pair Encoding (BPE)",
    "text": "2016: Byte Pair Encoding (BPE)\n\nBreaks words into frequent subword units (a.k.a. tokens)\n\n“unbelievable” → [“un”, “believ”, “able”]\n\nBalance between word level (large vocab) and character level (long sequences)\n\nSupports related words: [“Run”] and [“Run”, “ning”]\n30-50K tokens vs. 100K, and works well for non-English languages\n\nFoundations of today’s tokenization\n\nAPI costs are measured in tokens\nDifferent models use different tokenizers",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#search-for-context",
    "href": "src/01/slides.html#search-for-context",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Search for Context",
    "text": "Search for Context\n\nBPE provided efficiency and representation between words\nBut still didn’t solve context\n\ne.g., the River bank != Savings bank problem\n\nResearchers working on “attention tasks” using Recurrent Neural Networks (RNNs)\n\nBahdanau et al. introduce attention for translation (Bahdanau, Cho, and Bengio 2015)\nShowed that focusing on relevant parts of input improved translation quality",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#attention-is-all-you-need",
    "href": "src/01/slides.html#attention-is-all-you-need",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2017: “Attention is all you need”",
    "text": "2017: “Attention is all you need”\n\nGoogle researchers publish “Attention is all you need” (Vaswani et al. 2017)\n\nIntroduced the Transformer a novel Neural Network (NN) architecture, eliminating the need for RNNs for sequence-to-sequence models\nUsed BPE tokenization, and creates contextual embeddings during training process\nAttention mechanism allows the model to weigh the importance of words in a sequence\nAchieved State Of The Art (SOTA) performance on language translation, while also being faster to train",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-1",
    "href": "src/01/slides.html#introducing-the-transformer-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    Transformer[Transformer]\n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Transformer --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example",
    "href": "src/01/slides.html#example",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"Helsinki-NLP/opus-mt-fr-en\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-1",
    "href": "src/01/slides.html#example-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nfrench_text = \"Bonjour, comment allez-vous?\"\ninput_ids = tokenizer.encode(french_text, return_tensors=\"pt\")\nprint(input_ids[0])\nprint(\"Tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n\ntensor([8703,    2, 1027, 5682,   21,  682,   54,    0])\nTokens: ['▁Bonjour', ',', '▁comment', '▁allez', '-', 'vous', '?', '&lt;/s&gt;']",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-2",
    "href": "src/01/slides.html#example-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\noutput_ids = model.generate(input_ids)\nprint(output_ids)\n\ntensor([[59513, 10537,     2,   541,    52,    55,    54,     0]])",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-3",
    "href": "src/01/slides.html#example-3",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nenglish_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(\"Translation:\", english_text)\n\nTranslation: Hello, how are you?",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-2",
    "href": "src/01/slides.html#introducing-the-transformer-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    Transformer[Transformer]\n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Transformer --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-3",
    "href": "src/01/slides.html#introducing-the-transformer-3",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        Encoder[Encoder]\n        Decoder[Decoder]\n        Encoder --&gt; Decoder\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Encoder\n    Decoder --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-4",
    "href": "src/01/slides.html#introducing-the-transformer-4",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        direction LR\n        subgraph \"Encoder Stack (N layers)\"\n            E[Encoder&lt;br/&gt;Layers&lt;br/&gt;1...N]\n        end\n        \n        subgraph \"Decoder Stack (N layers)\"\n            D[Decoder&lt;br/&gt;Layers&lt;br/&gt;1...N]\n        end\n        \n        E -.-&gt;|Context| D\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; E\n    D --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-the-encoderdecoder-work",
    "href": "src/01/slides.html#how-does-the-encoderdecoder-work",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How Does the Encoder/Decoder Work?",
    "text": "How Does the Encoder/Decoder Work?\noutput_ids = model.generate(input_ids)\n\nTakes input ids, runs through encoder\n\nGenerates contextual vectors using self attention across input tokens\n\nRuns the decoder iteratively to generate one token at a time\n\nUses self attention on previously generated tokens\nUses cross-attention to attend to encoder output\n\nContinues until it generates an end-of-sequence token or hits max length",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-5",
    "href": "src/01/slides.html#introducing-the-transformer-5",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        direction TB\n        \n        subgraph \"Encoder Layer\"\n            direction TB\n            E_SelfAttn[Multi-Head&lt;br/&gt;Self-Attention]\n            E_AddNorm1[Add & Norm]\n            E_FFN[Feed-Forward&lt;br/&gt;Network]\n            E_AddNorm2[Add & Norm]\n            \n            E_SelfAttn --&gt; E_AddNorm1 --&gt; E_FFN --&gt; E_AddNorm2\n        end\n        \n        subgraph \"Decoder Layer\"\n            direction TB\n            D_SelfAttn[Masked Multi-Head&lt;br/&gt;Self-Attention]\n            D_AddNorm1[Add & Norm]\n            D_CrossAttn[Multi-Head&lt;br/&gt;Cross-Attention]\n            D_AddNorm2[Add & Norm]\n            D_FFN[Feed-Forward&lt;br/&gt;Network]\n            D_AddNorm3[Add & Norm]\n            \n            D_SelfAttn --&gt; D_AddNorm1 --&gt; D_CrossAttn --&gt; D_AddNorm2 --&gt; D_FFN --&gt; D_AddNorm3\n        end\n        \n        E_AddNorm2 -.-&gt;|Encoder&lt;br/&gt;Output| D_CrossAttn\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; E_SelfAttn\n    D_AddNorm3 --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#origin-of-gpt-1",
    "href": "src/01/slides.html#origin-of-gpt-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2018: Origin of “GPT”",
    "text": "2018: Origin of “GPT”\n\nGenerative Pre-trained Transformer\nName coined by OpenAI researchers in “Improving Language Understanding by Generative Pre-Training” (Radford et al. 2018)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-gpt",
    "href": "src/01/slides.html#what-is-a-gpt",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is a GPT?",
    "text": "What is a GPT?\n\n“Decoder-only” architecture\n\nSelf attention is causal/masked - tokens can only attend to previous tokens, not future ones\n\nPre-training objective: Next token prediction\n\nTrained on a massive text corpora\nLearns grammar, facts, reasoning patterns just from this objective",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-gpt-1",
    "href": "src/01/slides.html#what-is-a-gpt-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is a GPT?",
    "text": "What is a GPT?\n\nAutoregressive generation\n\nGenerates one token at a time, feeding back each output as input\nTemperature and sampling strategies\nSame prompt can produce different outputs\n\nContext window\n\nFixed maximum length (2048 for GPT-2)\nEverything must fit within this window during generation\nIntroduced the concept of “context” vs. “knowledge” (prompt vs. training)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#gpt-2",
    "href": "src/01/slides.html#gpt-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "GPT-2",
    "text": "GPT-2\n\nReleased in 2019 by OpenAI\n\nInitially, only 117M param model released in Feb 2019 due to safety concerns\nStaged releases throughout the year, 1.5B in Nov 2019\n\nTrained on WebText, 8 million web pages/40GB of text\nZero-shot task performance\n\nDid well on translation, summarization, and question answering without task-specific training",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-4",
    "href": "src/01/slides.html#example-4",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load pre-trained GPT-2 model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n# Set pad token\ntokenizer.pad_token = tokenizer.eos_token",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-5",
    "href": "src/01/slides.html#example-5",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nimport torch\n\ndef autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n    # Encode the prompt with attention mask\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    # Generate continuation\n    with torch.no_grad():\n        output = model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#temperature-top_k-and-top_p",
    "href": "src/01/slides.html#temperature-top_k-and-top_p",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Temperature, top_k, and top_p",
    "text": "Temperature, top_k, and top_p\n\nTemperature (0.0 - 1.0)\n\nLower for accuracy, factual summaries, etc.\nHigher for more creative, diverse ideas\n\ntop_k (top k tokens)\n\nNarrow the next tokens to the top k (ordered by probability)\n\ntop_p (cumulative probability)\n\nOnly return the top tokens whose culumative probability &lt; top_p",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-6",
    "href": "src/01/slides.html#example-6",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nprompt = \"Mary had a little lamb\"\ncompletion = autocomplete(prompt, max_length=80)\nprint(completion)\n\nMary had a little lamb, and the young woman asked her for a little lamb, and they gave it to her.\n\n\"Oh, my child, it is good to have a little lamb,\" said he, \"but it is not to be bought, for it is hard to make, and it is much more difficult to make.\n\n\"When you have a little lamb, it",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#limitations-of-gpt-2-1",
    "href": "src/01/slides.html#limitations-of-gpt-2-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Limitations of GPT-2",
    "text": "Limitations of GPT-2\n\nHallucinations / factual errors\nNo real-world grounding\nRepetition issues\nOnwards to GPT-3 and beyond…",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#looking-ahead-1",
    "href": "src/01/slides.html#looking-ahead-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nNew to Python?\n\nLearn Python with Jupyter\n\nInstruction-tuned models\nOpenAI specification\nGradio for chat-based UIs",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#references-1",
    "href": "src/01/slides.html#references-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "References",
    "text": "References\n\n\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. “Neural Machine Translation by Jointly Learning to Align and Translate.” In International Conference on Learning Representations. https://arxiv.org/abs/1409.0473.\n\n\nGage, Philip. 1994. “A New Algorithm for Data Compression.” The C Users Journal 12 (2): 23–38.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” In International Conference on Learning Representations. https://arxiv.org/abs/1301.3781.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” In Advances in Neural Information Processing Systems, 3111–19. https://arxiv.org/abs/1310.4546.\n\n\nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.” https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf.\n\n\nSennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1715–25. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1162.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems. Vol. 30.",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/assignment.html",
    "href": "src/01/assignment.html",
    "title": "Module 1 Assignment: Experiment with Text Continuation Styles",
    "section": "",
    "text": "Objective: Create a Colab notebook that uses GPT-2 to generate creative text continuations with different styles.\nRequirements:\n\nLoad a pre-trained GPT-2 model (using HuggingFace transformers - same approach as used in GPT-2.ipynb)\nCreate 3 different story starters in different genres/styles. For example:\n\nFantasy/Adventure: “In a land of dragons and magic…”\nSci-fi: “The year is 2157. Humanity has just…”\nMystery: “The detective examined the crime scene and noticed…”\n(or choose your own three)\n\nThen adjust for:\n\nGreedy decoding vs. sampling\nDifferent temperature values\nHow the opening sentence shapes the continuation (e.g., short vs. long)\n\nDocument your observations (using Markdown in the notebook)\n\nWhat differences do you notice between the strategies?\nWhat worked well (or surprised you!)\nWhat didn’t work that well\n\n\nDeliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation\nOutputs of generated text from GPT-2\nMarkdown cells explaining what each sampling strategy does and any observations\n\nHint:\nFor better results, use a larger GPT-2 model on Colab T4.\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Assignment"
    ]
  },
  {
    "objectID": "src/00/slides.html#course-description",
    "href": "src/00/slides.html#course-description",
    "title": "Welcome to CS-394/594!",
    "section": "Course Description",
    "text": "Course Description\n\nHow Generative AI Works focuses on the practical implementation of generative AI within custom software applications and games.\nThe course covers neural network architectures, including the impact of the Transformer model, customization of large language models across multiple vendors using APIs, and experimentation with multimodal models for image and audio recognition and generation.",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#course-description-1",
    "href": "src/00/slides.html#course-description-1",
    "title": "Welcome to CS-394/594!",
    "section": "Course Description",
    "text": "Course Description\n\nHands-on experience includes working with both hosted and locally run models, integrating AI with game engines such as Unity and Unreal, and developing AI agents that extend beyond simple chat-based interactions.\nEthical considerations and model evaluation are integrated throughout, emphasizing awareness of broader societal implications.\nThrough lectures, programming assignments, and a final project, the course provides the expertise needed to apply generative AI in creating innovative and interactive experiences.",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#learning-outcomes",
    "href": "src/00/slides.html#learning-outcomes",
    "title": "Welcome to CS-394/594!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the basic working principles and history of current LLMs (Large Language Models)\nUnderstand ethical and safety aspects of using generative models\nEvaluate and test generative models using industry benchmarks\nRun generative models on local, laptop-based hardware (using CPU, GPU, NPUs)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#learning-outcomes-1",
    "href": "src/00/slides.html#learning-outcomes-1",
    "title": "Welcome to CS-394/594!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCreate AI-based agents and tools based on the MCP (Model Context Protocol) specification\nAvoid hallucinations by increasing the accuracy of models through RAG (Retrieval Augmented Generation) and fine-tuning\nExplore and use multimodal models for image and audio recognition and generation\nCreate and deploy API-based clients, accessing LLMs hosted by different vendors (OpenAI, Meta, Google)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#in-summary",
    "href": "src/00/slides.html#in-summary",
    "title": "Welcome to CS-394/594!",
    "section": "In Summary",
    "text": "In Summary\n\nFocus on integration/augmentation vs. automation/using\nProvide a level of understanding beyond where most professional software developers are today\nBuild an exciting final project that you can add to your portfolio!",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#syllabus-1",
    "href": "src/00/slides.html#syllabus-1",
    "title": "Welcome to CS-394/594!",
    "section": "Syllabus",
    "text": "Syllabus\n\nSyllabus on OneDrive",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#schedule",
    "href": "src/00/slides.html#schedule",
    "title": "Welcome to CS-394/594!",
    "section": "Schedule",
    "text": "Schedule\n\nEvery Friday (Curie); 2pm - 4.50pm\n~1.5 hours of lecture\n~1.5 hours for in-class hands-on lab time and assignments\nExpectation of after-class work for assignments\nNo structured lectures for the weeks of the final project (3 hours of in-class lab time)\n\nAlthough we may do mini-lectures for common topics",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#schedule-1",
    "href": "src/00/slides.html#schedule-1",
    "title": "Welcome to CS-394/594!",
    "section": "Schedule",
    "text": "Schedule\n\nModules 1 through 4\nFeb 6 is Founders Day, so no classes\nModules 5 through 8\nMar 9 - 13 is Spring Break\nWeeks 9 through 14 - Final Project Work\nFinal presentations w/o Apr 20-24",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#during-class",
    "href": "src/00/slides.html#during-class",
    "title": "Welcome to CS-394/594!",
    "section": "During Class",
    "text": "During Class\n\nStrive for conversation and interactivity\n\nPlease ask questions, even mid-slide!\nThere are no wrong or bad questions!\nI enjoy going off on tangents / on the whiteboard\nUse hands on lab time to seek input / troubleshoot code",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#grading",
    "href": "src/00/slides.html#grading",
    "title": "Welcome to CS-394/594!",
    "section": "Grading",
    "text": "Grading\n\nModule Assignments: 40% of grade (8 x 5%)\nFinal Project: 60% of grade\nRubric for the weekly assignments\nRubric for the final project (CS-394)\nRubric for the final project (CS-594)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#handing-in-work",
    "href": "src/00/slides.html#handing-in-work",
    "title": "Welcome to CS-394/594!",
    "section": "Handing in Work",
    "text": "Handing in Work\n\nEverything submitted via GitHub\n\nRecommend creating a repo for weekly assignments\n\nFor most weeks, submission will be one Python notebook\n\nAnd (eventually) another repo for your final project\n\nDon’t forget to give me permissions! @simonguest",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#deadlines",
    "href": "src/00/slides.html#deadlines",
    "title": "Welcome to CS-394/594!",
    "section": "Deadlines",
    "text": "Deadlines\n\nWeekly Assignments\n\nAssignments are due by the following week’s lesson\ni.e., you get a week for each assignment\nIf you need more time/exception, please reach out via Teams\n\nFinal Project\n\nUp until Week 15 presentations\n(We’ll cover in detail later in the semester)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#ai-policy",
    "href": "src/00/slides.html#ai-policy",
    "title": "Welcome to CS-394/594!",
    "section": "AI Policy",
    "text": "AI Policy\n\nPermitted AI Usage\n\nYou may use AI tools to assist in understanding course materials\nIf AI is used to generate code, your must test and validate the code, must understand and be able to answer questions about the generated code, and include proper citations\nIf AI tools are used to assist with any part of an assignment, you must clearly cite the AI tool and explain how it was used",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#tools",
    "href": "src/00/slides.html#tools",
    "title": "Welcome to CS-394/594!",
    "section": "Tools",
    "text": "Tools\n\nWe will be introducing many tools\n\nColab Pro, OpenRouter, Hugging Face, etc.\nMost will be free\nExpect to need about $25 in API credits throughout the semester",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#languages",
    "href": "src/00/slides.html#languages",
    "title": "Welcome to CS-394/594!",
    "section": "Languages",
    "text": "Languages\n\nWe will be using (and learning) a lot of Python!\n\nMost of the in-class assignments will be in Python\nDon’t worry if you are new to Python as we’ll introduce concepts gradually\nAlthough recommend investing extra time (see resources in Week 1)\n\nFinal Project\n\nCan be any language\nProbably depending on what you choose to create",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#hardware",
    "href": "src/00/slides.html#hardware",
    "title": "Welcome to CS-394/594!",
    "section": "Hardware",
    "text": "Hardware\n\nWill will be training SLMs (Small Language Models) later in the semester\nThis training will require a decent GPU and VRAM\n\nColab Pro (CUDA)\nYour own NVIDIA-based laptop (CUDA)\nPotential of using MLX for any Mac users",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#need-help-1",
    "href": "src/00/slides.html#need-help-1",
    "title": "Welcome to CS-394/594!",
    "section": "Need Help?",
    "text": "Need Help?\n\nhttps://simonguest.github.io/CS-394\n\nSlides (current and prior lectures), Demo code, Resources, Rubrics, Assignments\nI will repost assignments and rubrics on the Meta-Moodle. (Grades will also be in Moodle.)\n\nOffice Hours\n\nThursdays 1pm - 3pm (On campus or virtually via Teams)\n\nTeams (CS394/594 combined channel)\n\nPrimary mechanism for updates, ask questions, request office hours, etc.",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#steep-learning-curve",
    "href": "src/00/slides.html#steep-learning-curve",
    "title": "Welcome to CS-394/594!",
    "section": "Steep Learning Curve",
    "text": "Steep Learning Curve\n\nWe will be using the latest tools and AI models\nLots of new tools, acronyms, frameworks, etc.\nMuch of the curriculum builds upon itself\n\nPlease try not to miss lectures\nAsk for help if you need to catch up",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#new-course-at-digipen",
    "href": "src/00/slides.html#new-course-at-digipen",
    "title": "Welcome to CS-394/594!",
    "section": "New Course at DigiPen!",
    "text": "New Course at DigiPen!\n\nThere may be some minor curriculum tweaks mid-flight\n\nEspecially for topics that need less/more time",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#fast-moving-space",
    "href": "src/00/slides.html#fast-moving-space",
    "title": "Welcome to CS-394/594!",
    "section": "Fast Moving Space",
    "text": "Fast Moving Space\n\nThere will be areas/questions I don’t have experience of\n\nMultiple new models are launched every week\n…or equations/algorithms that I don’t know\n\nWe will be learning some things together!\nBut that’s what makes it exciting!",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html",
    "href": "src/00/final-rubric-394.html",
    "title": "Rubric (Final Project CS-394)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#integration-of-ai-models-10",
    "href": "src/00/final-rubric-394.html#integration-of-ai-models-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#functionality-10",
    "href": "src/00/final-rubric-394.html#functionality-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "Functionality (10%)",
    "text": "Functionality (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nProject is largely non-functional with AI components not working. Major bugs and errors prevent basic usage.\nProject runs but AI components frequently fail or produce incorrect results. Significant functionality issues present.\nProject is functional with AI components working in most cases. Some bugs or limitations affect user experience.\nProject is fully working with AI components functioning reliably as intended. Minor issues may exist but don’t impact core functionality.\nProject is fully functional with flawless AI integration. All components work seamlessly together with robust error handling.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#innovation-and-creativity-10",
    "href": "src/00/final-rubric-394.html#innovation-and-creativity-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "Innovation and Creativity (10%)",
    "text": "Innovation and Creativity (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nGeneric AI usage with no creative application. Design is basic with poor aesthetics and no meaningful AI enhancement.\nAI used in conventional ways with limited creativity. Design shows some effort but AI doesn’t meaningfully enhance the experience.\nCreative AI application with interesting use cases. Design is competent with AI providing noticeable visual or interactive improvements.\nInnovative AI usage providing unique and compelling experiences. Strong aesthetic and functional design that effectively leverages AI capabilities.\nExceptional creativity with groundbreaking AI applications. Outstanding design that seamlessly integrates AI to create truly unique and compelling experiences.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#ethical-analysis-10",
    "href": "src/00/final-rubric-394.html#ethical-analysis-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "Ethical Analysis (10%)",
    "text": "Ethical Analysis (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nNo ethical analysis provided or only superficial acknowledgment of issues. No consideration of biases or societal impacts.\nLimited ethical analysis with minimal identification of potential issues. Brief mention of biases but no mitigation strategies proposed.\nAdequate evaluation of biases and ethical implications with some discussion of societal impacts. Basic mitigation proposals included.\nThorough evaluation of potential biases, ethical implications, and societal impacts. Clear and practical proposals for mitigating identified risks.\nComprehensive and insightful ethical analysis demonstrating deep understanding of AI implications. Sophisticated mitigation strategies with consideration of broader societal impacts.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#presentation-20",
    "href": "src/00/final-rubric-394.html#presentation-20",
    "title": "Rubric (Final Project CS-394)",
    "section": "Presentation (20%)",
    "text": "Presentation (20%)\n\n\n\n\n\n\n\n\n\n\n4%\n8%\n12%\n16%\n20%\n\n\n\n\nPresentation is unclear and disorganized with minimal explanation of AI features. If in a team, roles were undefined and collaboration issues evident.\nPresentation covers basic points but lacks engagement or clear explanation of AI integration process. If in a team, some team member contributions unclear.\nClear presentation highlighting main AI features and challenges faced. If in a team, collaboration is adequate with most roles and contributions identifiable.\nClear and engaging presentation effectively showcasing AI features and their purpose. Comprehensive explanation of integration process including model selection and solutions, with well-defined team roles (if applicable).\nOutstanding presentation that captivates audience while thoroughly explaining AI implementation. Exceptional teamwork (if applicable) with seamless collaboration and clearly articulated individual contributions.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS-394/594: How Generative AI Works",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CS-394/594 course.",
    "crumbs": [
      "CS-394/594"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "CS-394/594: How Generative AI Works",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CS-394/594 course.",
    "crumbs": [
      "CS-394/594"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html",
    "href": "src/00/final-rubric-594.html",
    "title": "Rubric (Final Project CS-594)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. No fine-tuning achieved. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited fine-tuning demonstrated. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Fine-tuning demonstrated, but not optimal. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Strong fine-tuning techniques demonstrated. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Excellent approach and results from fine-tuning. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#fine-tuning-and-integration-of-ai-models-10",
    "href": "src/00/final-rubric-594.html#fine-tuning-and-integration-of-ai-models-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. No fine-tuning achieved. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited fine-tuning demonstrated. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Fine-tuning demonstrated, but not optimal. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Strong fine-tuning techniques demonstrated. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Excellent approach and results from fine-tuning. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#functionality-10",
    "href": "src/00/final-rubric-594.html#functionality-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "Functionality (10%)",
    "text": "Functionality (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nProject is largely non-functional with AI components not working. Major bugs and errors prevent basic usage.\nProject runs but AI components frequently fail or produce incorrect results. Significant functionality issues present.\nProject is functional with AI components working in most cases. Some bugs or limitations affect user experience.\nProject is fully working with AI components functioning reliably as intended. Minor issues may exist but don’t impact core functionality.\nProject is fully functional with flawless AI integration. All components work seamlessly together with robust error handling.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#innovation-and-creativity-10",
    "href": "src/00/final-rubric-594.html#innovation-and-creativity-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "Innovation and Creativity (10%)",
    "text": "Innovation and Creativity (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nGeneric AI usage with no creative application. Design is basic with poor aesthetics and no meaningful AI enhancement.\nAI used in conventional ways with limited creativity. Design shows some effort but AI doesn’t meaningfully enhance the experience.\nCreative AI application with interesting use cases. Design is competent with AI providing noticeable visual or interactive improvements.\nInnovative AI usage providing unique and compelling experiences. Strong aesthetic and functional design that effectively leverages AI capabilities.\nExceptional creativity with groundbreaking AI applications. Outstanding design that seamlessly integrates AI to create truly unique and compelling experiences.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#ethical-analysis-10",
    "href": "src/00/final-rubric-594.html#ethical-analysis-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "Ethical Analysis (10%)",
    "text": "Ethical Analysis (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nNo ethical analysis provided or only superficial acknowledgment of issues. No consideration of biases or societal impacts.\nLimited ethical analysis with minimal identification of potential issues. Brief mention of biases but no mitigation strategies proposed.\nAdequate evaluation of biases and ethical implications with some discussion of societal impacts. Basic mitigation proposals included.\nThorough evaluation of potential biases, ethical implications, and societal impacts. Clear and practical proposals for mitigating identified risks.\nComprehensive and insightful ethical analysis demonstrating deep understanding of AI implications. Sophisticated mitigation strategies with consideration of broader societal impacts.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#presentation-and-research-20",
    "href": "src/00/final-rubric-594.html#presentation-and-research-20",
    "title": "Rubric (Final Project CS-594)",
    "section": "Presentation and Research (20%)",
    "text": "Presentation and Research (20%)\n\n\n\n\n\n\n\n\n\n\n4%\n8%\n12%\n16%\n20%\n\n\n\n\nPresentation is unclear and disorganized with minimal explanation of AI features. If in a team, roles were undefined and collaboration issues evident. No research evident and/or cited-papers.\nPresentation covers basic points but lacks engagement or clear explanation of AI integration process. If in a team, some team member contributions unclear. Limited research evident.\nClear presentation highlighting main AI features and challenges faced. If in a team, collaboration is adequate with most roles and contributions identifiable. Research evident, but with gaps or issues.\nClear and engaging presentation effectively showcasing AI features and their purpose. Comprehensive explanation of integration process including model selection and solutions, with well-defined team roles (if applicable). Strong, well-cited research evident.\nOutstanding presentation that captivates audience while thoroughly explaining AI implementation. Exceptional teamwork (if applicable) with seamless collaboration and clearly articulated individual contributions. Excellent and well-cited approach to research",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/weekly-rubric.html",
    "href": "src/00/weekly-rubric.html",
    "title": "Rubric (Weekly Assignments)",
    "section": "",
    "text": "1%\n2%\n3%\n4%\n5%\n\n\n\n\nThe submission fails to showcase any working features. The solution is practically unusable due to issues.\nOnly a few features are functional. Significant missing work or issues with the submission.\nMajor issues impact key features, but some functionality is evident. Noticeable gaps or confusing submission.\nMinor issues or bugs are present but do not significantly impact the functionality. Very few gaps, and a near-complete submission.\nAll features are fully functional. A complete submission that meets all requirements of the assignment.\n\n\n\n(0% for unsubmitted work)",
    "crumbs": [
      "**Welcome**",
      "Rubric (Weekly Assignments)"
    ]
  },
  {
    "objectID": "src/01/resources.html",
    "href": "src/01/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Visualize embeddings in 3D space, powered by EmbeddingGemma and Transformers.js\nOriginal Word2Vec Papers - Google Code archive with original papers\nWord2Vec Tutorial - The Skip-Gram Model - Chris McCormick’s detailed tutorial\nGensim Word2Vec Tutorial - Popular Python library for word embeddings\nA Visual Introduction to Vector Embeddings, Pamela Fox\nA Visual Exploration of Vectors, Pamela Fox",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#word2vec-and-word-embeddings",
    "href": "src/01/resources.html#word2vec-and-word-embeddings",
    "title": "Resources",
    "section": "",
    "text": "Visualize embeddings in 3D space, powered by EmbeddingGemma and Transformers.js\nOriginal Word2Vec Papers - Google Code archive with original papers\nWord2Vec Tutorial - The Skip-Gram Model - Chris McCormick’s detailed tutorial\nGensim Word2Vec Tutorial - Popular Python library for word embeddings\nA Visual Introduction to Vector Embeddings, Pamela Fox\nA Visual Exploration of Vectors, Pamela Fox",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#tokenization-and-byte-pair-encoding",
    "href": "src/01/resources.html#tokenization-and-byte-pair-encoding",
    "title": "Resources",
    "section": "Tokenization and Byte Pair Encoding",
    "text": "Tokenization and Byte Pair Encoding\n\nBPE Original Paper (1994) - Philip Gage’s original compression algorithm\nNeural Machine Translation with BPE - 2016 paper adapting BPE for NLP\nHugging Face Tokenizers - Understanding different tokenization strategies\nOpenAI Tokenizer Tool - Interactive tool to see how text is tokenized",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#transformers",
    "href": "src/01/resources.html#transformers",
    "title": "Resources",
    "section": "Transformers",
    "text": "Transformers\n\nThe Illustrated Transformer - Jay Alammar’s visual guide\n“Attention is All You Need” Paper - The original transformer paper (2017)\nThe Annotated Transformer - Harvard NLP’s line-by-line implementation guide\nTransformer Math 101 - Understanding transformer computation\nAttention Mechanism Explained - Visual explanation of attention",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#gpt-and-language-models",
    "href": "src/01/resources.html#gpt-and-language-models",
    "title": "Resources",
    "section": "GPT and Language Models",
    "text": "GPT and Language Models\n\nGPT-1 Paper: “Improving Language Understanding” - Original GPT paper (2018)\nGPT-2 Paper: “Language Models are Unsupervised Multitask Learners” - GPT-2 paper (2019)\nGPT-2 Release Blog Post - OpenAI’s staged release announcement\nUnderstanding Decoder-Only Models - Sebastian Raschka’s explanation\nAndrej Karpathy’s “Let’s build GPT” - Building GPT from scratch",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#python-and-programming-basics",
    "href": "src/01/resources.html#python-and-programming-basics",
    "title": "Resources",
    "section": "Python and Programming Basics",
    "text": "Python and Programming Basics\n\nLearn Python with Jupyter, Serena Bonaretti\nPython Official Documentation - Official Python 3 documentation\nPython for Beginners - Python.org’s getting started guide\nAutomate the Boring Stuff with Python - Free online book for Python beginners",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#python-package-management",
    "href": "src/01/resources.html#python-package-management",
    "title": "Resources",
    "section": "Python Package Management",
    "text": "Python Package Management\n\npip Documentation - Python’s standard package installer\nuv Documentation - Modern, fast Python package manager\nPyPI - Python Package Index - Repository of 500K+ Python packages",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#notebooks-and-development-environment",
    "href": "src/01/resources.html#notebooks-and-development-environment",
    "title": "Resources",
    "section": "Notebooks and Development Environment",
    "text": "Notebooks and Development Environment\n\nGoogle Colab Sign-up Page\nGoogle Colab Tips and Tricks - Making the most of Colab\nProject Jupyter Page\nJupyter Notebook Beginner Guide - Getting started with Jupyter\nVS Code Jupyter Extension - Run notebooks in VS Code",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#citations",
    "href": "src/01/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/assignment.html",
    "href": "src/02/assignment.html",
    "title": "Module 2 Assignment: Gradio Travel Planner",
    "section": "",
    "text": "Objective: Build a chat interface (in a separate Colab notebook) that helps a user complete tasks.\nRequirements:\n\nGradio chat interface with streaming responses\nSystem prompt that defines the AI as a travel planning expert (or pick your own scenario, if you have a better idea!)\nImplements a well-thought out system prompt (with feedback on the rationale behind it).\nDemonstrates at least 2 different models via OpenRouter as a dropdown in Gradio. (Not necessarily at the same time.)\n\nTry to pick models of different sizes to compare the differences. If you search for free models (https://openrouter.ai/models?q=free) you can also avoid incurring any costs.\n\nUse structured outputs when returning results to the user (e.g., a downloadable itinerary that the user can download)\n\nExample schema: Trip with fields like destination, duration_days, activities (list), budget_level, daily_schedule (list of day objects)\n\n\nDeliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation\nUses OPENROUTER_API_KEY for the API token. (Please do not include your API key in your notebook!)\nMarkdown cells explaining what the notebook does and any observations\n\nHint\n\nTo get streaming and structured outputs working in Gradio, you may want to think of this as two calls to the model:\n\nThe first call asks the model to think about the problem (e.g., “How can you help the user solve their travel question?”) You can use streaming to display the response token-by-token in Gradio.\nThe second call then takes the model’s prior answer and creates a new call (e.g., “From your thinking, create an itinerary.”) You can then use structured outputs to ensure that this matches the schema, and map that to a component in the Gradio UI (gr.JSON is fine to display this, unless you want to get more creative :).",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Assignment"
    ]
  },
  {
    "objectID": "src/02/slides.html#recap",
    "href": "src/02/slides.html#recap",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Recap",
    "text": "Recap\n\nExplored the history of vector embeddings and tokenization\nUnderstood the transformer architecture at a high level\nUsed our first transformer to translate language\nCovered a brief history of early generative transformers\nSetup and used Colab, and became familiar with the basics of notebooks and Python",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#lesson-objectives",
    "href": "src/02/slides.html#lesson-objectives",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand the evolution and licensing of models from GPT-2 through to modern day\nUnderstand instruction-tuned models, how they work, and how to configure\nSetup and use OpenRouter for accessing hosted models\nUnderstand the OpenAI API specification, the request/response payload, parameters, streaming, and structured outputs\nCreate and share a chatbot using a Gradio-based UI",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#from-gpt-2-to-gpt-3.5-1",
    "href": "src/02/slides.html#from-gpt-2-to-gpt-3.5-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "From GPT-2 to GPT-3.5",
    "text": "From GPT-2 to GPT-3.5\n\n\n\n\n\ntimeline\n    Feb 2019 : OpenAI releases GPT-2\n             : 1.5B parameters\n             : Initially withheld full model due to concerns about misuse\n             : Demonstrates impressive text generation capabilities with minimal fine-tuning\n\n    May 2020 : OpenAI releases GPT-3\n             : 175B parameters\n             : Demonstrates strong few-shot learning capabilities\n             : Marks a significant leap in model capabilities and scale\n\n    June 2020 : GPT-3 available through OpenAI API\n              : Still a completion model, not instruction-tuned\n\n    2021 : InstructGPT Development\n          : Built on GPT-3 with RLHF fine-tuning\n          : Trained to follow instructions and understand user intent\n          : Key innovation enabling ChatGPT\n    \n    Jan 2021 : Anthropic Founded\n             : Founded by Dario & Daniela Amodei with ~7 senior OpenAI employees\n             : Dario led GPT-2/3 development and co-invented RLHF\n\n    Nov 2022 : ChatGPT Launch\n              : Built on GPT-3.5 using RLHF\n              : 1M+ users in 5 days\n              : Sparked widespread interest in generative AI",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs. Instruction-Tuned",
    "text": "Completion vs. Instruction-Tuned\n\nCompletion Model just predicts the next token\n\nInput prompt: Mary had a little\nMax total tokens: 50\nTemperature: 0 - 1.0\ntop_k: consider only the top k tokens in the response\ntop_p: Nucleus sampling (probability cut off - 0 and 1.0)\n\nOutput\n\nMary had a little lamb, its fleece was white as snow... (up to max tokens)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-1",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs. Instruction-Tuned",
    "text": "Completion vs. Instruction-Tuned\n\nYou can’t really converse with it\nWhat should I do on my upcoming trip to Paris? (max tokens = 75)\nWhat should I do on my upcoming trip to Paris? Please provide a detailed plan of action to help me plan my trip to Paris. 1. Research the best time to travel to Paris:",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#instruction-tuned-models",
    "href": "src/02/slides.html#instruction-tuned-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Instruction-Tuned Models",
    "text": "Instruction-Tuned Models\n\nSupervised Fine-Tuning\n\nLarge datasets of questions/answers, tasks/completions, demonstrating helpful assistant behavior\n\nChat Templates\n\nStructured format to distinguish speakers in a conversation: Typically system, user, and assistant\n\nRLHF (Reinforcement Learning from Human Feedback)\n\nHuman raters rank different model responses, training a reward model",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#whats-a-chat-template",
    "href": "src/02/slides.html#whats-a-chat-template",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What’s a Chat Template?",
    "text": "What’s a Chat Template?\n\nThe format used to train instructional models on conversations involving system, user, and assistant prompts.\nEach model family uses a different format (there is no universal standard)\nWrong format will likely generate nonsense/garbage",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chatml-gpt-3.5-and-other-models",
    "href": "src/02/slides.html#chatml-gpt-3.5-and-other-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "ChatML (GPT-3.5 and other models)",
    "text": "ChatML (GPT-3.5 and other models)\n&lt;|im_start|&gt;system\nYou help travelers make plans for their trips.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHello&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nHi there! How can I help you?&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat should I do on my upcoming trip to Paris?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#system-user-assistant",
    "href": "src/02/slides.html#system-user-assistant",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "System, User, Assistant",
    "text": "System, User, Assistant\n\nSystem prompt sets the intention for the model, guiding the output\n\n“You are a helpful assistant”\n“You help students with their math homework”\n“You help travelers make plans for their trips”\nHas to come first in the conversation\nOnly one system prompt\nOptional for some models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#system-user-assistant-1",
    "href": "src/02/slides.html#system-user-assistant-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "System, User, Assistant",
    "text": "System, User, Assistant\n\nSystem Prompt best practices\n\nBe specific: “You are a Python programming tutor who explains concepts using simple analogies and provides code examples.”\nDefine output: “List no more than 3 suggestions. Always show your work step by step.”\nSet boundaries: “If you are asked questions outside coding, politely redirect the student back to the task.”",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#system-user-assistant-2",
    "href": "src/02/slides.html#system-user-assistant-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "System, User, Assistant",
    "text": "System, User, Assistant\n\nUser prompt is the message (request) from the user\n\n“How many ’r’s in Strawberry?”\n“What is linear algebra?”\n“What should I do on my upcoming trip to Paris?”\n\nAssistant prompt is the message (reply) from the model\n\n“There are three r’s in Strawberry”\n“Linear algebra is the branch of mathematics that studies vectors, etc.”\n“Here are some suggestions for your upcoming trip to Paris: 1. Explore the Louvre Museum: etc.”",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-templates-in-practice",
    "href": "src/02/slides.html#chat-templates-in-practice",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat Templates in Practice",
    "text": "Chat Templates in Practice\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\n\ninstruct_tokenizer.apply_chat_template(\n    messages, \n    tokenize=False,\n    add_generation_prompt=True  # Adds the assistant prompt\n)\n\n'&lt;|im_start|&gt;system\\nYou help travelers make plans for their trips&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nHello&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\nHi there!&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nWhat should I do on my upcoming trip to Paris?&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n'",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-2",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs. Instruction-Tuned",
    "text": "Completion vs. Instruction-Tuned\n\n\nbase_inputs = base_tokenizer(\"What should I do on my upcoming trip to Paris?\", return_tensors=\"pt\")\nbase_outputs = base_model.generate(\n    **base_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=base_tokenizer.eos_token_id\n)\nbase_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\nprint(base_response)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-3",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-3",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs. Instruction-Tuned",
    "text": "Completion vs. Instruction-Tuned\n\n\n\nWhat should I do on my upcoming trip to Paris? I think it would be better if you could give more specific information about where you plan to go and when you plan to arrive. Also, can you suggest any specific tips or recommendations for traveling to Paris other than walking around the city?\n\nI'm sorry, but as an AI language model, I don't have any specific information about your upcoming trip to Paris. However, I can suggest some general tips and recommendations for traveling to Paris other than walking around the city:\n\n1. Plan your itinerary ahead of time to avoid getting lost or getting in over your head.\n2. Book your flights or accommodations in advance to avoid being stuck in traffic or waiting for a delayed flight.\n3. Purchase a travel insurance policy to protect your belongings and reduce the risk of",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-4",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-4",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs. Instruction-Tuned",
    "text": "Completion vs. Instruction-Tuned\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\ninstruct_text = instruct_tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\ninstruct_inputs = instruct_tokenizer(instruct_text, return_tensors=\"pt\")\ninstruct_outputs = instruct_model.generate(\n    **instruct_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=instruct_tokenizer.eos_token_id,\n)\ninstruct_response = instruct_tokenizer.decode(\n    instruct_outputs[0], skip_special_tokens=True\n)\nprint(instruct_response)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-5",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-5",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs. Instruction-Tuned",
    "text": "Completion vs. Instruction-Tuned\n\n\n\nsystem\nYou help travelers make plans for their trips.\nuser\nHello\nassistant\nHi there!\nuser\nWhat should I do on my upcoming trip to Paris?\nassistant\nGreat question! On your next trip to Paris, you can start by visiting the iconic Eiffel Tower and the Louvre Museum. Don't miss exploring the Notre-Dame Cathedral and its stunning stained glass windows. For a bit of a break, consider visiting Montmartre for some beautiful art and architecture. If you're looking for something more adventurous, you could take a stroll through the charming streets of Montmartre or explore the vibrant nightlife of Le Marais. Have fun planning your trip to Paris!",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#model-evolution-gpt-3.5-onwards",
    "href": "src/02/slides.html#model-evolution-gpt-3.5-onwards",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Model Evolution (GPT 3.5 onwards)",
    "text": "Model Evolution (GPT 3.5 onwards)\n\n\n\n\n\ntimeline\n    Nov 2022 : ChatGPT Launch\n                  : Built on GPT-3.5 using RLHF\n                  : 1M+ users in 5 days\n                  : Sparked widespread interest in generative AI\n\n    Feb 2023 : Llama 1 Released\n                  : Meta's LLaMA (7B, 13B, 33B, 65B parameters)\n                  : 13B model exceeded GPT-3 (175B) on most benchmarks\n                  : Text completion only (Alpaca fine-tune added instructions)\n\n    Jul 2023 : Llama 2 Released\n              : Available in 7B, 13B, 70B sizes\n              : Trained on 40% more data than Llama 1\n              : First open-weights Llama for commercial use",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#closed-vs.-open-models",
    "href": "src/02/slides.html#closed-vs.-open-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Closed vs. Open Models",
    "text": "Closed vs. Open Models\n\nClosed Source:\n\nHosted models\nNo ability to inspect the weights of the models\nNo ability to download the models\nOpenAI GPT-5, Claude Sonnet 4.5, Google’s Gemini\nVery large models; often referred to as foundational models or frontier models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#closed-vs.-open-models-1",
    "href": "src/02/slides.html#closed-vs.-open-models-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Closed vs. Open Models",
    "text": "Closed vs. Open Models\n\nOpen Weight:\n\nDownloadable model files\nYou can download the model files with pretrained weights, but no training data\nNo training data == No ability to recreate the model from scratch\nMeta’s Llama, Google’s Gemma, Alibaba’s Qwen, OpenAI gpt-oss-120b\nRange from small to medium in size (1Gb - 500Gb+)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#closed-vs.-open-models-2",
    "href": "src/02/slides.html#closed-vs.-open-models-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Closed vs. Open Models",
    "text": "Closed vs. Open Models\n\nOpen Source:\n\nModels with access to the training data set\nYou can download the model files with pretrained weights and the training data used to train it\ni.e., you could create the model from scratch\nExamples: AI2’s OLMo, NVIDIA Nemotron",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#discovering-open-models",
    "href": "src/02/slides.html#discovering-open-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Discovering Open Models",
    "text": "Discovering Open Models\n\nSource: https://huggingface.co",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#what-is-hugging-face",
    "href": "src/02/slides.html#what-is-hugging-face",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What is Hugging Face?",
    "text": "What is Hugging Face?\n\nIt is to AI models what GitHub is to source code\n\nExplore, download models to run on local hardware\nUpload and share your own trained/fine-tuned models and datasets\nCreate “Spaces” - web-based apps for accessing models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#hugging-face-transformers",
    "href": "src/02/slides.html#hugging-face-transformers",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Hugging Face Transformers",
    "text": "Hugging Face Transformers\n\nSource: https://huggingface.co/docs/transformers",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#hugging-face-transformers-1",
    "href": "src/02/slides.html#hugging-face-transformers-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Hugging Face Transformers",
    "text": "Hugging Face Transformers\n\nWhat is the Hugging Face Transformers Library?\nOpen-source Python library to provide easy access to using various types of pre-trained transformer models\nBrings together all of the different formats under one interface\n\nDifferent models, vendors, types, chat templates\nDifferent implementations: PyTorch, TensorFlow, JAX\n\nA few lines of code to download and run the model",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#accessing-closed-models-1",
    "href": "src/02/slides.html#accessing-closed-models-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Accessing Closed Models",
    "text": "Accessing Closed Models\n\nConsumer Website / App\n\ne.g., ChatGPT website or AppStore App\nLimited free tier; monthly subscription for more usage\n\nAPI Access\n\nOpenAI’s API Platform; Create a developer account\nCredit card required\nCharged for tokens sent to the model and tokens returned from the model\nGPT 5.2 Chat = $1.75 per million tokens input; $14 per million tokens output",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#accessing-closed-models-2",
    "href": "src/02/slides.html#accessing-closed-models-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Accessing Closed Models",
    "text": "Accessing Closed Models\n\nHow much is going to cost?\n\nToken estimators (e.g., tiktoken from OpenAI)\nOr napkin math: 100 tokens ~= 75 English words",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#accessing-closed-models-3",
    "href": "src/02/slides.html#accessing-closed-models-3",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Accessing Closed Models",
    "text": "Accessing Closed Models\n\nExample\n\nInput from user = 75 words (100 tokens)\nOutput from model = 1500 words (2000 tokens)\nTotal cost = 100 input tokens + 2000 output tokens\nTotal cost = $0.000175 + $0.028 = $0.028175\n\nAt scale\n\n10,000 users / 1 request per month ~= $281.75/mo\n\n10,000 users / 1 request per day ~= $8452.50/mo",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#openai-chat-completions-api",
    "href": "src/02/slides.html#openai-chat-completions-api",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "OpenAI Chat Completions API",
    "text": "OpenAI Chat Completions API\n\n2020: OpenAI launched GPT-3 API with a /completions endpoint.\n\nFirst major LLM API\n\n2022: ChatGPT launch; massive adoption\n2023 /chat/completions endpoint released, becomes the dominant interface\n2023-2024: Other providers use the same API format for their own models vs. inventing their own\n\nBuild on the OpenAI developer ecosystem\n“OpenAI-compatible” became a selling point",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#openai-chat-completions-api-1",
    "href": "src/02/slides.html#openai-chat-completions-api-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "OpenAI Chat Completions API",
    "text": "OpenAI Chat Completions API\n\nWho uses the OpenAI Chat Completions API format?\n\nAnthropic (Claude API is very similar, with minor differences)\nOpenRouter, an inference provider for many models\nOpen source tools: LiteLLM, LangChain\nLocal serving: Ollama, vLLM, llama.cpp are all “OpenAI-compatible”",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-the-chat-completions-api",
    "href": "src/02/slides.html#using-the-chat-completions-api",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using the Chat Completions API",
    "text": "Using the Chat Completions API\n\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    api_key=OPENAI_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-the-chat-completions-api-1",
    "href": "src/02/slides.html#using-the-chat-completions-api-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using the Chat Completions API",
    "text": "Using the Chat Completions API\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-5\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://api.openai.com/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"gpt-5\"\n}\n==================================================",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-the-chat-completions-api-2",
    "href": "src/02/slides.html#using-the-chat-completions-api-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using the Chat Completions API",
    "text": "Using the Chat Completions API\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"chatcmpl-CuVn7EYuGJUEUEQ18Cl0SM2nNz9Mj\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Awesome! I can tailor a plan, but a few quick questions help:\\n- When are you going and for how many days?\\n- First time in Paris?\\n- Main interests (art, food, fashion, history, photography, nightlife, kid-friendly, etc.) and preferred pace (relaxed vs. packed)?\\n- Any must-sees or hard no’s?\\n- Rough budget and food needs (vegetarian, kosher/halal, allergies)?\\n- Where are you staying (neighborhood) and are day trips okay (Versailles, Champagne, Giverny, Disneyland)?\\n\\nIf you want a quick starter plan, here’s a flexible 4-day outline you can reshuffle by weather and museum closures:\\n\\nDay 1 – Islands + Latin Quarter\\n- Île de la Cité: Notre-Dame exterior, Sainte-Chapelle (timed ticket), Conciergerie.\\n- Stroll the Latin Quarter: Shakespeare & Company, Sorbonne, Luxembourg Gardens.\\n- Evening: Seine cruise or sunset along the river.\\n\\nDay 2 – Louvre to Arc de Triomphe\\n- Morning: Louvre (timed entry). Tuileries and Palais-Royal gardens.\\n- Covered passages (Véronique/Grand Cerf/Jouffroy) and Opéra Garnier.\\n- Sunset view: Arc de Triomphe rooftop or Galeries Lafayette/Printemps terrace.\\n\\nDay 3 – Montmartre + Left Bank art\\n- Montmartre: Sacré-Cœur, Place du Tertre, quieter backstreets (Rue de l’Abreuvoir).\\n- Afternoon: Musée d’Orsay and/or Orangerie.\\n- Evening: Saint-Germain wine bar or jazz.\\n\\nDay 4 – Le Marais or Day Trip\\n- Marais walk: Place des Vosges, Musée Carnavalet, Picasso Museum (check hours), Jewish quarter, trendy boutiques.\\n- Optional day trip: Versailles (palace + gardens; get the timed passport ticket).\\n- Night: Eiffel Tower area (view from Trocadéro or Champ de Mars; book tower tickets if going up).\\n\\nOther great adds by interest\\n- Art/architecture: Rodin Museum; Bourse de Commerce; Fondation Louis Vuitton. Note: check Centre Pompidou’s renovation status.\\n- Food: Morning market (Aligre or Rue Cler), cheese/wine tasting, pastry crawl, bistro lunch, cooking class.\\n- Unique: Catacombs (book ahead), Père Lachaise Cemetery, Canal Saint-Martin, covered markets (Le Marché des Enfants Rouges).\\n- With kids: Jardin des Plantes (zoo + galleries), Cité des Sciences, Jardin d’Acclimatation, Parc de la Villette.\\n- Day trips: Giverny (Apr–Oct), Reims/Epernay for Champagne, Fontainebleau, Auvers-sur-Oise, Disneyland Paris.\\n\\nBook these in advance\\n- Eiffel Tower, Louvre, Sainte-Chapelle, Catacombs, Versailles, Palais Garnier tours, popular restaurants.\\n- Consider the Paris Museum Pass (2/4/6 days) if you’ll visit several museums; the Louvre still needs a timed reservation even with the pass.\\n\\nPractical tips\\n- Closures: Many museums close one day/week (e.g., Orsay Mon, some Tue). Check hours.\\n- Getting around: The Métro is fastest. Use a contactless bank card to tap in, or get a reloadable Navigo Easy. For a Monday–Sunday stay with lots of rides, a Navigo Découverte weekly pass can be good value.\\n- Dining: Reserve for dinner, especially weekends. Tipping is minimal (service included); round up or leave 5–10% for great service.\\n- Safety: Watch for pickpockets in crowded areas and on the Metro.\\n\\nShare your dates, length of stay, and interests, and I’ll turn this into a detailed day-by-day plan with mapped routes and restaurant picks near each stop.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": [],\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1767584609,\n  \"model\": \"gpt-5-2025-08-07\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": \"default\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 2224,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 2268,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"reasoning_tokens\": 1408,\n      \"rejected_prediction_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0\n    }\n  }\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-history-management",
    "href": "src/02/slides.html#chat-history-management",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat History Management",
    "text": "Chat History Management\n\nKey Considerations\n\nModels don’t hold any state\nAPI sends full conversation on every request and the model reads through the full conversation on every call\nThe size of the conversation is known as the context\nThe maximum size the model can process is referred to as the context window",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-history-management-1",
    "href": "src/02/slides.html#chat-history-management-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat History Management",
    "text": "Chat History Management\n\nContext window sizes\n\nGPT-2 = 2048 tokens\nToday’s nano models ~= 32k tokens\nToday’s small models ~= 120k tokens\nToday’s frontier models ~= 1M tokens\n\nLarge conversations can cause challenges\n\nThey are expensive (you pay per token for whole conversation every time)\nSmall models often forget early details in long conversation histories",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-history-management-2",
    "href": "src/02/slides.html#chat-history-management-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat History Management",
    "text": "Chat History Management\n\nMitigation Strategies\n\nRemove older messages from the history\nImplement sliding window across the conversation history\nSummarize older messages and rewrite the history",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#calling-other-models-1",
    "href": "src/02/slides.html#calling-other-models-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Calling Other Models",
    "text": "Calling Other Models\n\nWe could just duplicate our notebook, change the URL to another provider (e.g., Claude, Google, etc.), but:\n\nA separate account with each provider\nA separate credit card with each provider\nA separate API key to use for each provider\nDuplicate notebooks for each provider\n\nWouldn’t it be nice to have a single service (inference provider) that exposed lots of different models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#introducing-openrouter",
    "href": "src/02/slides.html#introducing-openrouter",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Introducing OpenRouter",
    "text": "Introducing OpenRouter\n\nIntroducing OpenRouter (https://openrouter.ai)\n\nA unified API to hundreds of AI models through a single endpoint\n(Using OpenAI’s Chat Completion API)\nOpenAI, Claude, Gemini, Grok, Nova, Llama, DeepSeek, Qwen, and many others.\nPay per API call, often same cost as the provider\nNewer APIs tend to be free for a short period",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-openrouter",
    "href": "src/02/slides.html#using-openrouter",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using OpenRouter",
    "text": "Using OpenRouter\n\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-openrouter-1",
    "href": "src/02/slides.html#using-openrouter-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using OpenRouter",
    "text": "Using OpenRouter\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://openrouter.ai/api/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"openai/gpt-5.2-chat\"\n}\n==================================================",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-openrouter-2",
    "href": "src/02/slides.html#using-openrouter-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using OpenRouter",
    "text": "Using OpenRouter\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"gen-1767585819-snubWxcK6sJM3RdE9rJX\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Paris has something for almost every kind of traveler! Here’s a well‑rounded starting plan, and then I can tailor it more if you tell me your interests, travel dates, and how long you’ll be there.\\n\\n### Must‑See Highlights\\n- **Eiffel Tower** – Go up for the views or enjoy it from below at Trocadéro or Champ de Mars.\\n- **Louvre Museum** – Even if you don’t love museums, seeing the Mona Lisa and the building itself is worth it.\\n- **Notre‑Dame Cathedral** – Admire the exterior and surroundings; interior access is gradually reopening.\\n- **Montmartre & Sacré‑Cœur** – Charming streets, artists, and great city views.\\n\\n### Classic Paris Experiences\\n- **Stroll along the Seine** – Especially at sunset.\\n- **Café culture** – Sit at a café with a coffee or glass of wine and people‑watch.\\n- **Boulangeries & pastries** – Try croissants, pain au chocolat, macarons.\\n- **Seine river cruise** – Relaxing and great for first‑time visitors.\\n\\n### Art, History & Culture\\n- **Musée d’Orsay** – Impressionist masterpieces in a stunning former train station.\\n- **Le Marais** – Historic district with boutiques, museums, and lively streets.\\n- **Latin Quarter** – Bookshops, old streets, and student energy.\\n\\n### Food & Drink\\n- **Bistro dining** – Try classic French dishes like boeuf bourguignon or duck confit.\\n- **Food markets** – Marché des Enfants Rouges is a favorite.\\n- **Wine & cheese tasting** – Many small shops offer guided tastings.\\n\\n### Day Trips (if you have extra time)\\n- **Versailles** – Palace and gardens (half‑day or full‑day trip).\\n- **Giverny** – Monet’s gardens (spring/summer).\\n- **Champagne region** – For wine lovers.\\n\\n### Practical Tips\\n- Buy museum tickets in advance.\\n- Walk as much as possible—Paris is very walkable.\\n- Learn a few French phrases; locals appreciate the effort.\\n\\nIf you’d like, tell me:\\n- How many days you’ll be there  \\n- Your interests (food, art, history, shopping, nightlife, romance, family travel)  \\n- Your budget level  \\n\\nAnd I’ll create a personalized day‑by‑day itinerary for you.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": null,\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null,\n        \"reasoning\": null\n      },\n      \"native_finish_reason\": \"completed\"\n    }\n  ],\n  \"created\": 1767585819,\n  \"model\": \"openai/gpt-5.2-chat\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 506,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 550,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": null,\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"rejected_prediction_tokens\": null,\n      \"image_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0,\n      \"video_tokens\": 0\n    },\n    \"cost\": 0.007161,\n    \"is_byok\": false,\n    \"cost_details\": {\n      \"upstream_inference_cost\": null,\n      \"upstream_inference_prompt_cost\": 0.000077,\n      \"upstream_inference_completions_cost\": 0.007084\n    }\n  },\n  \"provider\": \"OpenAI\"\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#token-streaming",
    "href": "src/02/slides.html#token-streaming",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Token Streaming",
    "text": "Token Streaming\n\nIn our notebooks, responses can take a few seconds to be returned\n\nNot the best user experience, especially for consumer products\n\nNeed a way to support streaming of tokens as they are generated (a.k.a. “typewriter effect”)\n\nStreaming added to Chat Completions API in early 2023\nSupported by other major vendors (Anthropic, Cohere, etc.)\nNow expected as a baseline feature",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#how-does-token-streaming-work",
    "href": "src/02/slides.html#how-does-token-streaming-work",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "How Does Token Streaming Work?",
    "text": "How Does Token Streaming Work?\n\nUses SSE (Server-Sent Events)\n\nUnidirectional (server to client)\nUses standard HTTP/1.1 or HTTP/2\nServer sends a response with a text/event-stream MIME type\nClient uses built-in EventSource API to open the connection, listen to messages, and handle events.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#sse-data-format",
    "href": "src/02/slides.html#sse-data-format",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "SSE Data Format",
    "text": "SSE Data Format\ndata: {\"choices\":[{\"delta\":{\"content\":\"Hello\"}}]}\n\ndata: {\"choices\":[{\"delta\":{\"content\":\" world\"}}]}\n  \ndata: [DONE]\nData sent as chunks, prefixed with data: and separated by double newlines",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-token-streaming",
    "href": "src/02/slides.html#implementing-token-streaming",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Token Streaming",
    "text": "Implementing Token Streaming\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n    stream=True, # Enable streaming\n)\n\n# Iterate through the stream and print each token as it arrives\nfor chunk in response:\n    # Each chunk contains a delta with the new content\n    if chunk.choices[0].delta.content is not None:\n        token = chunk.choices[0].delta.content\n        print(token, end='', flush=True)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#structured-output-1",
    "href": "src/02/slides.html#structured-output-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nSo far, the models have generated non-structured output (i.e., free-form text)\nSometimes, paragraph. Sometimes, numbered list.\nBut often, you need structure\n\n“Return your result in JSON format”\n“Give me the coordinates for Paris”\n“What’s the temperature in Paris right now?”",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#structured-output-2",
    "href": "src/02/slides.html#structured-output-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nYou can try to use the system prompt\n\n“Return the result in JSON only”\n\nBut… it doesn’t always work\n\nEarly/small models struggle with correct JSON formatting\nEven larger models make mistakes (e.g., missing closing brace)\n\nSometimes the models just forget!\n\n“RETURN THE RESULT IN JSON ONLY. NO OTHER TEXT!!!”",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#structured-outputs-in-openai-api",
    "href": "src/02/slides.html#structured-outputs-in-openai-api",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Structured Outputs in OpenAI API",
    "text": "Structured Outputs in OpenAI API\n\nNov 2023: OpenAI added JSON mode\n\nresponse_format: {\"type\": \"json_object\"}\nGuaranteed valid JSON, but didn’t enforce schema\nSometimes mixed up/missed fields\n\nAug 2024: Structured Outputs launched\n\nresponse_format: {\"type\": \"json_object\", ...}\n100% reliability that output matches the your schema",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#how-structured-outputs-work",
    "href": "src/02/slides.html#how-structured-outputs-work",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "How Structured Outputs Work",
    "text": "How Structured Outputs Work\n\nConstrained Decoding\n\nWhen generating responses, the model normally samples from all possible next tokens\nWith constrained decoding, the next token is dynamically filtered to only allow tokens that keep the output schema valid\n\ne.g., if schema requires an integer, string tokens are masked out from the probability distribution",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#how-structured-outputs-work-1",
    "href": "src/02/slides.html#how-structured-outputs-work-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "How Structured Outputs Work",
    "text": "How Structured Outputs Work\n\nRuns on server (or in library) - not fine-tuning approach\nSlightly slower token generation due to computational overhead\nTechnically, it’s mathematically impossible to generate invalid output\n\n(Real world: I see ~1:7000 error rates with GPT-5.1 chat)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-structured-outputs",
    "href": "src/02/slides.html#implementing-structured-outputs",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Structured Outputs",
    "text": "Implementing Structured Outputs\n\n\nfrom pydantic import BaseModel\n\n# Define the model for a geographic location\nclass Location(BaseModel):\n  name: str\n  country: str\n  latitude: float\n  longitude: float",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-structured-outputs-1",
    "href": "src/02/slides.html#implementing-structured-outputs-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Structured Outputs",
    "text": "Implementing Structured Outputs\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.parse(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"What are the GPS coordinates for Paris?\"},\n    ],\n    response_format=Location\n)\n\ncompletion = response.choices[0].message\nprint(completion)\n\nParsedChatCompletionMessage[Location](content='{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=Location(name='Paris', country='France', latitude=48.8566, longitude=2.3522), reasoning=None)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-structured-outputs-2",
    "href": "src/02/slides.html#implementing-structured-outputs-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Structured Outputs",
    "text": "Implementing Structured Outputs\n\n\n# Display the JSON repesentation\nprint(completion.content)\n\n# Display the parsed type\nprint(completion.parsed)\n\n# Pretty-print\nif completion.parsed:\n  location: Location = completion.parsed\n  print(f\"{location.name}, {location.country} has GPS coordinates of {location.latitude}, {location.longitude}\")\n\n{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}\nname='Paris' country='France' latitude=48.8566 longitude=2.3522\nParis, France has GPS coordinates of 48.8566, 2.3522",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#creating-a-chat-ui-1",
    "href": "src/02/slides.html#creating-a-chat-ui-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Creating a Chat UI",
    "text": "Creating a Chat UI\n\nUp to now, we’ve been making requests and printing the responses\nGood for learning concepts, but not a “product” that others can use\nWe want to build a UI that supports conversation threads, streaming, rich inputs/outputs, etc.\nBut we don’t want to start from scratch!",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#introducing-gradio",
    "href": "src/02/slides.html#introducing-gradio",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Introducing Gradio",
    "text": "Introducing Gradio\n\nSource: https://www.gradio.app/",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#what-is-gradio",
    "href": "src/02/slides.html#what-is-gradio",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What is Gradio?",
    "text": "What is Gradio?\n\nCreated in 2019: Startup called Gradio developing demos for research/academia\nAcquired by Hugging Face in 2021: became the standard interface for Hugging Face Spaces\nNow industry standard: For ML demos - used by researchers, startups to showcase models without front-end expertise",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#what-is-gradio-1",
    "href": "src/02/slides.html#what-is-gradio-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What is Gradio?",
    "text": "What is Gradio?\n\nRapid UI creation with minimal code\n\n5-10 lines of Python for an interactive interface. No HTML, CSS, JS required.\n\nRich input/output types\n\nText, images, audio, video, files, dataframes, etc.\n\nML workflows\n\nSupports streaming, queues, flagging/feedback\n\nDeployment flexibility\n\nCan run locally, create temporary public links, or embed in production apps",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-1-basic-interface",
    "href": "src/02/slides.html#example-1-basic-interface",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 1: Basic Interface",
    "text": "Example 1: Basic Interface\n\n\nimport gradio as gr\n\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\n\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\")\ndemo.launch()",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-1-basic-interface-1",
    "href": "src/02/slides.html#example-1-basic-interface-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 1: Basic Interface",
    "text": "Example 1: Basic Interface\n\n\n\n* Running on local URL:  http://127.0.0.1:7862\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-2-basic-chat-interface",
    "href": "src/02/slides.html#example-2-basic-chat-interface",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 2: Basic Chat Interface",
    "text": "Example 2: Basic Chat Interface\n\n\nimport gradio as gr\n\ndef chat_with_history(message, history):\n    # Add current message\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Get response from API\n    response = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n    )\n    \n    return response.choices[0].message.content\n\n# Create a chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_history,\n    title=\"Basic Chat with Conversation History\",\n    type=\"messages\"\n)\n\ndemo.launch()",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-2-basic-chat-interface-1",
    "href": "src/02/slides.html#example-2-basic-chat-interface-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 2: Basic Chat Interface",
    "text": "Example 2: Basic Chat Interface\n\n\n\n* Running on local URL:  http://127.0.0.1:7863\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-3-streaming-chat-interface",
    "href": "src/02/slides.html#example-3-streaming-chat-interface",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 3: Streaming Chat Interface",
    "text": "Example 3: Streaming Chat Interface\n\n\nimport gradio as gr\n\ndef chat_with_streaming(message, history):\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Stream the response\n    stream = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n        stream=True,\n    )\n    \n    response_text = \"\"\n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            token = chunk.choices[0].delta.content\n            response_text += token\n            yield response_text\n\n# Create streaming chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_streaming,\n    title=\"AI Chat with Streaming\",\n    type=\"messages\"\n)\n\ndemo.launch()",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-3-streaming-chat-interface-1",
    "href": "src/02/slides.html#example-3-streaming-chat-interface-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 3: Streaming Chat Interface",
    "text": "Example 3: Streaming Chat Interface\n\n\n\n* Running on local URL:  http://127.0.0.1:7864\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#looking-ahead-1",
    "href": "src/02/slides.html#looking-ahead-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nExplore AI Agents\nCreate agents, building upon our knowledge of Gradio\nGive the agent documents and tools to perform functions beyond what an LLM can do",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#references-1",
    "href": "src/02/slides.html#references-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/resources.html",
    "href": "src/03/resources.html",
    "title": "Resources",
    "section": "",
    "text": "OpenAI Agents SDK (Python) - Official OpenAI agents framework for Python\nOpenAI Agents SDK Announcement - Blog post announcing the OpenAI Agents SDK (Mar 2025)\nOpenAI Agents Visualization - Tool for visualizing agent graphs and interactions\n\n\n\n\n\nLangGraph - Python framework for building stateful, multi-actor applications with LLMs\n\n\n\n\n\nCrew.ai - Framework for orchestrating role-playing, autonomous AI agents\n\n\n\n\n\nAutoGen - Microsoft’s framework for building conversational AI systems\nMicrosoft Semantic Kernel - SDK for integrating AI services with conventional programming languages\nMicrosoft Agent Framework - Converged agent framework supporting .NET",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#agent-frameworks",
    "href": "src/03/resources.html#agent-frameworks",
    "title": "Resources",
    "section": "",
    "text": "OpenAI Agents SDK (Python) - Official OpenAI agents framework for Python\nOpenAI Agents SDK Announcement - Blog post announcing the OpenAI Agents SDK (Mar 2025)\nOpenAI Agents Visualization - Tool for visualizing agent graphs and interactions\n\n\n\n\n\nLangGraph - Python framework for building stateful, multi-actor applications with LLMs\n\n\n\n\n\nCrew.ai - Framework for orchestrating role-playing, autonomous AI agents\n\n\n\n\n\nAutoGen - Microsoft’s framework for building conversational AI systems\nMicrosoft Semantic Kernel - SDK for integrating AI services with conventional programming languages\nMicrosoft Agent Framework - Converged agent framework supporting .NET",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#protocols-and-standards",
    "href": "src/03/resources.html#protocols-and-standards",
    "title": "Resources",
    "section": "Protocols and Standards",
    "text": "Protocols and Standards\n\nModel Context Protocol (MCP) - Standardized protocol for connecting AI models with external tools and data sources\nMCP SDK Documentation - SDKs for building MCP servers in Python, TypeScript, Go, Rust, C#, and more",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#agent-memory",
    "href": "src/03/resources.html#agent-memory",
    "title": "Resources",
    "section": "Agent Memory",
    "text": "Agent Memory\n\nSupermemory - Memory layer for AI applications\nLetta - Long-term memory for AI agents\nmem0 - Open source memory layer for AI applications",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#openai-platform",
    "href": "src/03/resources.html#openai-platform",
    "title": "Resources",
    "section": "OpenAI Platform",
    "text": "OpenAI Platform\n\nOpenAI Platform - OpenAI developer platform for API access\nOpenAI Traces Dashboard - Built-in tracing for debugging OpenAI Agents SDK applications",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#hosting-and-deployment",
    "href": "src/03/resources.html#hosting-and-deployment",
    "title": "Resources",
    "section": "Hosting and Deployment",
    "text": "Hosting and Deployment\n\nHugging Face Spaces - Free cloud hosting for ML demos and Gradio applications",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#articles-and-industry-reports",
    "href": "src/03/resources.html#articles-and-industry-reports",
    "title": "Resources",
    "section": "Articles and Industry Reports",
    "text": "Articles and Industry Reports\n\nWorld Economic Forum - Cognitive Enterprise - Article on the agentic business revolution\nCRN - Hottest Agentic AI Tools - Overview of the top agentic AI tools of 2025\nGartner Press Release - Gartner’s predictions about agentic AI project success rates\nAnthropic - Building Effective Agents - Engineering guide on building effective AI agents and patterns\nE2B - AI Agents Landscape - Overview of the AI agents ecosystem and available frameworks",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#citations",
    "href": "src/03/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/assignment.html",
    "href": "src/04/assignment.html",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "",
    "text": "Objective: Build a working application that demonstrates your understanding of multimedia or multimodal AI models.\nChoose Your Adventure: Pick ONE of the three options below.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#option-1-controlnet-scribble-app",
    "href": "src/04/assignment.html#option-1-controlnet-scribble-app",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "Option 1: ControlNet Scribble App",
    "text": "Option 1: ControlNet Scribble App\nBuild a Gradio application that transforms hand-drawn sketches into realistic images using ControlNet.\nRequirements:\n\nCreate a Gradio interface with a sketchpad/canvas input\nUse ControlNet (scribble or canny edge model) to condition image generation\nAllow users to enter a text prompt to guide the style/content\nGenerate and display the resulting image\nInclude at least one configurable parameter (e.g., guidance scale, number of steps)\n\nSuggested approach:\n\nUse Replicate’s ControlNet models for easier deployment, OR\nRun locally using HuggingFace diffusers with a ControlNet pipeline",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#option-2-replicate-model-pipeline",
    "href": "src/04/assignment.html#option-2-replicate-model-pipeline",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "Option 2: Replicate Model Pipeline",
    "text": "Option 2: Replicate Model Pipeline\nCreate a pipeline that chains multiple Replicate models together to transform images through a multi-step process.\nRequirements:\n\nChain at least 3 different models in sequence (e.g., depth estimation → ControlNet → upscaling)\nCreate a Gradio interface that accepts an input image and displays intermediate/final results\nDocument what each model in your pipeline does and why you chose it\nShow the transformation at each stage (not just the final output)\n\nExample pipeline ideas:\n\nPhoto → Depth Map → Stylized Scene → Upscaled Output\nPortrait → Pose Extraction → New Character in Same Pose → Background Replacement\nSketch → Colorized Image → Style Transfer → Final Composition",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#option-3-vision-language-model-application",
    "href": "src/04/assignment.html#option-3-vision-language-model-application",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "Option 3: Vision Language Model Application",
    "text": "Option 3: Vision Language Model Application\nImplement a practical application using a Vision Language Model (VLM) for a real-world use case.\nRequirements:\n\nUse an open-source VLM (e.g., Gemma 3, LLaVA, FastVLM) - not a closed API like GPT-4V or Claude\nBuild a Gradio interface that accepts image input\nImplement a specific, practical use case such as:\n\nAccessibility: Describe images for visually impaired users\nProduct Detection: Identify and catalog items from photos\nDocument Analysis: Extract information from receipts, forms, or charts\nEducational: Explain diagrams, equations, or scientific figures\n\nInclude thoughtful prompt engineering in your system prompt",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#deliverable-a-colabjupyter-notebook-with",
    "href": "src/04/assignment.html#deliverable-a-colabjupyter-notebook-with",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "Deliverable: A Colab/Jupyter notebook with:",
    "text": "Deliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation\nA working Gradio interface that can be launched and tested\nUses environment variables for any API keys (e.g., REPLICATE_API_TOKEN). Please do not include your API key in your notebook!\nMarkdown cells explaining:\n\nWhich option you chose and why\nYour design decisions and approach\nObservations about model behavior, quality, or limitations\nWhat worked well and what was challenging",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#hints",
    "href": "src/04/assignment.html#hints",
    "title": "Module 4 Assignment: Multimedia & Multimodal Applications",
    "section": "Hints",
    "text": "Hints\n\nOption 1: The gr.Sketchpad or gr.ImageEditor components in Gradio work well for drawing input. Start with a simple black-and-white sketch before adding complexity.\nOption 2: Plan your pipeline on paper first. Consider what each model needs as input and produces as output. The PBR notebook (pbr-creator.ipynb) demonstrates this chaining pattern.\nOption 3: Smaller models (like FastVLM-0.5B or Gemma 3 4B) can run on Colab’s T4 GPU. Focus on crafting a good system prompt that guides the model toward your specific use case.\nAll options: Test with multiple different inputs to understand the model’s capabilities and limitations.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/slides.html#recap",
    "href": "src/04/slides.html#recap",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Recap",
    "text": "Recap\n\nDescribed the fundamental concepts behind Agents/Agentic AI\nExplored and provided feedback on an existing multi-agent setup\nUnderstood available agent SDKs, how they differ, and advantages/disadvantages\nUsed the OpenAI Agents SDK to build a multi-agent system from scratch, including document indexing and retrieval\nUnderstood and implemented tool calls using OpenAI’s function calling and via MCP",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#lesson-objectives",
    "href": "src/04/slides.html#lesson-objectives",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand the fundamentals and history of diffuser models\nExplore and use models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet\nSetup and use Replicate to create a custom pipeline of production-grade models\nUnderstand the fundamentals and history of Vision Encoders and VLMs\nImplement/test a local VLM model for on-device inference",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#multimedia-vs.-multimodal-1",
    "href": "src/04/slides.html#multimedia-vs.-multimodal-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Multimedia vs. Multimodal",
    "text": "Multimedia vs. Multimodal\n\nMultimedia models\n\nSingle input/output models for images, video, audio, etc.\nAlso known as computer vision, audio models\n\nExamples\n\nText-to-Image (generate an image from a text prompt)\nImage-to-Image (generate an image from an existing image)\nImage-to-3D (generate a 3D object from an image)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#multimedia-vs.-multimodal-2",
    "href": "src/04/slides.html#multimedia-vs.-multimodal-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Multimedia vs. Multimodal",
    "text": "Multimedia vs. Multimodal\n\nMultimodal models\n\nProcess multiple datatypes such as text, images, and audio\nAlso known as VLMs (Vision-Language Models) or ALMs (Audio-Language Models)\n\nExamples\n\nImage-Text-to-Text (ask a question about this image)\nImage-Text-to-Image (decompose this image into multiple layers)\nAudio-Text-to-Text (what is this sound?)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#text-to-image",
    "href": "src/04/slides.html#text-to-image",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Text-to-Image",
    "text": "Text-to-Image\n\n\n“A photograph of an astronaut riding a horse.”\n\nBased on a concept called a diffusion transformer\nCommonly known as a diffuser\nTwo stage process, inspired by thermodynamics",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#introducing-the-diffuser",
    "href": "src/04/slides.html#introducing-the-diffuser",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Introducing the Diffuser",
    "text": "Introducing the Diffuser\n\nTraining\n\nDuring training, random noise is added to images in steps\nModel learns to predict what noise was added (forward diffusion process)\n\nInference (process runs in reverse)\n\nStart with pure random noise\nModel estimates what noise should be removed to create a realistic image\nUsing the text prompt, the model steers the process towards images that match the description",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-diffusion-models-in-2022",
    "href": "src/04/slides.html#image-diffusion-models-in-2022",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image Diffusion Models in 2022",
    "text": "Image Diffusion Models in 2022\n\n\n\n\n\ntimeline\n  August 2022 : Stable Diffusion v1.4\n              : First open-source high-quality model\n  September 2022 : Stable Diffusion v1.5\n                  : Refined version\n  October 2022 : eDiff-I (NVIDIA)\n                : Ensemble approach\n  November 2022 : Stable Diffusion v2.0/2.1\n                : Higher resolution (768x768)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-diffusion-models-in-2023",
    "href": "src/04/slides.html#image-diffusion-models-in-2023",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image Diffusion Models in 2023",
    "text": "Image Diffusion Models in 2023\n\n\n\n\n\ntimeline\n  March 2023 : Midjourney v5\n              : Exceptional artistic quality\n  April 2023 : ControlNet\n        : Precise spatial control\n        : AnimateDiff - Video generation\n  July 2023 : SDXL (Stable Diffusion XL)\n            : 1024x1024 native resolution\n  August 2023 : SDXL Turbo\n              : Real-time capable generation",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-diffusion-models-in-2024",
    "href": "src/04/slides.html#image-diffusion-models-in-2024",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image Diffusion Models in 2024",
    "text": "Image Diffusion Models in 2024\n\n\n\n\n\ntimeline\n  February 2024 : Stable Diffusion 3\n                : Improved text understanding\n  June 2024 : Stable Diffusion 3.5\n            : Multiple model sizes\n  2024 : FLUX.1 (Black Forest Labs)\n        : State-of-the-art open model\n        : Imagen 3 (Google DeepMind)\n        : Photorealistic quality",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-diffusion-models-in-2025",
    "href": "src/04/slides.html#image-diffusion-models-in-2025",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image Diffusion Models in 2025",
    "text": "Image Diffusion Models in 2025\n\n\n\n\n\ntimeline\n  May 2025 : Imagen 4 (Google DeepMind)\n           : Improved text rendering, 2K resolution\n  August 2025 : Nano Banana (Google)\n              : Autoregressive model in Gemini 2.5 Flash\n  November 2025 : FLUX.2 (Black Forest Labs)\n                : 32B parameters, multi-image references",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#text-to-image-with-sd-1.5",
    "href": "src/04/slides.html#text-to-image-with-sd-1.5",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Text-to-Image with SD 1.5",
    "text": "Text-to-Image with SD 1.5\n\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\n# Load a small diffusion model\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id,\n)\n\n# Move to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npipe = pipe.to(device)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#text-to-image-with-sd-1.5-1",
    "href": "src/04/slides.html#text-to-image-with-sd-1.5-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Text-to-Image with SD 1.5",
    "text": "Text-to-Image with SD 1.5\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nPROMPT = \"a photograph of an astronaut riding a horse\" #@param {type:\"string\"}\nSTEPS = 50 #@param {type:\"slider\", min:10, max:100, step:1}\nSEED = -1 #@param {type:\"integer\"}\n\nintermediate_images = []\n\ndef callback_fn(step, timestep, latents):\n    \"\"\"Capture intermediate denoising steps\"\"\"\n    # Decode latents to image every few steps\n    if step % 5 == 0 or step == 0:\n        with torch.no_grad():\n            # Decode the latent representation to an image\n            image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\n            image = pipe.image_processor.postprocess(image, output_type=\"pil\")[0]\n            intermediate_images.append((step, image))\n\nresult = pipe(\n    PROMPT,\n    num_inference_steps=STEPS,\n    callback=callback_fn,\n    callback_steps=1,\n    generator=torch.Generator().manual_seed(SEED) if SEED != -1 else None,\n).images[0]\n\n# Visualize the denoising process\nnum_steps_to_show = min(10, len(intermediate_images))\nstep_indices = np.linspace(0, len(intermediate_images)-1, num_steps_to_show, dtype=int)\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfig.suptitle(f'Real Diffusion Model Denoising Process\\nPrompt: \"{PROMPT}\"')\n\nfor idx, step_idx in enumerate(step_indices):\n    row = idx // 5\n    col = idx % 5\n    step_num, img = intermediate_images[step_idx]\n\n    axes[row, col].imshow(img)\n    axes[row, col].axis('off')\n    axes[row, col].set_title(f'Step {step_num}/{STEPS}')\n\nplt.tight_layout()\nplt.savefig('diffusion_process.png', dpi=150, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#text-to-image-with-sd-1.5-2",
    "href": "src/04/slides.html#text-to-image-with-sd-1.5-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Text-to-Image with SD 1.5",
    "text": "Text-to-Image with SD 1.5",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#text-to-image-with-sd-1.5-3",
    "href": "src/04/slides.html#text-to-image-with-sd-1.5-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Text-to-Image with SD 1.5",
    "text": "Text-to-Image with SD 1.5",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-what-are-these-pipelines",
    "href": "src/04/slides.html#sidebar-what-are-these-pipelines",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: What are these pipelines?",
    "text": "Sidebar: What are these pipelines?\n\nOur use of HF Transformers use so far\n\nConvert text to input tokens, pass to model, decode output tokens to text\n\nHF Pipelines provides a layer of abstraction\n\n(Setup the pipeline, then call pipe method)\nWhile still giving access to underlying components\n\nPipelines also standardize other areas\n\ne.g., pipe(prompt).images[0] works for all model types\n.to(\"cuda\") moves all components of the model to the GPU",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-seeds",
    "href": "src/04/slides.html#sidebar-seeds",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: Seeds",
    "text": "Sidebar: Seeds\n\nWhat is a seed?\n\n(Optional) Integer value used to initialize the image generation\nUsed to generate the initial random noise\nUsing the same seed will generate the same image\n\nWhy use a seed?\n\nControlling the seed allows you to then experiment with different prompts or parameters\nGives you more control/predictability vs. starting from random seed every time",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-1",
    "href": "src/04/slides.html#image-to-image-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image",
    "text": "Image-to-Image\n\nImage-to-Image: “Make this image different”\nOriginally solved by GAN approaches, but evolved into extension of the diffuser concept\n\nAdd noise to the original image (partial denoising)\nRegenerate it with modifications based on the prompt\nStrength parameter (0.0 - 1.0) to indicate the weight to the new image vs. original\nThe original image heavily influences the output structure",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-generate",
    "href": "src/04/slides.html#image-to-image-generate",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (Generate)",
    "text": "Image-to-Image (Generate)\n\n\ndef generate_image(strength):\n  return pipe(\n      prompt=PROMPT,\n      negative_prompt=NEGATIVE_PROMPT,\n      image=init_image,\n      strength=strength,\n      guidance_scale=7.5,\n      num_inference_steps=30,\n      generator=torch.Generator().manual_seed(SEED) if SEED != -1 else None,\n  ).images[0]",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-original",
    "href": "src/04/slides.html#image-to-image-original",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (Original)",
    "text": "Image-to-Image (Original)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-original-1",
    "href": "src/04/slides.html#image-to-image-original-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (Original)",
    "text": "Image-to-Image (Original)\nPrompt: “a goldendoodle wearing sunglasses, high quality, detailed”",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-original-2",
    "href": "src/04/slides.html#image-to-image-original-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (Original)",
    "text": "Image-to-Image (Original)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-0.3",
    "href": "src/04/slides.html#image-to-image-0.3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (0.3)",
    "text": "Image-to-Image (0.3)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-0.5",
    "href": "src/04/slides.html#image-to-image-0.5",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (0.5)",
    "text": "Image-to-Image (0.5)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-0.7",
    "href": "src/04/slides.html#image-to-image-0.7",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (0.7)",
    "text": "Image-to-Image (0.7)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-0.9",
    "href": "src/04/slides.html#image-to-image-0.9",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image (0.9)",
    "text": "Image-to-Image (0.9)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#beyond-sd-1.5-1",
    "href": "src/04/slides.html#beyond-sd-1.5-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Beyond SD 1.5",
    "text": "Beyond SD 1.5\n\nStable Diffusion 1.5\n\nGreat for learning about the diffusion process\nBut the image quality isn’t great!\n\nImage models get large quickly\n\nHigher resolutions demand more GPU/VRAM",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#introducing-replicate",
    "href": "src/04/slides.html#introducing-replicate",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Introducing Replicate",
    "text": "Introducing Replicate\n\nSource: https://replicate.com",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#introducing-replicate-1",
    "href": "src/04/slides.html#introducing-replicate-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Introducing Replicate",
    "text": "Introducing Replicate\n\nSimilar to OpenRouter\n\nBut with a focus on image and video models\nExtensive access to larger models (e.g., FLUX 2, Nano Banana, ImageGen)\nPay-per-call pricing (expect 2c per image for higher quality models; some free models)\nAPI access (with Python and NodeJS library)\nFine-tune and share your own models",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#flux-models",
    "href": "src/04/slides.html#flux-models",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "FLUX Models",
    "text": "FLUX Models\n\nFrom Black Forest Labs (founded by ex-Stability AI researchers)\nKey architectural differences from Stable Diffusion:\n\nUses a Multimodal Diffusion Transformer (MMDiT) instead of U-Net\nProcesses text and image tokens together in a unified transformer\nNative support for higher resolutions without quality degradation\n\nVariants: FLUX.1 [schnell] (fast), [dev] (quality), [pro] (commercial)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#using-the-replicate-api-1",
    "href": "src/04/slides.html#using-the-replicate-api-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Using the Replicate API",
    "text": "Using the Replicate API\n\n\nimport replicate\noutput = replicate.run(\n  \"black-forest-labs/flux-pro\",\n  input={\n      \"steps\": 28,\n      \"prompt\": \"lemon cupcake spelling out the words 'DigiPen' with sparklers, tasty, food photography, dynamic shot\",\n      \"seed\": 1564435,\n      \"output_format\": \"png\",\n      \"safety_tolerance\": 2,\n      \"prompt_upsampling\": False\n  },\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#using-the-replicate-api-2",
    "href": "src/04/slides.html#using-the-replicate-api-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Using the Replicate API",
    "text": "Using the Replicate API\n\nsteps: Number to steps to run through\nseed: Random seed value\nprompt: Prompt to use to guide the model\noutput_format: Output format to return\nsafety_tolerance: Safety tolerance (1 is most strict; 6 is most permissive)\nprompt_upsampling: Run the prompt through an LLM to be more descriptive/creative",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#using-the-replicate-api-3",
    "href": "src/04/slides.html#using-the-replicate-api-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Using the Replicate API",
    "text": "Using the Replicate API",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-safety-tolerance",
    "href": "src/04/slides.html#sidebar-safety-tolerance",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: Safety Tolerance",
    "text": "Sidebar: Safety Tolerance\n\nHow does safety tolerance work?\n\nSafety/guardrails are not typically embedded into the model\ne.g., SD 1.5 will generate NSFW images easily\n\nInstead, separate classifiers run alongside the model\n\nInput filtering: Analyzes the prompt prior to generation\nOutput filtering: Image classifier examines the model before showing it to the user\nThresholds used to control the classifiers",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#image-to-image-2",
    "href": "src/04/slides.html#image-to-image-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Image-to-Image",
    "text": "Image-to-Image\n\nCan also be used for…\n\nSuper resolution (increase the resolution of this image)\nStyle transfer (recreate this image in the style of…)\nColorization (grayscale to color)\nDepth maps",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps",
    "href": "src/04/slides.html#depth-maps",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps\n\n\n\nImages (often greyscale) where each pixel’s value represents the distance from the viewer\n\ni.e., objects in the foreground are lighter, background are darker",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-1",
    "href": "src/04/slides.html#depth-maps-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps\n\n\n\n\n\nHistorically, required custom hardware\n\nDepth Camera (e.g., RealSense) - ~$300-500\nModule/processing for realtime (60fps) sensing",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-2",
    "href": "src/04/slides.html#depth-maps-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps\n\nImage-to-Image depth estimation models\n\nDepth Anything, MiDaS, ZoeDepth\nLow latency (MiDaS 3.1 @ 20fps on embedded GPU)\n\nUsed for\n\n3D effects/estimation\nSimple/low-cost robotics\nControl input for other images",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-3",
    "href": "src/04/slides.html#depth-maps-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-4",
    "href": "src/04/slides.html#depth-maps-4",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps\n\n\nimport replicate\n\nMODEL = \"chenxwh/depth-anything-v2:b239ea33cff32bb7abb5db39ffe9a09c14cbc2894331d1ef66fe096eed88ebd4\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"image\": INPUT_IMAGE,\n  },\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-5",
    "href": "src/04/slides.html#depth-maps-5",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-maps-6",
    "href": "src/04/slides.html#depth-maps-6",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Maps",
    "text": "Depth Maps\n\nWhy do this?\n\nDepth map can be used as control image for new image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-map-as-control",
    "href": "src/04/slides.html#depth-map-as-control",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Map as Control",
    "text": "Depth Map as Control",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-map-as-control-1",
    "href": "src/04/slides.html#depth-map-as-control-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Map as Control",
    "text": "Depth Map as Control\n\n\nimport replicate\n\nMODEL = \"black-forest-labs/flux-depth-pro\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"control_image\": CONTROL_IMAGE,\n      \"output_format\": \"png\",\n      \"seed\": 12345,\n      \"prompt\": \"A futuristic building set in 2050, neon lighting, night shot, dynamic\"\n  },\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-map-as-control-2",
    "href": "src/04/slides.html#depth-map-as-control-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Map as Control",
    "text": "Depth Map as Control",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-map-as-control-3",
    "href": "src/04/slides.html#depth-map-as-control-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Map as Control",
    "text": "Depth Map as Control\n\n\nimport replicate\n\nMODEL = \"black-forest-labs/flux-depth-pro\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"control_image\": CONTROL_IMAGE,\n      \"output_format\": \"png\",\n      \"seed\": 12345,\n      \"prompt\": \"A historical castle set in medieval England, clear day, partially cloudy sky\"\n  },\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#depth-map-as-control-4",
    "href": "src/04/slides.html#depth-map-as-control-4",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Depth Map as Control",
    "text": "Depth Map as Control",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#inpainting",
    "href": "src/04/slides.html#inpainting",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Inpainting",
    "text": "Inpainting\n\nFilling in missing or masked regions of an image in a realistic way\nModel is given an image with certain areas masked out\nGenerates plausible content to fill those areas based on the surrounding context (and steered by a text prompt)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#inpainting-1",
    "href": "src/04/slides.html#inpainting-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Inpainting",
    "text": "Inpainting\n\nCan be challenging\n\nModel needs to understand context around the area\nGenerate content that matches the style, lighting, and perspective\nFollow a text prompt that was likely different from the original",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#inpainting-2",
    "href": "src/04/slides.html#inpainting-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Inpainting",
    "text": "Inpainting\n\n\nimport gradio as gr\nimport numpy as np\nfrom PIL import Image\nimport replicate\nimport io\n\ndef inpaint(image_data, prompt):\n    if image_data is None:\n        return None\n    \n    # Get the original image from the background\n    original_image = Image.fromarray(image_data['background'])\n    \n    # Get the mask from the layers\n    if image_data['layers'] and len(image_data['layers']) &gt; 0:\n        mask_layer = image_data['layers'][0]\n        mask_array = np.array(mask_layer)\n        \n        # Create binary mask: white where painted, black where not\n        alpha_channel = mask_array[:, :, 3]\n        binary_mask = np.where(alpha_channel &gt; 0, 255, 0).astype(np.uint8)\n        mask_image = Image.fromarray(binary_mask, mode='L')\n    else:\n        return None\n    \n    # Convert images to bytes for the replicate API\n    image_bytes = io.BytesIO()\n    original_image.save(image_bytes, format='PNG')\n    image_bytes.seek(0)\n    \n    mask_bytes = io.BytesIO()\n    mask_image.save(mask_bytes, format='PNG')\n    mask_bytes.seek(0)\n    \n    # Call the Replicate API\n    output = replicate.run(\n        \"black-forest-labs/flux-fill-pro\",\n        input={\n            \"image\": image_bytes,\n            \"mask\": mask_bytes,\n            \"prompt\": prompt,\n            \"steps\": 25,\n            \"guidance\": 75,\n            \"outpaint\": \"None\",\n            \"output_format\": \"jpg\",\n            \"safety_tolerance\": 2,\n            \"prompt_upsampling\": False\n        }\n    )\n    \n    # Read the FileOutput and convert to PIL Image\n    output_bytes = output.read()\n    output_image = Image.open(io.BytesIO(output_bytes))\n    \n    return output_image\n\ndemo = gr.Interface(\n    fn=inpaint,\n    inputs=[\n        gr.ImageEditor(\n            label=\"Image (paint over areas to inpaint)\",\n            brush=gr.Brush(color_mode=\"fixed\", colors=[\"#000000\"]),\n            layers=True\n        ),\n        gr.Textbox(label=\"Prompt\", placeholder=\"Describe what should replace the masked area...\")\n    ],\n    outputs=gr.Image(label=\"Output Image\"),\n    title=\"Inpainting using black-forest-labs/flux-fill-pro\"\n)\ndemo.launch()",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting",
    "href": "src/04/slides.html#outpainting",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting\n\nThe opposite of inpainting, kind of :)\nHow does it work?\n\nSupply a prompt: “2x zoom out this image”\nTreat the new empty regions around the image as masked areas\nUse inpainting technique to fill in the regions",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting-1",
    "href": "src/04/slides.html#outpainting-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting\n\nMore challenging\n\nLess context at the edges of the image vs. center/surrounded\nNeed to maintain the style, lighting, and perspective\nHas to be creative. Can’t just be a repetitive pattern.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting-2",
    "href": "src/04/slides.html#outpainting-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting-3",
    "href": "src/04/slides.html#outpainting-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting\n\n\nimport replicate\n\n# Call the Replicate API\noutput = replicate.run(\n    \"black-forest-labs/flux-fill-pro\",\n    input={\n        \"image\": INPUT_IMAGE,\n        \"prompt\": \"The main building of a technical college, no text\",\n        \"seed\": 123456,\n        \"steps\": 50,\n        \"guidance\": 60,\n        \"outpaint\": \"Zoom out 2x\",\n        \"output_format\": \"jpg\",\n        \"safety_tolerance\": 2,\n        \"prompt_upsampling\": False\n    }\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting-4",
    "href": "src/04/slides.html#outpainting-4",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#outpainting-5",
    "href": "src/04/slides.html#outpainting-5",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Outpainting",
    "text": "Outpainting",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#inpainting-and-outpainting",
    "href": "src/04/slides.html#inpainting-and-outpainting",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Inpainting and Outpainting",
    "text": "Inpainting and Outpainting\n\nPopular models\n\nStable Diffusion (Many inpainting variants)\nFlux Fill from Black Forest Labs\nLaMa: Large Mask inpainting\nIdeogram",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#i-want-more-control-1",
    "href": "src/04/slides.html#i-want-more-control-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "I Want More Control!",
    "text": "I Want More Control!\n\nPrompt engineering for text-to-image and image-to-image is important\n\nBad prompt: a cat\nGood prompt: A fluffy orange tabby cat sitting on a wooden windowsill, golden hour lighting, soft focus background of a garden, photorealistic, highly detailed fur texture, warm color palette, shot with 85mm lens, shallow depth of field",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#i-want-more-control-2",
    "href": "src/04/slides.html#i-want-more-control-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "I Want More Control!",
    "text": "I Want More Control!\n\nKey components to include in prompts:\n\nSubject: What you want\nStyle/Medium: photorealistic, oil painting, digital art\nLighting: studio lighting, dramatic shadows, soft diffused\nComposition: close-up, wide-angle, rule of thirds\nQuality: highly detailed, 4K, sharp focus\nTechnical specs: 85mm lens, f/1.8, bokeh",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#i-want-more-control-3",
    "href": "src/04/slides.html#i-want-more-control-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "I Want More Control!",
    "text": "I Want More Control!\n\nNegative prompts (popular in some models)\n\nTell the model what to avoid - particularly useful with Stable Diffusion\nblurry, low quality, distorted, deformed, ugly, bad anatomy, extra limbs, watermark, text, signature, overexposed, underexposed, cartoon",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#i-want-even-more-control",
    "href": "src/04/slides.html#i-want-even-more-control",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "I Want Even More Control!",
    "text": "I Want Even More Control!\n\nExtensive prompts (both positive and negative) can help, but only so far\n\nUltimately, “hoping the model guesses what I mean”\nFine-tuning possible, but it’s expensive and risks degrading quality (and potential overfitting)\nNeed a different approach…",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#introducing-controlnet-1",
    "href": "src/04/slides.html#introducing-controlnet-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Introducing ControlNet",
    "text": "Introducing ControlNet\n\nDeveloped by Lvmin Zhang and Maneesh Agrawala at Stanford University\nPublished in February 2023 (Zhang, Rao, and Agrawala 2023)\nControlNet represented a paradigm shift from “describe what you want” to “show the structure you want”.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works",
    "href": "src/04/slides.html#how-controlnet-works",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\nStable Diffusion’s U-Net has an encoder and decoder\nCreate a trainable copy of the encoder blocks\nTrain the copy of the encoder alongside the frozen SD model\n\nDuring training: use paired data (e.g., pose skeleton → original image)\nDuring inference: both encoders run together\nFeatures from both are combined via zero convolutions\n\nKey: The weights in the original SD model don’t change\nControlNet is analogous to a “Plug in” model",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-1",
    "href": "src/04/slides.html#how-controlnet-works-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\nExamples of conditioning types:\n\nDepth maps: 3D structure information\nHuman pose: skeleton/keypoint detection\nCanny edges: line drawings and edge detection\nScribbles: rough user drawings\nQR codes: blended into images",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-2",
    "href": "src/04/slides.html#how-controlnet-works-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-3",
    "href": "src/04/slides.html#how-controlnet-works-3",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\n\npose_image = openpose(input_image)\ndisplay(pose_image)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-4",
    "href": "src/04/slides.html#how-controlnet-works-4",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-5",
    "href": "src/04/slides.html#how-controlnet-works-5",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\n\nimport torch\nfrom diffusers import ControlNetModel\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"lllyasviel/control_v11p_sd15_openpose\",\n    torch_dtype=torch.float16\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-6",
    "href": "src/04/slides.html#how-controlnet-works-6",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\n\nfrom diffusers import StableDiffusionControlNetPipeline\n\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    use_safetensors=True\n)\npipe.to(\"cuda\")",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-7",
    "href": "src/04/slides.html#how-controlnet-works-7",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\n\nPROMPT = \"a robot with glowing LED lights, futuristic, sci-fi\"\nNEGATIVE_PROMPT = \"blurry, low quality, distorted, extra limbs, deformed\"\nSEED = 3434002\n\nresult = pipe(\n    prompt=PROMPT,\n    negative_prompt=NEGATIVE_PROMPT,\n    image=pose_image,\n    num_inference_steps=25,\n    guidance_scale=7.5,\n    controlnet_conditioning_scale=1.0,\n    generator=torch.manual_seed(SEED) if SEED != -1 else None\n).images[0]\ndisplay(result)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#how-controlnet-works-8",
    "href": "src/04/slides.html#how-controlnet-works-8",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#cnns-to-vision-transformer",
    "href": "src/04/slides.html#cnns-to-vision-transformer",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "CNNs to Vision Transformer",
    "text": "CNNs to Vision Transformer\nHistorically, computer vision has used classification models called CNNs (Convolutional Neural Networks)\n\nEnter the Vision Transformer (ViT)\n\n“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” (Dosovitskiy et al. 2021)\n\nTipping Point\n\nInitially, ViTs didn’t outperform CNNs\nBut exceeded SOTA CNNs on larger datasets (such as Google’s JFT-300M)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#popular-vision-transformers",
    "href": "src/04/slides.html#popular-vision-transformers",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Popular Vision Transformers",
    "text": "Popular Vision Transformers\n\nOpenAI’s CLIP\n\nTrained on 400M image-text pairs\nFoundation of most VLMs\n\nMeta’s DINO/DINO-2\n\n(Self DIstillation with NO Labels)\nSelf-supervised on 142M images\n\nMicrosoft’s Swin\n\nUse “shifted windows” approach\nExcels at dense prediction tasks",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#vision-language-models-vlms",
    "href": "src/04/slides.html#vision-language-models-vlms",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Vision Language Models (VLMs)",
    "text": "Vision Language Models (VLMs)\n\nViTs by themselves are only so useful\nIntroducing VLMs (Vision Language Models)\n\nA vision encoder\nAdapter/projector layer\nLanguage model (LLaMa or GPT)\n\nAlso known as “Multimodal”\n\nImage-Text-to-Text",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#demo-2",
    "href": "src/04/slides.html#demo-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Demo",
    "text": "Demo\nUsing Gemma 3 (4B) to describe an image\nNotebook: vlm-gemma-3-4b.ipynb",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-image-url-dereferencing",
    "href": "src/04/slides.html#sidebar-image-url-dereferencing",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: Image URL Dereferencing",
    "text": "Sidebar: Image URL Dereferencing\n\n\nfrom IPython.display import Image\n\nIMAGE_URL = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"\n\nImage(url=IMAGE_URL, width=500)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-image-url-dereferencing-1",
    "href": "src/04/slides.html#sidebar-image-url-dereferencing-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: Image URL Dereferencing",
    "text": "Sidebar: Image URL Dereferencing\n\nHandled within the pipeline library\n\nDetects the image URL in the message structure\nDownloads the image\nLoads as a PIL Image object\nPreprocessing (resizing, normalizing pixel values)\nConverts to tensors",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#sidebar-image-url-dereferencing-2",
    "href": "src/04/slides.html#sidebar-image-url-dereferencing-2",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Sidebar: Image URL Dereferencing",
    "text": "Sidebar: Image URL Dereferencing\n\nText gets tokenized into token IDs\nImage gets processed into pixel tensors\nBoth feed into their respective encoders (text encoder, vision encoder)\nFeatures are aligned via shared embedding space\n\ne.g., visual concepts (furry, four legs, whiskers, etc.) are close to the word “cat” in shared embedding space",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#use-cases-for-vlms",
    "href": "src/04/slides.html#use-cases-for-vlms",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Use Cases for VLMs",
    "text": "Use Cases for VLMs\n\nVisual Understanding: “What’s in this image?”\nAccessibility: Assisting users with visual impairments\nContent Moderation and Safety: Identifying harmful content\nRetail: Finding products with photos\nEducation: Helping students understand charts, diagrams, equations\nRobotics: Providing Robots with information to navigate their environment",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#vlms-vs.-object-detection-models",
    "href": "src/04/slides.html#vlms-vs.-object-detection-models",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "VLMs vs. Object Detection Models",
    "text": "VLMs vs. Object Detection Models\n\n\n\nRicher semantics (e.g., tabby cat vs. cat)\nContextual descriptions\nZero-shot: Can identify objects never seen during training\nBut less precise at localization (bounding boxes, pixel coordination)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#popular-vlms",
    "href": "src/04/slides.html#popular-vlms",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Popular VLMs",
    "text": "Popular VLMs\n\nClosed Source\n\nGPT4-V, Claude, Gemini Flash\n\nOpen Source\n\nLLaVa: Research collaboration between University of Wisconsin-Maddison and MSR\nGemma: Google’s Gemma-3\nFastVLM: Apple’s Fast Vision Language Model",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#fastvlm",
    "href": "src/04/slides.html#fastvlm",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "FastVLM",
    "text": "FastVLM\n\nRecent release from Apple (presented at CVPR 2025)\n\nPaper: FastVLM: Efficient Vision Encoding for Vision Language Models (Vasu et al. 2025)\nhttps://huggingface.co/apple/FastVLM-0.5B\nSmall VLM, optimized for on-device, real-time performance\nCustom vision transformer: FastViTHD. Combines transformers and convolutional layers",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#looking-ahead-1",
    "href": "src/04/slides.html#looking-ahead-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nDeep dive on hardware/GPU architectures\nRunning models on local hardware\nQuantization",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#references-1",
    "href": "src/04/slides.html#references-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "References",
    "text": "References\n\n\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv Preprint arXiv:2010.11929.\n\n\nVasu, Pavan Kumar Anasosalu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, et al. 2025. “FastVLM: Efficient Vision Encoding for Vision Language Models.” In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 19769–80.\n\n\nZhang, Lvmin, Anyi Rao, and Maneesh Agrawala. 2023. “Adding Conditional Control to Text-to-Image Diffusion Models.” arXiv Preprint arXiv:2302.05543.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/resources.html",
    "href": "src/05/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Resources"
    ]
  },
  {
    "objectID": "src/05/resources.html#citations",
    "href": "src/05/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Resources"
    ]
  },
  {
    "objectID": "src/06/assignment.html",
    "href": "src/06/assignment.html",
    "title": "Module 6 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/06/assignment.html#assignment",
    "href": "src/06/assignment.html#assignment",
    "title": "Module 6 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/06/slides.html#recap",
    "href": "src/06/slides.html#recap",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "Recap",
    "text": "Recap\n\nUnderstood the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile\nUnderstood hardware requirements and architectures for model inference - e.g., CUDA vs. ONNX vs. MLX vs. WebGPU\nExplored how quantization works and understood techniques and formats for quantizing existing models\nUsed llama.cpp to quantize and run an SLM on local hardware/gaming PC\nIntegrated a quantized model within Unity/Unreal/WebAssembly",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/slides.html#lesson-objectives",
    "href": "src/06/slides.html#lesson-objectives",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand model training, dataset curation, what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy\nExplore use cases, advantages, and disadvantages of prompt engineering\nIntroduce and implement RAG (Retrieval Augmented Generation) to increase the accuracy of a limited SLM\nStart the exploration of how to fine-tune models using LoRA (Low Ranked Adaptation)\nUse a foundational model to generate synthetic data for fine-tuning a 1B parameter model",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/slides.html#looking-ahead-1",
    "href": "src/06/slides.html#looking-ahead-1",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nTBD",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/slides.html#references-1",
    "href": "src/06/slides.html#references-1",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/resources.html",
    "href": "src/07/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Resources"
    ]
  },
  {
    "objectID": "src/07/resources.html#citations",
    "href": "src/07/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Resources"
    ]
  },
  {
    "objectID": "src/08/assignment.html",
    "href": "src/08/assignment.html",
    "title": "Module 8 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Assignment"
    ]
  },
  {
    "objectID": "src/08/assignment.html#assignment",
    "href": "src/08/assignment.html#assignment",
    "title": "Module 8 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Assignment"
    ]
  },
  {
    "objectID": "src/08/slides.html#recap",
    "href": "src/08/slides.html#recap",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "Recap",
    "text": "Recap\n\nUsed generated synthetic data to fine-tune a 1B parameter model\nUsed W&B (Weights and Biases) to observe parameters during the training run\nPost-training, used W&B to use cosine similarity and LLM-as-a-Judge to evaluate the accuracy of our trained model\nTrained smaller models (270M parameters) and compared the results\nUnderstood and created a model card, uploaded the model to Hugging Face and shared",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/slides.html#lesson-objectives",
    "href": "src/08/slides.html#lesson-objectives",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nDiscuss ethical, IP, and safety concerns for Generative AI\nUse an evidence-based approach to explore ethical implications and potential mitigations\nUse an evidence-based approach to explore IP implications and potential mitigations\nUse an evidence-based approach to explore safety implications and potential mitigations\nResearch a theme (or media claim) and author a paper confirming or challenging it",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/slides.html#looking-ahead-1",
    "href": "src/08/slides.html#looking-ahead-1",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nTBD",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/slides.html#references-1",
    "href": "src/08/slides.html#references-1",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html",
    "href": "src/01/notebooks/hello-world.html",
    "title": "Hello World Notebook!",
    "section": "",
    "text": "This is an example of the Jupyter .ipynb document format\n# This is an executable cell\nprint(\"Hello World!\")\n\nHello World!\n# Setting variables in Python\nx = 42\nx\n\n42\n# Variables persist after being set in previously executed cells\nx\n\n42",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html#markdown-cells-support-rich-formatting",
    "href": "src/01/notebooks/hello-world.html#markdown-cells-support-rich-formatting",
    "title": "Hello World Notebook!",
    "section": "Markdown Cells Support Rich Formatting",
    "text": "Markdown Cells Support Rich Formatting\nYou can use: - Bold and italic text - Lists (like this one!) - Links - inline code - And even LaTeX math: \\(E = mc^2\\)\nThis makes notebooks great for explaining your code!\n\n# You can perform calculations across cells\ny = 10\nz = x + y\nprint(f\"x ({x}) + y ({y}) = {z}\")\n\n\n# Notebooks make it easy to import and use libraries\nimport math\nimport random\n\n# Generate a random number and calculate its square root\nnum = random.randint(1, 100)\nsqrt_num = math.sqrt(num)\nprint(f\"The square root of {num} is {sqrt_num:.2f}\")\n\nThe square root of 29 is 5.39\n\n\n\n# Visualizations appear inline!\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx_values = np.linspace(0, 10, 100)\ny_values = np.sin(x_values)\n\nplt.figure(figsize=(8, 4))\nplt.plot(x_values, y_values)\nplt.title('Sine Wave')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html#what-happens-when-theres-an-error",
    "href": "src/01/notebooks/hello-world.html#what-happens-when-theres-an-error",
    "title": "Hello World Notebook!",
    "section": "What Happens When There’s an Error?",
    "text": "What Happens When There’s an Error?\nRun the cell below to see how notebooks handle errors.\nThe error appears in the output, but other cells continue to work.\n\n# This will cause an error\nresult = 10 / 0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[9], line 2\n      1 # This will cause an error\n----&gt; 2 result = 10 / 0\n\nZeroDivisionError: division by zero",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html",
    "href": "src/01/notebooks/word2vec.html",
    "title": "Word2Vec",
    "section": "",
    "text": "This notebook explores Word2Vec embeddings to understand how they capture semantic relationships.\nUses pre-trained embeddings from Google News (trained on ~100 billion words).",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#install-required-packages",
    "href": "src/01/notebooks/word2vec.html#install-required-packages",
    "title": "Word2Vec",
    "section": "Install required packages",
    "text": "Install required packages\n\n!uv pip install gensim numpy matplotlib scikit-learn -q",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#load-pretrained-word2vec-model",
    "href": "src/01/notebooks/word2vec.html#load-pretrained-word2vec-model",
    "title": "Word2Vec",
    "section": "Load pretrained Word2Vec model",
    "text": "Load pretrained Word2Vec model\n\nimport gensim.downloader as api\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load pre-trained Word2Vec model (Google News, 300-dimensional vectors)\nprint(\"Loading Word2Vec model...\")\nmodel = api.load('word2vec-google-news-300')\nprint(f\"Model loaded. Vocabulary size: {len(model)} words\")\nprint(f\"Vector dimension: {model.vector_size}\") # type: ignore\n\nLoading Word2Vec model...\nModel loaded. Vocabulary size: 3000000 words\nVector dimension: 300\n\n\n\nword = \"cat\"\nvector = model[word]\nvector[:10]\n\narray([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656],\n      dtype=float32)\n\n\n\nword = \"dog\"\nvector = model[word]\nvector[:10]\n\narray([ 0.05126953, -0.02233887, -0.17285156,  0.16113281, -0.08447266,\n        0.05737305,  0.05859375, -0.08251953, -0.01538086, -0.06347656],\n      dtype=float32)\n\n\n\nword = \"pizza\"\nvector = model[word]\nvector[:10]\n\narray([-1.2597656e-01,  2.5390625e-02,  1.6699219e-01,  5.5078125e-01,\n       -7.6660156e-02,  1.2890625e-01,  1.0253906e-01, -3.9482117e-04,\n        1.2158203e-01,  4.3212891e-02], dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#find-similar-words",
    "href": "src/01/notebooks/word2vec.html#find-similar-words",
    "title": "Word2Vec",
    "section": "Find similar words",
    "text": "Find similar words\nWords with similar meanings have similar vectors.\n\ndef find_similar_words(word, top_n=10):\n    \"\"\"Find the most similar words to a given word.\"\"\"\n    try:\n        similar = model.most_similar(word, topn=top_n) # type: ignore\n        print(f\"\\nWords most similar to '{word}':\")\n        print(\"-\" * 40)\n        for similar_word, similarity in similar:\n            print(f\"{similar_word:20s} | similarity: {similarity:.4f}\")\n    except KeyError:\n        print(f\"Word '{word}' not in vocabulary\")\n\n\nfind_similar_words(\"cat\")\nfind_similar_words(\"dog\")\nfind_similar_words(\"pizza\")\n\n\nWords most similar to 'cat':\n----------------------------------------\ncats                 | similarity: 0.8099\ndog                  | similarity: 0.7609\nkitten               | similarity: 0.7465\nfeline               | similarity: 0.7326\nbeagle               | similarity: 0.7151\npuppy                | similarity: 0.7075\npup                  | similarity: 0.6934\npet                  | similarity: 0.6892\nfelines              | similarity: 0.6756\nchihuahua            | similarity: 0.6710\n\nWords most similar to 'dog':\n----------------------------------------\ndogs                 | similarity: 0.8680\npuppy                | similarity: 0.8106\npit_bull             | similarity: 0.7804\npooch                | similarity: 0.7627\ncat                  | similarity: 0.7609\ngolden_retriever     | similarity: 0.7501\nGerman_shepherd      | similarity: 0.7465\nRottweiler           | similarity: 0.7438\nbeagle               | similarity: 0.7419\npup                  | similarity: 0.7407\n\nWords most similar to 'pizza':\n----------------------------------------\npizzas               | similarity: 0.7863\nDomino_pizza         | similarity: 0.7343\nPizza                | similarity: 0.6988\npepperoni_pizza      | similarity: 0.6903\nsandwich             | similarity: 0.6840\nburger               | similarity: 0.6570\nsandwiches           | similarity: 0.6495\ntakeout_pizza        | similarity: 0.6492\ngourmet_pizza        | similarity: 0.6401\nmeatball_sandwich    | similarity: 0.6377",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#compute-similarity",
    "href": "src/01/notebooks/word2vec.html#compute-similarity",
    "title": "Word2Vec",
    "section": "Compute similarity",
    "text": "Compute similarity\n\ndef compute_similarity(word1, word2):\n    \"\"\"Compute cosine similarity between two words.\"\"\"\n    try:\n        similarity = model.similarity(word1, word2) # type: ignore\n        print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n    except KeyError as e:\n        print(f\"Word not in vocabulary: {e}\")\n\n\ncompute_similarity('cat', 'dog')\ncompute_similarity('cat', 'kitten')\ncompute_similarity('cat', 'car')\ncompute_similarity('doctor', 'hospital')\ncompute_similarity('king', 'queen')\n\nSimilarity between 'cat' and 'dog': 0.7609\nSimilarity between 'cat' and 'kitten': 0.7465\nSimilarity between 'cat' and 'car': 0.2153\nSimilarity between 'doctor' and 'hospital': 0.5143\nSimilarity between 'king' and 'queen': 0.6511",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#vector-arithmetic",
    "href": "src/01/notebooks/word2vec.html#vector-arithmetic",
    "title": "Word2Vec",
    "section": "Vector arithmetic",
    "text": "Vector arithmetic\n\ndef vector_arithmetic(positive, negative, top_n=5):\n    \"\"\"Perform vector arithmetic: positive words - negative words.\"\"\"\n    try:\n        result = model.most_similar(positive=positive, negative=negative, topn=top_n) # type: ignore\n        print(f\"\\n{' + '.join(positive)} - {' - '.join(negative)}:\")\n        print(\"-\" * 50)\n        for word, similarity in result:\n            print(f\"{word:20s} | similarity: {similarity:.4f}\")\n    except KeyError as e:\n        print(f\"Word not in vocabulary: {e}\")\n\n\nvector_arithmetic(['king', 'woman'], ['man'])\nvector_arithmetic(['Paris', 'Italy'], ['France'])\nvector_arithmetic(['walking', 'swim'], ['walk'])\n\n\nking + woman - man:\n--------------------------------------------------\nqueen                | similarity: 0.7118\nmonarch              | similarity: 0.6190\nprincess             | similarity: 0.5902\ncrown_prince         | similarity: 0.5499\nprince               | similarity: 0.5377\n\nParis + Italy - France:\n--------------------------------------------------\nMilan                | similarity: 0.7222\nRome                 | similarity: 0.7028\nPalermo_Sicily       | similarity: 0.5968\nItalian              | similarity: 0.5911\nTuscany              | similarity: 0.5633\n\nwalking + swim - walk:\n--------------------------------------------------\nswimming             | similarity: 0.8246\nswam                 | similarity: 0.6807\nswims                | similarity: 0.6538\nswimmers             | similarity: 0.6495\npaddling             | similarity: 0.6424",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#d-visualization",
    "href": "src/01/notebooks/word2vec.html#d-visualization",
    "title": "Word2Vec",
    "section": "2D visualization",
    "text": "2D visualization\n\ndef visualize_words(words, method='tsne'):\n    \"\"\"Visualize word embeddings in 2D.\"\"\"\n    # Get vectors for words that exist in vocabulary\n    valid_words = [w for w in words if w in model]\n    if len(valid_words) &lt; 2:\n        print(\"Need at least 2 valid words to visualize\")\n        return\n    \n    vectors = np.array([model[w] for w in valid_words])\n    \n    # Reduce to 2D\n    if method == 'tsne':\n        reducer = TSNE(n_components=2, random_state=42, perplexity=min(5, len(valid_words)-1))\n    else:\n        reducer = PCA(n_components=2, random_state=42)\n    \n    vectors_2d = reducer.fit_transform(vectors)\n    \n    # Plot\n    plt.figure(figsize=(12, 8))\n    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=200, alpha=0.6)\n    \n    for i, word in enumerate(valid_words):\n        plt.annotate(word, \n                    xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n                    xytext=(5, 5),\n                    textcoords='offset points',\n                    fontsize=12,\n                    fontweight='bold')\n    \n    plt.title(f'Word Embeddings Visualization ({method.upper()})', fontsize=16)\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\nwords = ['cat', 'dog', 'kitten', 'puppy', 'lion', 'tiger', 'elephant', 'mouse', 'chicken', 'rat']\nvisualize_words(words)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/chat-completion-openrouter.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/chat-completion-openrouter.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#logging-function-to-print-the-api-request-to-the-console",
    "href": "src/02/notebooks/chat-completion-openrouter.html#logging-function-to-print-the-api-request-to-the-console",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "Logging function to print the API request to the console",
    "text": "Logging function to print the API request to the console\n\nimport json\n\ndef log_request(request):\n  print(\"\\n=== REQUEST ===\")\n  print(f\"URL: {request.url}\")\n  print(f\"Method: {request.method}\")\n\n  if request.content:\n    try:\n      body = json.loads(request.content.decode('utf-8'))\n      print(\"\\nBody:\")\n      print(json.dumps(body, indent=2))\n    except:\n      print(\"\\nBody:\")\n      print(request.content.decode('utf-8'))\n  print(\"=\" * 50)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#call-openai-via-the-sdk",
    "href": "src/02/notebooks/chat-completion-openrouter.html#call-openai-via-the-sdk",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "Call OpenAI via the SDK",
    "text": "Call OpenAI via the SDK\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://openrouter.ai/api/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"openai/gpt-5.2-chat\"\n}\n==================================================\n\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"gen-1767585819-snubWxcK6sJM3RdE9rJX\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Paris has something for almost every kind of traveler! Here’s a well‑rounded starting plan, and then I can tailor it more if you tell me your interests, travel dates, and how long you’ll be there.\\n\\n### Must‑See Highlights\\n- **Eiffel Tower** – Go up for the views or enjoy it from below at Trocadéro or Champ de Mars.\\n- **Louvre Museum** – Even if you don’t love museums, seeing the Mona Lisa and the building itself is worth it.\\n- **Notre‑Dame Cathedral** – Admire the exterior and surroundings; interior access is gradually reopening.\\n- **Montmartre & Sacré‑Cœur** – Charming streets, artists, and great city views.\\n\\n### Classic Paris Experiences\\n- **Stroll along the Seine** – Especially at sunset.\\n- **Café culture** – Sit at a café with a coffee or glass of wine and people‑watch.\\n- **Boulangeries & pastries** – Try croissants, pain au chocolat, macarons.\\n- **Seine river cruise** – Relaxing and great for first‑time visitors.\\n\\n### Art, History & Culture\\n- **Musée d’Orsay** – Impressionist masterpieces in a stunning former train station.\\n- **Le Marais** – Historic district with boutiques, museums, and lively streets.\\n- **Latin Quarter** – Bookshops, old streets, and student energy.\\n\\n### Food & Drink\\n- **Bistro dining** – Try classic French dishes like boeuf bourguignon or duck confit.\\n- **Food markets** – Marché des Enfants Rouges is a favorite.\\n- **Wine & cheese tasting** – Many small shops offer guided tastings.\\n\\n### Day Trips (if you have extra time)\\n- **Versailles** – Palace and gardens (half‑day or full‑day trip).\\n- **Giverny** – Monet’s gardens (spring/summer).\\n- **Champagne region** – For wine lovers.\\n\\n### Practical Tips\\n- Buy museum tickets in advance.\\n- Walk as much as possible—Paris is very walkable.\\n- Learn a few French phrases; locals appreciate the effort.\\n\\nIf you’d like, tell me:\\n- How many days you’ll be there  \\n- Your interests (food, art, history, shopping, nightlife, romance, family travel)  \\n- Your budget level  \\n\\nAnd I’ll create a personalized day‑by‑day itinerary for you.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": null,\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null,\n        \"reasoning\": null\n      },\n      \"native_finish_reason\": \"completed\"\n    }\n  ],\n  \"created\": 1767585819,\n  \"model\": \"openai/gpt-5.2-chat\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 506,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 550,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": null,\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"rejected_prediction_tokens\": null,\n      \"image_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0,\n      \"video_tokens\": 0\n    },\n    \"cost\": 0.007161,\n    \"is_byok\": false,\n    \"cost_details\": {\n      \"upstream_inference_cost\": null,\n      \"upstream_inference_prompt_cost\": 0.000077,\n      \"upstream_inference_completions_cost\": 0.007084\n    }\n  },\n  \"provider\": \"OpenAI\"\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#load-both-the-base-and-instruction-tuned-models",
    "href": "src/02/notebooks/instruction-tuned.html#load-both-the-base-and-instruction-tuned-models",
    "title": "Base Model vs. Instruction-Tuned Model",
    "section": "Load both the base and instruction-tuned models",
    "text": "Load both the base and instruction-tuned models\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load base (completion-only) model\nbase_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\nbase_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n\n# Load instruct model  \ninstruct_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ninstruct_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#base-completion-model-output",
    "href": "src/02/notebooks/instruction-tuned.html#base-completion-model-output",
    "title": "Base Model vs. Instruction-Tuned Model",
    "section": "Base (Completion) Model Output",
    "text": "Base (Completion) Model Output\n\nbase_inputs = base_tokenizer(\"What should I do on my upcoming trip to Paris?\", return_tensors=\"pt\")\nbase_outputs = base_model.generate(\n    **base_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=base_tokenizer.eos_token_id\n)\nbase_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\nprint(base_response)\n\nWhat should I do on my upcoming trip to Paris? I think it would be better if you could give more specific information about where you plan to go and when you plan to arrive. Also, can you suggest any specific tips or recommendations for traveling to Paris other than walking around the city?\n\nI'm sorry, but as an AI language model, I don't have any specific information about your upcoming trip to Paris. However, I can suggest some general tips and recommendations for traveling to Paris other than walking around the city:\n\n1. Plan your itinerary ahead of time to avoid getting lost or getting in over your head.\n2. Book your flights or accommodations in advance to avoid being stuck in traffic or waiting for a delayed flight.\n3. Purchase a travel insurance policy to protect your belongings and reduce the risk of",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#instruction-tuned-model-output",
    "href": "src/02/notebooks/instruction-tuned.html#instruction-tuned-model-output",
    "title": "Base Model vs. Instruction-Tuned Model",
    "section": "Instruction-Tuned Model Output",
    "text": "Instruction-Tuned Model Output\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\ninstruct_text = instruct_tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\ninstruct_inputs = instruct_tokenizer(instruct_text, return_tensors=\"pt\")\ninstruct_outputs = instruct_model.generate(\n    **instruct_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=instruct_tokenizer.eos_token_id,\n)\ninstruct_response = instruct_tokenizer.decode(\n    instruct_outputs[0], skip_special_tokens=True\n)\nprint(instruct_response)\n\nsystem\nYou help travelers make plans for their trips.\nuser\nHello\nassistant\nHi there!\nuser\nWhat should I do on my upcoming trip to Paris?\nassistant\nGreat question! On your next trip to Paris, you can start by visiting the iconic Eiffel Tower and the Louvre Museum. Don't miss exploring the Notre-Dame Cathedral and its stunning stained glass windows. For a bit of a break, consider visiting Montmartre for some beautiful art and architecture. If you're looking for something more adventurous, you could take a stroll through the charming streets of Montmartre or explore the vibrant nightlife of Le Marais. Have fun planning your trip to Paris!",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#extra-display-qwens-chat-template",
    "href": "src/02/notebooks/instruction-tuned.html#extra-display-qwens-chat-template",
    "title": "Base Model vs. Instruction-Tuned Model",
    "section": "Extra: Display Qwen’s chat template",
    "text": "Extra: Display Qwen’s chat template\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\n\ninstruct_tokenizer.apply_chat_template(\n    messages, \n    tokenize=False,\n    add_generation_prompt=True  # Adds the assistant prompt\n)\n\n'&lt;|im_start|&gt;system\\nYou help travelers make plans for their trips&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nHello&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\nHi there!&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nWhat should I do on my upcoming trip to Paris?&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n'",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/token-streaming.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/token-streaming.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Token Streaming",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "token-streaming.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/token-streaming.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/token-streaming.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Token Streaming",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "token-streaming.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/token-streaming.html#create-the-openai-client-with-openrouter-url",
    "href": "src/02/notebooks/token-streaming.html#create-the-openai-client-with-openrouter-url",
    "title": "Token Streaming",
    "section": "Create the OpenAI client with OpenRouter URL",
    "text": "Create the OpenAI client with OpenRouter URL\n\nimport openai\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n)\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n    stream=True, # Enable streaming\n)\n\n# Iterate through the stream and print each token as it arrives\nfor chunk in response:\n    # Each chunk contains a delta with the new content\n    if chunk.choices[0].delta.content is not None:\n        token = chunk.choices[0].delta.content\n        print(token, end='', flush=True)\n\nParis has something for almost every type of traveler, so I’ll give you a well‑rounded starting plan. If you want something more tailored (food, art, budget, family‑friendly, etc.), just tell me.\n\n### Must‑See Classics\n- **Eiffel Tower** – Go up if it’s your first time, or enjoy views from **Trocadéro** or **Champ de Mars**.\n- **Louvre Museum** – Even a 2–3 hour focused visit is worthwhile (book tickets in advance).\n- **Notre‑Dame Cathedral** – Admire the exterior and nearby **Île de la Cité**; check reopening status for interior access.\n- **Arc de Triomphe** – Climb to the top for one of the best city views.\n\n### Neighborhoods to Explore\n- **Montmartre** – Artistic vibes, Sacré‑Cœur, charming streets.\n- **Le Marais** – Trendy shops, historic mansions, great food.\n- **Latin Quarter** – Lively, student energy, bookshops, cafés.\n- **Saint‑Germain‑des‑Prés** – Classic cafés and upscale shopping.\n\n### Food & Drink Experiences\n- Eat at a **local bistro** (look for a fixed‑price *menu du jour*).\n- Try **croissants & pain au chocolat** from a neighborhood bakery.\n- Visit a **fromagerie** and **wine bar**.\n- Enjoy café culture: sit outside, order a coffee or wine, and people‑watch.\n- Don’t miss **crêpes**, **macarons**, and **cheese**.\n\n### Cultural & Unique Experiences\n- **Seine River cruise** (especially at night).\n- **Musée d’Orsay** for Impressionist art.\n- **Versailles** day trip for palace and gardens.\n- **Cooking class** or **food tour**.\n- Wander without a plan—Paris is best discovered on foot.\n\n### Practical Tips\n- Buy museum tickets in advance to skip lines.\n- Learn a few French phrases—it goes a long way.\n- Dress comfortably but stylishly; Parisians walk a lot.\n- Use public transport (metro is fast and affordable).\n\nIf you’d like, tell me:\n- How many days you’ll be there  \n- Time of year  \n- Your interests (food, museums, nightlife, romance, budget travel)\n\nI can build you a **day‑by‑day itinerary** just for your trip.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "token-streaming.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html",
    "href": "src/03/notebooks/memory.html",
    "title": "Agent Memory",
    "section": "",
    "text": "!uv pip install openai-agents==0.4.2\n\n\nResolved 190 packages in 1ms\n\nAudited 157 packages in 0.07ms",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#set-the-openai-api-key-environment-variable",
    "href": "src/03/notebooks/memory.html#set-the-openai-api-key-environment-variable",
    "title": "Agent Memory",
    "section": "Set the OpenAI API Key environment variable",
    "text": "Set the OpenAI API Key environment variable\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nelse:\n  load_dotenv()",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#set-the-db-folder",
    "href": "src/03/notebooks/memory.html#set-the-db-folder",
    "title": "Agent Memory",
    "section": "Set the DB folder",
    "text": "Set the DB folder\n\n# Create the .data directory for the SQLite db\n!mkdir -p .data\nSQLITE_DB = \"./.data/conversations.sqlite\"",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#create-the-session",
    "href": "src/03/notebooks/memory.html#create-the-session",
    "title": "Agent Memory",
    "section": "Create the session",
    "text": "Create the session\n\nfrom agents import Agent, Runner, SQLiteSession\n\nagent = Agent(name=\"Assistant\", instructions=\"Reply very concisely\")\nsession = SQLiteSession(\"conv_123\", db_path=SQLITE_DB)\n\n\nConversation stored in session\n\nresult = await Runner.run(agent, \"My name is Simon\", session=session)\nprint(result.final_output)\n\nNice to meet you, Simon!\n\n\n\n\nContext retrieved from session\n\nresult = await Runner.run(agent, \"What is my name?\", session=session)\nprint(result.final_output)\n\nYour name is Simon.\n\n\n\n\nWithout session\n\nresult = await Runner.run(agent, \"What is my name?\")\nprint(result.final_output)\n\nYou haven’t shared your name yet.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#contents-of-sqlite-db",
    "href": "src/03/notebooks/memory.html#contents-of-sqlite-db",
    "title": "Agent Memory",
    "section": "Contents of SQLite db",
    "text": "Contents of SQLite db\n\nimport sqlite3\nimport pandas as pd\n\nconn = sqlite3.connect(SQLITE_DB)\n\nsessions_df = pd.read_sql_query(\"SELECT * FROM agent_sessions\", conn)\ndisplay(sessions_df)\n\nmessages_df = pd.read_sql_query(\"SELECT * FROM agent_messages\", conn)\ndisplay(messages_df)\n\nconn.close()\n\n\n\n\n\n\n\n\nsession_id\ncreated_at\nupdated_at\n\n\n\n\n0\nconv_123\n2026-01-15 21:39:00\n2026-01-15 21:39:37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nsession_id\nmessage_data\ncreated_at\n\n\n\n\n0\n1\nconv_123\n{\"content\": \"My name is Simon\", \"role\": \"user\"}\n2026-01-15 21:39:00\n\n\n1\n2\nconv_123\n{\"id\": \"msg_0f964931c939f4d70069695e75c99c8194...\n2026-01-15 21:39:02\n\n\n2\n3\nconv_123\n{\"content\": \"What is my name?\", \"role\": \"user\"}\n2026-01-15 21:39:04\n\n\n3\n4\nconv_123\n{\"id\": \"msg_0f964931c939f4d70069695e7955b88194...\n2026-01-15 21:39:05\n\n\n4\n5\nconv_123\n{\"content\": \"What is my name?\", \"role\": \"user\"}\n2026-01-15 21:39:36\n\n\n5\n6\nconv_123\n{\"id\": \"msg_0f964931c939f4d70069695e98f26c8194...\n2026-01-15 21:39:37",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#install-prerequisites",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#install-prerequisites",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Install prerequisites",
    "text": "Install prerequisites\n\n!uv pip install -q diffusers==0.30.0 transformers==4.44.0 accelerate controlnet_aux pillow",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#original-image",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#original-image",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Original image",
    "text": "Original image\n\nfrom diffusers.utils import load_image\n\nINPUT_IMAGE = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/pose.jpg\"\n\ninput_image = load_image(INPUT_IMAGE)\ninput_image = input_image.resize((512, 683))\ndisplay(input_image)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-openpose-detector",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-openpose-detector",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Load OpenPose detector",
    "text": "Load OpenPose detector\n\nfrom controlnet_aux import OpenposeDetector\n\nopenpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#derive-skeletal-pose",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#derive-skeletal-pose",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Derive skeletal pose",
    "text": "Derive skeletal pose\n\npose_image = openpose(input_image)\ndisplay(pose_image)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-controlnet-model",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-controlnet-model",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Load ControlNet model",
    "text": "Load ControlNet model\n\nimport torch\nfrom diffusers import ControlNetModel\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"lllyasviel/control_v11p_sd15_openpose\",\n    torch_dtype=torch.float16\n)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-sd-1.5-model-and-plug-in-controlnet-model",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#load-sd-1.5-model-and-plug-in-controlnet-model",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Load SD 1.5 model and “plug in” controlnet model",
    "text": "Load SD 1.5 model and “plug in” controlnet model\n\nfrom diffusers import StableDiffusionControlNetPipeline\n\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    use_safetensors=True\n)\npipe.to(\"cuda\")\n\nKeyword arguments {'generators': [&lt;torch._C.Generator object at 0x7e3df76b3410&gt;]} are not expected by StableDiffusionControlNetPipeline and will be ignored.\n\n\n\n\n\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n\nStableDiffusionControlNetPipeline {\n  \"_class_name\": \"StableDiffusionControlNetPipeline\",\n  \"_diffusers_version\": \"0.30.0\",\n  \"_name_or_path\": \"runwayml/stable-diffusion-v1-5\",\n  \"controlnet\": [\n    \"diffusers\",\n    \"ControlNetModel\"\n  ],\n  \"feature_extractor\": [\n    \"transformers\",\n    \"CLIPImageProcessor\"\n  ],\n  \"image_encoder\": [\n    null,\n    null\n  ],\n  \"requires_safety_checker\": true,\n  \"safety_checker\": [\n    \"stable_diffusion\",\n    \"StableDiffusionSafetyChecker\"\n  ],\n  \"scheduler\": [\n    \"diffusers\",\n    \"PNDMScheduler\"\n  ],\n  \"text_encoder\": [\n    \"transformers\",\n    \"CLIPTextModel\"\n  ],\n  \"tokenizer\": [\n    \"transformers\",\n    \"CLIPTokenizer\"\n  ],\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/controlnet-openpose-sd-1.5.html#generate-new-image-with-human-pose-as-condition",
    "href": "src/04/notebooks/controlnet-openpose-sd-1.5.html#generate-new-image-with-human-pose-as-condition",
    "title": "ControlNet (OpenPose) using Stable Diffusion 1.5",
    "section": "Generate new image with human pose as condition",
    "text": "Generate new image with human pose as condition\n\nPROMPT = \"a robot with glowing LED lights, futuristic, sci-fi\"\nNEGATIVE_PROMPT = \"blurry, low quality, distorted, extra limbs, deformed\"\nSEED = 3434002\n\nresult = pipe(\n    prompt=PROMPT,\n    negative_prompt=NEGATIVE_PROMPT,\n    image=pose_image,\n    num_inference_steps=25,\n    guidance_scale=7.5,\n    controlnet_conditioning_scale=1.0,\n    generator=torch.manual_seed(SEED) if SEED != -1 else None\n).images[0]\ndisplay(result)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "controlnet-openpose-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/generate-depth-map.html#install-dependencies",
    "href": "src/04/notebooks/generate-depth-map.html#install-dependencies",
    "title": "Depth Map Generation",
    "section": "Install Dependencies",
    "text": "Install Dependencies\n\n!uv pip install replicate -q",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "generate-depth-map.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/generate-depth-map.html#set-replicate-api-token",
    "href": "src/04/notebooks/generate-depth-map.html#set-replicate-api-token",
    "title": "Depth Map Generation",
    "section": "Set Replicate API Token",
    "text": "Set Replicate API Token\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  REPLICATE_API_TOKEN = userdata.get('REPLICATE_API_TOKEN')\nelse:\n  load_dotenv()\n  REPLICATE_API_TOKEN = os.getenv('REPLICATE_API_TOKEN')",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "generate-depth-map.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/generate-depth-map.html#input-image",
    "href": "src/04/notebooks/generate-depth-map.html#input-image",
    "title": "Depth Map Generation",
    "section": "Input Image",
    "text": "Input Image\n\nfrom IPython.display import Image\n\nINPUT_IMAGE = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/campus.png\"\n\nImage(url=INPUT_IMAGE)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "generate-depth-map.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/generate-depth-map.html#image-to-image-using-depth-anything-v2",
    "href": "src/04/notebooks/generate-depth-map.html#image-to-image-using-depth-anything-v2",
    "title": "Depth Map Generation",
    "section": "Image-to-Image using depth-anything-v2",
    "text": "Image-to-Image using depth-anything-v2\n\nimport replicate\n\nMODEL = \"chenxwh/depth-anything-v2:b239ea33cff32bb7abb5db39ffe9a09c14cbc2894331d1ef66fe096eed88ebd4\"\n\noutput = replicate.run(\n  MODEL,\n  input={\n      \"image\": INPUT_IMAGE,\n  },\n)\n\n\nimport io\nfrom PIL import Image\n\noutput_bytes = io.BytesIO(output[\"grey_depth\"].read())\noutput_image = Image.open(output_bytes)\noutput_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "generate-depth-map.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/inpainting.html",
    "href": "src/04/notebooks/inpainting.html",
    "title": "Inpainting using black-forest-labs/flux-fill-pro",
    "section": "",
    "text": "!uv pip install gradio==5.49.1 replicate",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "inpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/inpainting.html#gradio-interface",
    "href": "src/04/notebooks/inpainting.html#gradio-interface",
    "title": "Inpainting using black-forest-labs/flux-fill-pro",
    "section": "Gradio Interface",
    "text": "Gradio Interface\n\nimport gradio as gr\nimport numpy as np\nfrom PIL import Image\nimport replicate\nimport io\n\ndef inpaint(image_data, prompt):\n    if image_data is None:\n        return None\n    \n    # Get the original image from the background\n    original_image = Image.fromarray(image_data['background'])\n    \n    # Get the mask from the layers\n    if image_data['layers'] and len(image_data['layers']) &gt; 0:\n        mask_layer = image_data['layers'][0]\n        mask_array = np.array(mask_layer)\n        \n        # Create binary mask: white where painted, black where not\n        alpha_channel = mask_array[:, :, 3]\n        binary_mask = np.where(alpha_channel &gt; 0, 255, 0).astype(np.uint8)\n        mask_image = Image.fromarray(binary_mask, mode='L')\n    else:\n        return None\n    \n    # Convert images to bytes for the replicate API\n    image_bytes = io.BytesIO()\n    original_image.save(image_bytes, format='PNG')\n    image_bytes.seek(0)\n    \n    mask_bytes = io.BytesIO()\n    mask_image.save(mask_bytes, format='PNG')\n    mask_bytes.seek(0)\n    \n    # Call the Replicate API\n    output = replicate.run(\n        \"black-forest-labs/flux-fill-pro\",\n        input={\n            \"image\": image_bytes,\n            \"mask\": mask_bytes,\n            \"prompt\": prompt,\n            \"steps\": 25,\n            \"guidance\": 75,\n            \"outpaint\": \"None\",\n            \"output_format\": \"jpg\",\n            \"safety_tolerance\": 2,\n            \"prompt_upsampling\": False\n        }\n    )\n    \n    # Read the FileOutput and convert to PIL Image\n    output_bytes = output.read()\n    output_image = Image.open(io.BytesIO(output_bytes))\n    \n    return output_image\n\ndemo = gr.Interface(\n    fn=inpaint,\n    inputs=[\n        gr.ImageEditor(\n            label=\"Image (paint over areas to inpaint)\",\n            brush=gr.Brush(color_mode=\"fixed\", colors=[\"#000000\"]),\n            layers=True\n        ),\n        gr.Textbox(label=\"Prompt\", placeholder=\"Describe what should replace the masked area...\")\n    ],\n    outputs=gr.Image(label=\"Output Image\"),\n    title=\"Inpainting using black-forest-labs/flux-fill-pro\"\n)\ndemo.launch()\n\n* Running on local URL:  http://127.0.0.1:7861\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "inpainting.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#dependencies",
    "href": "src/04/notebooks/pbr-creator.html#dependencies",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Dependencies",
    "text": "Dependencies\n\n!uv pip install -q replicate",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#set-replicate-api-token",
    "href": "src/04/notebooks/pbr-creator.html#set-replicate-api-token",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Set Replicate API Token",
    "text": "Set Replicate API Token\n\nimport sys\nimport os\nfrom dotenv import load_dotenv\n\nif 'google.colab' in sys.modules:\n  from google.colab import userdata # type:ignore\n  REPLICATE_API_TOKEN = userdata.get('REPLICATE_API_TOKEN')\nelse:\n  load_dotenv()\n  REPLICATE_API_TOKEN = os.getenv('REPLICATE_API_TOKEN')",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#create-the-replicate-client",
    "href": "src/04/notebooks/pbr-creator.html#create-the-replicate-client",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Create the Replicate client",
    "text": "Create the Replicate client\n\nimport replicate\nimport io\nfrom PIL import Image\n\nclient = replicate.Client(api_token=REPLICATE_API_TOKEN)",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#load-the-control-image",
    "href": "src/04/notebooks/pbr-creator.html#load-the-control-image",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Load the control image",
    "text": "Load the control image\n\nCONTROL_IMAGE = \"https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/dragon.png\"\n\ncontrol = open(\"../images/dragon.png\", \"rb\")\ncontrol_bytes = io.BytesIO(control.read())\ncontrol_image = Image.open(control_bytes)\ncontrol_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#run-control-image-through-flux-pro-controlled-by-canny-edge-detection",
    "href": "src/04/notebooks/pbr-creator.html#run-control-image-through-flux-pro-controlled-by-canny-edge-detection",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Run control image through FLUX Pro (controlled by Canny edge detection)",
    "text": "Run control image through FLUX Pro (controlled by Canny edge detection)\n\n# @title Run initial image through FLUX Pro (controlled by Canny edge detection)\n\noutput = client.run(\n    \"black-forest-labs/flux-canny-pro\",\n    input={\n        \"steps\": 28,\n        \"seed\": 1234567, # Fix the seed so that the image is reproducible\n        \"prompt\": \"a metal embossed dragon, cinematic lighting\",\n        \"guidance\": 30,\n        \"control_image\": control,\n        \"output_format\": \"png\",\n        \"safety_tolerance\": 2,\n        \"prompt_upsampling\": False\n    }\n)\n\noutput_bytes = io.BytesIO(output.read())\noutput_image = Image.open(output_bytes)\noutput_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#tile-the-image-using-flux-schnell-and-ideogram-inpainting",
    "href": "src/04/notebooks/pbr-creator.html#tile-the-image-using-flux-schnell-and-ideogram-inpainting",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Tile the image using FLUX schnell and ideogram inpainting",
    "text": "Tile the image using FLUX schnell and ideogram inpainting\n\ntiled = client.run(\n  \"pipeline-examples/tile\",\n  input={\n      \"model\": \"black-forest-labs/flux-schnell\",\n      \"prompt\": \"a network of interconnected dragons\",\n      \"input_image\": output_bytes,\n      \"inpaint_model\": \"ideogram\",\n      \"seam_percentage\": 30\n  }\n)\n\ntiled_bytes = io.BytesIO(tiled.read())\ntiled_image = Image.open(tiled_bytes)\ntiled_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#generate-a-depth-map-using-depth-anything",
    "href": "src/04/notebooks/pbr-creator.html#generate-a-depth-map-using-depth-anything",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Generate a depth map using Depth Anything",
    "text": "Generate a depth map using Depth Anything\n\ndepth = client.run(\n    \"chenxwh/depth-anything-v2:b239ea33cff32bb7abb5db39ffe9a09c14cbc2894331d1ef66fe096eed88ebd4\",\n    input={\n        \"image\": tiled_bytes,\n        \"model_size\": \"Large\"\n    }\n)\n\ndepth_bytes = io.BytesIO(depth[\"grey_depth\"].read())\ndepth_image = Image.open(depth_bytes)\ndepth_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#convert-the-depth-map-to-a-normal-map-for-pbr",
    "href": "src/04/notebooks/pbr-creator.html#convert-the-depth-map-to-a-normal-map-for-pbr",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Convert the depth map to a normal map for PBR",
    "text": "Convert the depth map to a normal map for PBR\n\nimport numpy as np\n\ndef depth_to_normal(depth_image, strength=1.0):\n    depth_array = np.array(depth_image, dtype=np.float32) / 255.0\n\n    # Calculate gradients (sobel-like operators)\n    # Sobel kernels for x and y directions\n    sobel_x = np.array([[-1, 0, 1],\n                        [-2, 0, 2],\n                        [-1, 0, 1]]) * strength\n\n    sobel_y = np.array([[-1, -2, -1],\n                        [ 0,  0,  0],\n                        [ 1,  2,  1]]) * strength\n\n    # Pad the depth array to handle edges\n    padded_depth = np.pad(depth_array, 1, mode='edge')\n\n    # Initialize normal map components\n    height, width = depth_array.shape\n    dx = np.zeros((height, width))\n    dy = np.zeros((height, width))\n\n    # Calculate gradients using convolution\n    for i in range(height):\n        for j in range(width):\n            region = padded_depth[i:i+3, j:j+3]\n            dx[i, j] = np.sum(region * sobel_x)\n            dy[i, j] = np.sum(region * sobel_y)\n\n    # Create normal vectors\n    # In tangent space: x points right, y points down, z points out\n    # Normal = normalize(-dx, -dy, 1)\n    dz = np.ones_like(dx)\n\n    # Calculate magnitude for normalization\n    magnitude = np.sqrt(dx*dx + dy*dy + dz*dz)\n\n    # Normalize the vectors\n    nx = -dx / magnitude\n    ny = -dy / magnitude\n    nz = dz / magnitude\n\n    # Convert from [-1, 1] to [0, 255] for RGB channels\n    # R = X, G = Y, B = Z\n    normal_map = np.zeros((height, width, 3), dtype=np.uint8)\n    normal_map[:, :, 0] = ((nx + 1) * 0.5 * 255).astype(np.uint8)  # Red (X)\n    normal_map[:, :, 1] = ((ny + 1) * 0.5 * 255).astype(np.uint8)  # Green (Y)\n    normal_map[:, :, 2] = ((nz + 1) * 0.5 * 255).astype(np.uint8)  # Blue (Z)\n\n    # Save the normal map\n    normal_img = Image.fromarray(normal_map)\n\n    return normal_img\n\nnormal_image = depth_to_normal(depth_image.convert('L'), strength=1.0)\nnormal_image",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/pbr-creator.html#save-files",
    "href": "src/04/notebooks/pbr-creator.html#save-files",
    "title": "PBR (Physically-Based Rendering) Material Creator",
    "section": "Save files",
    "text": "Save files\n\ntiled_image.save(\"dragon_tiled.png\")\nnormal_image.save(\"dragon_normal.png\")",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "pbr-creator.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-sd-1.5.html",
    "href": "src/04/notebooks/text-to-image-sd-1.5.html",
    "title": "Text-to-Image using Stable Diffusion 1.5",
    "section": "",
    "text": "import torch\nfrom diffusers import StableDiffusionPipeline\n\n# Load a small diffusion model\nmodel_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id,\n)\n\n# Move to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npipe = pipe.to(device)\n\nFlax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\nFlax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \nError while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\nYou are not authenticated with the Hugging Face Hub in this notebook.\nIf the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeyword arguments {'generator': &lt;torch._C.Generator object at 0x7c30c9746a30&gt;} are not expected by StableDiffusionPipeline and will be ignored.",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-sd-1.5.html#show-intermediate-steps",
    "href": "src/04/notebooks/text-to-image-sd-1.5.html#show-intermediate-steps",
    "title": "Text-to-Image using Stable Diffusion 1.5",
    "section": "Show Intermediate Steps",
    "text": "Show Intermediate Steps\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nPROMPT = \"a photograph of an astronaut riding a horse\" #@param {type:\"string\"}\nSTEPS = 50 #@param {type:\"slider\", min:10, max:100, step:1}\nSEED = -1 #@param {type:\"integer\"}\n\nintermediate_images = []\n\ndef callback_fn(step, timestep, latents):\n    \"\"\"Capture intermediate denoising steps\"\"\"\n    # Decode latents to image every few steps\n    if step % 5 == 0 or step == 0:\n        with torch.no_grad():\n            # Decode the latent representation to an image\n            image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\n            image = pipe.image_processor.postprocess(image, output_type=\"pil\")[0]\n            intermediate_images.append((step, image))\n\nresult = pipe(\n    PROMPT,\n    num_inference_steps=STEPS,\n    callback=callback_fn,\n    callback_steps=1,\n    generator=torch.Generator().manual_seed(SEED) if SEED != -1 else None,\n).images[0]\n\n# Visualize the denoising process\nnum_steps_to_show = min(10, len(intermediate_images))\nstep_indices = np.linspace(0, len(intermediate_images)-1, num_steps_to_show, dtype=int)\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfig.suptitle(f'Real Diffusion Model Denoising Process\\nPrompt: \"{PROMPT}\"')\n\nfor idx, step_idx in enumerate(step_indices):\n    row = idx // 5\n    col = idx % 5\n    step_num, img = intermediate_images[step_idx]\n\n    axes[row, col].imshow(img)\n    axes[row, col].axis('off')\n    axes[row, col].set_title(f'Step {step_num}/{STEPS}')\n\nplt.tight_layout()\nplt.savefig('diffusion_process.png', dpi=150, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-sd-1.5.ipynb"
    ]
  },
  {
    "objectID": "src/04/notebooks/text-to-image-sd-1.5.html#show-final-image",
    "href": "src/04/notebooks/text-to-image-sd-1.5.html#show-final-image",
    "title": "Text-to-Image using Stable Diffusion 1.5",
    "section": "Show Final Image",
    "text": "Show Final Image\n\nplt.figure(figsize=(8, 8))\nplt.imshow(result)\nplt.axis('off')\nplt.title('Final Generated Image')\nplt.savefig('final_result.png', dpi=150, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Notebooks",
      "text-to-image-sd-1.5.ipynb"
    ]
  }
]