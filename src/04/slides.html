<!DOCTYPE html>
<html lang="en"><head>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.26">

  <title>CS-394 – Module 4: Multimedia and Multimodal Models</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #24292e;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #24292e; } /* Normal */
    code span.al { color: #ff5555; font-weight: bold; } /* Alert */
    code span.an { color: #6a737d; } /* Annotation */
    code span.at { color: #d73a49; } /* Attribute */
    code span.bn { color: #005cc5; } /* BaseN */
    code span.bu { color: #d73a49; } /* BuiltIn */
    code span.cf { color: #d73a49; } /* ControlFlow */
    code span.ch { color: #032f62; } /* Char */
    code span.cn { color: #005cc5; } /* Constant */
    code span.co { color: #6a737d; } /* Comment */
    code span.cv { color: #6a737d; } /* CommentVar */
    code span.do { color: #6a737d; } /* Documentation */
    code span.dt { color: #d73a49; } /* DataType */
    code span.dv { color: #005cc5; } /* DecVal */
    code span.er { color: #ff5555; text-decoration: underline; } /* Error */
    code span.ex { color: #d73a49; font-weight: bold; } /* Extension */
    code span.fl { color: #005cc5; } /* Float */
    code span.fu { color: #6f42c1; } /* Function */
    code span.im { color: #032f62; } /* Import */
    code span.in { color: #6a737d; } /* Information */
    code span.kw { color: #d73a49; } /* Keyword */
    code span.op { color: #24292e; } /* Operator */
    code span.ot { color: #6f42c1; } /* Other */
    code span.pp { color: #d73a49; } /* Preprocessor */
    code span.re { color: #6a737d; } /* RegionMarker */
    code span.sc { color: #005cc5; } /* SpecialChar */
    code span.ss { color: #032f62; } /* SpecialString */
    code span.st { color: #032f62; } /* String */
    code span.va { color: #e36209; } /* Variable */
    code span.vs { color: #032f62; } /* VerbatimString */
    code span.wa { color: #ff5555; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-eeafb82a00776dbd0312b01cd21cfa25.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <meta name="mermaid-theme" content="neutral">
  <script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
  <script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
  <link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
  <script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>
  
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Module 4: Multimedia and Multimodal Models</h1>

<div class="quarto-title-authors">
</div>

</section>
<section id="recap" class="slide level2">
<h2>Recap</h2>
<ul>
<li class="fragment">Described the fundamental concepts behind Agents/Agentic AI</li>
<li class="fragment">Explored and provided feedback on an existing multi-agent setup</li>
<li class="fragment">Understood available agent SDKs, how they differ, and advantages/disadvantages</li>
<li class="fragment">Used the OpenAI Agents SDK to build a multi-agent system from scratch, including document indexing and retrieval</li>
<li class="fragment">Understood and implemented tool calls using OpenAI’s function calling and via MCP</li>
</ul>
</section>
<section id="lesson-objectives" class="slide level2">
<h2>Lesson Objectives</h2>
<ul>
<li class="fragment">Understand the fundamentals and history of diffuser models</li>
<li class="fragment">Explore and use models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet</li>
<li class="fragment">Setup and use Replicate to create a custom pipeline of production-grade models</li>
<li class="fragment">Understand the fundamentals and history of Vision Encoders and VLMs</li>
<li class="fragment">Implement/test a local VLM model for on-device inference</li>
</ul>
</section>
<section>
<section id="multimedia-vs.-multimodal" class="title-slide slide level1 center">
<h1>Multimedia vs.&nbsp;Multimodal</h1>

</section>
<section id="multimedia-vs.-multimodal-1" class="slide level2">
<h2>Multimedia vs.&nbsp;Multimodal</h2>
<ul>
<li class="fragment"><strong>Multimedia</strong> models
<ul>
<li class="fragment">Single input/output models for images, video, audio, etc.</li>
<li class="fragment">Also known as computer vision, audio models</li>
</ul></li>
<li class="fragment">Examples
<ul>
<li class="fragment"><strong>Text-to-Image</strong> (generate an image from a text prompt)</li>
<li class="fragment"><strong>Image-to-Image</strong> (generate an image from an existing image)</li>
<li class="fragment"><strong>Image-to-3D</strong> (generate a 3D object from an image)</li>
</ul></li>
</ul>
</section>
<section id="multimedia-vs.-multimodal-2" class="slide level2">
<h2>Multimedia vs.&nbsp;Multimodal</h2>
<ul>
<li class="fragment"><strong>Multimodal</strong> models
<ul>
<li class="fragment">Process multiple datatypes such as text, images, and audio</li>
<li class="fragment">Also known as VLMs (Vision-Language Models) or ALMs (Audio-Language Models)</li>
</ul></li>
<li class="fragment">Examples
<ul>
<li class="fragment"><strong>Image-Text-to-Text</strong> (ask a question about this image)</li>
<li class="fragment"><strong>Image-Text-to-Image</strong> (decompose this image into multiple layers)</li>
<li class="fragment"><strong>Audio-Text-to-Text</strong> (what is this sound?)</li>
</ul></li>
</ul>
</section>
<section id="text-to-image" class="slide level2">
<h2>Text-to-Image</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>“A photograph of an astronaut riding a horse.”</p>
<ul>
<li class="fragment">Based on a concept called a diffusion transformer</li>
<li class="fragment">Commonly known as a <strong>diffuser</strong></li>
<li class="fragment">Two stage process, inspired by thermodynamics</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="./images/astronaut.png"></p>
</div></div>
</section>
<section id="introducing-the-diffuser" class="slide level2">
<h2>Introducing the Diffuser</h2>
<ul>
<li class="fragment">Training
<ul>
<li class="fragment">During training, random noise is added to images in steps</li>
<li class="fragment">Model learns to predict what noise was added (forward diffusion process)</li>
</ul></li>
<li class="fragment">Inference (process runs in reverse)
<ul>
<li class="fragment">Start with pure random noise</li>
<li class="fragment">Model estimates what noise should be removed to create a realistic image</li>
<li class="fragment">Using the text prompt, the model steers the process towards images that match the description</li>
</ul></li>
</ul>
</section>
<section id="image-diffusion-models-in-2022" class="slide level2">
<h2>Image Diffusion Models in 2022</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
  August 2022 : Stable Diffusion v1.4
              : First open-source high-quality model
  September 2022 : Stable Diffusion v1.5
                  : Refined version
  October 2022 : eDiff-I (NVIDIA)
                : Ensemble approach
  November 2022 : Stable Diffusion v2.0/2.1
                : Higher resolution (768x768)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="image-diffusion-models-in-2023" class="slide level2">
<h2>Image Diffusion Models in 2023</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
  March 2023 : Midjourney v5
              : Exceptional artistic quality
  April 2023 : ControlNet
        : Precise spatial control
        : AnimateDiff - Video generation
  July 2023 : SDXL (Stable Diffusion XL)
            : 1024x1024 native resolution
  August 2023 : SDXL Turbo
              : Real-time capable generation
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="image-diffusion-models-in-2024" class="slide level2">
<h2>Image Diffusion Models in 2024</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
  February 2024 : Stable Diffusion 3
                : Improved text understanding
  June 2024 : Stable Diffusion 3.5
            : Multiple model sizes
  2024 : FLUX.1 (Black Forest Labs)
        : State-of-the-art open model
        : Imagen 3 (Google DeepMind)
        : Photorealistic quality
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="image-diffusion-models-in-2025" class="slide level2">
<h2>Image Diffusion Models in 2025</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
  May 2025 : Imagen 4 (Google DeepMind)
           : Improved text rendering, 2K resolution
  August 2025 : Nano Banana (Google)
              : Autoregressive model in Gemini 2.5 Flash
  November 2025 : FLUX.2 (Black Forest Labs)
                : 32B parameters, multi-image references
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section></section>
<section>
<section id="text-to-image-1" class="title-slide slide level1 center">
<h1>Text-to-Image</h1>

</section>
<section id="text-to-image-with-sd-1.5" class="slide level2">
<h2>Text-to-Image with SD 1.5</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/text-to-image-sd-1.5.ipynb" data-notebook-title="Text-to-Image using Stable Diffusion 1.5" data-notebook-cellid="load-model">
<div id="load-model" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:704,&quot;referenced_widgets&quot;:[&quot;6a6e4539180343119f27baf41cfb6ce3&quot;,&quot;d3dda5fbb32c4930a85148ed43086e09&quot;,&quot;2d9f47f108124a46a21119c3c23d4882&quot;,&quot;425b2a085a5c47ee8bb8f6cf1d3e102b&quot;,&quot;b1e2419536774c92a73d6d96460e1e66&quot;,&quot;1f5c1eb431a941c9806737cc14f6781b&quot;,&quot;f644cb048e414dbc96407ada75d7cf9f&quot;,&quot;4a914c9fdb59498f9f6a5244771fb6ee&quot;,&quot;24e791938be547f38cb613438f2960ec&quot;,&quot;56a094c1e5474bdfa6f5bb62ac79200f&quot;,&quot;fa4e6b3deb9f40e088748ffa578008f5&quot;,&quot;50f05b48fb5c44f099f50397993d73c1&quot;,&quot;0ca1fb71b7fa4d34801f8b7e57d3b754&quot;,&quot;5a2ed13d725a4787997c557ad24c35c2&quot;,&quot;73e41ea12a1b42d9a05c507c6d8b39f3&quot;,&quot;1e6d1d21b5804070bbfb13374d5aaf74&quot;,&quot;dc6f6f161c8940d897c4e59e8c17387f&quot;,&quot;093baf3319534e9f88e33db901ce3620&quot;,&quot;6ba6781b4bf74d25b1e61d81cdcf4c74&quot;,&quot;a7dd1f1183be4a5aa8265bf56834b484&quot;,&quot;b1600503e3034d8888a7d28009d260a8&quot;,&quot;1a5cde824d1b4934b2931956aa5a9a98&quot;,&quot;5a3265a897844e3d9058e78105746cf6&quot;,&quot;965ad8614ec74b8aa5fb9c0096575f06&quot;,&quot;88a71acf15b94f979f77296e7410e574&quot;,&quot;0f334c0fac644f22bbc56437704a0037&quot;,&quot;247c647f64f54d8fa25a757b3043d9cf&quot;,&quot;9777a0166eaf43aab20893b36e754617&quot;,&quot;0ec09341266a441dadfa54b8264e2cdf&quot;,&quot;ef78e956bf2e4b52affdd102c94a744c&quot;,&quot;cff0ca23d886498d8c164365359bf2ac&quot;,&quot;8af9b903d42c465f9cc7b92b9e5fff5b&quot;,&quot;8fb3c0de10124fd6bbbb5f7be31b2de7&quot;,&quot;c9c208e5534f49f0b292e95c08eb34b5&quot;,&quot;c01101010de84782b692186434797533&quot;,&quot;2ee3ed7c07b34a17bb92da3617f34002&quot;,&quot;f4455eb4e4474400b3be05163e2117c4&quot;,&quot;a91fa258668147348a492eb49c4eb21f&quot;,&quot;959e11c1068f4ff0b58a285518690b18&quot;,&quot;b7f5f635623d4ebe932ba24bee4a41dc&quot;,&quot;af8d70e3d8d34da196fb767ec0f3c994&quot;,&quot;9172866563ce46b5b5b90de353bc54a1&quot;,&quot;39eed96f2c8a488482e8b29cdd27fa3f&quot;,&quot;130ed0f0ceaa46209a16e1852951bd7f&quot;,&quot;87b2ba02c8ce47df82fe38261645f8b5&quot;,&quot;2ff08cc2866d48b4b9ec9d04afc78dbf&quot;,&quot;de1f5fa929ff407bad01dbb6195b44b7&quot;,&quot;6722e1a9c3a54a8595c2b97ef20f05e6&quot;,&quot;da2e289b0e3a4056a91575c1863c1482&quot;,&quot;6a5c20d23b294e3caf5e6a4505b42d52&quot;,&quot;3791e7acbfd445e39f66b330894e7c2a&quot;,&quot;3dfb7fe59f374602910175117c972d1e&quot;,&quot;351be0b1b8674439bccac206862a733b&quot;,&quot;c7dda0e1a03c45ccb057834d47cebd0a&quot;,&quot;0690f77f39b944e0b844da7f2ad2a957&quot;,&quot;25a61b1fed2344218149fadb20cf1b65&quot;,&quot;a85874aaf3aa424aa64fe17f76175d67&quot;,&quot;a3767aa39a094260a461ab8a6c5f93ab&quot;,&quot;823dd20991784750a2cdfc008a7cbedc&quot;,&quot;765d57df30a74f3bbb83603cabda774b&quot;,&quot;8c64d1f3e6ed4405b5fecd5676a4f384&quot;,&quot;b36c137aed454065bc41768bf0999641&quot;,&quot;e80d2059ff8b40dc878ad2fcb5e6375b&quot;,&quot;529e953c401b4e94b2c3fd4f5de9d558&quot;,&quot;7e9c6f758a854feb99bbcdf9821c1caa&quot;,&quot;0becee6f59b645f0ac9615b53b735ee3&quot;,&quot;b167e78172e344f8bcfe89b17a252d8e&quot;,&quot;101270501e7e410e96c31da986c805c3&quot;,&quot;da722f5e4c1c4b759a7dbe9ce10c7dde&quot;,&quot;3fa679b964d548bd96d96277b9d06e64&quot;,&quot;8c293b8b43dc48cf93b3449a63320a81&quot;,&quot;98e6acc37a9740a1824ec52e17614813&quot;,&quot;c6f2c008d65b4a23980b69669accb485&quot;,&quot;9a6f80a8ea5248b2933d78205b252b36&quot;,&quot;365c09d742c04bccb9f6eb9669731a81&quot;,&quot;a780d02ac43a485aa3ec2081990688a1&quot;,&quot;2dd598e4a3064101a1e35072eb44f126&quot;,&quot;e497adf9e9954bf7b1bf1b041af46732&quot;,&quot;737e346ba3de45868defd94ba33694a6&quot;,&quot;e1d3f81330d24393bcd1901bb9bb9353&quot;,&quot;e6f3d0174dc04464b52653e539a78150&quot;,&quot;ba937f390a37427b9ed0cb6c330a8f33&quot;,&quot;291f766042f3424d8157be7d2c89d265&quot;,&quot;8ee03fdd3bc94e3b9ee0ca33a04a1d78&quot;,&quot;f7d5f86f7b3b40168bdbd5c3106cde15&quot;,&quot;2d594927021648559daf946e84264650&quot;,&quot;22d1895e3b68476f8dd1637fc8fc17e6&quot;,&quot;a2d757f83fc4476fb8b6e7fa9925ce60&quot;,&quot;f85ae4df068a4930a04ad12e1cf755dd&quot;,&quot;ea09d58f0a3b494980415d1d2878086a&quot;,&quot;ceec13316752479fa9c9475539f15160&quot;,&quot;443bfb1eda4e4b228a818c162069eb0c&quot;,&quot;c25153e473294e4abe1a12883a1d259c&quot;,&quot;a55720aec6ab40cea72370f4ff16b993&quot;,&quot;e83869149cee4e918ca417da8e1b55f8&quot;,&quot;a54484cbc7d047049ac8df868f0f49ee&quot;,&quot;66949aad545047428ba59ddd8f140bb4&quot;,&quot;05929c91f7ff4486a87b095d24480334&quot;,&quot;1f4685dd64e84e7ab244be2b25ce79e5&quot;,&quot;8966c6cf5e6f43928082f3d097be769f&quot;,&quot;b77b7dce80f3487a9260cd025c47d956&quot;,&quot;6840b54b73c04871a4dfab3701d93017&quot;,&quot;ef93577232fa49db9737af1c77feee5b&quot;,&quot;56f89368fcd046dfbd977b51ffe1dec3&quot;,&quot;550c44211c2847adb4b55c4f775886b2&quot;,&quot;4f0aa7be1d08477b9a2999d86f8b0e23&quot;,&quot;9bc5746b65c144098b7f27129766ab98&quot;,&quot;72e59640be5c4d3384f42c7a8dd89779&quot;,&quot;668833a6e902492aaab4562bd666f176&quot;,&quot;d9225e1d3ef94bd2bf868d59fa04338b&quot;,&quot;bddb58f605b946cc847bbbaa6c550ced&quot;,&quot;68c813b164ea4141a2ef7aaf56e23af3&quot;,&quot;8e1c2f4af3c847128aa7ba8d35f07ddc&quot;,&quot;f2ae5ca3880948e3b5f2b2d1459eae54&quot;,&quot;5950e404ac8b4f6aa54ebf8186a97fcb&quot;,&quot;5cd1996ac6024761958e853b826a6771&quot;,&quot;f8add8bdb401476f9a108b1b410e11b0&quot;,&quot;07d5900298974cb28ac7e8df75a70311&quot;,&quot;6deba887540d42559080e3d4a77fe540&quot;,&quot;ff841401828247b38a452e48f62c76ce&quot;,&quot;0dc30621e09944a4a43fbcc34aeeb9f7&quot;,&quot;dd3344bf6a0146e4bfc8abaf6b2a5caa&quot;,&quot;8435896e0a2e4f209b69191cc8d1ae65&quot;,&quot;29abcb32ee5546458d262b631b51e1c2&quot;,&quot;a10fd205ac85402a9c672a135394d792&quot;,&quot;bca87d88458f4ec7bd0dae9fecd181f8&quot;,&quot;83d1029ad7b04b3dab0c42754631c615&quot;,&quot;0f5b92e52a404a898f6aedcbe8fcab11&quot;,&quot;7116d359572d471c851f5d477437e9aa&quot;,&quot;425bd2ba9edc43409a8df5f3f8954b99&quot;,&quot;6d7a094154a946cf902033866b9a73c6&quot;,&quot;9905fe5e26bf4d909cd89f42ef216382&quot;,&quot;24cd050ae41f4cd6b84e522f249ce253&quot;,&quot;28008a40f6bc475d8ca8399ade1b358e&quot;,&quot;9d30e7ac7b8844ea82567e05b44dd3e2&quot;,&quot;daa35d0978154c03b926d0abbf23ee5e&quot;,&quot;61a7f81e34b24a5c93a0bd5959358ae9&quot;,&quot;f14667ff96424ef6b9a0c400172c3ecf&quot;,&quot;b9327c85b7d3414594b590fe2d490340&quot;,&quot;17e1e39276e547bcb727a88d9c650f19&quot;,&quot;c61c0330442345f2ab3dbbc3bc88a45a&quot;,&quot;0e57aa03cd81436683d049b1b52a1454&quot;,&quot;82ba207a9ecb41f78d02aa676dcfc11d&quot;,&quot;5d427a0b0e004f4b8674e0d296141b14&quot;,&quot;4151d2399a2e4e4c94557d9a0e761b55&quot;,&quot;ce94113e99824b509bc40db44ade2024&quot;,&quot;5cf3707226d849bdaaa78432815c5cda&quot;,&quot;22faa37c9b2d4f9bbe988087a4adeaf7&quot;,&quot;c2b5799b36bf473fb67177851c5e2fcc&quot;,&quot;081c46450f41478395323baf0baba4f3&quot;,&quot;c2c40e56b5bf458b80ff8cb603f8026e&quot;,&quot;0904b46d95ba4e529b0377819a877618&quot;,&quot;085082ebc5434a37a4e07259d6d207a2&quot;,&quot;2a117e6d992a4b78a0fe187a3948ec74&quot;,&quot;18c5df9e514f4ffa8cf167aa290d00c6&quot;,&quot;9f71ae8524c44cda832c199882d3b5c8&quot;,&quot;2924f7a015f0420aa99b2aac0920bb5b&quot;,&quot;7f2976314b5f42798b3add26b8938eff&quot;,&quot;5cbd62ed419e4207b677af86960a8a03&quot;,&quot;bed359cb0c6d4b5da15eddd85952499f&quot;,&quot;18384c9cb8994fbdbdfbf117d6876e58&quot;,&quot;38b0c47cc0ce41338b5efe0bf3a908cd&quot;,&quot;31e059f34ad44b5cacec044506c9ea09&quot;,&quot;63c2791bbe274b289d1daf943d80d01b&quot;,&quot;5f5bd8d2c6df40afb6d904f29e814397&quot;,&quot;01fa4ed1115342a2a99018ad0fea4faf&quot;,&quot;b348835ba86640a89539ff214cd4e216&quot;,&quot;555ee4f7621543709c1fb98fb4323bea&quot;,&quot;513c12ce601c4c869783342b048f53fd&quot;,&quot;96a2ebaef3444f4d8178eae6efe8f388&quot;,&quot;17fe6dc03a0247cbb7dfbe1c2a9e4573&quot;,&quot;c2de9ca0d5534ecca7662bc0b03daa7d&quot;,&quot;b99a5c53d5024099999746560f6c6f23&quot;,&quot;12ef08f721354508a7e1d854dc8db703&quot;,&quot;3d6e773ecea044c2aa4412788651b8e0&quot;,&quot;bf1201e97ffa4dac80e6d78e5386a3fa&quot;,&quot;500f361fdda940ec925766ada97820fb&quot;,&quot;c79d7a42a4d344e380960ddcb834743c&quot;,&quot;c0250d26977f47d89cce56bde18c5f8e&quot;,&quot;6cb2a00b7f0546b2839f230731571475&quot;,&quot;de3d3adfb0a746a49b0f07db954c32a9&quot;,&quot;f6736c5bdf4d47969c80808df0d9d3f3&quot;,&quot;eeaf9efc90194ef0a0b873025f0d0b7a&quot;,&quot;ee58b578e5234652b9891f56cac70222&quot;,&quot;dd420d4d85734969ad27fad7cef8dd6d&quot;,&quot;8e854a92c3b945948dffdb8e0e4f34d6&quot;,&quot;6e61552bc2d84f66b4e4b31ebbdd7ac6&quot;]}}" data-outputid="c990ac0b-aaef-4e18-d82b-8963ce6f9e5b" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a></a><span class="im">from</span> diffusers <span class="im">import</span> StableDiffusionPipeline</span>
<span id="cb1-3"><a></a></span>
<span id="cb1-4"><a></a><span class="co"># Load a small diffusion model</span></span>
<span id="cb1-5"><a></a>model_id <span class="op">=</span> <span class="st">"stable-diffusion-v1-5/stable-diffusion-v1-5"</span></span>
<span id="cb1-6"><a></a>pipe <span class="op">=</span> StableDiffusionPipeline.from_pretrained(</span>
<span id="cb1-7"><a></a>    model_id,</span>
<span id="cb1-8"><a></a>)</span>
<span id="cb1-9"><a></a></span>
<span id="cb1-10"><a></a><span class="co"># Move to GPU if available</span></span>
<span id="cb1-11"><a></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb1-12"><a></a>pipe <span class="op">=</span> pipe.to(device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="text-to-image-with-sd-1.5-1" class="slide level2">
<h2>Text-to-Image with SD 1.5</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/text-to-image-sd-1.5.ipynb" data-notebook-title="Text-to-Image using Stable Diffusion 1.5" data-notebook-cellid="steps">
<div id="steps" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:642,&quot;referenced_widgets&quot;:[&quot;0c3b24430ad54eb5b1ba5bc15f3cb1bc&quot;,&quot;6c15b3e33b404b7e862ec525682b6740&quot;,&quot;a3cc418dd7114427a2d107deaa3d264c&quot;,&quot;6357f9e4901c42d8be7384d07396e70d&quot;,&quot;04a724a5299342b6bd96712e56be5213&quot;,&quot;0b2ecb007bbd475bb19ba88672a0bc9e&quot;,&quot;3edee87a1373437d9078f089e61a0a51&quot;,&quot;0d37474e0b8f4e369d5d655f7b5ea353&quot;,&quot;83cea04b54174496abe3211413066571&quot;,&quot;b3e4f06470c64e9fa1c3fb3e1eefe39a&quot;,&quot;131db35a17194ea28b3779b10c264b7d&quot;]}}" data-outputid="94274b75-ed5e-4813-b57c-2b5b24e99d65" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a></a></span>
<span id="cb2-4"><a></a>PROMPT <span class="op">=</span> <span class="st">"a photograph of an astronaut riding a horse"</span> <span class="co">#@param {type:"string"}</span></span>
<span id="cb2-5"><a></a>STEPS <span class="op">=</span> <span class="dv">50</span> <span class="co">#@param {type:"slider", min:10, max:100, step:1}</span></span>
<span id="cb2-6"><a></a>SEED <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="co">#@param {type:"integer"}</span></span>
<span id="cb2-7"><a></a></span>
<span id="cb2-8"><a></a>intermediate_images <span class="op">=</span> []</span>
<span id="cb2-9"><a></a></span>
<span id="cb2-10"><a></a><span class="kw">def</span> callback_fn(step, timestep, latents):</span>
<span id="cb2-11"><a></a>    <span class="co">"""Capture intermediate denoising steps"""</span></span>
<span id="cb2-12"><a></a>    <span class="co"># Decode latents to image every few steps</span></span>
<span id="cb2-13"><a></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">5</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> step <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-14"><a></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-15"><a></a>            <span class="co"># Decode the latent representation to an image</span></span>
<span id="cb2-16"><a></a>            image <span class="op">=</span> pipe.vae.decode(latents <span class="op">/</span> pipe.vae.config.scaling_factor, return_dict<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]</span>
<span id="cb2-17"><a></a>            image <span class="op">=</span> pipe.image_processor.postprocess(image, output_type<span class="op">=</span><span class="st">"pil"</span>)[<span class="dv">0</span>]</span>
<span id="cb2-18"><a></a>            intermediate_images.append((step, image))</span>
<span id="cb2-19"><a></a></span>
<span id="cb2-20"><a></a>result <span class="op">=</span> pipe(</span>
<span id="cb2-21"><a></a>    PROMPT,</span>
<span id="cb2-22"><a></a>    num_inference_steps<span class="op">=</span>STEPS,</span>
<span id="cb2-23"><a></a>    callback<span class="op">=</span>callback_fn,</span>
<span id="cb2-24"><a></a>    callback_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb2-25"><a></a>    generator<span class="op">=</span>torch.Generator().manual_seed(SEED) <span class="cf">if</span> SEED <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span> <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb2-26"><a></a>).images[<span class="dv">0</span>]</span>
<span id="cb2-27"><a></a></span>
<span id="cb2-28"><a></a><span class="co"># Visualize the denoising process</span></span>
<span id="cb2-29"><a></a>num_steps_to_show <span class="op">=</span> <span class="bu">min</span>(<span class="dv">10</span>, <span class="bu">len</span>(intermediate_images))</span>
<span id="cb2-30"><a></a>step_indices <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="bu">len</span>(intermediate_images)<span class="op">-</span><span class="dv">1</span>, num_steps_to_show, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb2-31"><a></a></span>
<span id="cb2-32"><a></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb2-33"><a></a>fig.suptitle(<span class="ss">f'Real Diffusion Model Denoising Process</span><span class="ch">\n</span><span class="ss">Prompt: "</span><span class="sc">{</span>PROMPT<span class="sc">}</span><span class="ss">"'</span>)</span>
<span id="cb2-34"><a></a></span>
<span id="cb2-35"><a></a><span class="cf">for</span> idx, step_idx <span class="kw">in</span> <span class="bu">enumerate</span>(step_indices):</span>
<span id="cb2-36"><a></a>    row <span class="op">=</span> idx <span class="op">//</span> <span class="dv">5</span></span>
<span id="cb2-37"><a></a>    col <span class="op">=</span> idx <span class="op">%</span> <span class="dv">5</span></span>
<span id="cb2-38"><a></a>    step_num, img <span class="op">=</span> intermediate_images[step_idx]</span>
<span id="cb2-39"><a></a></span>
<span id="cb2-40"><a></a>    axes[row, col].imshow(img)</span>
<span id="cb2-41"><a></a>    axes[row, col].axis(<span class="st">'off'</span>)</span>
<span id="cb2-42"><a></a>    axes[row, col].set_title(<span class="ss">f'Step </span><span class="sc">{</span>step_num<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>STEPS<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-43"><a></a></span>
<span id="cb2-44"><a></a>plt.tight_layout()</span>
<span id="cb2-45"><a></a>plt.savefig(<span class="st">'diffusion_process.png'</span>, dpi<span class="op">=</span><span class="dv">150</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb2-46"><a></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="text-to-image-with-sd-1.5-2" class="slide level2">
<h2>Text-to-Image with SD 1.5</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/text-to-image-sd-1.5.ipynb" data-notebook-title="Text-to-Image using Stable Diffusion 1.5" data-notebook-cellid="steps">
<div id="steps" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:642,&quot;referenced_widgets&quot;:[&quot;0c3b24430ad54eb5b1ba5bc15f3cb1bc&quot;,&quot;6c15b3e33b404b7e862ec525682b6740&quot;,&quot;a3cc418dd7114427a2d107deaa3d264c&quot;,&quot;6357f9e4901c42d8be7384d07396e70d&quot;,&quot;04a724a5299342b6bd96712e56be5213&quot;,&quot;0b2ecb007bbd475bb19ba88672a0bc9e&quot;,&quot;3edee87a1373437d9078f089e61a0a51&quot;,&quot;0d37474e0b8f4e369d5d655f7b5ea353&quot;,&quot;83cea04b54174496abe3211413066571&quot;,&quot;b3e4f06470c64e9fa1c3fb3e1eefe39a&quot;,&quot;131db35a17194ea28b3779b10c264b7d&quot;]}}" data-outputid="94274b75-ed5e-4813-b57c-2b5b24e99d65" data-execution_count="14">
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major":2,"version_minor":0,"model_id":"0c3b24430ad54eb5b1ba5bc15f3cb1bc","quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-text-to-image-sd-1.5-cell-3-output-2.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="text-to-image-with-sd-1.5-3" class="slide level2">
<h2>Text-to-Image with SD 1.5</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/text-to-image-sd-1.5.ipynb" data-notebook-title="Text-to-Image using Stable Diffusion 1.5" data-notebook-cellid="final">
<div id="final" class="cell" data-outputid="492823ab-15e3-4d9a-a353-49a9046c18e2" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:675}}" data-execution_count="15">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-text-to-image-sd-1.5-cell-4-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="sidebar-what-are-these-pipelines" class="slide level2">
<h2>Sidebar: What are these pipelines?</h2>
<ul>
<li class="fragment">Our use of HF Transformers use so far
<ul>
<li class="fragment">Convert text to input tokens, pass to model, decode output tokens to text</li>
</ul></li>
<li class="fragment">HF Pipelines provides a layer of abstraction
<ul>
<li class="fragment">(Setup the pipeline, then call <code>pipe</code> method)</li>
<li class="fragment">While still giving access to underlying components</li>
</ul></li>
<li class="fragment">Pipelines also standardize other areas
<ul>
<li class="fragment">e.g., <code>pipe(prompt).images[0]</code> works for all model types</li>
<li class="fragment"><code>.to("cuda")</code> moves all components of the model to the GPU</li>
</ul></li>
</ul>
</section>
<section id="sidebar-seeds" class="slide level2">
<h2>Sidebar: Seeds</h2>
<ul>
<li class="fragment">What is a seed?
<ul>
<li class="fragment">(Optional) Integer value used to initialize the image generation</li>
<li class="fragment">Used to generate the initial random noise</li>
<li class="fragment">Using the same seed will generate the same image</li>
</ul></li>
<li class="fragment">Why use a seed?
<ul>
<li class="fragment">Controlling the seed allows you to then experiment with different prompts or parameters</li>
<li class="fragment">Gives you more control/predictability vs.&nbsp;starting from random seed every time</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="image-to-image" class="title-slide slide level1 center">
<h1>Image-to-Image</h1>

</section>
<section id="image-to-image-1" class="slide level2">
<h2>Image-to-Image</h2>
<ul>
<li class="fragment">Image-to-Image: “Make this image different”</li>
<li class="fragment">Originally solved by GAN approaches, but evolved into extension of the diffuser concept
<ul>
<li class="fragment">Add noise to the original image (partial denoising)</li>
<li class="fragment">Regenerate it with modifications based on the prompt</li>
<li class="fragment">Strength parameter (0.0 - 1.0) to indicate the weight to the new image vs.&nbsp;original</li>
<li class="fragment">The original image heavily influences the output structure</li>
</ul></li>
</ul>
</section>
<section id="image-to-image-generate" class="slide level2">
<h2>Image-to-Image (Generate)</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/image-to-image-sd-1.5.ipynb" data-notebook-title="Image-to-Image using Stable Diffusion 1.5" data-notebook-cellid="generate">
<div id="generate" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="kw">def</span> generate_image(strength):</span>
<span id="cb3-2"><a></a>  <span class="cf">return</span> pipe(</span>
<span id="cb3-3"><a></a>      prompt<span class="op">=</span>PROMPT,</span>
<span id="cb3-4"><a></a>      negative_prompt<span class="op">=</span>NEGATIVE_PROMPT,</span>
<span id="cb3-5"><a></a>      image<span class="op">=</span>init_image,</span>
<span id="cb3-6"><a></a>      strength<span class="op">=</span>strength,</span>
<span id="cb3-7"><a></a>      guidance_scale<span class="op">=</span><span class="fl">7.5</span>,</span>
<span id="cb3-8"><a></a>      num_inference_steps<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb3-9"><a></a>      generator<span class="op">=</span>torch.Generator().manual_seed(SEED) <span class="cf">if</span> SEED <span class="op">!=</span> <span class="op">-</span><span class="dv">1</span> <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb3-10"><a></a>  ).images[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="image-to-image-original" class="slide level2">
<h2>Image-to-Image (Original)</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/image-to-image-sd-1.5.ipynb" data-notebook-title="Image-to-Image using Stable Diffusion 1.5" data-notebook-cellid="original">
<div id="original" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-image-to-image-sd-1.5-cell-4-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="image-to-image-0.3" class="slide level2">
<h2>Image-to-Image (0.3)</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/image-to-image-sd-1.5.ipynb" data-notebook-title="Image-to-Image using Stable Diffusion 1.5" data-notebook-cellid="0.3">
<div id="0.3" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:761,&quot;referenced_widgets&quot;:[&quot;31318fb940c94c6ca103572f43fdc294&quot;,&quot;9a2a4db9117f4529934b77f9533d564c&quot;,&quot;be68c97799b74b60a9076a0eefb474d1&quot;,&quot;1b07d50df8f14f0cbecb051c8fec948f&quot;,&quot;c63f0701aa684223aff2fea1e1b13912&quot;,&quot;08fdac44edaa4b08ba4aab950f232e47&quot;,&quot;7b1b062ced9642949005196bbf4ed6b6&quot;,&quot;fe65f76ed7d44ae6ba7550372985bf98&quot;,&quot;23d074a6c1eb4883bf6c81195efb782d&quot;,&quot;c7a0fed66f9e4070a8528debe309bbe6&quot;,&quot;f5588beb122f44e095d6b059ac781090&quot;]}}" data-outputid="3bcb551f-520f-462f-f99a-290a34c49bd6" data-execution_count="18">
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"31318fb940c94c6ca103572f43fdc294","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-image-to-image-sd-1.5-cell-6-output-2.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="image-to-image-0.5" class="slide level2">
<h2>Image-to-Image (0.5)</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/image-to-image-sd-1.5.ipynb" data-notebook-title="Image-to-Image using Stable Diffusion 1.5" data-notebook-cellid="0.5">
<div id="0.5" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:761,&quot;referenced_widgets&quot;:[&quot;bec89e2be1194f169ccf3ca7afba8161&quot;,&quot;9c8692b89cbc4847927c06b3621b4ddc&quot;,&quot;4f7d4e2c6e8c4901b44dbd41dba043fb&quot;,&quot;5d4a92c36e8f4b7bbfc129a829886f47&quot;,&quot;6e4a2fc4892544259bd253b5a0c4fe27&quot;,&quot;975b9bfaa3bc404c821db4bbf84a23f4&quot;,&quot;29d31c9c8eaa4a9dbe08d4ecf48e69dc&quot;,&quot;ab71c339a2b84fdb9332c79df260a548&quot;,&quot;d5ef43fce3e646638d61b9fe7c60ef5c&quot;,&quot;44b024a3ea0245e1b893af16c2f62e0d&quot;,&quot;50869837f3e64b109dcdbd0231fbad2f&quot;]}}" data-outputid="e200ed27-e8d2-43c7-d6d4-e1d7017d4a47" data-execution_count="19">
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bec89e2be1194f169ccf3ca7afba8161","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-image-to-image-sd-1.5-cell-7-output-2.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="image-to-image-0.7" class="slide level2">
<h2>Image-to-Image (0.7)</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/image-to-image-sd-1.5.ipynb" data-notebook-title="Image-to-Image using Stable Diffusion 1.5" data-notebook-cellid="0.7">
<div id="0.7" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:761,&quot;referenced_widgets&quot;:[&quot;133781b908a7408e8191f8a9478a9b50&quot;,&quot;f4d6a6bc20e74641b102723023457a8e&quot;,&quot;6a5fb7b98e5644e494a0d5904492efac&quot;,&quot;c602e205bcd44cc8a607f4e1b0e1c43c&quot;,&quot;6dde480280d543b4974d711d476bd84c&quot;,&quot;3bba1cd06a1f42058916b5c61dbba7d4&quot;,&quot;59eb14daeee14999b7444e5fc383880f&quot;,&quot;fa85604b23894820bef696a2a3e0c125&quot;,&quot;0b9cf376c41448998215224202a6f60c&quot;,&quot;6642e8cd0def4c2d8421b03760e8e781&quot;,&quot;fa1c000aaff84e938f12f1623ab479de&quot;]}}" data-outputid="2134cb2c-ae0e-4c1f-8313-e6005a199be4" data-execution_count="20">
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"133781b908a7408e8191f8a9478a9b50","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-image-to-image-sd-1.5-cell-8-output-2.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="image-to-image-0.9" class="slide level2">
<h2>Image-to-Image (0.9)</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/image-to-image-sd-1.5.ipynb" data-notebook-title="Image-to-Image using Stable Diffusion 1.5" data-notebook-cellid="0.9">
<div id="0.9" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:761,&quot;referenced_widgets&quot;:[&quot;93dce114893f4356801271cbf19fab48&quot;,&quot;00f055017b0043729579bfd4c2870464&quot;,&quot;ae3a8be06588432c9c12488f8347d311&quot;,&quot;3aca39e10c3a4e9c9ddfa739e83e3936&quot;,&quot;8308496751c544aa9a9183074de302b7&quot;,&quot;9b853441f862442898997323d2377e51&quot;,&quot;ea5ea0b150e94dc2a8c4c033adc25309&quot;,&quot;d75f4344d36d46f19dc27570738d1eea&quot;,&quot;e9db4bd4342c42a4ae25bb141ce94428&quot;,&quot;e411121048da43d4bd2b08396fe6d57b&quot;,&quot;83200deb11fe4b41afc852b77138e838&quot;]}}" data-outputid="44a74615-8ca5-48b1-f15d-c5406d20418a" data-execution_count="21">
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"93dce114893f4356801271cbf19fab48","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-image-to-image-sd-1.5-cell-9-output-2.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section></section>
<section id="hands-on" class="title-slide slide level1 center">
<h1>Hands-on</h1>
<p>Explore Text-to-Image and Image-to-Image notebooks (text-to-image-sd-1.5.ipynb and image-to-image-sd-1.5.ipynb)</p>
<p>Test different prompts, images, seed values, and steps.</p>
</section>

<section>
<section id="beyond-sd-1.5" class="title-slide slide level1 center">
<h1>Beyond SD 1.5</h1>

</section>
<section id="beyond-sd-1.5-1" class="slide level2">
<h2>Beyond SD 1.5</h2>
<ul>
<li class="fragment">Stable Diffusion 1.5
<ul>
<li class="fragment">Great for learning about the diffusion process</li>
<li class="fragment">But the image quality isn’t great!</li>
</ul></li>
<li class="fragment">Image models get large quickly
<ul>
<li class="fragment">Higher resolutions demand more GPU/VRAM</li>
</ul></li>
</ul>
</section>
<section id="introducing-replicate" class="slide level2">
<h2>Introducing Replicate</h2>

<img data-src="./images/replicate.png" class="r-stretch quarto-figure-center"><p class="caption">Source: https://replicate.com</p></section>
<section id="introducing-replicate-1" class="slide level2">
<h2>Introducing Replicate</h2>
<ul>
<li class="fragment">Similar to OpenRouter
<ul>
<li class="fragment">But with a focus on image and video models</li>
<li class="fragment">Extensive access to larger models (e.g., FLUX 2, Nano Banana, ImageGen)</li>
<li class="fragment">Pay-per-call pricing (expect 2c per image for higher quality models)</li>
<li class="fragment">API access (with Python and NodeJS library)</li>
<li class="fragment">Fine-tune and share your own models</li>
</ul></li>
</ul>
</section></section>
<section id="demo" class="title-slide slide level1 center">
<h1>Demo</h1>
<p>Browsing models on Replicate</p>
</section>

<section>
<section id="using-the-replicate-api" class="title-slide slide level1 center">
<h1>Using the Replicate API</h1>

</section>
<section id="using-the-replicate-api-1" class="slide level2">
<h2>Using the Replicate API</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/text-to-image-replicate.ipynb" data-notebook-title="Text-to-Image using Replicate" data-notebook-cellid="run">
<div id="run" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">import</span> replicate</span>
<span id="cb4-2"><a></a>output <span class="op">=</span> replicate.run(</span>
<span id="cb4-3"><a></a>  <span class="st">"black-forest-labs/flux-pro"</span>,</span>
<span id="cb4-4"><a></a>  <span class="bu">input</span><span class="op">=</span>{</span>
<span id="cb4-5"><a></a>      <span class="st">"steps"</span>: <span class="dv">28</span>,</span>
<span id="cb4-6"><a></a>      <span class="st">"prompt"</span>: <span class="st">"lemon cupcake spelling out the words 'DigiPen' with sparklers, tasty, food photography, dynamic shot"</span>,</span>
<span id="cb4-7"><a></a>      <span class="st">"seed"</span>: <span class="dv">1564435</span>,</span>
<span id="cb4-8"><a></a>      <span class="st">"output_format"</span>: <span class="st">"png"</span>,</span>
<span id="cb4-9"><a></a>      <span class="st">"safety_tolerance"</span>: <span class="dv">2</span>,</span>
<span id="cb4-10"><a></a>      <span class="st">"prompt_upsampling"</span>: <span class="va">False</span></span>
<span id="cb4-11"><a></a>  },</span>
<span id="cb4-12"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="using-the-replicate-api-2" class="slide level2">
<h2>Using the Replicate API</h2>
<ul>
<li class="fragment"><strong>steps</strong>: Number to steps to run through</li>
<li class="fragment"><strong>seed</strong>: Random seed value</li>
<li class="fragment"><strong>prompt</strong>: Prompt to use to guide the model</li>
<li class="fragment"><strong>output_format</strong>: Output format to return</li>
<li class="fragment"><strong>safety_tolerance</strong>: Safety tolerance (1 is most strict; 6 is most permissive)</li>
<li class="fragment"><strong>prompt_upsampling</strong>: Run the prompt through an LLM to be more descriptive/creative</li>
</ul>
</section>
<section id="using-the-replicate-api-3" class="slide level2">
<h2>Using the Replicate API</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/text-to-image-replicate.ipynb" data-notebook-title="Text-to-Image using Replicate" data-notebook-cellid="result">
<div id="result" class="cell" data-execution_count="10">
<div class="cell-output cell-output-display" data-execution_count="10">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-text-to-image-replicate-cell-5-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div></div>
</section>
<section id="sidebar-safety-tolerance" class="slide level2">
<h2>Sidebar: Safety Tolerance</h2>
<ul>
<li class="fragment">TBD: How does safety tolerance networks work?</li>
</ul>
</section>
<section id="image-to-image-2" class="slide level2">
<h2>Image-to-Image</h2>
<ul>
<li class="fragment">Can also be used for…
<ul>
<li class="fragment">Super resolution (increase the resolution of this image)</li>
<li class="fragment">Style transfer (recreate this image in the style of…)</li>
<li class="fragment">Colorization (grayscale to color)</li>
<li class="fragment">Depth maps</li>
</ul></li>
</ul>
</section>
<section id="depth-maps" class="slide level2">
<h2>Depth Maps</h2>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li class="fragment">Images (often greyscale) where each pixel’s value represents the distance from the viewer
<ul>
<li class="fragment">i.e., objects in the foreground are lighter, background are darker</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="./images/example-depth.png"></p>
</div></div>
</section>
<section id="depth-maps-1" class="slide level2">
<h2>Depth Maps</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="./images/real-sense.jpg"></p>
</div><div class="column" style="width:50%;">
<ul>
<li class="fragment">Historically, required custom hardware
<ul>
<li class="fragment">Depth Camera (e.g., RealSense) - ~$300-500</li>
<li class="fragment">Module/processing for realtime (60fps) sensing</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="depth-maps-2" class="slide level2">
<h2>Depth Maps</h2>
<ul>
<li class="fragment">Image-to-Image depth estimation models
<ul>
<li class="fragment">Depth Anything, MiDaS, ZoeDepth</li>
<li class="fragment">Low latency (MiDaS 3.1 @ 20fps on embedded GPU)</li>
</ul></li>
<li class="fragment">Used for
<ul>
<li class="fragment">3D effects/estimation</li>
<li class="fragment">Simple/low-cost robotics</li>
<li class="fragment">Control input for other images</li>
</ul></li>
</ul>
</section>
<section id="depth-maps-3" class="slide level2">
<h2>Depth Maps</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/generate-depth-map.ipynb" data-notebook-title="Depth Map Generation" data-notebook-cellid="input-image">
<div id="input-image" class="cell">
<div class="cell-output cell-output-display" data-execution_count="1">
<img src="https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/campus.png">
</div>
</div>
</div>
</section>
<section id="depth-maps-4" class="slide level2">
<h2>Depth Maps</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/generate-depth-map.ipynb" data-notebook-title="Depth Map Generation" data-notebook-cellid="run">
<div id="run" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="im">import</span> replicate</span>
<span id="cb5-2"><a></a></span>
<span id="cb5-3"><a></a>MODEL <span class="op">=</span> <span class="st">"chenxwh/depth-anything-v2:b239ea33cff32bb7abb5db39ffe9a09c14cbc2894331d1ef66fe096eed88ebd4"</span></span>
<span id="cb5-4"><a></a></span>
<span id="cb5-5"><a></a>output <span class="op">=</span> replicate.run(</span>
<span id="cb5-6"><a></a>  MODEL,</span>
<span id="cb5-7"><a></a>  <span class="bu">input</span><span class="op">=</span>{</span>
<span id="cb5-8"><a></a>      <span class="st">"image"</span>: INPUT_IMAGE,</span>
<span id="cb5-9"><a></a>  },</span>
<span id="cb5-10"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="depth-maps-5" class="slide level2">
<h2>Depth Maps</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/generate-depth-map.ipynb" data-notebook-title="Depth Map Generation" data-notebook-cellid="result">
<div id="result" class="cell">
<div class="cell-output cell-output-display" data-execution_count="9">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-generate-depth-map-cell-6-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="depth-maps-6" class="slide level2">
<h2>Depth Maps</h2>
<ul>
<li class="fragment">Why do this?
<ul>
<li class="fragment">Depth map can be used as control image for new image</li>
</ul></li>
</ul>
</section>
<section id="depth-map-as-control" class="slide level2">
<h2>Depth Map as Control</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/depth-map-as-context.ipynb" data-notebook-title="Depth Map as Context" data-notebook-cellid="control-image">
<div id="control-image" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display" data-execution_count="4">
<img src="https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/campus-depth.png">
</div>
</div>
</div>
</section>
<section id="depth-map-as-control-1" class="slide level2">
<h2>Depth Map as Control</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/depth-map-as-context.ipynb" data-notebook-title="Depth Map as Context" data-notebook-cellid="run">
<div id="run" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a><span class="im">import</span> replicate</span>
<span id="cb6-2"><a></a></span>
<span id="cb6-3"><a></a>MODEL <span class="op">=</span> <span class="st">"black-forest-labs/flux-depth-pro"</span></span>
<span id="cb6-4"><a></a></span>
<span id="cb6-5"><a></a>output <span class="op">=</span> replicate.run(</span>
<span id="cb6-6"><a></a>  MODEL,</span>
<span id="cb6-7"><a></a>  <span class="bu">input</span><span class="op">=</span>{</span>
<span id="cb6-8"><a></a>      <span class="st">"control_image"</span>: CONTROL_IMAGE,</span>
<span id="cb6-9"><a></a>      <span class="st">"output_format"</span>: <span class="st">"png"</span>,</span>
<span id="cb6-10"><a></a>      <span class="st">"seed"</span>: <span class="dv">12345</span>,</span>
<span id="cb6-11"><a></a>      <span class="st">"prompt"</span>: <span class="st">"A futuristic building set in 2050, neon lighting, night shot, dynamic"</span></span>
<span id="cb6-12"><a></a>  },</span>
<span id="cb6-13"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="depth-map-as-control-2" class="slide level2">
<h2>Depth Map as Control</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/depth-map-as-context.ipynb" data-notebook-title="Depth Map as Context" data-notebook-cellid="result">
<div id="result" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display" data-execution_count="8">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-depth-map-as-context-cell-6-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="depth-map-as-control-3" class="slide level2">
<h2>Depth Map as Control</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/depth-map-as-context.ipynb" data-notebook-title="Depth Map as Context" data-notebook-cellid="run-2">
<div id="run-2" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a><span class="im">import</span> replicate</span>
<span id="cb7-2"><a></a></span>
<span id="cb7-3"><a></a>MODEL <span class="op">=</span> <span class="st">"black-forest-labs/flux-depth-pro"</span></span>
<span id="cb7-4"><a></a></span>
<span id="cb7-5"><a></a>output <span class="op">=</span> replicate.run(</span>
<span id="cb7-6"><a></a>  MODEL,</span>
<span id="cb7-7"><a></a>  <span class="bu">input</span><span class="op">=</span>{</span>
<span id="cb7-8"><a></a>      <span class="st">"control_image"</span>: CONTROL_IMAGE,</span>
<span id="cb7-9"><a></a>      <span class="st">"output_format"</span>: <span class="st">"png"</span>,</span>
<span id="cb7-10"><a></a>      <span class="st">"seed"</span>: <span class="dv">12345</span>,</span>
<span id="cb7-11"><a></a>      <span class="st">"prompt"</span>: <span class="st">"A historical castle set in medieval England, clear day, partially cloudy sky"</span></span>
<span id="cb7-12"><a></a>  },</span>
<span id="cb7-13"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="depth-map-as-control-4" class="slide level2">
<h2>Depth Map as Control</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/depth-map-as-context.ipynb" data-notebook-title="Depth Map as Context" data-notebook-cellid="result-2">
<div id="result-2" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display" data-execution_count="6">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-depth-map-as-context-cell-8-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="inpainting" class="slide level2">
<h2>Inpainting</h2>
<ul>
<li class="fragment">Filling in missing or masked regions of an image in a realistic way</li>
<li class="fragment">Model is given an image with certain areas masked out</li>
<li class="fragment">Generates plausible content to fill those areas based on the surrounding context (and steered by a text prompt)</li>
</ul>
</section>
<section id="inpainting-1" class="slide level2">
<h2>Inpainting</h2>
<ul>
<li class="fragment">Can be challenging
<ul>
<li class="fragment">Model needs to understand context around the area</li>
<li class="fragment">Generate content that matches the style, lighting, and perspective</li>
<li class="fragment">Follow a text prompt that was likely different from the original</li>
</ul></li>
</ul>
</section>
<section id="inpainting-2" class="slide level2">
<h2>Inpainting</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/inpainting.ipynb" data-notebook-title="Inpainting using black-forest-labs/flux-fill-pro" data-notebook-cellid="gradio">
<div id="gradio" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb8-2"><a></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-3"><a></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb8-4"><a></a><span class="im">import</span> replicate</span>
<span id="cb8-5"><a></a><span class="im">import</span> io</span>
<span id="cb8-6"><a></a></span>
<span id="cb8-7"><a></a><span class="kw">def</span> inpaint(image_data, prompt):</span>
<span id="cb8-8"><a></a>    <span class="cf">if</span> image_data <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb8-9"><a></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb8-10"><a></a>    </span>
<span id="cb8-11"><a></a>    <span class="co"># Get the original image from the background</span></span>
<span id="cb8-12"><a></a>    original_image <span class="op">=</span> Image.fromarray(image_data[<span class="st">'background'</span>])</span>
<span id="cb8-13"><a></a>    </span>
<span id="cb8-14"><a></a>    <span class="co"># Get the mask from the layers</span></span>
<span id="cb8-15"><a></a>    <span class="cf">if</span> image_data[<span class="st">'layers'</span>] <span class="kw">and</span> <span class="bu">len</span>(image_data[<span class="st">'layers'</span>]) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb8-16"><a></a>        mask_layer <span class="op">=</span> image_data[<span class="st">'layers'</span>][<span class="dv">0</span>]</span>
<span id="cb8-17"><a></a>        mask_array <span class="op">=</span> np.array(mask_layer)</span>
<span id="cb8-18"><a></a>        </span>
<span id="cb8-19"><a></a>        <span class="co"># Create binary mask: white where painted, black where not</span></span>
<span id="cb8-20"><a></a>        alpha_channel <span class="op">=</span> mask_array[:, :, <span class="dv">3</span>]</span>
<span id="cb8-21"><a></a>        binary_mask <span class="op">=</span> np.where(alpha_channel <span class="op">&gt;</span> <span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>).astype(np.uint8)</span>
<span id="cb8-22"><a></a>        mask_image <span class="op">=</span> Image.fromarray(binary_mask, mode<span class="op">=</span><span class="st">'L'</span>)</span>
<span id="cb8-23"><a></a>    <span class="cf">else</span>:</span>
<span id="cb8-24"><a></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb8-25"><a></a>    </span>
<span id="cb8-26"><a></a>    <span class="co"># Convert images to bytes for the replicate API</span></span>
<span id="cb8-27"><a></a>    image_bytes <span class="op">=</span> io.BytesIO()</span>
<span id="cb8-28"><a></a>    original_image.save(image_bytes, <span class="bu">format</span><span class="op">=</span><span class="st">'PNG'</span>)</span>
<span id="cb8-29"><a></a>    image_bytes.seek(<span class="dv">0</span>)</span>
<span id="cb8-30"><a></a>    </span>
<span id="cb8-31"><a></a>    mask_bytes <span class="op">=</span> io.BytesIO()</span>
<span id="cb8-32"><a></a>    mask_image.save(mask_bytes, <span class="bu">format</span><span class="op">=</span><span class="st">'PNG'</span>)</span>
<span id="cb8-33"><a></a>    mask_bytes.seek(<span class="dv">0</span>)</span>
<span id="cb8-34"><a></a>    </span>
<span id="cb8-35"><a></a>    <span class="co"># Call the Replicate API</span></span>
<span id="cb8-36"><a></a>    output <span class="op">=</span> replicate.run(</span>
<span id="cb8-37"><a></a>        <span class="st">"black-forest-labs/flux-fill-pro"</span>,</span>
<span id="cb8-38"><a></a>        <span class="bu">input</span><span class="op">=</span>{</span>
<span id="cb8-39"><a></a>            <span class="st">"image"</span>: image_bytes,</span>
<span id="cb8-40"><a></a>            <span class="st">"mask"</span>: mask_bytes,</span>
<span id="cb8-41"><a></a>            <span class="st">"prompt"</span>: prompt,</span>
<span id="cb8-42"><a></a>            <span class="st">"steps"</span>: <span class="dv">25</span>,</span>
<span id="cb8-43"><a></a>            <span class="st">"guidance"</span>: <span class="dv">75</span>,</span>
<span id="cb8-44"><a></a>            <span class="st">"outpaint"</span>: <span class="st">"None"</span>,</span>
<span id="cb8-45"><a></a>            <span class="st">"output_format"</span>: <span class="st">"jpg"</span>,</span>
<span id="cb8-46"><a></a>            <span class="st">"safety_tolerance"</span>: <span class="dv">2</span>,</span>
<span id="cb8-47"><a></a>            <span class="st">"prompt_upsampling"</span>: <span class="va">False</span></span>
<span id="cb8-48"><a></a>        }</span>
<span id="cb8-49"><a></a>    )</span>
<span id="cb8-50"><a></a>    </span>
<span id="cb8-51"><a></a>    <span class="co"># Read the FileOutput and convert to PIL Image</span></span>
<span id="cb8-52"><a></a>    output_bytes <span class="op">=</span> output.read()</span>
<span id="cb8-53"><a></a>    output_image <span class="op">=</span> Image.<span class="bu">open</span>(io.BytesIO(output_bytes))</span>
<span id="cb8-54"><a></a>    </span>
<span id="cb8-55"><a></a>    <span class="cf">return</span> output_image</span>
<span id="cb8-56"><a></a></span>
<span id="cb8-57"><a></a>demo <span class="op">=</span> gr.Interface(</span>
<span id="cb8-58"><a></a>    fn<span class="op">=</span>inpaint,</span>
<span id="cb8-59"><a></a>    inputs<span class="op">=</span>[</span>
<span id="cb8-60"><a></a>        gr.ImageEditor(</span>
<span id="cb8-61"><a></a>            label<span class="op">=</span><span class="st">"Image (paint over areas to inpaint)"</span>,</span>
<span id="cb8-62"><a></a>            brush<span class="op">=</span>gr.Brush(color_mode<span class="op">=</span><span class="st">"fixed"</span>, colors<span class="op">=</span>[<span class="st">"#000000"</span>]),</span>
<span id="cb8-63"><a></a>            layers<span class="op">=</span><span class="va">True</span></span>
<span id="cb8-64"><a></a>        ),</span>
<span id="cb8-65"><a></a>        gr.Textbox(label<span class="op">=</span><span class="st">"Prompt"</span>, placeholder<span class="op">=</span><span class="st">"Describe what should replace the masked area..."</span>)</span>
<span id="cb8-66"><a></a>    ],</span>
<span id="cb8-67"><a></a>    outputs<span class="op">=</span>gr.Image(label<span class="op">=</span><span class="st">"Output Image"</span>),</span>
<span id="cb8-68"><a></a>    title<span class="op">=</span><span class="st">"Inpainting using black-forest-labs/flux-fill-pro"</span></span>
<span id="cb8-69"><a></a>)</span>
<span id="cb8-70"><a></a>demo.launch()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="outpainting" class="slide level2">
<h2>Outpainting</h2>
<ul>
<li class="fragment">The opposite of inpainting, kind of :)</li>
<li class="fragment">How does it work?
<ul>
<li class="fragment">Supply a prompt: “2x zoom out this image”</li>
<li class="fragment">Treat the new empty regions around the image as masked areas</li>
<li class="fragment">Use impainting technique to fill in the regions</li>
</ul></li>
</ul>
</section>
<section id="outpainting-1" class="slide level2">
<h2>Outpainting</h2>
<ul>
<li class="fragment">More challenging
<ul>
<li class="fragment">Less context at the edges of the image vs.&nbsp;center/surrounded</li>
<li class="fragment">Need to maintain the style, lighting, and perspective</li>
<li class="fragment">Has to be creative. Can’t just be a repetitive pattern.</li>
</ul></li>
</ul>
</section>
<section id="outpainting-2" class="slide level2">
<h2>Outpainting</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/outpainting.ipynb" data-notebook-title="Outpainting using black-forest-labs/flux-fill-pro" data-notebook-cellid="input">
<div id="input" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display" data-execution_count="1">
<img src="https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/campus.png">
</div>
</div>
</div>
</section>
<section id="outpainting-3" class="slide level2">
<h2>Outpainting</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/outpainting.ipynb" data-notebook-title="Outpainting using black-forest-labs/flux-fill-pro" data-notebook-cellid="run">
<div id="run" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a><span class="im">import</span> replicate</span>
<span id="cb9-2"><a></a></span>
<span id="cb9-3"><a></a><span class="co"># Call the Replicate API</span></span>
<span id="cb9-4"><a></a>output <span class="op">=</span> replicate.run(</span>
<span id="cb9-5"><a></a>    <span class="st">"black-forest-labs/flux-fill-pro"</span>,</span>
<span id="cb9-6"><a></a>    <span class="bu">input</span><span class="op">=</span>{</span>
<span id="cb9-7"><a></a>        <span class="st">"image"</span>: INPUT_IMAGE,</span>
<span id="cb9-8"><a></a>        <span class="st">"prompt"</span>: <span class="st">"The main building of a technical college, no text"</span>,</span>
<span id="cb9-9"><a></a>        <span class="st">"seed"</span>: <span class="dv">123456</span>,</span>
<span id="cb9-10"><a></a>        <span class="st">"steps"</span>: <span class="dv">50</span>,</span>
<span id="cb9-11"><a></a>        <span class="st">"guidance"</span>: <span class="dv">60</span>,</span>
<span id="cb9-12"><a></a>        <span class="st">"outpaint"</span>: <span class="st">"Zoom out 2x"</span>,</span>
<span id="cb9-13"><a></a>        <span class="st">"output_format"</span>: <span class="st">"jpg"</span>,</span>
<span id="cb9-14"><a></a>        <span class="st">"safety_tolerance"</span>: <span class="dv">2</span>,</span>
<span id="cb9-15"><a></a>        <span class="st">"prompt_upsampling"</span>: <span class="va">False</span></span>
<span id="cb9-16"><a></a>    }</span>
<span id="cb9-17"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="outpainting-4" class="slide level2">
<h2>Outpainting</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/outpainting.ipynb" data-notebook-title="Outpainting using black-forest-labs/flux-fill-pro" data-notebook-cellid="input">
<div id="input" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display" data-execution_count="1">
<img src="https://raw.githubusercontent.com/simonguest/CS-394/refs/heads/main/src/04/images/campus.png">
</div>
</div>
</div>
</section>
<section id="outpainting-5" class="slide level2">
<h2>Outpainting</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/04/notebooks/outpainting.ipynb" data-notebook-title="Outpainting using black-forest-labs/flux-fill-pro" data-notebook-cellid="result">
<div id="result" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display" data-execution_count="9">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-outpainting-cell-5-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="inpainting-and-outpainting" class="slide level2">
<h2>Inpainting and Outpainting</h2>
<ul>
<li class="fragment">Popular models
<ul>
<li class="fragment">Stable Diffusion (Many inpainting variants)</li>
<li class="fragment">Flux Fill from Black Forest Labs</li>
<li class="fragment">LaMa: Large Mask inpainting</li>
<li class="fragment">Ideogram</li>
</ul></li>
</ul>
</section></section>
<section id="demo-1" class="title-slide slide level1 center">
<h1>Demo</h1>
<p>Inpainting and Outpainting using black-forest-labs/flux-fill-pro</p>
</section>

<section>
<section id="i-want-more-control" class="title-slide slide level1 center">
<h1>I Want More Control!</h1>

</section>
<section id="i-want-more-control-1" class="slide level2">
<h2>I Want More Control!</h2>
<ul>
<li class="fragment"><strong>Text-to-Image</strong> and <strong>Image-to-Image</strong> don’t give you that much control
<ul>
<li class="fragment">Extensive prompts (both positive and negative) can help, but only so far</li>
<li class="fragment">Ultimately, “hoping the model guesses what I mean”</li>
<li class="fragment">Fine-tuning possible, but it’s expensive and risks degrading quality (and potential overfitting)</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="introducing-controlnet" class="title-slide slide level1 center">
<h1>Introducing ControlNet</h1>

</section>
<section id="introducing-controlnet-1" class="slide level2">
<h2>Introducing ControlNet</h2>
<ul>
<li class="fragment">Developed by Lvmin Zhang and Maneesh Agrawala at Stanford University</li>
<li class="fragment">Published in February 2023 <span class="citation" data-cites="zhang2023controlnet">(<a href="#/references-1" role="doc-biblioref" onclick="">Zhang, Rao, and Agrawala 2023</a>)</span></li>
<li class="fragment">ControlNet represented a paradigm shift from “describe what you want” to “show the structure you want”.</li>
</ul>
</section>
<section id="how-controlnet-works" class="slide level2">
<h2>How ControlNet Works</h2>
<ul>
<li class="fragment">Stable Diffusion’s U-Net has an encoder and decoder</li>
<li class="fragment">Create a trainable copy of the encoder blocks</li>
<li class="fragment">Train the copy of the encoder alongside the frozen SD model
<ul>
<li class="fragment">During training: use paired data (e.g., pose skeleton → original image)</li>
<li class="fragment">During inference: both encoders run together</li>
<li class="fragment">Features from both are combined via zero convolutions</li>
</ul></li>
<li class="fragment">Key: The weights in the original SD model don’t change</li>
<li class="fragment">ControlNet is analogous to a “Plug in” model</li>
</ul>
</section>
<section id="how-controlnet-works-1" class="slide level2">
<h2>How ControlNet Works</h2>
<ul>
<li class="fragment">TBD: notebook for human pose</li>
</ul>
</section>
<section id="how-controlnet-works-2" class="slide level2">
<h2>How ControlNet Works</h2>
<ul>
<li class="fragment">TBD: notebook for Flux kontext model</li>
</ul>
</section>
<section id="how-controlnet-works-3" class="slide level2">
<h2>How ControlNet Works</h2>
<p>Examples of conditioning types:</p>
<ul>
<li class="fragment">Human pose (OpenPose): skeleton/keypoint detection</li>
<li class="fragment">Depth maps: 3D structure information</li>
<li class="fragment">Canny edges: line drawings and edge detection</li>
<li class="fragment">Scribbles: rough user drawings</li>
<li class="fragment">QR codes: blended into images</li>
</ul>
</section></section>
<section id="demo-putting-this-all-together" class="title-slide slide level1 center">
<h1>Demo: Putting this all together</h1>
<p>Using text-to-image, ControlNet, inpainting, and image-to-image (depth map) to create PBR materials</p>
<p>TBD - Mega notebook</p>
</section>

<section>
<section id="using-transformers-for-computer-vision" class="title-slide slide level1 center">
<h1>Using Transformers for Computer Vision</h1>

</section>
<section id="cnns-to-vision-transformer" class="slide level2">
<h2>CNNs to Vision Transformer</h2>
<p>Historically, computer vision has used classification models called CNNs (Convolutional Neural Networks)</p>
<ul>
<li class="fragment">Enter the Vision Transformer (ViT)
<ul>
<li class="fragment">“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” <span class="citation" data-cites="dosovitskiy2021vit">(<a href="#/references-1" role="doc-biblioref" onclick="">Dosovitskiy et al. 2021</a>)</span></li>
</ul></li>
<li class="fragment">Tipping Point
<ul>
<li class="fragment">Initially, ViTs didn’t outperform CNNs</li>
<li class="fragment">But exceeded SOTA CNNs on larger datasets (such as Google’s JFT-300M)</li>
</ul></li>
</ul>
</section>
<section id="popular-vision-transformers" class="slide level2">
<h2>Popular Vision Transformers</h2>
<ul>
<li class="fragment">OpenAI’s CLIP
<ul>
<li class="fragment">Trained on 400M image-text pairs</li>
<li class="fragment">Foundation of most VLMs</li>
</ul></li>
<li class="fragment">Meta’s DINO/DINO-2
<ul>
<li class="fragment">(Self DIstillation with NO Labels)</li>
<li class="fragment">Self-supervised on 142M images</li>
</ul></li>
<li class="fragment">Microsoft’s Swin
<ul>
<li class="fragment">Use “shifted windows” approach</li>
<li class="fragment">Excels at dense predication tasks</li>
</ul></li>
</ul>
</section>
<section id="vision-language-models-vlms" class="slide level2">
<h2>Vision Language Models (VLMs)</h2>
<ul>
<li class="fragment">ViTs by themselves are only so useful</li>
<li class="fragment">Introducing VLMs (Vision Language Models)
<ul>
<li class="fragment">A vision encoder</li>
<li class="fragment">Adapter/projector layer</li>
<li class="fragment">Language model (LLaMa or GPT)</li>
</ul></li>
<li class="fragment">Also known as “Multimodal”
<ul>
<li class="fragment">Image-Text-to-Text</li>
</ul></li>
</ul>
</section>
<section id="demo-2" class="slide level2">
<h2>Demo</h2>
<ul>
<li class="fragment">TBD</li>
</ul>
</section>
<section id="use-cases-for-vlms" class="slide level2">
<h2>Use Cases for VLMs</h2>
<ul>
<li class="fragment">Visual Understanding: “What’s in this image?”</li>
<li class="fragment">Accessibility: Assisting users with visual impairments</li>
<li class="fragment">Content Moderation and Safety: Identifying harmful content</li>
<li class="fragment">Retail: Finding products with photos</li>
<li class="fragment">Education: Helping students understand charts, diagrams, equations</li>
<li class="fragment">Robotics: Providing Robots with information to navigate their environment</li>
</ul>
</section>
<section id="popular-vlms" class="slide level2">
<h2>Popular VLMs</h2>
<ul>
<li class="fragment">Closed Source
<ul>
<li class="fragment">GPT4-V, Claude, Gemini Flash</li>
</ul></li>
<li class="fragment">Open Source
<ul>
<li class="fragment">LLaVa: Research collaboration between University of Wisconsin-Maddison and MSR</li>
<li class="fragment">Gemma: Google’s Gemma-3</li>
<li class="fragment">FastVLM: Apple’s Fast Vision Language Model</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="looking-ahead" class="title-slide slide level1 center">
<h1>Looking Ahead</h1>

</section>
<section id="looking-ahead-1" class="slide level2">
<h2>Looking Ahead</h2>
<ul>
<li class="fragment"><a href="https://simonguest.github.io/CS-394/src/04/assignment.html" class="external" target="_blank">This week’s assignment!</a></li>
<li class="fragment">TBD</li>
</ul>
</section></section>
<section>
<section id="references" class="title-slide slide level1 center">
<h1>References</h1>

</section>
<section id="references-1" class="slide level2 smaller scrollable">
<h2>References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-dosovitskiy2021vit" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <em>arXiv Preprint arXiv:2010.11929</em>.
</div>
<div id="ref-zhang2023controlnet" class="csl-entry" role="listitem">
Zhang, Lvmin, Anyi Rao, and Maneesh Agrawala. 2023. <span>“Adding Conditional Control to Text-to-Image Diffusion Models.”</span> <em>arXiv Preprint arXiv:2302.05543</em>.
</div>
</div>
</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../theme/logos/DigiPen_RGB_Red.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.15,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/simonguest\.github\.io\/CS-394\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>