{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa21b89d",
   "metadata": {},
   "source": [
    "# Create Hugging Face Dataset\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/simonguest/CS-394/blob/main/src/06/notebooks/create-dataset.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/simonguest/CS-394/raw/refs/heads/main/src/06/notebooks/create-dataset.ipynb\">\n",
    "  <img src=\"https://img.shields.io/badge/Download_.ipynb-blue\" alt=\"Download .ipynb\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2fe3b0",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8862dfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.1 environment at: /Users/simon/Dev/CS-394/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff82977",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52033058",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"../code/train.jsonl\"\n",
    "VALIDATION_FILE = \"../code/validation.jsonl\"\n",
    "TEST_FILE = \"../code/test.jsonl\"\n",
    "\n",
    "DATASET_REPO = \"simonguest/test-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd712d6",
   "metadata": {},
   "source": [
    "## Create dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 10\n",
      "Validation samples: 3\n",
      "Test samples: 1\n",
      "\n",
      "Sample entry: {'messages': [{'content': 'import keyword\\nprint(keyword.kwlist)', 'role': 'user'}, {'content': \"This code first imports Python's built‑in `keyword` module, which contains information about the language's reserved words. Then it prints the list of all keywords that Python recognizes, so you can see exactly which words are reserved and cannot be used as variable names or function names.\\n\\nThink of the keyword list like a list of stop signs at an intersection: each stop sign tells drivers (or in this case, the Python interpreter) that they must stop and follow a specific rule. Just as you can't drive through a stop sign, you can't use these words as your own identifiers—they're reserved for special meanings in the language.\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            try:\n",
    "                # Try parsing the line\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Error parsing line {line_num}: {e}\")\n",
    "                print(f\"Problematic line: {line[:200]}...\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_hf_dataset(train_file, val_file, test_file):\n",
    "    # Load the data\n",
    "    train_data = load_jsonl(train_file)\n",
    "    val_data = load_jsonl(val_file)\n",
    "    test_data = load_jsonl(test_file)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "    # Combine into DatasetDict\n",
    "    dataset_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": train_dataset,\n",
    "            \"validation\": val_dataset,\n",
    "            \"test\": test_dataset,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset_dict\n",
    "\n",
    "\n",
    "# Create and validate the dataset is ready to upload\n",
    "dataset = create_hf_dataset(TRAIN_FILE, VALIDATION_FILE, TEST_FILE)\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "print(f\"\\nSample entry: {dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5948f",
   "metadata": {},
   "source": [
    "## Upload to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "upload",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd4b3b3a2574ce38ca221ea12ecfcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c7920e9cde4f5abcc2ea8dfc7dc43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e67e91ff30a4c5ca5e56f361afd0a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4d57f714be4230bf4d20ed9c11c845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a3c6b93e254cd4857f54c960a6d799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed3bc932837470ebab286cb1bef544c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c099ed68ba74831a34423f75cdbc389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fbea9083b849bcb09dc496739b36d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8254620c2364fe0b39b4984ebfbe746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555c208edd5046f5837b2e5fd0e3e0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b886948c6b42aa8013341473e49e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f670014b4e141bb8cb16bdb47d5407e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded successfully to: https://huggingface.co/datasets/simonguest/test-dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def upload_dataset(dataset_dict, repo_name, token):\n",
    "    \"\"\"Upload dataset to Hugging Face Hub\"\"\"\n",
    "    \n",
    "    # Push to hub\n",
    "    dataset_dict.push_to_hub(\n",
    "        repo_name,\n",
    "        token=token,\n",
    "        private=False  # Set to True if you want it private\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset uploaded successfully to: https://huggingface.co/datasets/{repo_name}\")\n",
    "\n",
    "upload_dataset(dataset, DATASET_REPO, os.environ.get(\"HF_TOKEN\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-394 (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
