{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa21b89d",
   "metadata": {},
   "source": [
    "# Create Hugging Face Dataset\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/simonguest/CS-394/blob/main/src/07/notebooks/create-dataset.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/simonguest/CS-394/raw/refs/heads/main/src/07/notebooks/create-dataset.ipynb\">\n",
    "  <img src=\"https://img.shields.io/badge/Download_.ipynb-blue\" alt=\"Download .ipynb\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2fe3b0",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8862dfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.1 environment at: /Users/simon/Dev/CS-394/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 24ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff82977",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**Note**: If you are running this notebook on Colab, be sure to first upload your training data (.jsonl) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52033058",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"./train.jsonl\"\n",
    "VALIDATION_FILE = \"./validation.jsonl\"\n",
    "TEST_FILE = \"./test.jsonl\"\n",
    "\n",
    "DATASET_REPO = \"simonguest/test-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd712d6",
   "metadata": {},
   "source": [
    "## Create dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "create-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 5000\n",
      "Validation samples: 500\n",
      "Test samples: 10\n",
      "\n",
      "Sample entry: {'messages': [{'content': 'x = 10\\ny = 3.5\\nname = \"Alex\"\\nis_student = True', 'role': 'user'}, {'content': 'This code creates four variables: x stores a whole number, y stores a decimal number, name stores text, and is_student stores a True/False value. Each variable holds information that your program can use later.\\n\\nYou can think of variables like labeled boxes. You place different kinds of items into different boxes, and each box has a name so you can easily find what you stored inside.', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            try:\n",
    "                # Try parsing the line\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Error parsing line {line_num}: {e}\")\n",
    "                print(f\"Problematic line: {line[:200]}...\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_hf_dataset(train_file, val_file, test_file):\n",
    "    # Load the data\n",
    "    train_data = load_jsonl(train_file)\n",
    "    val_data = load_jsonl(val_file)\n",
    "    test_data = load_jsonl(test_file)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "    # Combine into DatasetDict\n",
    "    dataset_dict = DatasetDict(\n",
    "        {\n",
    "            \"train\": train_dataset,\n",
    "            \"validation\": val_dataset,\n",
    "            \"test\": test_dataset,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset_dict\n",
    "\n",
    "\n",
    "# Create and validate the dataset is ready to upload\n",
    "dataset = create_hf_dataset(TRAIN_FILE, VALIDATION_FILE, TEST_FILE)\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "print(f\"\\nSample entry: {dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e788b",
   "metadata": {},
   "source": [
    "## Get Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2326726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded env vars from .env\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import userdata # type:ignore\n",
    "  os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
    "  print(\"HF API Token set for Colab\")\n",
    "else:\n",
    "  load_dotenv()\n",
    "  print(\"Loaded env vars from .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5948f",
   "metadata": {},
   "source": [
    "## Upload to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f0ef9f74db45a58341e82288738a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33942bc984194d4b86b57437c840e946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaebe7865dce479ba5707d52100b2d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46fe1db61ba4a5583bf9782cbca7501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcad37c48434019b6c50564a3f5279d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48a9c6415aa4b2d851f3f5612318af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d94aea48d84114b9e533927225bd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5307608f640b43be907ce12cc71c8951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3760b9e5a4eb49dba446367d55c7614f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60300b6e21dc4ad0a59e1542468166a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ff393f23474f8dacf913dcf9e55105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1ff86f378c454e8c2a212bcc11f5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded successfully to: https://huggingface.co/datasets/simonguest/test-dataset\n"
     ]
    }
   ],
   "source": [
    "def upload_dataset(dataset, repo_name, token):\n",
    "    dataset.push_to_hub(\n",
    "        repo_name,\n",
    "        token=token,\n",
    "        private=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset uploaded successfully to: https://huggingface.co/datasets/{repo_name}\")\n",
    "\n",
    "upload_dataset(dataset, DATASET_REPO, os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5df305",
   "metadata": {},
   "source": [
    "## Create the dataset card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e8c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import DatasetCard\n",
    "\n",
    "card_content = f\"\"\"---\n",
    "---\n",
    "pretty_name: \"Test Dataset for CS-394\"\n",
    "license: mit\n",
    "---\n",
    "\n",
    "# Test Dataset\n",
    "\n",
    "This is a test dataset of Python code snippets and explanations, used in DigiPen's CS-394 course.\n",
    "\"\"\"\n",
    "\n",
    "card = DatasetCard(card_content)\n",
    "card.save(f\"./README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ebd8d",
   "metadata": {},
   "source": [
    "## Upload the dataset card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3b0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/simonguest/test-dataset/commit/debbc8c9e4e94646b5ee769874a27abc00da093c', commit_message='Upload README.md with huggingface_hub', commit_description='', oid='debbc8c9e4e94646b5ee769874a27abc00da093c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/simonguest/test-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='simonguest/test-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=DATASET_REPO,\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-394 (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
