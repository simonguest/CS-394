---
title: "Module 4: Multimedia and Multimodal Models"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Described the fundamental concepts behind Agents/Agentic AI
- Explored and provided feedback on an existing multi-agent setup
- Understood available agent SDKs, how they differ, and advantages/disadvantages
- Used the OpenAI Agents SDK to build a multi-agent system from scratch, including document indexing and retrieval
- Understood and implemented tool calls using OpenAI's function calling and via MCP

## Lesson Objectives

- Understand the fundamentals and history of diffuser models
- Explore and use models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet
- Setup and use Replicate to create a custom pipeline of production-grade models
- Understand the fundamentals and history of Vision Encoders and VLMs
- Implement/test a local VLM model for on-device inference

# Image Models

## Image Models

- Text-to-Image
- Image-to-Image
- TBD

## Text-to-Image

:::: {.columns}

::: {.column width="50%"}
"A photograph of an astronaut riding a horse."

- Based on a concept called a diffusion transformer
- Commonly known as a **diffuser**
- Two stage process, inspired by thermodynamics
:::

::: {.column width="50%"}
![](./images/astronaut.png)
:::
::::

## Introducing the Diffuser

- Training
  - During training, random noise is added to images in steps
  - Model learns to predict what noise was added (forward diffusion process)
- Inference (process runs in reverse)
  - Start with pure random noise
  - Model estimates what noise should be removed to create a realistic image
  - Using the text prompt, the model steers the process towards images that match the description


## Image Diffusion Models in 2022

```{mermaid}
timeline
  August 2022 : Stable Diffusion v1.4
              : First open-source high-quality model
  September 2022 : Stable Diffusion v1.5
                  : Refined version
  October 2022 : eDiff-I (NVIDIA)
                : Ensemble approach
  November 2022 : Stable Diffusion v2.0/2.1
                : Higher resolution (768x768)
```

## Image Diffusion Models in 2023

```{mermaid}
timeline
  March 2023 : Midjourney v5
              : Exceptional artistic quality
  April 2023 : ControlNet
        : Precise spatial control
        : AnimateDiff - Video generation
  July 2023 : SDXL (Stable Diffusion XL)
            : 1024x1024 native resolution
  August 2023 : SDXL Turbo
              : Real-time capable generation
```

## Image Diffusion Models in 2024

```{mermaid}
timeline
  February 2024 : Stable Diffusion 3
                : Improved text understanding
  June 2024 : Stable Diffusion 3.5
            : Multiple model sizes
  2024 : FLUX.1 (Black Forest Labs)
        : State-of-the-art open model
        : Imagen 3 (Google DeepMind)
        : Photorealistic quality
```

## Image Diffusion Models in 2025

```{mermaid}
timeline
  May 2025 : Imagen 4 (Google DeepMind)
           : Improved text rendering, 2K resolution
  August 2025 : Nano Banana (Google)
              : Autoregressive model in Gemini 2.5 Flash
  November 2025 : FLUX.2 (Black Forest Labs)
                : 32B parameters, multi-image references
```

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#load-model echo=true outputs=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#steps echo=true outputs=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#steps echo=false >}}

## Introducing HF Pipelines

- TBD






# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/04/assignment.html){.external target="_blank"}
- TBD

# References

## References
