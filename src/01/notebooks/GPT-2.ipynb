{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRd5FboLjx1b"
      },
      "source": [
        "# Pre-trained GPT-2 Notebook\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/simonguest/CS-394/blob/main/src/01/GPT-2.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a target=\"_blank\" href=\"https://github.com/simonguest/CS-394/raw/refs/heads/main/src/01/GPT-2.ipynb\">\n",
        "  <img src=\"https://img.shields.io/badge/Download_.ipynb-blue\" alt=\"Download .ipynb\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "load-model",
      "metadata": {
        "id": "Z_RRNTep1FDj"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "autocomplete",
      "metadata": {
        "id": "UPVXBYRZ0Lsm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n",
        "    # Encode the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate continuation\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prompts",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiM3AC_x1dJB",
        "outputId": "7decbea9-1b4b-4208-a441-a65f9be042d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Mary had a little lamb\n",
            "--------------------------------------------------\n",
            "Output: Mary had a little lamb on her lap and a little black bean, but I think she was just as happy to eat it as I was to drink it. She was quite surprised when I told her I had eaten it.\n",
            "\n",
            "\"I've been saying that for a while now, but I've been talking to you all the time.\"\n",
            "\n",
            "\"I didn't have any tea,\" said\n",
            "\n",
            "\n",
            "Prompt: The future of artificial intelligence\n",
            "--------------------------------------------------\n",
            "Output: The future of artificial intelligence is not so much a matter of what will happen to humans but rather what we do with them.\n",
            "\n",
            "In short, what's in your fridge?\n",
            "\n",
            "What's in your fridge?\n",
            "\n",
            "What's in your fridge?\n",
            "\n",
            "What's in your fridge?\n",
            "\n",
            "What's in your fridge?\n",
            "\n",
            "What's in your fridge?\n",
            "\n",
            "What's\n",
            "\n",
            "\n",
            "Prompt: In a galaxy far, far away\n",
            "--------------------------------------------------\n",
            "Output: In a galaxy far, far away, where the universe is a vast expanse of nothingness, the only place where life exists is at the bottom of the ocean.\n",
            "\n",
            "The only place where life exists is at the bottom of the ocean.\n",
            "\n",
            "The only place where life exists is at the bottom of the ocean.\n",
            "\n",
            "The only place where life exists is at the bottom of the\n",
            "\n",
            "\n",
            "Prompt: DigiPen is a place where\n",
            "--------------------------------------------------\n",
            "Output: DigiPen is a place where you can get the most out of your pen. It's not just about writing with pen tools or with pens. The main thing you will notice is that the pen is not just a pen, it's a pen. It's a pen.\n",
            "\n",
            "The pen is a pen. It's a pen.\n",
            "\n",
            "The pen is a pen. It's a\n",
            "\n",
            "\n",
            "Prompt: def calculate_fibonacci(n):\n",
            "--------------------------------------------------\n",
            "Output: def calculate_fibonacci(n): # calculate the Fibonacci number 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "    \"Mary had a little lamb\",\n",
        "    \"The future of artificial intelligence\",\n",
        "    \"In a galaxy far, far away\",\n",
        "    \"DigiPen is a place where\",\n",
        "    \"def calculate_fibonacci(n):\"\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(\"-\" * 50)\n",
        "    completion = autocomplete(prompt, max_length=80)\n",
        "    print(f\"Output: {completion}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
