---
title: "Module 6: Increasing Model Accuracy (Part 1)"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Understood the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile
- Understood hardware requirements and architectures for model inference - e.g., CUDA vs. ONNX vs. MLX vs. WebGPU
- Explored how quantization works and understood techniques and formats for quantizing existing models
- Used llama.cpp to quantize and run an SLM on local hardware/gaming PC
- Integrated a quantized model within Unity/Unreal/WebAssembly

## Lesson Objectives

- Understand model training, dataset curation, what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy
- Explore use cases, advantages, and disadvantages of prompt engineering
- Introduce and implement RAG (Retrieval-Augmented Generation) to increase the accuracy of a limited SLM
- Start exploring how to fine-tune models
- Generate synthetic data for fine-tuning a small language model

# Hallucinations

## Hallucinations

- Breaking News: Models hallucinate!
  - Especially noticable on older and smaller models
  - Many hallucinations can be difficult to spot as they sound so plausible
  - Plausible sounding requests (i.e., asking about something that doesn't exist) often results in a hallucination

## Hallucinations

{{< embed notebooks/hallucinations.ipynb#setup-client echo=true outputs=false >}}

## Hallucinations: Citations

{{< embed notebooks/hallucinations.ipynb#hallucinate-1 echo=true outputs=false >}}

## Hallucinations: Citations

{{< embed notebooks/hallucinations.ipynb#hallucinate-1 echo=false >}}

## Hallucinations: Local Knowledge

{{< embed notebooks/hallucinations.ipynb#hallucinate-2 echo=true outputs=false >}}

## Hallucinations: Local Knowledge

{{< embed notebooks/hallucinations.ipynb#hallucinate-2 echo=false >}}

## Hallucinations: Summarization

{{< embed notebooks/hallucinations.ipynb#hallucinate-3 echo=true outputs=false >}}

## Hallucinations: Summarization

{{< embed notebooks/hallucinations.ipynb#hallucinate-3 echo=false >}}

## Why Do Models Hallucinate?

- A language model is not a database
  - It's a stochastic prediction machine
  - Models often don't know how to say "I don't know"
  - Instead, they are designed to come up with the most plausible continuation, not to retrieve verified facts
  - (Hallucination isn't a bug in the traditional sense)

## Why Do Models Hallucinate?

- Models are trained on a large corpora of Internet data
  - Data on the Internet is often uneven
  - Full of gaps
  - Often contradictory

## Why Do Models Hallucinate?

- Model size vs. Training set size
  - Many of the latest models hold billions of parameters
  - But they are trained on trillions of tokens
  - Thus, they can't memorize everything and instead recognize patterns to recall information

# Model Accuracy

## Model Accuracy

- Fortunately, we can improve the accuracy of models using several techniques:
  - Prompt Engineering
  - Reasoning/Thinking Models
  - Context Injection
  - Fine-tuning

# Prompt Engineering

## Prompt Engineering

- Best practices (introduced in Module 2)
  - **Be specific**: *"You are a Python programming tutor who explains concepts using simple analogies and provides code examples."*
  - **Define output**: *"List no more than 3 suggestions. Always show your work step by step."*
  - **Set boundaries**: *"If you are asked questions outside coding, politely redirect the student back to the task."*

## Prompt Engineering

- Output-formatting examples:
  - Providing 2-5 examples of desired output can dramatically improve performance
  - *"Please provide your output as..."*
  - *"Format your answer in the following way..."*
  - (Of course, structured outputs, if it's available, can be more reliable)

## Prompt Engineering

- Few-shot examples:
  - Providing 2-5 examples of input/output pairs dramatically improves performance
  - Example: *"Q: What's 15% of 80? A: Let me calculate: 80 × 0.15 = 12"*
  - Especially effective for formatting, tone, or structure-heavy tasks

## Prompt Engineering

- Role/persona assignment:
  - It can be common to add role/persona to help guide the model
  - *"You are a senior engineer reviewing code for a junior developer"*
  - *"You are a geography teacher working with high school students"*

## Prompt Engineering

- Negative samples:
  - Showing what not to do alongside positive examples
  - *"Do not add ```python markdown code fences before generating code"*
- Chain-of-Thought:
  - Explicitly asking the model to "think step-by-step"
  - Which introduces our next technique: reasoning/thinking models

# Reasoning/Thinking Models

## Reasoning/Thinking Models

- In 2022, Google researchers showed that prompting models to think "step by step" dramatically improved performance on math and reasoning tasks.
  - Paper: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" [@wei2022chain]
  - This prompt engineering technique is known as "Chain-of-Thought"
  - i.e., showing your work helped arrive at better answers

## Reasoning/Thinking Models

- **2023:** Models began to be trained with reinforcement learning on reasoning processes, not just final answers
- **2024-25:** OpenAI released o1 and o3 models. "Thinking tokens" are a separate hidden phase before generating visible output.
- **Today:** Thousands of models on Hugging Face, often suffixed with **-Thinking**

## Reasoning/Thinking Models

- How do reasoning/thinking models work?
  - Models are fine-tuned to produce "thinking tokens" before their final answer
  - The model learns to use the thinking phase as a "scratch space" where it can explore, self-correct, and reason
  - Thinking tokens in open-weight models are often in-between ```<think>``` and ```</think>``` tokens (or just a single ```</think>``` token to divide the two)
  - Thinking tokens for closed models (e.g., o1/o3) are typically hidden

## Reasoning/Thinking Models

{{< embed notebooks/thinking.ipynb#pipe echo=true outputs=false >}}

## Reasoning/Thinking Models

{{< embed notebooks/thinking.ipynb#pipe echo=false >}}

# Hands-on

Test out different prompts in the hallucinations notebook (hallucinations.ipynb)

Try some of the hallucination prompts with Qwen 4B Thinking (thinking.ipynb)

## Reasoning/Thinking Models

"Tell me more about FilterGenie..."

```text
Okay, the user is asking about FilterGenie, a macOS software for applying dynamic filters to images. Hmm, I need to recall what I know about this. Wait, I think FilterGenie might be a bit misleading here. Let me think carefully.\n\nFirst, I remember that there\'s a popular app called FilterGenie, but it\'s actually a Windows-only application. The user specifically mentioned it\'s for Mac. That\'s a red flag. I should double-check this to avoid giving wrong information. \n\nLet me mentally scan through common image editing apps for macOS. There\'s Lightroom, Affinity Photo, Photoshop, but those aren\'t called FilterGenie. Oh! I think there\'s a confusion with "FilterGenie" versus "FilterGenie" - wait, no. Actually, I recall that FilterGenie is a tool that\'s primarily for Windows. Maybe the user heard about it from a source that mixed up the platforms?\n\nI should consider if there\'s any Mac app with a similar name. Let me think... There\'s "Filter Magic" for Mac, but that\'s different. Or "Snapseed" by Google, but that\'s not it either. Hmm. \n\nWait a minute - I think the user might be mixing up FilterGen
```

## Reasoning/Thinking Models

- Prompt engineering and reasoning help the model increase it's accuracy for the data that it's been trained on
- But it doesn't account for data that it doesn't know about
- Enter context injection...

# Context Injection

## Context Injection

- A model's training data will only only answer so much
  - Training data has a cut-off: Your model won't know anything after that
  - Your model won't know if anything has changed since being trained
  - Function calling can help bridge this gap, but context injection is often simpler and more flexible

## Context Injection

- How it works
  - Take the user's prompt (e.g., "Who teaches CS-394?")
  - Before sending the prompt to the model, do a database look-up to find relevant information (context)
  - Inject that context into the system prompt
  - Call the model with the modified system prompt

## Context Injection

- The injected information is called 'context', and adding it is called 'augmenting' the prompt
  - Hence the popular term, augmented generation
  - Two examples:
    - SQL-Augmented Generation
    - Retrieval-Augmented Generation

## SQL-Augmented Generation

{{< embed notebooks/sql-augmented.ipynb#call-without echo=true outputs=false >}}

## SQL-Augmented Generation

{{< embed notebooks/sql-augmented.ipynb#call-without echo=false >}}

## SQL-Augmented Generation

![](diagrams/sql.svg){.lightbox fig-align="center" width="600px"}

## SQL-Augmented Generation

{{< embed notebooks/sql-augmented.ipynb#create-db echo=true outputs=false >}}

## SQL-Augmented Generation

{{< embed notebooks/sql-augmented.ipynb#build-context echo=true outputs=false >}}

## SQL-Augmented Generation

{{< embed notebooks/sql-augmented.ipynb#call-with echo=true outputs=false >}}

## SQL-Augmented Generation

{{< embed notebooks/sql-augmented.ipynb#call-with echo=false >}}

## SQL-Augmented Generation

- Improved results, but...
  - Requires the course code to be in the user prompt, matched to a RegEx pattern
  - Will answer *"Who teaches CS-394?"* and *"When is CS-394 held?"*
  - Won't answer broader queries:
    - *"What courses teach generative AI?"*
    - *"Which courses are on Friday afternoons?"*
  - Could make a free-text search through the database...
  - But there's a better way...

## RAG: Retrieval-Augmented Generation

- Roots of RAG trace back to 1950s and 60s, when researchers were working with vector space models
- Term was coined in 2020 in "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" [@lewis2020retrieval]
- *“We definitely would have put more thought into the name had we known our work would become so widespread”*

## How RAG Works

- Create "documents"
  - Strings of text - can be from existing db queries or scraped from PDFs
  - Generate embeddings (using a Sentence Transformer)
  - Store embeddings in a database
- User prompt is converted into an embedding
- Find the closest set of embeddings that match and inject into the system prompt

## Sidebar: Sentence Transformers

- In Module 1, we covered Word2Vec
  - Creating embeddings from words
- A sentence transformer creates embeddings from sentences
- Many models available (e.g., all-MiniLM-L6-v2, a 384-dim vector space)

## How RAG Works

![](diagrams/rag.svg){.lightbox fig-align="center" width="600px"}

## How RAG Works

{{< embed notebooks/rag.ipynb#create-db echo=true outputs=false >}}

## How RAG Works

{{< embed notebooks/rag.ipynb#build-context echo=true outputs=false >}}

## How RAG Works

{{< embed notebooks/rag.ipynb#call-with echo=true outputs=false >}}

## How RAG Works

- *"Which courses teach about generative AI?"*

## How RAG Works

{{< embed notebooks/rag.ipynb#call-with echo=false >}}

# Hands-on

Investigate sql-augmented.ipynb and rag.ipynb

Try different queries. Which work well on both?

Recreate the database with different data

## RAG for Photo Indexing

- Documents can be created using VLMs (Vision Language Models)
  - Descriptions from VLMs (together with photo metadata) can then be used for lookup
  - *"Find me photos with two dogs next to the beach"*
  - *"Show me all my photos of sunsets in Hawaii"*

## RAG for Photo Indexing

![](diagrams/rag-photo.svg){.lightbox fig-align="center" width="600px"}

## RAG Databases

- Extensions
  - **sqlite-vec:** SQLite Extension (we've been using that!)
  - **pgvector:** PostgreSQL extension
- Libraries
  - **FAISS:** (From Meta) In-memory vector index vs. database. Very fast.

## RAG Databases

- **Pinecone:** Popular commercial managed/cloud option.
- **Qdrant:** Open-source dedicated db, written in Rust.
- **Milvus:** Open-source. Heavier to operate, but can exceed a billion embeddings.

# Side Topic: Evaluating Models

## Evaluating Models

- How do we know whether we are improving a model's accuracy?
  - Run evaluations (commonly known as "evals")
  - Allows us to benchmark progress, especially for fine-tuning
  - We also want to know if performance has regressed (degraded) on certain tasks
  - Evals also help grade models within certain domains (e.g., what is the SOTA model for math, coding, medical, law, etc.)

## Evaluating Models

- How evals are created:
  - Curated question sets: Experts write or gather questions based on ground truth answers
  - Human-validated responses: For open-ended tasks (e.g., coding) human evaluators score model responses - or models are testing against validation logic (e.g., unit tests)
  - Adversarial design: The best evals try to anticipate how models might "cheat" - e.g., avoiding questions in training data, including distractors and edge cases

## Popular Evals: MMLU Pro

[https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro){.external target="_blank"}

- **What it tests:** 12K questions across 14 subject areas that cover STEM, humanities, social sciences, and professional domains. 
  - Multiple-choice questions (10 options) testing both factual knowledge and reasoning across elementary to professional levels.
- **History:** Original MMLU published in 2021 by Dan Hendrycks et al. 
  - MMLU-Pro created because models were plateauing on MMLU (hitting 85-90%+)
  - Pro released in June 2024 by the TIGER-AI-Lab team and accepted at NeurIPS 2024

## Popular Evals: GPQA

[https://huggingface.co/datasets/Idavidrein/gpqa](https://huggingface.co/datasets/Idavidrein/gpqa){.external target="_blank"}

- **What it tests:** Graduate/PhD-level scientific reasoning in biology, physics, and chemistry through multiple-choice questions
  - Require deep domain understanding, not just fact recall
  - Cannot be solved by web search (the "Google-proof" property)
  - Test genuine conceptual reasoning and problem-solving
- **History:**
  - Created Nov 2023 by David Rein et al. (includes researchers from NYU, Anthropic, and other institutions)

## Popular Evals: SWE-Bench

[https://www.swebench.com/](https://www.swebench.com/){.external target="_blank"}

- **What it tests:** AI systems' ability to solve real-world software engineering tasks by resolving actual GitHub issues from popular open-source Python repositories
- **History:** Created: 2023 by Princeton NLP group (Carlos E. Jimenez et al.)
  - Original dataset was 2,294 GitHub issues from 12 popular Python repositories (Django, scikit-learn, Flask, Matplotlib, Requests, SymPy, pytest, Sphinx, etc.)
  - Three major variants: Lite, Verified, and Pro
  - Microsoft also publishes a "Live" version with monthly curated updates

## Popular Evals: HLE

[https://huggingface.co/datasets/cais/hle](https://huggingface.co/datasets/cais/hle){.external target="_blank"}

- **What it tests:** 2,500 expert-level questions across dozens of subjects, including mathematics, humanities, and the natural sciences. 
  - HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading.
  - Require multimodal, multi-step reasoning rather than pattern matching or recall
  - Be "Google-proof" (can't be quickly answered by web search)
- **History:** Created in late 2024 by the Center for AI Safety (CAIS) and Scale AI
  - Led by: Dan Hendrycks (who also created MMLU and MATH benchmarks)

## Challenges With Evals

- Dataset contamination
  - Many popular benchmarks (MMLU, HumanEval) have been around for years and "leaked" into training data
  - Models can achieve a high score my memorizing the questions/answers vs. true reasoning
- Gaming the benchmark
  - Researchers may optimize models specifically for benchmark performance without improving general capability - also known as "teaching to the test"

## Challenges With Evals

- Synthetic vs. human generated problems
  - Many evals use multiple-choice, clean, well formatted problems.
  - Real users index towards more open-ended questions, which may have typos, missing context, and other edge cases.
- Bias and fairness
  - Standard evals rarely measure performance across demographics - a model might excel on average, but fail for under-represented groups or languages.

## What's Next For Evals?

- **Agentic and multi-step workflows:** Moving beyond single-question tests to measure performance on longer tasks like research, multi-file codebases, and customer support conversations
- **Contamination-resistant design:** Hugging Face's Benchmarks and dynamic benchmarks that refresh regularly (like SWE-Bench Live) or generate novel questions programmatically to avoid training data leakage
- **Process evaluation over outcomes:** Testing *how* models reason, not just final answers — can the model self-correct, identify its mistakes, and explain its reasoning?

# Fine-tuning

## Fine-tuning

- Prompt engineering, using thinking models, and context injection (RAG) can help improve the output and accuracy of models
- However, sometimes you need the model to have more consistency in its style, structure, or behavior
- Enter fine-tuning...

## Fine-tuning

- Fine-tuning is the process of taking a pre-trained model and continuing to train it on a smaller, task-specific dataset using curated examples
- This allows it to adapt its general capabilities toward a particular **style**, **structure** or **behavior** without learning from scratch

## Fine-tuning

- Examples
  - **Style:** Adopting a different voice at scale: Speak to students using "Hinglish" at a 5th grade level
  - **Structure**: Generating outputs in a precise format: Player movement instructions in a game, in JSON format
  - **Behavior**: Always refuse off-topic questions and redirect to the subject matter (guardrail)

## History of Fine-tuning

- **2010s:** Fine-tuning became prevalent in computer vision (e.g., ImageNet-trained CNNs for medical)
- **2018:** BERT demonstrated that a language model could be fine-tuned for sentiment, Q&A, and classification
- **2022:** InstructGPT fine-tuning on human-curated instruction-response pairs
- **Today:** Very popular technique for adapting trained models to different use cases

## When To Pick What

- Prompt Engineering shapes how the model responds to input
- Context Injection (RAG) augments the context with data
- Fine-tuning reshapes the model itself
- (A combination of all three are often used together)

## Sidebar: Can We Train Models From Scratch?

- [NanoChat](https://github.com/karpathy/nanochat){.external target="_blank"} shows that it's possible
  - 60x faster and 652x cheaper than original GPT-2 in 2019
- But you may not have the data nor the GPU compute to build a capable model
  - Trained on [fineweb-edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu){.external target="_blank"}
  - GPT-2 level: 3 hours on a 8xH100 GPU

# Generating Training Data

## Generating Training Data

- Before we can fine-tune a model, we need to generate training data
  - Data are conversations or input/output pairs that demonstrate **style**, **structure**, or **behavior** you want the model to adopt

## Generating Training Data

```json
{"messages": [{"role": "user", "content": "s = \"Hello\\nWorld\"\nprint(s)\n"}, {"role": "assistant", "content": "The code defines a string that contains the escape sequence \\n, which tells Python to start a new line when the string is displayed. When you run the print statement, \"Hello\" appears on one line and \"World\" appears on the next line, just like pressing Enter moves the cursor down in a text editor.\n\nImagine you are writing a story on a typewriter. When you finish a line, you press the \u201cEnter\u201d key to move to the next line and continue typing. The \\n character works the same way for strings in Python, signaling that the following text should start on a new line."}]}
```

## Generating Training Data

- Three datasets
  - **Training set:** What the model will learn from. Largest portion, about 70-80% of total data
  - **Validation set:** Separate dataset, used to monitor how well the model is generalizing. About 10-15%.
  - **Test set:** Separate dataset, provides an unbiased final measure of performance after training. About 10-15%

## Generating Training Data

- Diversity of training data
  - Needs to cover the range of inputs that the model will encounter in production.
  - A model trained only on short questions will struggle when users ask anything more complex
  - Datasets should have different dimensions

## Generating Training Data

- Diversity dimensions
  - Topics (areas, sub-domains)
  - Audience (e.g., school grade)
  - Length (e.g., short, med, long)
  - Formats (e.g., script vs. romanized)
  - Conversation turns (single / multi)
  - Negative answers (e.g., if implementing safety)
- Each of these can be weighted (e.g., 60% short, 20% med, 20% long)

## Generating Training Data

- How much data do you need? It depends :)
- Behaviour and style shifts: 50-500 high quality examples
- Multi-turn dialogue: 1K-10K examples
- Smaller models often need more examples
- Optimal dataset size is often discovered during training

## Generating Training Data

- Dataset Format
  - Typically JSON Lines (jsonl) - JSON objects separated with newline
- Converted to Hugging Face Datasets (and uploaded)
- During training, these get converted to the chat template/format used by the model

## Generating Training Data

- How do we generate this data?
  - Create the file by hand!
  - Generate **synthetic data** from a more capable model (and review for quality)
- Synthetic Data
  - *"Generate n examples of students asking their teacher a geography question"*
  - Often use structured outputs to map to jsonl conversational format

# Demo

Synthetic data generation using the generate-synthetic.ipynb notebook

# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/06/assignment.html){.external target="_blank"}
- Use your generated synthetic data to fine-tune a small model
- Use W&B (Weights and Biases) to observe parameters during the training run
- Understand and create a model card, upload your model to Hugging Face and share

# References

## References
