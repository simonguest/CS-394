---
title: "Week 6: Increasing Model Accuracy (Part 1)"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap of Last Week's Lecture

- Understood the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile
- Understood hardware requirements and architectures for model inference - e.g., CUDA vs. ONNX vs. MLX vs. WebGPU
- Explored how quantization works and understood techniques and formats for quantizing existing models
- Used llama.cpp to quantize and run an SLM on local hardware/gaming PC
- Integrated a quantized model within Unity/Unreal/WebAssembly

## Lesson Objectives

- Understand model training, dataset curation, what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy
- Explore use cases, advantages, and disadvantages of prompt engineering
- Introduce and implement RAG (Retrieval Augmented Generation) to increase the accuracy of a limited SLM
- Start the exploration of how to fine-tune models using LoRA (Low Ranked Adaptation)
- Use a foundational model to generate synthetic data for fine-tuning a 1B parameter model

# Looking Ahead to Next Week

## Looking Ahead to Next Week

- [This week's assignment!](https://simonguest.github.io/CS-394/src/06/assignment.html){.external target="_blank"}
- TBD

# References

## References
