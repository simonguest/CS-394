{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRd5FboLjx1b"
      },
      "source": [
        "# Pre-trained GPT-2 Notebook\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/simonguest/CS-394/blob/main/src/01/notebooks/GPT-2.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a target=\"_blank\" href=\"https://github.com/simonguest/CS-394/raw/refs/heads/main/src/01/notebooks/GPT-2.ipynb\">\n",
        "  <img src=\"https://img.shields.io/badge/Download_.ipynb-blue\" alt=\"Download .ipynb\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "load-model",
      "metadata": {
        "id": "Z_RRNTep1FDj"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set pad token\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "autocomplete",
      "metadata": {
        "id": "UPVXBYRZ0Lsm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n",
        "    # Encode the prompt with attention mask\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    # Generate continuation\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "prompts",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiM3AC_x1dJB",
        "outputId": "7decbea9-1b4b-4208-a441-a65f9be042d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mary had a little lamb, and the young woman asked her for a little lamb, and they gave it to her.\n",
            "\n",
            "\"Oh, my child, it is good to have a little lamb,\" said he, \"but it is not to be bought, for it is hard to make, and it is much more difficult to make.\n",
            "\n",
            "\"When you have a little lamb, it\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Mary had a little lamb\"\n",
        "completion = autocomplete(prompt, max_length=80)\n",
        "print(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Mary had a little lamb\n",
            "--------------------------------------------------\n",
            "Output: Mary had a little lamb, and the child was very hungry, and so he took a small lamb and brought it to her, and she and the child were very merry. So the child went home and the lamb was brought to her. So she and the child went to the priest and he gave her a piece of bread and said to her, \"This is good bread for you, but what\n",
            "\n",
            "\n",
            "Prompt: The future of artificial intelligence\n",
            "--------------------------------------------------\n",
            "Output: The future of artificial intelligence is uncertain, but its future is bright.\n",
            "\n",
            "And so, we are all waiting for a breakthrough.\n",
            "\n",
            "And that's why I think that it's important to understand how AI is coming to the table.\n",
            "\n",
            "One of the big questions we have right now is how AI will be able to take over a world, and how it will be able to take\n",
            "\n",
            "\n",
            "Prompt: In a galaxy far, far away\n",
            "--------------------------------------------------\n",
            "Output: In a galaxy far, far away, there is only one thing that matters. The fate of our galaxy.\n",
            "\n",
            "And it matters only to you.\n",
            "\n",
            "A New Frontier for Space\n",
            "\n",
            "It's been almost two years since I first wrote a post about this book. And that's because I've been busy.\n",
            "\n",
            "In the last month or so, I've been working on an\n",
            "\n",
            "\n",
            "Prompt: DigiPen is a place where\n",
            "--------------------------------------------------\n",
            "Output: DigiPen is a place where you can share your creations.\n",
            "\n",
            "Don't let the name fool you. This is the place to share your creations and to share your creativity.\n",
            "\n",
            "Don't let the name fool you. This is the place to share your creations and to share your creativity.\n",
            "\n",
            "Don't let the name fool you. This is the place to share your creations and\n",
            "\n",
            "\n",
            "Prompt: def calculate_fibonacci(n):\n",
            "--------------------------------------------------\n",
            "Output: def calculate_fibonacci(n):\n",
            "\n",
            "fibonacci(n) = 0.01\n",
            "\n",
            "return f(n)\n",
            "\n",
            "def calculate_fibonacci(n):\n",
            "\n",
            "fibonacci(n) = 0.01\n",
            "\n",
            "return f(n)\n",
            "\n",
            "def calculate_fibonacci(n):\n",
            "\n",
            "fibonacci\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "    \"Mary had a little lamb\",\n",
        "    \"The future of artificial intelligence\",\n",
        "    \"In a galaxy far, far away\",\n",
        "    \"DigiPen is a place where\",\n",
        "    \"def calculate_fibonacci(n):\"\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(\"-\" * 50)\n",
        "    completion = autocomplete(prompt, max_length=80)\n",
        "    print(f\"Output: {completion}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
