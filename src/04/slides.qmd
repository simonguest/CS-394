---
title: "Module 4: Multimedia and Multimodal Models"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Described the fundamental concepts behind Agents/Agentic AI
- Explored and provided feedback on an existing multi-agent setup
- Understood available agent SDKs, how they differ, and advantages/disadvantages
- Used the OpenAI Agents SDK to build a multi-agent system from scratch, including document indexing and retrieval
- Understood and implemented tool calls using OpenAI's function calling and via MCP

## Lesson Objectives

- Understand the fundamentals and history of diffuser models
- Explore and use models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet
- Setup and use Replicate to create a custom pipeline of production-grade models
- Understand the fundamentals and history of Vision Encoders and VLMs
- Implement/test a local VLM model for on-device inference

# Multimedia vs. Multimodal

## Multimedia vs. Multimodal

- **Multimedia** models
  - Single input/output models for images, video, audio, etc.
  - Also known as computer vision, audio models
- Examples
  - **Text-to-Image** (generate an image from a text prompt)
  - **Image-to-Image** (generate an image from an existing image)
  - **Image-to-3D** (generate a 3D object from an image)

## Multimedia vs. Multimodal

- **Multimodal** models
  - Process multiple datatypes such as text, images, and audio
  - Also known as VLMs (Vision-Language Models) or ALMs (Audio-Language Models)
- Examples
  - **Image-Text-to-Text** (ask a question about this image)
  - **Image-Text-to-Image** (decompose this image into multiple layers)
  - **Audio-Text-to-Text** (what is this sound?)

## Text-to-Image

:::: {.columns}

::: {.column width="50%"}
"A photograph of an astronaut riding a horse."

- Based on a concept called a diffusion transformer
- Commonly known as a **diffuser**
- Two stage process, inspired by thermodynamics
:::

::: {.column width="50%"}
![](./images/astronaut.png)
:::
::::

## Introducing the Diffuser

- Training
  - During training, random noise is added to images in steps
  - Model learns to predict what noise was added (forward diffusion process)
- Inference (process runs in reverse)
  - Start with pure random noise
  - Model estimates what noise should be removed to create a realistic image
  - Using the text prompt, the model steers the process towards images that match the description

## Image Diffusion Models in 2022

```{mermaid}
timeline
  August 2022 : Stable Diffusion v1.4
              : First open-source high-quality model
  September 2022 : Stable Diffusion v1.5
                  : Refined version
  October 2022 : eDiff-I (NVIDIA)
                : Ensemble approach
  November 2022 : Stable Diffusion v2.0/2.1
                : Higher resolution (768x768)
```

## Image Diffusion Models in 2023

```{mermaid}
timeline
  March 2023 : Midjourney v5
              : Exceptional artistic quality
  April 2023 : ControlNet
        : Precise spatial control
        : AnimateDiff - Video generation
  July 2023 : SDXL (Stable Diffusion XL)
            : 1024x1024 native resolution
  August 2023 : SDXL Turbo
              : Real-time capable generation
```

## Image Diffusion Models in 2024

```{mermaid}
timeline
  February 2024 : Stable Diffusion 3
                : Improved text understanding
  June 2024 : Stable Diffusion 3.5
            : Multiple model sizes
  2024 : FLUX.1 (Black Forest Labs)
        : State-of-the-art open model
        : Imagen 3 (Google DeepMind)
        : Photorealistic quality
```

## Image Diffusion Models in 2025

```{mermaid}
timeline
  May 2025 : Imagen 4 (Google DeepMind)
           : Improved text rendering, 2K resolution
  August 2025 : Nano Banana (Google)
              : Autoregressive model in Gemini 2.5 Flash
  November 2025 : FLUX.2 (Black Forest Labs)
                : 32B parameters, multi-image references
```

# Text-to-Image

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#load-model echo=true outputs=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#steps echo=true outputs=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#steps echo=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#final echo=false >}}

## Sidebar: What are these pipelines?

- Our use of HF Transformers use so far
  - Convert text to input tokens, pass to model, decode output tokens to text
- HF Pipelines provides a layer of abstraction
  - (Setup the pipeline, then call `pipe` method)
  - While still giving access to underlying components
- Pipelines also standardize other areas
  - e.g., `pipe(prompt).images[0]` works for all model types
  - `.to("cuda")` moves all components of the model to the GPU

## Sidebar: Seeds

- What is a seed?
  - (Optional) Integer value used to initialize the image generation
  - Used to generate the initial random noise
  - Using the same seed will generate the same image
- Why use a seed?
  - Controlling the seed allows you to then experiment with different prompts or parameters
  - Gives you more control/predictability vs. starting from random seed every time

# Image-to-Image

## Image-to-Image

- Image-to-Image: "Make this image different"
- Originally solved by GAN approaches, but evolved into extension of the diffuser concept
  - Add noise to the original image (partial denoising)
  - Regenerate it with modifications based on the prompt
  - Strength parameter (0.0 - 1.0) to indicate the weight to the new image vs. original
  - The original image heavily influences the output structure

## Image-to-Image (Generate)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#generate echo=true >}}

## Image-to-Image (Original)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#original echo=false >}}

## Image-to-Image (0.3)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#0.3 echo=false >}}

## Image-to-Image (0.5)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#0.5 echo=false >}}

## Image-to-Image (0.7)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#0.7 echo=false >}}

## Image-to-Image (0.9)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#0.9 echo=false >}}

# Hands-on

Explore Text-to-Image and Image-to-Image notebooks (text-to-image-sd-1.5.ipynb and image-to-image-sd-1.5.ipynb)

Test different prompts, images, seed values, and steps.

# Beyond SD 1.5

## Beyond SD 1.5

- Stable Diffusion 1.5
  - Great for learning about the diffusion process
  - But the image quality isn't great!
- Image models get large quickly
  - Higher resolutions demand more GPU/VRAM

## Introducing Replicate

![Source: https://replicate.com](./images/replicate.png)

## Introducing Replicate

- Similar to OpenRouter
  - But with a focus on image and video models
  - Extensive access to larger models (e.g., FLUX 2, Nano Banana, ImageGen)
  - Pay-per-call pricing (expect 2c per image for higher quality models)
  - API access (with Python and NodeJS library)
  - Fine-tune and share your own models

# Demo

Browsing models on Replicate

# Using the Replicate API

## Using the Replicate API

{{< embed notebooks/text-to-image-replicate.ipynb#run echo=true >}}

## Using the Replicate API

- **steps**: Number to steps to run through
- **seed**: Random seed value
- **prompt**: Prompt to use to guide the model
- **output_format**: Output format to return
- **safety_tolerance**: Safety tolerance (1 is most strict; 6 is most permissive)
- **prompt_upsampling**: Run the prompt through an LLM to be more descriptive/creative

## Using the Replicate API

:::: {.columns}
::: {.column width="50%"}
{{< embed notebooks/text-to-image-replicate.ipynb#result echo=false >}}
:::
::::

## Sidebar: Safety Tolerance

- TBD: How does safety tolerance networks work?

## Image-to-Image

- Can also be used for...
  - Super resolution (increase the resolution of this image)
  - Style transfer (recreate this image in the style of...)
  - Colorization (grayscale to color)
  - Depth maps

## Depth Maps

:::: {.columns}
::: {.column width="50%"}

- Images (often greyscale) where each pixel's value represents the distance from the viewer
  - i.e., objects in the foreground are lighter, background are darker

:::
::: {.column width="50%"}
![](./images/example-depth.png)
:::
::::

## Depth Maps

- Historically, required custom hardware
  - Depth Camera (e.g., RealSense) - ~$300-500
  - Module/processing for realtime (60fps) sensing

## Depth Maps

- Image-to-Image depth estimation models
  - Depth Anything, MiDaS, ZoeDepth
  - Low latency (MiDaS 3.1 @ 20fps on embedded GPU)
- Used for
  - 3D effects/estimation
  - Simple/low-cost robotics
  - Control input for other images

## Depth Maps

{{< embed notebooks/generate-depth-map.ipynb#input-image echo=false >}}

## Depth Maps

{{< embed notebooks/generate-depth-map.ipynb#run echo=true >}}

## Depth Maps

{{< embed notebooks/generate-depth-map.ipynb#result echo=false >}}

## Depth Maps

- Why do this?
  - Depth map can be used as control image for new image

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#control-image echo=false >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#run echo=true >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#result echo=false >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#run-2 echo=true >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#result-2 echo=false >}}

## Inpainting

:::: {.columns}
::: {.column width="50%"}
Filling in missing or masked regions of an image in a realistic way

- Model is given an image with certain areas masked out
- Generates plausible content to fill those areas based on the surrounding context (and steered by a text prompt)
:::
::: {.column width="50%"}
<video data-autoplay src="./images/cherries-oranges-bananas.mp4"></video>
:::
::::

## Inpainting

:::: {.columns}
::: {.column width="50%"}
Filling in missing or masked regions of an image in a realistic way

- Model needs to understand context around the area
- Generate content that matches the style, lighting, and perspective
- Follow a text prompt that was likely different from the original
:::
::: {.column width="50%"}
<video data-autoplay src="./images/cherries-oranges-bananas.mp4"></video>
:::
::::

## Outpainting

Inpainting applied to the edges of an image to extend it beyond it's original size

![Source: https://replicate.com/black-forest-labs/flux-fill-pro](./images/outpainter1.png)

## Outpainting

Inpainting applied to the edges of an image to extend it beyond it's original size

![Source: https://replicate.com/black-forest-labs/flux-fill-pro](./images/outpainter2.png)

## Outpainting

- How does it work?
  - Supply a prompt: "2x zoom out this image"
  - Treat the new empty regions around the image as masked areas
  - Use impainting technique to fill in the regions

## Outpainting

- More challenging
  - Less context at the edges of the image vs. center/surrounded
  - Need to maintain the style, lighting, and perspective
  - Has to be creative. Can't just be a repetitive pattern.

## Inpainting and Outpainting

- Popular models
  - Stable Diffusion (Many inpainting variants)
  - Flux Fill from Black Forest Labs
  - LaMa: Large Mask inpainting
  - Ideogram

## Inpainting and Outpainting

- TBD: Notebook that shows flux fill

# I Want More Control!

## I Want More Control!

- **Text-to-Image** and **Image-to-Image** don't give you that much control
  - Extensive prompts (both positive and negative) can help, but only so far
  - Ultimately, "hoping the model guesses what I mean"
  - Fine-tuning possible, but it's expensive and risks degrading quality (and potential overfitting)

# Introducing ControlNet

## Introducing ControlNet

- Developed by Lvmin Zhang and Maneesh Agrawala at Stanford University
- Published in February 2023 [@zhang2023controlnet]
- ControlNet represented a paradigm shift from "describe what you want" to "show the structure you want".

## How ControlNet Works

- Stable Diffusion's U-Net has an encoder and decoder
- Create a trainable copy of the encoder blocks
- Train the copy of the encoder alongside the frozen SD model
  - During training: use paired data (e.g., pose skeleton â†’ original image)
  - During inference: both encoders run together
  - Features from both are combined via zero convolutions
- Key: The weights in the original SD model don't change
- ControlNet is analogous to a "Plug in" model

## How ControlNet Works

- TBD: notebook for human pose

## How ControlNet Works

- TBD: notebook for Flux kontext model

## How ControlNet Works

Examples of conditioning types:

- Human pose (OpenPose): skeleton/keypoint detection
- Depth maps: 3D structure information
- Canny edges: line drawings and edge detection
- Scribbles: rough user drawings
- QR codes: blended into images

# Demo: Putting this all together

Using text-to-image, ControlNet, inpainting, and image-to-image (depth map) to create PBR materials

TBD - Mega notebook

# Using Transformers for Computer Vision

## CNNs to Vision Transformer

Historically, computer vision has used classification models called CNNs (Convolutional Neural Networks)

- Enter the Vision Transformer (ViT)
  - "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" [@dosovitskiy2021vit]
- Tipping Point
  - Initially, ViTs didn't outperform CNNs
  - But exceeded SOTA CNNs on larger datasets (such as Google's JFT-300M)

## Popular Vision Transformers

- OpenAI's CLIP
  - Trained on 400M image-text pairs
  - Foundation of most VLMs
- Meta's DINO/DINO-2
  - (Self DIstillation with NO Labels)
  - Self-supervised on 142M images
- Microsoft's Swin
  - Use "shifted windows" approach
  - Excels at dense predication tasks

## Vision Language Models (VLMs)

- ViTs by themselves are only so useful
- Introducing VLMs (Vision Language Models)
  - A vision encoder
  - Adapter/projector layer
  - Language model (LLaMa or GPT)
- Also known as "Multimodal"
  - Image-Text-to-Text

## Demo

- TBD

## Use Cases for VLMs

- Visual Understanding: "What's in this image?"
- Accessibility: Assisting users with visual impairments
- Content Moderation and Safety: Identifying harmful content
- Retail: Finding products with photos
- Education: Helping students understand charts, diagrams, equations
- Robotics: Providing Robots with information to navigate their environment

## Popular VLMs

- Closed Source
  - GPT4-V, Claude, Gemini Flash
- Open Source
  - LLaVa: Research collaboration between University of Wisconsin-Maddison and MSR
  - Gemma: Google's Gemma-3
  - FastVLM: Apple's Fast Vision Language Model

# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/04/assignment.html){.external target="_blank"}
- TBD

# References

## References
