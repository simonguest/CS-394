[
  {
    "objectID": "src/03/notebooks/open-meteo-mcp.html#install-pre-requisites",
    "href": "src/03/notebooks/open-meteo-mcp.html#install-pre-requisites",
    "title": "OpenAI Agents SDK with MCP Server",
    "section": "Install pre-requisites",
    "text": "Install pre-requisites\n\n!uv add openai-agents==0.4.2",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "open-meteo-mcp.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/open-meteo-mcp.html#mcp-server-setup",
    "href": "src/03/notebooks/open-meteo-mcp.html#mcp-server-setup",
    "title": "OpenAI Agents SDK with MCP Server",
    "section": "MCP Server Setup",
    "text": "MCP Server Setup\n\nimport logging\n\n# Suppress MCP client warnings about non-JSON messages from the server\nlogging.getLogger(\"mcp\").setLevel(logging.CRITICAL)\n\nfrom agents import Agent, Runner\nfrom agents.mcp import MCPServerStdio\n\n\nList Available Tools\n\nasync with MCPServerStdio(\n    name=\"Open-Meteo Weather Server\",\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"open-meteo-mcp-server\"],\n    },\n) as server:\n    tools = await server.list_tools()\n    print(f\"Available tools: {[tool.name for tool in tools]}\")\n\nAvailable tools: ['weather_forecast', 'weather_archive', 'air_quality', 'marine_weather', 'elevation', 'flood_forecast', 'seasonal_forecast', 'climate_projection', 'ensemble_forecast', 'geocoding', 'dwd_icon_forecast', 'gfs_forecast', 'meteofrance_forecast', 'ecmwf_forecast', 'jma_forecast', 'metno_forecast', 'gem_forecast']\n\n\n\n\nSimple Weather Query\n\nasync with MCPServerStdio(\n    name=\"Open-Meteo Weather Server\",\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"open-meteo-mcp-server\"],\n    },\n) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n    \n    result = await Runner.run(agent, \"What's the weather forecast for Seattle this week?\")\n    print(result.final_output)\n\nHere is the weather forecast for Seattle this week:\n\n- Maximum temperatures will range from about 8°C to 12°C.\n- Minimum temperatures will be between 3°C and 5°C.\n- Dry weather is expected all week, with no significant precipitation.\n- Most days will be partly cloudy to clear, with occasional cloudier periods.\n\nIf you need a forecast in Fahrenheit or more details (such as hourly data), just ask!\n\n\n\n\nAir Quality Query\n\nasync with MCPServerStdio(\n    name=\"Open-Meteo Weather Server\",\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"open-meteo-mcp-server\"],\n    },\n) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n    \n    result = await Runner.run(agent, \"What's the current air quality in Los Angeles?\")\n    print(result.final_output)\n\nLos Angeles air quality (latest hourly value, **2026-01-15 00:00** local time, America/Los_Angeles):\n\n- **PM2.5:** 22.6 µg/m³  \n- **PM10:** 22.7 µg/m³  \n- **Ozone (O₃):** 1 µg/m³  \n- **Nitrogen dioxide (NO₂):** 64.1 µg/m³  \n- **Carbon monoxide (CO):** 307 µg/m³  \n- **Sulphur dioxide (SO₂):** 9.7 µg/m³  \n\nIf you want, tell me your neighborhood/ZIP in LA and I can check a closer point estimate (levels can vary a lot across the basin).\n\n\n\n\nHistorical Weather Data\n\nasync with MCPServerStdio(\n    name=\"Open-Meteo Weather Server\",\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"open-meteo-mcp-server\"],\n    },\n) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n    \n    result = await Runner.run(agent, \"What was the average temperature in Tokyo during January 2024?\")\n    print(result.final_output)\n\nUsing ERA5 reanalysis data for central Tokyo (Asia/Tokyo), the **average temperature in Tokyo during January 2024** was **about 5.8 °C**.\n\n(Computed as the mean of each day’s average temperature: \\((T_{\\max}+T_{\\min})/2\\) averaged over **2024-01-01 to 2024-01-31**.)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "open-meteo-mcp.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#install-pre-requisites",
    "href": "src/03/notebooks/campus-agent.html#install-pre-requisites",
    "title": "DigiPen Campus Agent",
    "section": "Install pre-requisites",
    "text": "Install pre-requisites\n\n!uv add openai-agents==0.4.2 gradio==5.49.1\n\n\nResolved 190 packages in 1ms\n\nInstalled 10 packages in 33ms12.0                           \n\n + colorama==0.4.6\n\n + cryptography==46.0.3\n\n + griffe==1.15.0\n\n + httpx-sse==0.4.3\n\n + mcp==1.25.0\n\n + openai-agents==0.4.2\n\n + pydantic-settings==2.12.0\n\n + pyjwt==2.10.1\n\n + sse-starlette==3.1.2\n\n + types-requests==2.32.4.20260107",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#agents",
    "href": "src/03/notebooks/campus-agent.html#agents",
    "title": "DigiPen Campus Agent",
    "section": "Agents",
    "text": "Agents\n\nfrom agents import Agent, Runner, FileSearchTool\n\n\nCafe Agent\n\nfrom agents import function_tool\n\n@function_tool\ndef get_bytes_cafe_menu(date: str) -&gt; any:\n    return {\n        f\"{date}\": {\n            \"daily byte\": {\n                \"name\": \"Steak Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Flank steak, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"vegetarian\": {\n                \"name\": \"Impossible Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Impossible plant based product, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"international\": {\n                \"name\": \"Chicken Curry\",\n                \"price\": 12,\n                \"description\": \"Chicken thighs, onion, carrot, potato, curry sauce served over rice\",\n            },\n        }\n    }\n\n\ncafe_agent = Agent(\n    name=\"Cafe Agent\",\n    instructions=\"You help students locate and provide information about the Bytes Cafe.\",\n    tools=[\n        get_bytes_cafe_menu,\n    ],\n)\n\n\n\nSet Vector Store\n\nVECTOR_STORE_ID = \"vs_6896d8c959008191981d645850b42313\"\n\n\n\nBuilding Agent\n\nbuilding_agent = Agent(\n    name=\"Building Agent\",\n    instructions=\"You help students locate and provide information about buildings and rooms on campus. Be descriptive when giving locations.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=3,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)\n\n\n\nCourse Agent\n\ncourse_agent = Agent(\n    name=\"Course Agent\",\n    instructions=\"You help students find out information about courses held at DigiPen.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)\n\n\n\nHandbook Agent\n\nhandbook_agent = Agent(\n    name=\"Handbook Agent\",\n    instructions=\"You help students navigate the school handbook, providing information about campus policies and student conduct.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)\n\n\n\nCampus Agent\n\nagent = Agent(\n    name=\"DigiPen Campus Agent\",\n    instructions=\"You are a helpful campus agent that can plan and execute tasks for students at DigiPen. Please be concise and accurate in handing off tasks to other agents as needed.\",\n    handoffs=[building_agent, course_agent, handbook_agent, cafe_agent],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/campus-agent.html#gradio-interface",
    "href": "src/03/notebooks/campus-agent.html#gradio-interface",
    "title": "DigiPen Campus Agent",
    "section": "Gradio Interface",
    "text": "Gradio Interface\n\nChat function and display of tool calls\n\nfrom gradio import ChatMessage\n\nasync def chat_with_agent(user_msg: str, history: list):\n    messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in history]\n    messages.append({\"role\": \"user\", \"content\": user_msg})\n    responses = []\n    reply_created = False\n    active_agent = None\n\n    result = Runner.run_streamed(agent, messages)\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\":\n            if event.data.type == \"response.output_text.delta\":\n                if not reply_created:\n                    responses.append(ChatMessage(role=\"assistant\", content=\"\"))\n                    reply_created = True\n                responses[-1].content += event.data.delta\n        elif event.type == \"agent_updated_stream_event\":\n            active_agent = event.new_agent.name\n            responses.append(\n                ChatMessage(\n                    content=event.new_agent.name,\n                    metadata={\"title\": \"Agent Now Running\", \"id\": active_agent},\n                )\n            )\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                if event.item.raw_item.type == \"file_search_call\":\n                    responses.append(\n                        ChatMessage(\n                            content=f\"Query used: {event.item.raw_item.queries}\",\n                            metadata={\n                                \"title\": \"File Search Completed\",\n                                \"parent_id\": active_agent,\n                            },\n                        )\n                    )\n                else:\n                    tool_name = getattr(event.item.raw_item, \"name\", \"unknown_tool\")\n                    tool_args = getattr(event.item.raw_item, \"arguments\", {})\n                    responses.append(\n                        ChatMessage(\n                            content=f\"Calling tool {tool_name} with arguments {tool_args}\",\n                            metadata={\"title\": \"Tool Call\", \"parent_id\": active_agent},\n                        )\n                    )\n            if event.item.type == \"tool_call_output_item\":\n                responses.append(\n                    ChatMessage(\n                        content=f\"Tool output: '{event.item.raw_item['output']}'\",\n                        metadata={\"title\": \"Tool Output\", \"parent_id\": active_agent},\n                    )\n                )\n            if event.item.type == \"handoff_call_item\":\n                responses.append(\n                    ChatMessage(\n                        content=f\"Name: {event.item.raw_item.name}\",\n                        metadata={\n                            \"title\": \"Handing Off Request\",\n                            \"parent_id\": active_agent,\n                        },\n                    )\n                )\n        yield responses\n\n\n\nLaunch Gradio\n\nimport gradio as gr\n\ndemo = gr.ChatInterface(\n    chat_with_agent,\n    title=\"DigiPen Campus Agent\",\n    theme=gr.themes.Soft(\n        primary_hue=\"red\", secondary_hue=\"slate\", font=[gr.themes.GoogleFont(\"Inter\")]\n    ),\n    examples=[\n        \"I'm trying to find the WANIC classrooms. Can you help?\",\n        \"What's the policy for eating in auditoriums?\",\n        \"What's today's vegetarian dish at the Bytes Cafe?\",\n        \"What are the prerequisites for FLM201?\"\n    ],\n    submit_btn=True,\n    flagging_mode=\"manual\",\n    flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n    type=\"messages\",\n    save_history=False,\n)\n\ndemo.launch(share=False, debug=True)\n\n* Running on local URL:  http://127.0.0.1:7863\n* To create a public link, set `share=True` in `launch()`.\n\n\n\n\n\nKeyboard interruption in main thread... closing server.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "campus-agent.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/structured-outputs.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Structured Outputs",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/structured-outputs.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Structured Outputs",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#create-the-openai-client-with-openrouter-url",
    "href": "src/02/notebooks/structured-outputs.html#create-the-openai-client-with-openrouter-url",
    "title": "Structured Outputs",
    "section": "Create the OpenAI client with OpenRouter URL",
    "text": "Create the OpenAI client with OpenRouter URL\n\nimport openai\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#create-the-location-type",
    "href": "src/02/notebooks/structured-outputs.html#create-the-location-type",
    "title": "Structured Outputs",
    "section": "Create the “Location” type",
    "text": "Create the “Location” type\n\nfrom pydantic import BaseModel\n\n# Define the model for a geographic location\nclass Location(BaseModel):\n  name: str\n  country: str\n  latitude: float\n  longitude: float",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/structured-outputs.html#send-request-with-response_format-set",
    "href": "src/02/notebooks/structured-outputs.html#send-request-with-response_format-set",
    "title": "Structured Outputs",
    "section": "Send request with response_format set",
    "text": "Send request with response_format set\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.parse(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"What are the GPS coordinates for Paris?\"},\n    ],\n    response_format=Location\n)\n\ncompletion = response.choices[0].message\nprint(completion)\n\nParsedChatCompletionMessage[Location](content='{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=Location(name='Paris', country='France', latitude=48.8566, longitude=2.3522), reasoning=None)\n\n\n\n# Display the JSON repesentation\nprint(completion.content)\n\n# Display the parsed type\nprint(completion.parsed)\n\n# Pretty-print\nif completion.parsed:\n  location: Location = completion.parsed\n  print(f\"{location.name}, {location.country} has GPS coordinates of {location.latitude}, {location.longitude}\")\n\n{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}\nname='Paris' country='France' latitude=48.8566 longitude=2.3522\nParis, France has GPS coordinates of 48.8566, 2.3522",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "structured-outputs.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#install-required-packages",
    "href": "src/02/notebooks/gradio.html#install-required-packages",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Install required packages",
    "text": "Install required packages\n\n!uv add gradio==5.49.1 openai\n\n\nResolved 190 packages in 1ms\n\nInstalled 49 packages in 208ms                              \n\n + aiofiles==24.1.0\n\n + annotated-doc==0.0.4\n\n + annotated-types==0.7.0\n\n + audioop-lts==0.2.2\n\n + brotli==1.2.0\n\n + click==8.3.1\n\n + contourpy==1.3.3\n\n + cycler==0.12.1\n\n + distro==1.9.0\n\n + fastapi==0.128.0\n\n + ffmpy==1.0.0\n\n + filelock==3.20.1\n\n + fonttools==4.61.1\n\n + fsspec==2025.12.0\n\n + gradio==5.49.1\n\n + gradio-client==1.13.3\n\n + groovy==0.1.2\n\n + hf-xet==1.2.0\n\n + huggingface-hub==0.36.0\n\n + jiter==0.12.0\n\n + kiwisolver==1.4.9\n\n + markdown-it-py==4.0.0\n\n + matplotlib==3.10.8\n\n + mdurl==0.1.2\n\n + numpy==2.4.0\n\n + openai==2.14.0\n\n + orjson==3.11.5\n\n + pandas==2.3.3\n\n + pillow==11.3.0\n\n + pydantic==2.11.10\n\n + pydantic-core==2.33.2\n\n + pydub==0.25.1\n\n + pyparsing==3.3.1\n\n + python-dotenv==1.2.1\n\n + python-multipart==0.0.21\n\n + pytz==2025.2\n\n + rich==14.2.0\n\n + ruff==0.14.12\n\n + safehttpx==0.1.7\n\n + semantic-version==2.10.0\n\n + shellingham==1.5.4\n\n + sniffio==1.3.1\n\n + starlette==0.50.0\n\n + tomlkit==0.13.3\n\n + tqdm==4.67.1\n\n + typer==0.21.0\n\n + typing-inspection==0.4.2\n\n + uvicorn==0.40.0\n\n + websockets==15.0.1",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/gradio.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 from google.colab import userdata\n      2 OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n\nModuleNotFoundError: No module named 'google'",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/gradio.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#initialize-the-openai-client",
    "href": "src/02/notebooks/gradio.html#initialize-the-openai-client",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Initialize the OpenAI client",
    "text": "Initialize the OpenAI client\n\nimport openai\n\n# Initialize OpenAI client\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#example-1-basic-gradio-interface",
    "href": "src/02/notebooks/gradio.html#example-1-basic-gradio-interface",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Example 1: Basic Gradio interface",
    "text": "Example 1: Basic Gradio interface\n\nimport gradio as gr\n\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\n\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\")\ndemo.launch()\n\n* Running on local URL:  http://127.0.0.1:7860\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#example-2-basic-chat-interface-with-conversation-history",
    "href": "src/02/notebooks/gradio.html#example-2-basic-chat-interface-with-conversation-history",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Example 2: Basic chat interface with conversation history",
    "text": "Example 2: Basic chat interface with conversation history\n\nimport gradio as gr\n\ndef chat_with_history(message, history):\n    # Add current message\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Get response from API\n    response = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n    )\n    \n    return response.choices[0].message.content\n\n# Create a chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_history,\n    title=\"Basic Chat with Conversation History\"\n)\n\ndemo.launch()\n\n/Users/simon/Dev/CS-394/.venv/lib/python3.13/site-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  self.chatbot = Chatbot(\n\n\n* Running on local URL:  http://127.0.0.1:7861\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/gradio.html#example-3-streaming-chat-interface",
    "href": "src/02/notebooks/gradio.html#example-3-streaming-chat-interface",
    "title": "Gradio with OpenAI API (via OpenRouter)",
    "section": "Example 3: Streaming chat interface",
    "text": "Example 3: Streaming chat interface\n\ndef chat_with_streaming(message, history):\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Stream the response\n    stream = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n        stream=True,\n    )\n    \n    response_text = \"\"\n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            token = chunk.choices[0].delta.content\n            response_text += token\n            yield response_text\n\n# Create streaming chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_streaming,\n    title=\"AI Chat with Streaming\",\n)\n\ndemo.launch()\n\n/Users/simon/Dev/CS-394/.venv/lib/python3.13/site-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  self.chatbot = Chatbot(\n\n\n* Running on local URL:  http://127.0.0.1:7862\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "gradio.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openai.html#or-grab-the-openai-api-key-if-running-locally",
    "href": "src/02/notebooks/chat-completion-openai.html#or-grab-the-openai-api-key-if-running-locally",
    "title": "Chat Completion API (via OpenAI)",
    "section": "(Or grab the OpenAI API key if running locally)",
    "text": "(Or grab the OpenAI API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openai.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openai.html#logging-function-to-print-the-api-request-to-the-console",
    "href": "src/02/notebooks/chat-completion-openai.html#logging-function-to-print-the-api-request-to-the-console",
    "title": "Chat Completion API (via OpenAI)",
    "section": "Logging function to print the API request to the console",
    "text": "Logging function to print the API request to the console\n\nimport json\n\ndef log_request(request):\n  print(\"\\n=== REQUEST ===\")\n  print(f\"URL: {request.url}\")\n  print(f\"Method: {request.method}\")\n\n  if request.content:\n    try:\n      body = json.loads(request.content.decode('utf-8'))\n      print(\"\\nBody:\")\n      print(json.dumps(body, indent=2))\n    except:\n      print(\"\\nBody:\")\n      print(request.content.decode('utf-8'))\n  print(\"=\" * 50)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openai.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openai.html#call-openai-via-the-sdk",
    "href": "src/02/notebooks/chat-completion-openai.html#call-openai-via-the-sdk",
    "title": "Chat Completion API (via OpenAI)",
    "section": "Call OpenAI via the SDK",
    "text": "Call OpenAI via the SDK\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    api_key=OPENAI_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-5\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://api.openai.com/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"gpt-5\"\n}\n==================================================\n\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"chatcmpl-CuVn7EYuGJUEUEQ18Cl0SM2nNz9Mj\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Awesome! I can tailor a plan, but a few quick questions help:\\n- When are you going and for how many days?\\n- First time in Paris?\\n- Main interests (art, food, fashion, history, photography, nightlife, kid-friendly, etc.) and preferred pace (relaxed vs. packed)?\\n- Any must-sees or hard no’s?\\n- Rough budget and food needs (vegetarian, kosher/halal, allergies)?\\n- Where are you staying (neighborhood) and are day trips okay (Versailles, Champagne, Giverny, Disneyland)?\\n\\nIf you want a quick starter plan, here’s a flexible 4-day outline you can reshuffle by weather and museum closures:\\n\\nDay 1 – Islands + Latin Quarter\\n- Île de la Cité: Notre-Dame exterior, Sainte-Chapelle (timed ticket), Conciergerie.\\n- Stroll the Latin Quarter: Shakespeare & Company, Sorbonne, Luxembourg Gardens.\\n- Evening: Seine cruise or sunset along the river.\\n\\nDay 2 – Louvre to Arc de Triomphe\\n- Morning: Louvre (timed entry). Tuileries and Palais-Royal gardens.\\n- Covered passages (Véronique/Grand Cerf/Jouffroy) and Opéra Garnier.\\n- Sunset view: Arc de Triomphe rooftop or Galeries Lafayette/Printemps terrace.\\n\\nDay 3 – Montmartre + Left Bank art\\n- Montmartre: Sacré-Cœur, Place du Tertre, quieter backstreets (Rue de l’Abreuvoir).\\n- Afternoon: Musée d’Orsay and/or Orangerie.\\n- Evening: Saint-Germain wine bar or jazz.\\n\\nDay 4 – Le Marais or Day Trip\\n- Marais walk: Place des Vosges, Musée Carnavalet, Picasso Museum (check hours), Jewish quarter, trendy boutiques.\\n- Optional day trip: Versailles (palace + gardens; get the timed passport ticket).\\n- Night: Eiffel Tower area (view from Trocadéro or Champ de Mars; book tower tickets if going up).\\n\\nOther great adds by interest\\n- Art/architecture: Rodin Museum; Bourse de Commerce; Fondation Louis Vuitton. Note: check Centre Pompidou’s renovation status.\\n- Food: Morning market (Aligre or Rue Cler), cheese/wine tasting, pastry crawl, bistro lunch, cooking class.\\n- Unique: Catacombs (book ahead), Père Lachaise Cemetery, Canal Saint-Martin, covered markets (Le Marché des Enfants Rouges).\\n- With kids: Jardin des Plantes (zoo + galleries), Cité des Sciences, Jardin d’Acclimatation, Parc de la Villette.\\n- Day trips: Giverny (Apr–Oct), Reims/Epernay for Champagne, Fontainebleau, Auvers-sur-Oise, Disneyland Paris.\\n\\nBook these in advance\\n- Eiffel Tower, Louvre, Sainte-Chapelle, Catacombs, Versailles, Palais Garnier tours, popular restaurants.\\n- Consider the Paris Museum Pass (2/4/6 days) if you’ll visit several museums; the Louvre still needs a timed reservation even with the pass.\\n\\nPractical tips\\n- Closures: Many museums close one day/week (e.g., Orsay Mon, some Tue). Check hours.\\n- Getting around: The Métro is fastest. Use a contactless bank card to tap in, or get a reloadable Navigo Easy. For a Monday–Sunday stay with lots of rides, a Navigo Découverte weekly pass can be good value.\\n- Dining: Reserve for dinner, especially weekends. Tipping is minimal (service included); round up or leave 5–10% for great service.\\n- Safety: Watch for pickpockets in crowded areas and on the Metro.\\n\\nShare your dates, length of stay, and interests, and I’ll turn this into a detailed day-by-day plan with mapped routes and restaurant picks near each stop.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": [],\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1767584609,\n  \"model\": \"gpt-5-2025-08-07\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": \"default\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 2224,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 2268,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"reasoning_tokens\": 1408,\n      \"rejected_prediction_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0\n    }\n  }\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openai.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html",
    "href": "src/01/notebooks/translation-transformer.html",
    "title": "Translation Transformer",
    "section": "",
    "text": "In this notebook, we use a small transformer (Helsinki-NLP/opus-mt-fr-en) to translate from French to English.",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#load-model",
    "href": "src/01/notebooks/translation-transformer.html#load-model",
    "title": "Translation Transformer",
    "section": "Load model",
    "text": "Load model\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"Helsinki-NLP/opus-mt-fr-en\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n\n\n\n\n\n\n\n\n\n/Users/simon/Dev/CS-394/.venv/lib/python3.13/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#tokenize",
    "href": "src/01/notebooks/translation-transformer.html#tokenize",
    "title": "Translation Transformer",
    "section": "Tokenize",
    "text": "Tokenize\n\nfrench_text = \"Bonjour, comment allez-vous?\"\ninput_ids = tokenizer.encode(french_text, return_tensors=\"pt\")\nprint(input_ids[0])\nprint(\"Tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n\ntensor([8703,    2, 1027, 5682,   21,  682,   54,    0])\nTokens: ['▁Bonjour', ',', '▁comment', '▁allez', '-', 'vous', '?', '&lt;/s&gt;']\n\n\n\n# @title Demonstrate contextual vectors using the encoder\n\n# French: \"Bonjour , comment allez  - vous  ?\"\n#          ↓       ↓    ↓      ↓    ↓  ↓    ↓\n# Encoder: [v1]   [v2] [v3]  [v4] [v5][v6][v7]  ← 7 vectors, each 512-dim\n#          └─────────────────────────────────┘\n\nencoder = model.get_encoder()\nencoder_output = encoder(input_ids)\nprint(\"Encoder output shape:\", encoder_output.last_hidden_state.shape)\nprint(\"Encoder output:\", encoder_output)\n\nEncoder output shape: torch.Size([1, 8, 512])\nEncoder output: BaseModelOutput(last_hidden_state=tensor([[[-0.3943,  0.4660,  0.0190,  ..., -0.5069,  0.2120, -0.3190],\n         [ 0.0957,  0.0780,  0.1918,  ..., -0.0854,  0.2138,  0.1528],\n         [-0.6160,  0.0295,  0.1918,  ..., -0.3886,  0.0770,  0.2311],\n         ...,\n         [-0.1839, -0.3798,  0.1832,  ..., -0.0041, -0.3633, -0.5455],\n         [ 0.0153,  0.0264,  0.1122,  ...,  0.1966, -0.3027, -0.3659],\n         [-0.0484,  0.0147,  0.0078,  ..., -0.1359, -0.0295, -0.0799]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), hidden_states=None, attentions=None)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#run-through-tokenizer",
    "href": "src/01/notebooks/translation-transformer.html#run-through-tokenizer",
    "title": "Translation Transformer",
    "section": "Run through tokenizer",
    "text": "Run through tokenizer\n\noutput_ids = model.generate(input_ids)\nprint(output_ids)\n\ntensor([[59513, 10537,     2,   541,    52,    55,    54,     0]])",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/translation-transformer.html#decode-back-to-tokens-to-complete-the-translation",
    "href": "src/01/notebooks/translation-transformer.html#decode-back-to-tokens-to-complete-the-translation",
    "title": "Translation Transformer",
    "section": "Decode back to tokens to complete the translation",
    "text": "Decode back to tokens to complete the translation\n\nenglish_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(\"Translation:\", english_text)\n\nTranslation: Hello, how are you?",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "translation-transformer.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/GPT-2.html",
    "href": "src/01/notebooks/GPT-2.html",
    "title": "Pre-trained GPT-2 Notebook",
    "section": "",
    "text": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load pre-trained GPT-2 model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n# Set pad token\ntokenizer.pad_token = tokenizer.eos_token\n\n\nimport torch\n\ndef autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n    # Encode the prompt with attention mask\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    # Generate continuation\n    with torch.no_grad():\n        output = model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n\n\nprompt = \"Mary had a little lamb\"\ncompletion = autocomplete(prompt, max_length=80)\nprint(completion)\n\nMary had a little lamb, and the young woman asked her for a little lamb, and they gave it to her.\n\n\"Oh, my child, it is good to have a little lamb,\" said he, \"but it is not to be bought, for it is hard to make, and it is much more difficult to make.\n\n\"When you have a little lamb, it\n\n\n\nprompts = [\n    \"Mary had a little lamb\",\n    \"The future of artificial intelligence\",\n    \"In a galaxy far, far away\",\n    \"DigiPen is a place where\",\n    \"def calculate_fibonacci(n):\"\n]\n\nfor prompt in prompts:\n    print(f\"\\nPrompt: {prompt}\")\n    print(\"-\" * 50)\n    completion = autocomplete(prompt, max_length=80)\n    print(f\"Output: {completion}\\n\")\n\n\nPrompt: Mary had a little lamb\n--------------------------------------------------\nOutput: Mary had a little lamb, and the child was very hungry, and so he took a small lamb and brought it to her, and she and the child were very merry. So the child went home and the lamb was brought to her. So she and the child went to the priest and he gave her a piece of bread and said to her, \"This is good bread for you, but what\n\n\nPrompt: The future of artificial intelligence\n--------------------------------------------------\nOutput: The future of artificial intelligence is uncertain, but its future is bright.\n\nAnd so, we are all waiting for a breakthrough.\n\nAnd that's why I think that it's important to understand how AI is coming to the table.\n\nOne of the big questions we have right now is how AI will be able to take over a world, and how it will be able to take\n\n\nPrompt: In a galaxy far, far away\n--------------------------------------------------\nOutput: In a galaxy far, far away, there is only one thing that matters. The fate of our galaxy.\n\nAnd it matters only to you.\n\nA New Frontier for Space\n\nIt's been almost two years since I first wrote a post about this book. And that's because I've been busy.\n\nIn the last month or so, I've been working on an\n\n\nPrompt: DigiPen is a place where\n--------------------------------------------------\nOutput: DigiPen is a place where you can share your creations.\n\nDon't let the name fool you. This is the place to share your creations and to share your creativity.\n\nDon't let the name fool you. This is the place to share your creations and to share your creativity.\n\nDon't let the name fool you. This is the place to share your creations and\n\n\nPrompt: def calculate_fibonacci(n):\n--------------------------------------------------\nOutput: def calculate_fibonacci(n):\n\nfibonacci(n) = 0.01\n\nreturn f(n)\n\ndef calculate_fibonacci(n):\n\nfibonacci(n) = 0.01\n\nreturn f(n)\n\ndef calculate_fibonacci(n):\n\nfibonacci",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "GPT-2.ipynb"
    ]
  },
  {
    "objectID": "src/08/resources.html",
    "href": "src/08/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Resources"
    ]
  },
  {
    "objectID": "src/08/resources.html#citations",
    "href": "src/08/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Resources"
    ]
  },
  {
    "objectID": "src/07/slides.html#recap",
    "href": "src/07/slides.html#recap",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "Recap",
    "text": "Recap\n\nUnderstood model training, dataset curation, what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy\nExplored use cases, advantages, and disadvantages of prompt engineering\nIntroduced and implemented RAG (Retrieval Augmented Generation) to increase the accuracy of a limited SLM\nStarted the exploration of how to fine-tune models using LoRA (Low Ranked Adaptation)\nUsed a foundational model to generate synthetic data for fine-tuning a 1B parameter model",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/slides.html#lesson-objectives",
    "href": "src/07/slides.html#lesson-objectives",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUse generated synthetic data to fine-tune a 1B parameter model\nUse W&B (Weights and Biases) to observe parameters during the training run\nPost-training, use W&B to use cosine similarity and LLM-as-a-Judge to evaluate the accuracy of our trained model\nTrain smaller models (270M parameters) and compare the results\nUnderstand and create a model card, upload the model to Hugging Face and share",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/slides.html#looking-ahead-1",
    "href": "src/07/slides.html#looking-ahead-1",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nTBD",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/slides.html#references-1",
    "href": "src/07/slides.html#references-1",
    "title": "Module 7: Increasing Model Accuracy (Part 2)",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/assignment.html",
    "href": "src/07/assignment.html",
    "title": "Module 7 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/07/assignment.html#assignment",
    "href": "src/07/assignment.html#assignment",
    "title": "Module 7 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/06/resources.html",
    "href": "src/06/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Resources"
    ]
  },
  {
    "objectID": "src/06/resources.html#citations",
    "href": "src/06/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Resources"
    ]
  },
  {
    "objectID": "src/05/slides.html#recap",
    "href": "src/05/slides.html#recap",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Recap",
    "text": "Recap\n\nUnderstood the fundamentals and history of diffuser models\nExplored and used models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet\nSetup and used Replicate to create a custom pipeline of production-grade models\nUnderstood the fundamentals and history of Vision Encoders and VLMs\nImplemented/tested a local VLM model for on-device inference",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#lesson-objectives",
    "href": "src/05/slides.html#lesson-objectives",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile\nUnderstand hardware requirements and architectures for model inference - e.g., CUDA vs. ONNX vs. MLX vs. WebGPU\nExplore how quantization works and understand techniques and formats for quantizing existing models\nUse llama.cpp to quantize and run an SLM on local hardware/gaming PC\nIntegrate a quantized model within Unity/Unreal/WebAssembly",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#looking-ahead-1",
    "href": "src/05/slides.html#looking-ahead-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nTBD",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/slides.html#references-1",
    "href": "src/05/slides.html#references-1",
    "title": "Module 5: Running Models on Local Hardware",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/assignment.html",
    "href": "src/05/assignment.html",
    "title": "Module 5 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Assignment"
    ]
  },
  {
    "objectID": "src/05/assignment.html#assignment",
    "href": "src/05/assignment.html#assignment",
    "title": "Module 5 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/resources.html",
    "href": "src/04/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/resources.html#citations",
    "href": "src/04/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/slides.html#recap",
    "href": "src/03/slides.html#recap",
    "title": "Module 3: Agents and Tools",
    "section": "Recap",
    "text": "Recap\n\nUnderstood the evolution and licensing of models from GPT-2 through to modern day\nUnderstood instruction-tuned models, how they work, and how to configure\nSetup and used OpenRouter for accessing hosted models\nUnderstood the OpenAI API specification, the request/response payload, parameters, streaming, and structured output\nCreated and shared a chatbot using a Gradio-based UI",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#lesson-objectives",
    "href": "src/03/slides.html#lesson-objectives",
    "title": "Module 3: Agents and Tools",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nDescribe the fundamental concepts behind Agents/Agentic AI\nExplore and provide feedback on an existing multi-agent setup\nUnderstand available agent SDKs, how they differ, and advantages/disadvantages\nUse the OpenAI Agents SDK to build a multi-agent system from scratch, including document indexing and retrieval\nUnderstand and implement tool calls and implement using OpenAI’s function calling and via MCP",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-agents-1",
    "href": "src/03/slides.html#why-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Why Agents?",
    "text": "Why Agents?\n\nLimitations of our prior chatbots\n\nNeeds constant human input every turn; No ability to plan beyond a single interaction\nSingle model with single context (conversation)\nNo ability to interact with external systems",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent",
    "href": "src/03/slides.html#what-is-an-agent",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.youtube.com/watch?v=bwXaJXgezf4",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-1",
    "href": "src/03/slides.html#what-is-an-agent-1",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.weforum.org/stories/2025/06/cognitive-enterprise-agentic-business-revolution/",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-2",
    "href": "src/03/slides.html#what-is-an-agent-2",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.crn.com/news/ai/2025/10-hottest-agentic-ai-tools-and-agents-of-2025-so-far",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-3",
    "href": "src/03/slides.html#what-is-an-agent-3",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#what-is-an-agent-4",
    "href": "src/03/slides.html#what-is-an-agent-4",
    "title": "Module 3: Agents and Tools",
    "section": "What is an Agent?",
    "text": "What is an Agent?\nImagine a DigiPen Campus Assistant: An AI agent that can help you navigate anything and everything at DigiPen!\n\n“Where can I find the ‘Hopper’ room?”\n“Can you tell me more about FLM201?”\n“Oh, and what’s today’s vegetarian option at the Bytes Cafe?”",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents",
    "href": "src/03/slides.html#five-characteristics-of-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Planners\n\n\nAgents are driven by goals\nAnd they can put together a plan for the steps to complete that goal.\n\n“First, I will discover where course information is located”\n“Then I will search for any courses that reference FLM201”\n“Then I summarize all of the key points for the student”",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-1",
    "href": "src/03/slides.html#five-characteristics-of-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Autonomous\n\n\nAgents can then go off and execute the plan, independent of human input\nThe concept of “human in the loop” still applies for confirmation\n\ne.g. “Do you really want to place this order at the Bytes Cafe?”",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-2",
    "href": "src/03/slides.html#five-characteristics-of-agents-2",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Reactive\n\n\nAgents can change mid-course depending on what they find and/or the environment.\n\ne.g. “I couldn’t find any course information on FLM201. I’m going to check if there are other 200-level FLM courses before responding to the student.”",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-3",
    "href": "src/03/slides.html#five-characteristics-of-agents-3",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents have Persistence\n\n\nAgents often have memory systems beyond the current conversation\nBroadly classified as short and long-term memory\n\nShort-term memory could be your order request at the Bytes cafe\nLong-term memory could be your food preferences",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#five-characteristics-of-agents-4",
    "href": "src/03/slides.html#five-characteristics-of-agents-4",
    "title": "Module 3: Agents and Tools",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents can Interact with external systems\n\n\nAgents can delegate to other agents for complex tasks\n\n(Or for tasks where other agents are better suited for.)\ne.g., Campus Agent -&gt; delegating to a Course Agent\n\nAgents can also be given access to external tools\n\ne.g., File search, Web search, access to the Bytes Cafe API",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-1",
    "href": "src/03/slides.html#openai-agents-sdk-1",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK",
    "text": "OpenAI Agents SDK\n\nAnnounced in Mar 2025\n\nTogether with web search, file search, and computer use\nAnd a new Responses API (formerly Assistants API)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-2",
    "href": "src/03/slides.html#openai-agents-sdk-2",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK",
    "text": "OpenAI Agents SDK\n\nCreated to address the gap between chat completions (what we were using last week) and multi-step systems\n\nvs. building your own, which a lot of developers were doing at the time\n\nIntegrates function calling, handoffs, and session management in the same package\nSupports Python and TypeScript; MIT licensed",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#not-the-only-agent-sdk-in-town",
    "href": "src/03/slides.html#not-the-only-agent-sdk-in-town",
    "title": "Module 3: Agents and Tools",
    "section": "Not the only Agent SDK in town!",
    "text": "Not the only Agent SDK in town!\n\nSource: https://e2b.dev",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#langgraph",
    "href": "src/03/slides.html#langgraph",
    "title": "Module 3: Agents and Tools",
    "section": "LangGraph",
    "text": "LangGraph\n\nhttps://langchain-ai.github.io/langgraph/\nPython only\nMIT License\nOne of the first agent frameworks, building on LangChain\n\nIMO, too abstract/complex/bloated",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#crew.ai",
    "href": "src/03/slides.html#crew.ai",
    "title": "Module 3: Agents and Tools",
    "section": "Crew.ai",
    "text": "Crew.ai\n\nhttps://github.com/crewaiinc/crewai\nPython only\nOne of the more popular commercial offerings\n\n(Although they do have an MIT License/freemium model)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#microsoft",
    "href": "src/03/slides.html#microsoft",
    "title": "Module 3: Agents and Tools",
    "section": "Microsoft",
    "text": "Microsoft\n\nAutoGen\n\nhttps://microsoft.github.io/autogen/stable/\nPython (.NET coming soon)\nMIT License\n\nMicrosoft Semantic Kernel\n\nhttps://github.com/microsoft/semantic-kernel\nPython, .NET, Java\nMIT License",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#microsoft-1",
    "href": "src/03/slides.html#microsoft-1",
    "title": "Module 3: Agents and Tools",
    "section": "Microsoft",
    "text": "Microsoft\n\nNow converging into the Microsoft Agent Framework\n(One of the few agent SDKs to support .NET)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#agent-structure",
    "href": "src/03/slides.html#agent-structure",
    "title": "Module 3: Agents and Tools",
    "section": "Agent Structure",
    "text": "Agent Structure\n\nAgents and tools for the DigiPen Campus Agent",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#campus-agent",
    "href": "src/03/slides.html#campus-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Campus Agent",
    "text": "Campus Agent\n\n\nagent = Agent(\n    name=\"DigiPen Campus Agent\",\n    instructions=\"You are a helpful campus agent that can plan and execute tasks for students at DigiPen. Please be concise and accurate in handing off tasks to other agents as needed.\",\n    handoffs=[building_agent, course_agent, handbook_agent, cafe_agent],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#building-agent",
    "href": "src/03/slides.html#building-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Building Agent",
    "text": "Building Agent\n\n\nbuilding_agent = Agent(\n    name=\"Building Agent\",\n    instructions=\"You help students locate and provide information about buildings and rooms on campus. Be descriptive when giving locations.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=3,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#course-agent",
    "href": "src/03/slides.html#course-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Course Agent",
    "text": "Course Agent\n\n\ncourse_agent = Agent(\n    name=\"Course Agent\",\n    instructions=\"You help students find out information about courses held at DigiPen.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#handbook-agent",
    "href": "src/03/slides.html#handbook-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Handbook Agent",
    "text": "Handbook Agent\n\n\nhandbook_agent = Agent(\n    name=\"Handbook Agent\",\n    instructions=\"You help students navigate the school handbook, providing information about campus policies and student conduct.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=5,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store",
    "href": "src/03/slides.html#whats-a-vector-store",
    "title": "Module 3: Agents and Tools",
    "section": "What’s a Vector Store?",
    "text": "What’s a Vector Store?\n\nA way to provide domain-specific knowledge beyond training data\n\ne.g., current semester course information that is newer than GPT-5.2’s cutoff date\n\nWe could just insert these into the context window\n\nBut doesn’t scale to more than a few documents",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store-1",
    "href": "src/03/slides.html#whats-a-vector-store-1",
    "title": "Module 3: Agents and Tools",
    "section": "What’s a Vector Store?",
    "text": "What’s a Vector Store?\n\nInstead, we use a vector store\n\nConverts documents, paragraphs, or sentences into vector embeddings\n(Remember these from module 1? :)\n\nSimilar concepts are close to each other in vector space\n\nWhich makes it efficient to query\n\nQueries return the document (or pages within a document) that match\n\ne.g., “Michelangelo” returns “Page 1 of the floor map”",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store-2",
    "href": "src/03/slides.html#whats-a-vector-store-2",
    "title": "Module 3: Agents and Tools",
    "section": "What’s a Vector Store?",
    "text": "What’s a Vector Store?\n\nFoundation for RAG (Retrieval Augmented Generation)\n\n(Which we will cover in module 6)\n\nMany different types of vectors stores/databases\nFor now, we will be using the OpenAI built in one",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#whats-a-vector-store-3",
    "href": "src/03/slides.html#whats-a-vector-store-3",
    "title": "Module 3: Agents and Tools",
    "section": "What’s a Vector Store?",
    "text": "What’s a Vector Store?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#agent-structure-1",
    "href": "src/03/slides.html#agent-structure-1",
    "title": "Module 3: Agents and Tools",
    "section": "Agent Structure",
    "text": "Agent Structure\n\nAgents and tools for the DigiPen Campus Agent",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-do-agents-need-tools",
    "href": "src/03/slides.html#why-do-agents-need-tools",
    "title": "Module 3: Agents and Tools",
    "section": "Why Do Agents Need Tools?",
    "text": "Why Do Agents Need Tools?\n\nThe scope of the agent’s ability is contained within the model\nTools enable the agent to reach out to systems beyond the model\nExamples\n\nRead a file from disk or search the web (built in)\nCalculator (because LLMs aren’t great at math)\nCode interpreter (running code on the fly)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-tool-calling",
    "href": "src/03/slides.html#openai-tool-calling",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Tool Calling",
    "text": "OpenAI Tool Calling\n\nIntroduced by OpenAI in June 2023\nOriginally called Function Calling\nModels are fine-tuned to return a structured function_call JSON object, specifying which function to call and with what arguments.\nTools are provided as functions\nOption for the LLM to decide when to call the tool (always, never, auto)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#cafe-agent",
    "href": "src/03/slides.html#cafe-agent",
    "title": "Module 3: Agents and Tools",
    "section": "Cafe Agent",
    "text": "Cafe Agent\n\n\ncafe_agent = Agent(\n    name=\"Cafe Agent\",\n    instructions=\"You help students locate and provide information about the Bytes Cafe.\",\n    tools=[\n        get_bytes_cafe_menu,\n    ],\n)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#cafe-agent-tool",
    "href": "src/03/slides.html#cafe-agent-tool",
    "title": "Module 3: Agents and Tools",
    "section": "Cafe Agent Tool",
    "text": "Cafe Agent Tool\n\n\nfrom agents import function_tool\n\n@function_tool\ndef get_bytes_cafe_menu(date: str) -&gt; any:\n    return {\n        f\"{date}\": {\n            \"daily byte\": {\n                \"name\": \"Steak Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Flank steak, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"vegetarian\": {\n                \"name\": \"Impossible Quesadilla\",\n                \"price\": 12,\n                \"description\": \"Impossible plant based product, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n            \"international\": {\n                \"name\": \"Chicken Curry\",\n                \"price\": 12,\n                \"description\": \"Chicken thighs, onion, carrot, potato, curry sauce served over rice\",\n            },\n        }\n    }",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n\nHow do models know when they should call a tool?\n\nModels are fine-tuned on conversations with tool call examples\nThe model learns patterns like “when the user asks about the weather, call the get_weather tool”\nThe request to call the tool is returned as a JSON payload",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-1",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-1",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n{\n  \"role\": \"assistant\",\n  \"content\": null,\n  \"tool_calls\": [\n    {\n      \"id\": \"call_abc123\",\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_weather\",\n        \"arguments\": \"{\\\"location\\\": \\\"San Francisco\\\", \\\"unit\\\": \\\"celsius\\\"}\"\n      }\n    }\n  ]\n}",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-2",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-2",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n\nThe client then calls the tool with the required parameters\nAnd returns the result back to the model as a “tool” role message",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-3",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-3",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n{\n  \"role\": \"tool\",\n  \"tool_call_id\": \"call_abc123\",\n  \"content\": \"{\\\"temperature\\\": 18, \\\"condition\\\": \\\"partly cloudy\\\", \\\"humidity\\\": 65, \\\"wind_speed\\\": 12}\"\n}",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-4",
    "href": "src/03/slides.html#sidebar-fine-tuning-models-for-tools-4",
    "title": "Module 3: Agents and Tools",
    "section": "Sidebar: Fine-tuning models for Tools",
    "text": "Sidebar: Fine-tuning models for Tools\n\nRLHF is used to improve the accuracy for tool selection\n\nRewards are given for correctly choosing the right tool for a task\nOr penalized for hallucinating tools and methods that don’t exist",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#lets-run-the-campus-agent-1",
    "href": "src/03/slides.html#lets-run-the-campus-agent-1",
    "title": "Module 3: Agents and Tools",
    "section": "Let’s Run the Campus Agent",
    "text": "Let’s Run the Campus Agent\n\nDoes the OpenAI Agents SDK work with OpenRouter?\n\nYes and No :)\n\nYes to core functionality\n\nCreating an agent, handoffs, calling custom tools\n\nNo to calling built-in OpenAI tools\n\nFile search, Web search, Code interpreter",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#lets-run-the-campus-agent-2",
    "href": "src/03/slides.html#lets-run-the-campus-agent-2",
    "title": "Module 3: Agents and Tools",
    "section": "Let’s Run the Campus Agent",
    "text": "Let’s Run the Campus Agent\n\nWe’ll need to create an OpenAI developer account\nPotentially add some credits to it",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-multiple-agents",
    "href": "src/03/slides.html#why-multiple-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Why Multiple Agents?",
    "text": "Why Multiple Agents?\n\nContext window limitations\nEach agent can have a different system prompt (instructions)\nMakes tool separation cleaner and more accurate\nEach agent can have a different underlying model\n\nSpecialized models (e.g., a vision encoder)\nOr to blend cost",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#why-multiple-agents-1",
    "href": "src/03/slides.html#why-multiple-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Why Multiple Agents?",
    "text": "Why Multiple Agents?\n\nCost considerations are really important with agents\n\nLarge number of tokens for reasoning\nHandoffs with multiple API calls\nEven more tokens with verbose tool calls\n\nMitigations\n\nDoes every agent need full GPT / frontier model capability?\nAgents doing primarily function calling (e.g., file search) can be much smaller/cheaper",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#example-of-multiple-agents",
    "href": "src/03/slides.html#example-of-multiple-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Example of Multiple Agents",
    "text": "Example of Multiple Agents\n\nCode generation\n\nAgents for ‘architect’, code writer, tester, debugger, etc.\n\nContent generation\n\nAgent to create content, other agents to generate images, translate content, etc.\n\nTravel booking\n\nAgent to book flights, hotels, cars, etc. for packages",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#patterns-for-agents",
    "href": "src/03/slides.html#patterns-for-agents",
    "title": "Module 3: Agents and Tools",
    "section": "Patterns for Agents",
    "text": "Patterns for Agents\n\nAs you get deeper into building agents, patterns start to emerge\n\nRouter (which is what we used in our demo) - hand off of tasks\nOrchestrator (using other agents as tools)\nParallel agents (calling other agents in parallel and aggregating results)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#patterns-for-agents-1",
    "href": "src/03/slides.html#patterns-for-agents-1",
    "title": "Module 3: Agents and Tools",
    "section": "Patterns for Agents",
    "text": "Patterns for Agents\n\nSource: https://www.anthropic.com/engineering/building-effective-agents",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#when-things-go-wrong-1",
    "href": "src/03/slides.html#when-things-go-wrong-1",
    "title": "Module 3: Agents and Tools",
    "section": "When Things Go Wrong",
    "text": "When Things Go Wrong\n\nAgents can be difficult to debug\n\nIncorrect handoffs\nInfinite loops can be common\nFailed to call the right tool at the right time",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#when-things-go-wrong-2",
    "href": "src/03/slides.html#when-things-go-wrong-2",
    "title": "Module 3: Agents and Tools",
    "section": "When Things Go Wrong",
    "text": "When Things Go Wrong\n\nOpenAI Agents SDK includes built-in tracing\n\nActually enabled by default!\n\nComprehensive record of:\n\ngenerations\ntool calls\nhandoffs\nguardrails\ncustom events",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#the-need-for-memory",
    "href": "src/03/slides.html#the-need-for-memory",
    "title": "Module 3: Agents and Tools",
    "section": "The Need for Memory",
    "text": "The Need for Memory\n\nJust like API calls, agents need the conversation/context every call\nThis can be challenging with agents working on long-running tasks\n\nAnd/or agents working on multiple threads with other agents\n\nShort-term and long-term memory",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory",
    "href": "src/03/slides.html#short-term-memory",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\nUsed to store/retreive the current conversation thread\nBuilt-in to most SDKs\nIn OpenAI Agents SDK called a session",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-1",
    "href": "src/03/slides.html#short-term-memory-1",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nfrom agents import Agent, Runner, SQLiteSession\n\nagent = Agent(name=\"Assistant\", instructions=\"Reply very concisely\")\nsession = SQLiteSession(\"conv_123\", db_path=SQLITE_DB)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-2",
    "href": "src/03/slides.html#short-term-memory-2",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nresult = await Runner.run(agent, \"My name is Simon\", session=session)\nprint(result.final_output)\n\nNice to meet you, Simon!",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-3",
    "href": "src/03/slides.html#short-term-memory-3",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nresult = await Runner.run(agent, \"What is my name?\", session=session)\nprint(result.final_output)\n\nYour name is Simon.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#short-term-memory-4",
    "href": "src/03/slides.html#short-term-memory-4",
    "title": "Module 3: Agents and Tools",
    "section": "Short-term Memory",
    "text": "Short-term Memory\n\n\nresult = await Runner.run(agent, \"What is my name?\")\nprint(result.final_output)\n\nYou haven’t shared your name yet.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#long-term-memory",
    "href": "src/03/slides.html#long-term-memory",
    "title": "Module 3: Agents and Tools",
    "section": "Long-term Memory",
    "text": "Long-term Memory\n\nMore challenging\nYou don’t want to store/retrieve the entire conversations\nLong-term memory types\n\nFactual: General facts (e.g., name, address, seating preferences)\nEpisodic: Past conversations (e.g., user booked a trip to Paris)\nProcedural: Learnings (e.g., the best hotel site to book accommodation)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#implementing-long-term-memory",
    "href": "src/03/slides.html#implementing-long-term-memory",
    "title": "Module 3: Agents and Tools",
    "section": "Implementing Long-term Memory",
    "text": "Implementing Long-term Memory\n\nLots of startup options!\n\nSupermemory\nLetta\nmem0\n…and lots more",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#implementing-long-term-memory-mem0",
    "href": "src/03/slides.html#implementing-long-term-memory-mem0",
    "title": "Module 3: Agents and Tools",
    "section": "Implementing Long-term Memory (mem0)",
    "text": "Implementing Long-term Memory (mem0)\nfrom mem0 import Memory\nmemory = Memory()\n\n# Create new memories from the conversation\nmessages.append({\"role\": \"assistant\", \"content\": assistant_response})\nmemory.add(messages, user_id=user_id)\n\n# Retrieve relevant memories\nrelevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n# (append these to the system prompt)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#implementing-long-term-memory-1",
    "href": "src/03/slides.html#implementing-long-term-memory-1",
    "title": "Module 3: Agents and Tools",
    "section": "Implementing Long-term Memory",
    "text": "Implementing Long-term Memory\n\nOr “roll your own”\nLong-term memory types\n\nFactual: General facts (e.g., name, address, seating preferences)\nEpisodic: Past conversations (e.g., user booked a trip to Paris)\nProcedural: Learnings (e.g., the best hotel site to book accomodation)\n\nCreate tools for factual storage\n\ne.g., a profile tool with set/get options\n\nUse LLM to summarize short-term session conversations to store episodic and procedural learnings.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#beyond-tool-calling-1",
    "href": "src/03/slides.html#beyond-tool-calling-1",
    "title": "Module 3: Agents and Tools",
    "section": "Beyond Tool Calling",
    "text": "Beyond Tool Calling\n\nTool calling is super useful, but…\n\nYou need to write the function(s) yourself\nAnd then expose them to OpenAI using the @function_tool method\n\nWhat if there was a way to standardize this?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol",
    "href": "src/03/slides.html#mcp-model-context-protocol",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)\n\nReleased by Anthropic in Nov 2024\nProvides a standard interface for tools - akin to a USB standard for peripherals\nImplementations are known as “MCP servers”\n\nA server exposes one or more tools (functions)\nUses JSON-RPC 2.0 as underlying RPC protocol\nServers can run remotely over HTTP (supports SSE)\nOr can be hosted locally and accessed via stdio\nMany servers hosted using Node.js",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol-1",
    "href": "src/03/slides.html#mcp-model-context-protocol-1",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol-2",
    "href": "src/03/slides.html#mcp-model-context-protocol-2",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#mcp-model-context-protocol-3",
    "href": "src/03/slides.html#mcp-model-context-protocol-3",
    "title": "Module 3: Agents and Tools",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-and-mcp-1",
    "href": "src/03/slides.html#openai-agents-sdk-and-mcp-1",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK and MCP",
    "text": "OpenAI Agents SDK and MCP\n\nMCP supported in OpenAI Agents SDK (as of Sep 2025)\nExposes MCPServerStdio and MCPServerSse to connect to local and remote servers",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-and-mcp-2",
    "href": "src/03/slides.html#openai-agents-sdk-and-mcp-2",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK and MCP",
    "text": "OpenAI Agents SDK and MCP\n\n\nasync with MCPServerStdio(\n    name=\"Open-Meteo Weather Server\",\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"open-meteo-mcp-server\"],\n    },\n) as server:\n    tools = await server.list_tools()\n    print(f\"Available tools: {[tool.name for tool in tools]}\")\n\nAvailable tools: ['weather_forecast', 'weather_archive', 'air_quality', 'marine_weather', 'elevation', 'flood_forecast', 'seasonal_forecast', 'climate_projection', 'ensemble_forecast', 'geocoding', 'dwd_icon_forecast', 'gfs_forecast', 'meteofrance_forecast', 'ecmwf_forecast', 'jma_forecast', 'metno_forecast', 'gem_forecast']",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#openai-agents-sdk-and-mcp-3",
    "href": "src/03/slides.html#openai-agents-sdk-and-mcp-3",
    "title": "Module 3: Agents and Tools",
    "section": "OpenAI Agents SDK and MCP",
    "text": "OpenAI Agents SDK and MCP\n\n\nasync with MCPServerStdio(\n    name=\"Open-Meteo Weather Server\",\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"open-meteo-mcp-server\"],\n    },\n) as server:\n    agent = Agent(\n        name=\"Weather Agent\",\n        model=\"gpt-5.2\",\n        instructions=\"You are a helpful weather assistant. Use the available tools to answer questions about weather forecasts, historical weather data, and air quality. Always provide clear, concise answers.\",\n        mcp_servers=[server],\n    )\n    \n    result = await Runner.run(agent, \"What's the weather forecast for Seattle this week?\")\n    print(result.final_output)\n\nHere is the weather forecast for Seattle this week:\n\n- Maximum temperatures will range from about 8°C to 12°C.\n- Minimum temperatures will be between 3°C and 5°C.\n- Dry weather is expected all week, with no significant precipitation.\n- Most days will be partly cloudy to clear, with occasional cloudier periods.\n\nIf you need a forecast in Fahrenheit or more details (such as hourly data), just ask!",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#creating-your-own-mcp-server",
    "href": "src/03/slides.html#creating-your-own-mcp-server",
    "title": "Module 3: Agents and Tools",
    "section": "Creating Your Own MCP Server",
    "text": "Creating Your Own MCP Server\n\nMultiple SDKs on https://modelcontextprotocol.io/docs/sdk\n\nPython, TypeScript, Go, Rust, C#, and more\n\nVery similar to tool calling\n\nDefine your MCP server\nAnnotate your functions with @mcp.tool()\nAdd descriptions to the tool methods to help the LLM select which tool to call",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#example-microbit-mcp-server",
    "href": "src/03/slides.html#example-microbit-mcp-server",
    "title": "Module 3: Agents and Tools",
    "section": "Example: micro:bit MCP Server",
    "text": "Example: micro:bit MCP Server\n\n\nhttps://simonguest.com/p/microbit-mcp/\nhttps://simonguest.com/p/microbit-mcp/",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#hugging-face-spaces-1",
    "href": "src/03/slides.html#hugging-face-spaces-1",
    "title": "Module 3: Agents and Tools",
    "section": "Hugging Face Spaces",
    "text": "Hugging Face Spaces\n\nWe’ve been using Gradio, but hosting via notebooks isn’t ideal\n\nEven with share=True you have to keep the notebook running\n\nWouldn’t it be nice if we could easily host our Gradio app?",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#hugging-face-spaces-2",
    "href": "src/03/slides.html#hugging-face-spaces-2",
    "title": "Module 3: Agents and Tools",
    "section": "Hugging Face Spaces",
    "text": "Hugging Face Spaces",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#hugging-face-spaces-3",
    "href": "src/03/slides.html#hugging-face-spaces-3",
    "title": "Module 3: Agents and Tools",
    "section": "Hugging Face Spaces",
    "text": "Hugging Face Spaces\n\nFree cloud hosting for ML demos and applications\n\nSupports Gradio, Streamlit, and static HTML/JS\n\nFor Gradio, either upload a main.py or a Docker configuration file\nHugging Face handles resource allocation\n\nSleeps the space if it’s inactive\nIntegrates with the queuing mechanism of Gradio to batch requests\nSupports multiple GPU types (if signed up for Pro account)",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#looking-ahead-1",
    "href": "src/03/slides.html#looking-ahead-1",
    "title": "Module 3: Agents and Tools",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nLeave text behind and explore image-based models!\nIntroduce the diffuser\nAnd go other way with vision encoders",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/slides.html#references-1",
    "href": "src/03/slides.html#references-1",
    "title": "Module 3: Agents and Tools",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/assignment.html",
    "href": "src/03/assignment.html",
    "title": "Module 3 Assignment: Extend the Campus Agent",
    "section": "",
    "text": "Objective: Extend the DigiPen Campus Agent by adding a new specialized agent that handles a specific domain.\nBackground:\nThe Campus Agent currently has four specialized agents: Building Agent, Course Agent, Handbook Agent, and Cafe Agent. Your task is to add a fifth agent that serves a new purpose on campus.\nSuggested Agent Ideas (pick one, or propose your own):\n\nEvents Agent - Helps students find information about campus events, club meetings, and activities\nIT Agent - Assists with common IT issues, lab software, and printing\nLibrary Agent - Helps students find resources, reserve study rooms, or check hours\nCareer Services Agent - Provides information about internships, resume reviews, and career fairs\nTransportation Agent - Helps with parking, shuttle schedules, and commute options\n\nRequirements:\n\nCreate a new agent with appropriate instructions (system prompt)\nImplement knowledge retrieval using one of these approaches:\n\nAdd documents to the vector store and use FileSearchTool, OR\nCreate a custom tool using @function_tool that returns relevant data OR\nCreate a new (or select an existing) MCP server\n\nIntegrate with the main Campus Agent via handoff\nTest your agent with at least 3 different queries and document the results\nUse the OpenAI Traces Dashboard to debug at least one interaction and include a screenshot or description of what you observed\n\nDeliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation (building on the campus-agent.ipynb starter)\nUses OPENAI_API_KEY for the API token. (Please do not include your API key in your notebook!)\nMarkdown cells explaining:\n\nWhat agent you chose and why\nHow you implemented knowledge retrieval (vector store vs. custom tool)\nYour test queries and the agent’s responses\nWhat you learned from the Traces Dashboard (what worked, what didn’t, any debugging insights)\n\n\nBonus:\nDeploy your extended Campus Agent to Hugging Face Spaces and include the URL in your submission. Remember to add your OPENAI_API_KEY as a secret in your Space settings (not in the code!).\nHints:\n\nStart by copying the campus-agent.ipynb notebook and modifying it\nKeep your agent’s scope focused - a narrower domain with good data works better than a broad domain with sparse data\nIf using the vector store, you can upload files via the OpenAI Platform Storage\nIf using a custom tool, remember that the function’s docstring helps the LLM understand when to call it",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Assignment"
    ]
  },
  {
    "objectID": "src/02/resources.html",
    "href": "src/02/resources.html",
    "title": "Resources",
    "section": "",
    "text": "GPT-2 Release Blog Post - OpenAI’s original GPT-2 announcement\nGPT-3 Paper - “Language Models are Few-Shot Learners”\nInstructGPT Paper - “Training language models to follow instructions with human feedback”\nMeta’s Llama Models - Official Llama model family page",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#model-history-and-background",
    "href": "src/02/resources.html#model-history-and-background",
    "title": "Resources",
    "section": "",
    "text": "GPT-2 Release Blog Post - OpenAI’s original GPT-2 announcement\nGPT-3 Paper - “Language Models are Few-Shot Learners”\nInstructGPT Paper - “Training language models to follow instructions with human feedback”\nMeta’s Llama Models - Official Llama model family page",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#chat-templates-and-model-formats",
    "href": "src/02/resources.html#chat-templates-and-model-formats",
    "title": "Resources",
    "section": "Chat Templates and Model Formats",
    "text": "Chat Templates and Model Formats\n\nHugging Face Chat Templates Guide - Comprehensive guide to chat templates",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#openai-api",
    "href": "src/02/resources.html#openai-api",
    "title": "Resources",
    "section": "OpenAI API",
    "text": "OpenAI API\n\nOpenAI Platform - Create an account and get API keys\nOpenAI API Documentation - Official API reference\nChat Completions Guide - How to use the chat completions endpoint\nStructured Outputs Guide - Guide to structured outputs",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#openrouter",
    "href": "src/02/resources.html#openrouter",
    "title": "Resources",
    "section": "OpenRouter",
    "text": "OpenRouter\n\nOpenRouter Home Page - Unified API for hundreds of models\nOpenRouter Documentation - API docs and model listings\nOpenRouter Models - Browse available models and pricing",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#server-sent-events-sse-and-streaming",
    "href": "src/02/resources.html#server-sent-events-sse-and-streaming",
    "title": "Resources",
    "section": "Server-Sent Events (SSE) and Streaming",
    "text": "Server-Sent Events (SSE) and Streaming\n\nMDN: Server-Sent Events - Technical overview of SSE\nEventSource API - Browser API for SSE\nOpenAI Streaming Guide - How to implement streaming with OpenAI API",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#gradio",
    "href": "src/02/resources.html#gradio",
    "title": "Resources",
    "section": "Gradio",
    "text": "Gradio\n\nGradio Home Page\nGradio Documentation - Official documentation\nGradio ChatInterface - Chat interface component documentation\nGradio Guides - Tutorials and examples\nHugging Face Spaces - Platform for deploying Gradio apps",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#rlhf-and-model-training",
    "href": "src/02/resources.html#rlhf-and-model-training",
    "title": "Resources",
    "section": "RLHF and Model Training",
    "text": "RLHF and Model Training\n\nRLHF Explainer - Hugging Face’s comprehensive guide to RLHF\nReinforcement Learning from Human Feedback Paper - Original RLHF research",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#context-windows-and-token-management",
    "href": "src/02/resources.html#context-windows-and-token-management",
    "title": "Resources",
    "section": "Context Windows and Token Management",
    "text": "Context Windows and Token Management\n\nUnderstanding Context Windows - Anthropic’s post on long context\nToken Counting Best Practices - OpenAI cookbook example\nTiktoken - Token counting library for estimating costs",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/resources.html#citations",
    "href": "src/02/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/slides.html#module-objectives",
    "href": "src/01/slides.html#module-objectives",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nExplore the history of vector embeddings and tokenization\nUnderstand the transformer architecture at a high level\nUse our first transformer to translate language\nCover a brief history of early generative transformers\nSetup and use Colab, and become familiar with the basics of notebooks and Python (if you haven’t used them already)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#rewind-to-2013",
    "href": "src/01/slides.html#rewind-to-2013",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Rewind To 2013",
    "text": "Rewind To 2013\n\nNLP (Natural Language Processing) was the thing!\n\nSentiment analysis, named entity recognition, parsing, etc.\n\nBut, you had limited options…\n\nOne-hot encoding\nHand crafted features\nNeural language models",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#word2vec-released",
    "href": "src/01/slides.html#word2vec-released",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2013: Word2Vec Released",
    "text": "2013: Word2Vec Released\n\nWord2Vec introduced by Mikolov and colleagues at Google Research in two papers\n\nSkip-gram and Continuous Bag-of-Words (CBOW) (Mikolov, Chen, et al. 2013)\nNegative sampling and subsampling techniques (Mikolov, Sutskever, et al. 2013)\n\nParadigm shift from count-based methods\n\nUsed Neural Networks (NNs) to predict words vs. large matrices\n\nFoundation for modern NLP tasks",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work",
    "href": "src/01/slides.html#how-does-word2vec-work",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\nWord Embeddings are meaningful numerical representations of words\n\nRepresentations where words are encoded into multi-dimensional space\nLarge number of dimensions (200-500 is typical)\nSimilar words have similar numbers",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-1",
    "href": "src/01/slides.html#how-does-word2vec-work-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"cat\"\nvector = model[word]\nvector[:10]\n\narray([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656],\n      dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-2",
    "href": "src/01/slides.html#how-does-word2vec-work-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"dog\"\nvector = model[word]\nvector[:10]\n\narray([ 0.05126953, -0.02233887, -0.17285156,  0.16113281, -0.08447266,\n        0.05737305,  0.05859375, -0.08251953, -0.01538086, -0.06347656],\n      dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-3",
    "href": "src/01/slides.html#how-does-word2vec-work-3",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nword = \"pizza\"\nvector = model[word]\nvector[:10]\n\narray([-1.2597656e-01,  2.5390625e-02,  1.6699219e-01,  5.5078125e-01,\n       -7.6660156e-02,  1.2890625e-01,  1.0253906e-01, -3.9482117e-04,\n        1.2158203e-01,  4.3212891e-02], dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#why-do-this",
    "href": "src/01/slides.html#why-do-this",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Why Do This?",
    "text": "Why Do This?\n\nMapping words to multi-dimensional vectors enables\n\nTest for similarity\nCompute similarity\nPerform vector arithmetic\nExplore sets of words through visualizations",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-4",
    "href": "src/01/slides.html#how-does-word2vec-work-4",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nfind_similar_words(\"cat\")\nfind_similar_words(\"dog\")\nfind_similar_words(\"pizza\")\n\n\nWords most similar to 'cat':\n----------------------------------------\ncats                 | similarity: 0.8099\ndog                  | similarity: 0.7609\nkitten               | similarity: 0.7465\nfeline               | similarity: 0.7326\nbeagle               | similarity: 0.7151\npuppy                | similarity: 0.7075\npup                  | similarity: 0.6934\npet                  | similarity: 0.6892\nfelines              | similarity: 0.6756\nchihuahua            | similarity: 0.6710\n\nWords most similar to 'dog':\n----------------------------------------\ndogs                 | similarity: 0.8680\npuppy                | similarity: 0.8106\npit_bull             | similarity: 0.7804\npooch                | similarity: 0.7627\ncat                  | similarity: 0.7609\ngolden_retriever     | similarity: 0.7501\nGerman_shepherd      | similarity: 0.7465\nRottweiler           | similarity: 0.7438\nbeagle               | similarity: 0.7419\npup                  | similarity: 0.7407\n\nWords most similar to 'pizza':\n----------------------------------------\npizzas               | similarity: 0.7863\nDomino_pizza         | similarity: 0.7343\nPizza                | similarity: 0.6988\npepperoni_pizza      | similarity: 0.6903\nsandwich             | similarity: 0.6840\nburger               | similarity: 0.6570\nsandwiches           | similarity: 0.6495\ntakeout_pizza        | similarity: 0.6492\ngourmet_pizza        | similarity: 0.6401\nmeatball_sandwich    | similarity: 0.6377",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-5",
    "href": "src/01/slides.html#how-does-word2vec-work-5",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\ncompute_similarity('cat', 'dog')\ncompute_similarity('cat', 'kitten')\ncompute_similarity('cat', 'car')\ncompute_similarity('doctor', 'hospital')\ncompute_similarity('king', 'queen')\n\nSimilarity between 'cat' and 'dog': 0.7609\nSimilarity between 'cat' and 'kitten': 0.7465\nSimilarity between 'cat' and 'car': 0.2153\nSimilarity between 'doctor' and 'hospital': 0.5143\nSimilarity between 'king' and 'queen': 0.6511",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-6",
    "href": "src/01/slides.html#how-does-word2vec-work-6",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?\n\n\nvector_arithmetic(['king', 'woman'], ['man'])\nvector_arithmetic(['Paris', 'Italy'], ['France'])\nvector_arithmetic(['walking', 'swim'], ['walk'])\n\n\nking + woman - man:\n--------------------------------------------------\nqueen                | similarity: 0.7118\nmonarch              | similarity: 0.6190\nprincess             | similarity: 0.5902\ncrown_prince         | similarity: 0.5499\nprince               | similarity: 0.5377\n\nParis + Italy - France:\n--------------------------------------------------\nMilan                | similarity: 0.7222\nRome                 | similarity: 0.7028\nPalermo_Sicily       | similarity: 0.5968\nItalian              | similarity: 0.5911\nTuscany              | similarity: 0.5633\n\nwalking + swim - walk:\n--------------------------------------------------\nswimming             | similarity: 0.8246\nswam                 | similarity: 0.6807\nswims                | similarity: 0.6538\nswimmers             | similarity: 0.6495\npaddling             | similarity: 0.6424",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-word2vec-work-7",
    "href": "src/01/slides.html#how-does-word2vec-work-7",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How does Word2Vec Work?",
    "text": "How does Word2Vec Work?",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-python",
    "href": "src/01/slides.html#what-is-python",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is Python?",
    "text": "What is Python?\n\nInterpreted language (vs. compiled like C++ or C#)\n\nNo compilation step - code runs directly\nInteractive and flexible, great for experimentation\n\nCreated by Guido van Rossum in 1991\n\nPython 2 (2000-2020), Python 3 (2008-present)\nWe’ll use Python 3.13\n\nCross-platform: Runs on Windows, macOS, Linux\nDynamically typed: No need to declare variable types\nThe language of AI/ML: Vast ecosystem of libraries (NumPy, TensorFlow, PyTorch, Transformers)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#variables-and-data-types",
    "href": "src/01/slides.html#variables-and-data-types",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\n\nVariables store data (no type declaration needed)\n\nx = 42 (integer)\nname = \"Alice\" (string)\npi = 3.14 (float)\n\nLists hold multiple values\n\nnumbers = [1, 2, 3, 4, 5]\nwords = [\"cat\", \"dog\", \"bird\"]\n\nAccess with square brackets: numbers[0] returns 1",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#functions",
    "href": "src/01/slides.html#functions",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Functions",
    "text": "Functions\n\nFunctions perform actions\n\nBuilt-in: print(\"Hello\"), len([1, 2, 3])\nDefine your own: def greet(name): return f\"Hello {name}\"\nIndentation vs. braces\nSupport for classes (although used rarely in AI/ML)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#libraries-and-packages",
    "href": "src/01/slides.html#libraries-and-packages",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Libraries and Packages",
    "text": "Libraries and Packages\n\nLibraries extend Python’s capabilities\n\nimport math - mathematical functions\nfrom transformers import AutoModel - import specific components\n\nUse dot notation to access: math.sqrt(16)\nPackage management\n\npip - standard package installer (similar to NuGet for C#)\nuv - modern, faster alternative to pip\nPyPI (Python Package Index) - central repository with 500K+ packages",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-notebook",
    "href": "src/01/slides.html#what-is-a-notebook",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is a Notebook?",
    "text": "What is a Notebook?\n\nAn interactive document that combines:\n\nLive code that can be executed\nRich text explanations (markdown)\nVisualizations and outputs\n\nThink of it as a computational narrative\n\nTell a story with code, data, and explanations\n\nOriginally designed for data science and research\nAlso used for learning, experimenting, and sharing results",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#a-brief-history-of-notebooks",
    "href": "src/01/slides.html#a-brief-history-of-notebooks",
    "title": "Module 1: Foundations of Generative AI",
    "section": "A Brief History of Notebooks",
    "text": "A Brief History of Notebooks\n\n2011: IPython Notebook project begins\n\nInteractive Python shell → web-based notebook\n\n2014: Renamed to Jupyter (Julia, Python, R)\n\nNow supports 40+ programming languages\nPython is most popular by far\n\n2017: Google launches Colab\n\nFree cloud-based Jupyter notebooks\nFree access to GPUs and TPUs\n\nToday: Industry standard for ML/AI development",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#anatomy-of-a-python-notebook",
    "href": "src/01/slides.html#anatomy-of-a-python-notebook",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Anatomy of a Python Notebook",
    "text": "Anatomy of a Python Notebook\n\nFormat: Extension is .ipynb\n\nJSON format, using Jupyter Document Schema\n\nCells: Building blocks of notebooks\n\nCode cells: Executable Python code\nMarkdown cells: Text, headings, images, equations\n\nKernel: The computational engine running your code\n\nMaintains state between cell executions\n\nOutputs: Results appear directly below code cells\n\nText, tables, plots, interactive widgets",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-to-run-notebooks",
    "href": "src/01/slides.html#how-to-run-notebooks",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How to Run Notebooks",
    "text": "How to Run Notebooks\n\nJupyter Notebook Server (Classic approach)\n\nWeb interface on localhost\n\nVS Code (Local development)\n\nJupyter extension for VS Code\nRun on your own machine\n\nGoogle Colab (Recommended)\n\nBrowser-based, no installation needed\nFree(-ish) GPU access\nCan also access local GPU",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#why-recommend-google-colab",
    "href": "src/01/slides.html#why-recommend-google-colab",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Why Recommend Google Colab?",
    "text": "Why Recommend Google Colab?\n\nAccess to GPUs and TPUs for AI-based tasks\n\ne.g., A100 and H100 with 40Gb/80Gb VRAM\n\nModel downloaded between cloud vendors\n\nvs. downloading large models via the DigiPen network\n\nMany libraries pre-installed\nEasy to share notebooks with others\nGenerous (free) GPU limits for students!",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#challenges-with-word-embeddings-1",
    "href": "src/01/slides.html#challenges-with-word-embeddings-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Challenges with Word Embeddings",
    "text": "Challenges with Word Embeddings\n\nLarge vocabularies\n\n100K+ words\nAnd not particularly friendly to non-English vocabularies\n\nLittle representation between certain words\n\n“Run” and “Running” should be related\n\nLack of context\n\nEmbedding for the word “bank” is the same, regardless of context\nRiver bank != Savings bank",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#challenges-with-word-embeddings-2",
    "href": "src/01/slides.html#challenges-with-word-embeddings-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Challenges with Word Embeddings",
    "text": "Challenges with Word Embeddings\n\nSome researchers tried character-level models\n\nSmall vocabulary (26 letters + punctuation for English)\nBut very long sequences\nAnd hard to extract meaning",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#byte-pair-encoding-bpe",
    "href": "src/01/slides.html#byte-pair-encoding-bpe",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2016: Byte Pair Encoding (BPE)",
    "text": "2016: Byte Pair Encoding (BPE)\n\nOriginally developed in 1994 as a simple compression algorithm (Gage 1994)\n\nFrequent pairs of adjacent bytes represented as a single byte\n\nIn 2016, adapted to neural machine translation (Sennrich, Haddow, and Birch 2016)\n\nApplied BPE to break words into subword units for better handling of rare words",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#byte-pair-encoding-bpe-1",
    "href": "src/01/slides.html#byte-pair-encoding-bpe-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2016: Byte Pair Encoding (BPE)",
    "text": "2016: Byte Pair Encoding (BPE)\n\nBreaks words into frequent subword units (a.k.a. tokens)\n\n“unbelievable” → [“un”, “believ”, “able”]\n\nBalance between word level (large vocab) and character level (long sequences)\n\nSupports related words: [“Run”] and [“Run”, “ning”]\n30-50K tokens vs. 100K, and works well for non-English languages\n\nFoundations of today’s tokenization\n\nAPI costs are measured in tokens\nDifferent models use different tokenizers",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#search-for-context",
    "href": "src/01/slides.html#search-for-context",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Search for Context",
    "text": "Search for Context\n\nBPE provided efficiency and representation between words\nBut still didn’t solve context\n\ne.g., the River bank != Savings bank problem\n\nResearchers working on “attention tasks” using Recurrent Neural Networks (RNNs)\n\nBahdanau et al. introduce attention for translation (Bahdanau, Cho, and Bengio 2015)\nShowed that focusing on relevant parts of input improved translation quality",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#attention-is-all-you-need",
    "href": "src/01/slides.html#attention-is-all-you-need",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2017: “Attention is all you need”",
    "text": "2017: “Attention is all you need”\n\nGoogle researchers publish “Attention is all you need” (Vaswani et al. 2017)\n\nIntroduced the Transformer a novel Neural Network (NN) architecture, eliminating the need for RNNs for sequence-to-sequence models\nUsed BPE tokenization, and creates contextual embeddings during training process\nAttention mechanism allows the model to weigh the importance of words in a sequence\nAchieved State Of The Art (SOTA) performance on language translation, while also being faster to train",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-1",
    "href": "src/01/slides.html#introducing-the-transformer-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    Transformer[Transformer]\n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Transformer --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example",
    "href": "src/01/slides.html#example",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"Helsinki-NLP/opus-mt-fr-en\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-1",
    "href": "src/01/slides.html#example-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nfrench_text = \"Bonjour, comment allez-vous?\"\ninput_ids = tokenizer.encode(french_text, return_tensors=\"pt\")\nprint(input_ids[0])\nprint(\"Tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n\ntensor([8703,    2, 1027, 5682,   21,  682,   54,    0])\nTokens: ['▁Bonjour', ',', '▁comment', '▁allez', '-', 'vous', '?', '&lt;/s&gt;']",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-2",
    "href": "src/01/slides.html#example-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\noutput_ids = model.generate(input_ids)\nprint(output_ids)\n\ntensor([[59513, 10537,     2,   541,    52,    55,    54,     0]])",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-3",
    "href": "src/01/slides.html#example-3",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nenglish_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(\"Translation:\", english_text)\n\nTranslation: Hello, how are you?",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-2",
    "href": "src/01/slides.html#introducing-the-transformer-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    Transformer[Transformer]\n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Transformer --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-3",
    "href": "src/01/slides.html#introducing-the-transformer-3",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        Encoder[Encoder]\n        Decoder[Decoder]\n        Encoder --&gt; Decoder\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; Encoder\n    Decoder --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-4",
    "href": "src/01/slides.html#introducing-the-transformer-4",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        direction LR\n        subgraph \"Encoder Stack (N layers)\"\n            E[Encoder&lt;br/&gt;Layers&lt;br/&gt;1...N]\n        end\n        \n        subgraph \"Decoder Stack (N layers)\"\n            D[Decoder&lt;br/&gt;Layers&lt;br/&gt;1...N]\n        end\n        \n        E -.-&gt;|Context| D\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; Tokenize --&gt; E\n    D --&gt; Decode --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#how-does-the-encoderdecoder-work",
    "href": "src/01/slides.html#how-does-the-encoderdecoder-work",
    "title": "Module 1: Foundations of Generative AI",
    "section": "How Does the Encoder/Decoder Work?",
    "text": "How Does the Encoder/Decoder Work?\noutput_ids = model.generate(input_ids)\n\nTakes input ids, runs through encoder\n\nGenerates contextual vectors using self attention across input tokens\n\nRuns the decoder iteratively to generate one token at a time\n\nUses self attention on previously generated tokens\nUses cross-attention to attend to encoder output\n\nContinues until it generates an end-of-sequence token or hits max length",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#introducing-the-transformer-5",
    "href": "src/01/slides.html#introducing-the-transformer-5",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\n\n\n\n\ngraph LR\n    Input[\"Input: 'Bonjour, comment allez-vous?'\"]\n    \n    subgraph Transformer\n        direction TB\n        \n        subgraph \"Encoder Layer\"\n            direction TB\n            E_SelfAttn[Multi-Head&lt;br/&gt;Self-Attention]\n            E_AddNorm1[Add & Norm]\n            E_FFN[Feed-Forward&lt;br/&gt;Network]\n            E_AddNorm2[Add & Norm]\n            \n            E_SelfAttn --&gt; E_AddNorm1 --&gt; E_FFN --&gt; E_AddNorm2\n        end\n        \n        subgraph \"Decoder Layer\"\n            direction TB\n            D_SelfAttn[Masked Multi-Head&lt;br/&gt;Self-Attention]\n            D_AddNorm1[Add & Norm]\n            D_CrossAttn[Multi-Head&lt;br/&gt;Cross-Attention]\n            D_AddNorm2[Add & Norm]\n            D_FFN[Feed-Forward&lt;br/&gt;Network]\n            D_AddNorm3[Add & Norm]\n            \n            D_SelfAttn --&gt; D_AddNorm1 --&gt; D_CrossAttn --&gt; D_AddNorm2 --&gt; D_FFN --&gt; D_AddNorm3\n        end\n        \n        E_AddNorm2 -.-&gt;|Encoder&lt;br/&gt;Output| D_CrossAttn\n    end\n    \n    Output[\"Output: 'Hello, how are you?'\"]\n    \n    Input --&gt; E_SelfAttn\n    D_AddNorm3 --&gt; Output",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#origin-of-gpt-1",
    "href": "src/01/slides.html#origin-of-gpt-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "2018: Origin of “GPT”",
    "text": "2018: Origin of “GPT”\n\nGenerative Pre-trained Transformer\nName coined by OpenAI researchers in “Improving Language Understanding by Generative Pre-Training” (Radford et al. 2018)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-gpt",
    "href": "src/01/slides.html#what-is-a-gpt",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is a GPT?",
    "text": "What is a GPT?\n\n“Decoder-only” architecture\n\nSelf attention is causal/masked - tokens can only attend to previous tokens, not future ones\n\nPre-training objective: Next token prediction\n\nTrained on a massive text corpora\nLearns grammar, facts, reasoning patterns just from this objective",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#what-is-a-gpt-1",
    "href": "src/01/slides.html#what-is-a-gpt-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "What is a GPT?",
    "text": "What is a GPT?\n\nAutoregressive generation\n\nGenerates one token at a time, feeding back each output as input\nTemperature and sampling strategies\nSame prompt can produce different outputs\n\nContext window\n\nFixed maximum length (2048 for GPT-2)\nEverything must fit within this window during generation\nIntroduced the concept of “context” vs. “knowledge” (prompt vs. training)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#gpt-2",
    "href": "src/01/slides.html#gpt-2",
    "title": "Module 1: Foundations of Generative AI",
    "section": "GPT-2",
    "text": "GPT-2\n\nReleased in 2019 by OpenAI\n\nInitially, only 117M param model released in Feb 2019 due to safety concerns\nStaged releases throughout the year, 1.5B in Nov 2019\n\nTrained on WebText, 8 million web pages/40GB of text\nZero-shot task performance\n\nDid well on translation, summarization, and question answering without task-specific training",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-4",
    "href": "src/01/slides.html#example-4",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load pre-trained GPT-2 model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n# Set pad token\ntokenizer.pad_token = tokenizer.eos_token",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-5",
    "href": "src/01/slides.html#example-5",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nimport torch\n\ndef autocomplete(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n    # Encode the prompt with attention mask\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    # Generate continuation\n    with torch.no_grad():\n        output = model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#temperature-top_k-and-top_p",
    "href": "src/01/slides.html#temperature-top_k-and-top_p",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Temperature, top_k, and top_p",
    "text": "Temperature, top_k, and top_p\n\nTemperature (0.0 - 1.0)\n\nLower for accuracy, factual summaries, etc.\nHigher for more creative, diverse ideas\n\ntop_k (top k tokens)\n\nNarrow the next tokens to the top k (ordered by probability)\n\ntop_p (cumulative probability)\n\nOnly return the top tokens whose culumative probability &lt; top_p",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#example-6",
    "href": "src/01/slides.html#example-6",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Example",
    "text": "Example\n\n\nprompt = \"Mary had a little lamb\"\ncompletion = autocomplete(prompt, max_length=80)\nprint(completion)\n\nMary had a little lamb, and the young woman asked her for a little lamb, and they gave it to her.\n\n\"Oh, my child, it is good to have a little lamb,\" said he, \"but it is not to be bought, for it is hard to make, and it is much more difficult to make.\n\n\"When you have a little lamb, it",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#limitations-of-gpt-2-1",
    "href": "src/01/slides.html#limitations-of-gpt-2-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Limitations of GPT-2",
    "text": "Limitations of GPT-2\n\nHallucinations / factual errors\nNo real-world grounding\nRepetition issues\nOnwards to GPT-3 and beyond…",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#looking-ahead-1",
    "href": "src/01/slides.html#looking-ahead-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nNew to Python?\n\nLearn Python with Jupyter\n\nInstruction-tuned models\nOpenAI specification\nGradio for chat-based UIs",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/slides.html#references-1",
    "href": "src/01/slides.html#references-1",
    "title": "Module 1: Foundations of Generative AI",
    "section": "References",
    "text": "References\n\n\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. “Neural Machine Translation by Jointly Learning to Align and Translate.” In International Conference on Learning Representations. https://arxiv.org/abs/1409.0473.\n\n\nGage, Philip. 1994. “A New Algorithm for Data Compression.” The C Users Journal 12 (2): 23–38.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” In International Conference on Learning Representations. https://arxiv.org/abs/1301.3781.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” In Advances in Neural Information Processing Systems, 3111–19. https://arxiv.org/abs/1310.4546.\n\n\nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.” https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf.\n\n\nSennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1715–25. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1162.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems. Vol. 30.",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/assignment.html",
    "href": "src/01/assignment.html",
    "title": "Module 1 Assignment: Experiment with Text Continuation Styles",
    "section": "",
    "text": "Objective: Create a Colab notebook that uses GPT-2 to generate creative text continuations with different styles.\nRequirements:\n\nLoad a pre-trained GPT-2 model (using HuggingFace transformers - same approach as used in GPT-2.ipynb)\nCreate 3 different story starters in different genres/styles. For example:\n\nFantasy/Adventure: “In a land of dragons and magic…”\nSci-fi: “The year is 2157. Humanity has just…”\nMystery: “The detective examined the crime scene and noticed…”\n(or choose your own three)\n\nThen adjust for:\n\nGreedy decoding vs. sampling\nDifferent temperature values\nHow the opening sentence shapes the continuation (e.g., short vs. long)\n\nDocument your observations (using Markdown in the notebook)\n\nWhat differences do you notice between the strategies?\nWhat worked well (or surprised you!)\nWhat didn’t work that well\n\n\nDeliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation\nOutputs of generated text from GPT-2\nMarkdown cells explaining what each sampling strategy does and any observations\n\nHint:\nFor better results, use a larger GPT-2 model on Colab T4.\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Assignment"
    ]
  },
  {
    "objectID": "src/00/slides.html#course-description",
    "href": "src/00/slides.html#course-description",
    "title": "Welcome to CS-394/594!",
    "section": "Course Description",
    "text": "Course Description\n\nHow Generative AI Works focuses on the practical implementation of generative AI within custom software applications and games.\nThe course covers neural network architectures, including the impact of the Transformer model, customization of large language models across multiple vendors using APIs, and experimentation with multimodal models for image and audio recognition and generation.",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#course-description-1",
    "href": "src/00/slides.html#course-description-1",
    "title": "Welcome to CS-394/594!",
    "section": "Course Description",
    "text": "Course Description\n\nHands-on experience includes working with both hosted and locally run models, integrating AI with game engines such as Unity and Unreal, and developing AI agents that extend beyond simple chat-based interactions.\nEthical considerations and model evaluation are integrated throughout, emphasizing awareness of broader societal implications.\nThrough lectures, programming assignments, and a final project, the course provides the expertise needed to apply generative AI in creating innovative and interactive experiences.",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#learning-outcomes",
    "href": "src/00/slides.html#learning-outcomes",
    "title": "Welcome to CS-394/594!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the basic working principles and history of current LLMs (Large Language Models)\nUnderstand ethical and safety aspects of using generative models\nEvaluate and test generative models using industry benchmarks\nRun generative models on local, laptop-based hardware (using CPU, GPU, NPUs)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#learning-outcomes-1",
    "href": "src/00/slides.html#learning-outcomes-1",
    "title": "Welcome to CS-394/594!",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCreate AI-based agents and tools based on the MCP (Model Context Protocol) specification\nAvoid hallucinations by increasing the accuracy of models through RAG (Retrieval Augmented Generation) and fine-tuning\nExplore and use multimodal models for image and audio recognition and generation\nCreate and deploy API-based clients, accessing LLMs hosted by different vendors (OpenAI, Meta, Google)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#in-summary",
    "href": "src/00/slides.html#in-summary",
    "title": "Welcome to CS-394/594!",
    "section": "In Summary",
    "text": "In Summary\n\nFocus on integration/augmentation vs. automation/using\nProvide a level of understanding beyond where most professional software developers are today\nBuild an exciting final project that you can add to your portfolio!",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#syllabus-1",
    "href": "src/00/slides.html#syllabus-1",
    "title": "Welcome to CS-394/594!",
    "section": "Syllabus",
    "text": "Syllabus\n\nSyllabus on OneDrive",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#schedule",
    "href": "src/00/slides.html#schedule",
    "title": "Welcome to CS-394/594!",
    "section": "Schedule",
    "text": "Schedule\n\nEvery Friday (Curie); 2pm - 4.50pm\n~1.5 hours of lecture\n~1.5 hours for in-class hands-on lab time and assignments\nExpectation of after-class work for assignments\nNo structured lectures for the weeks of the final project (3 hours of in-class lab time)\n\nAlthough we may do mini-lectures for common topics",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#schedule-1",
    "href": "src/00/slides.html#schedule-1",
    "title": "Welcome to CS-394/594!",
    "section": "Schedule",
    "text": "Schedule\n\nModules 1 through 4\nFeb 6 is Founders Day, so no classes\nModules 5 through 8\nMar 9 - 13 is Spring Break\nWeeks 9 through 14 - Final Project Work\nFinal presentations w/o Apr 20-24",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#during-class",
    "href": "src/00/slides.html#during-class",
    "title": "Welcome to CS-394/594!",
    "section": "During Class",
    "text": "During Class\n\nStrive for conversation and interactivity\n\nPlease ask questions, even mid-slide!\nThere are no wrong or bad questions!\nI enjoy going off on tangents / on the whiteboard\nUse hands on lab time to seek input / troubleshoot code",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#grading",
    "href": "src/00/slides.html#grading",
    "title": "Welcome to CS-394/594!",
    "section": "Grading",
    "text": "Grading\n\nModule Assignments: 40% of grade (8 x 5%)\nFinal Project: 60% of grade\nRubric for the weekly assignments\nRubric for the final project (CS-394)\nRubric for the final project (CS-594)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#handing-in-work",
    "href": "src/00/slides.html#handing-in-work",
    "title": "Welcome to CS-394/594!",
    "section": "Handing in Work",
    "text": "Handing in Work\n\nEverything submitted via GitHub\n\nRecommend creating a repo for weekly assignments\n\nFor most weeks, submission will be one Python notebook\n\nAnd (eventually) another repo for your final project\n\nDon’t forget to give me permissions! @simonguest",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#deadlines",
    "href": "src/00/slides.html#deadlines",
    "title": "Welcome to CS-394/594!",
    "section": "Deadlines",
    "text": "Deadlines\n\nWeekly Assignments\n\nAssignments are due by the following week’s lesson\ni.e., you get a week for each assignment\nIf you need more time/exception, please reach out via Teams\n\nFinal Project\n\nUp until Week 15 presentations\n(We’ll cover in detail later in the semester)",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#ai-policy",
    "href": "src/00/slides.html#ai-policy",
    "title": "Welcome to CS-394/594!",
    "section": "AI Policy",
    "text": "AI Policy\n\nPermitted AI Usage\n\nYou may use AI tools to assist in understanding course materials\nIf AI is used to generate code, your must test and validate the code, must understand and be able to answer questions about the generated code, and include proper citations\nIf AI tools are used to assist with any part of an assignment, you must clearly cite the AI tool and explain how it was used",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#tools",
    "href": "src/00/slides.html#tools",
    "title": "Welcome to CS-394/594!",
    "section": "Tools",
    "text": "Tools\n\nWe will be introducing many tools\n\nColab Pro, OpenRouter, Hugging Face, etc.\nMost will be free\nExpect to need about $25 in API credits throughout the semester",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#languages",
    "href": "src/00/slides.html#languages",
    "title": "Welcome to CS-394/594!",
    "section": "Languages",
    "text": "Languages\n\nWe will be using (and learning) a lot of Python!\n\nMost of the in-class assignments will be in Python\nDon’t worry if you are new to Python as we’ll introduce concepts gradually\nAlthough recommend investing extra time (see resources in Week 1)\n\nFinal Project\n\nCan be any language\nProbably depending on what you choose to create",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#hardware",
    "href": "src/00/slides.html#hardware",
    "title": "Welcome to CS-394/594!",
    "section": "Hardware",
    "text": "Hardware\n\nWill will be training SLMs (Small Language Models) later in the semester\nThis training will require a decent GPU and VRAM\n\nColab Pro (CUDA)\nYour own NVIDIA-based laptop (CUDA)\nPotential of using MLX for any Mac users",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#need-help-1",
    "href": "src/00/slides.html#need-help-1",
    "title": "Welcome to CS-394/594!",
    "section": "Need Help?",
    "text": "Need Help?\n\nhttps://simonguest.github.io/CS-394\n\nSlides (current and prior lectures), Demo code, Resources, Rubrics, Assignments\nI will repost assignments and rubrics on the Meta-Moodle. (Grades will also be in Moodle.)\n\nOffice Hours\n\nThursdays 1pm - 3pm (On campus or virtually via Teams)\n\nTeams (CS394/594 combined channel)\n\nPrimary mechanism for updates, ask questions, request office hours, etc.",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#steep-learning-curve",
    "href": "src/00/slides.html#steep-learning-curve",
    "title": "Welcome to CS-394/594!",
    "section": "Steep Learning Curve",
    "text": "Steep Learning Curve\n\nWe will be using the latest tools and AI models\nLots of new tools, acronyms, frameworks, etc.\nMuch of the curriculum builds upon itself\n\nPlease try not to miss lectures\nAsk for help if you need to catch up",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#new-course-at-digipen",
    "href": "src/00/slides.html#new-course-at-digipen",
    "title": "Welcome to CS-394/594!",
    "section": "New Course at DigiPen!",
    "text": "New Course at DigiPen!\n\nThere may be some minor curriculum tweaks mid-flight\n\nEspecially for topics that need less/more time",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/slides.html#fast-moving-space",
    "href": "src/00/slides.html#fast-moving-space",
    "title": "Welcome to CS-394/594!",
    "section": "Fast Moving Space",
    "text": "Fast Moving Space\n\nThere will be areas/questions I don’t have experience of\n\nMultiple new models are launched every week\n…or equations/algorithms that I don’t know\n\nWe will be learning some things together!\nBut that’s what makes it exciting!",
    "crumbs": [
      "**Welcome**",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html",
    "href": "src/00/final-rubric-394.html",
    "title": "Rubric (Final Project CS-394)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#integration-of-ai-models-10",
    "href": "src/00/final-rubric-394.html#integration-of-ai-models-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#functionality-10",
    "href": "src/00/final-rubric-394.html#functionality-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "Functionality (10%)",
    "text": "Functionality (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nProject is largely non-functional with AI components not working. Major bugs and errors prevent basic usage.\nProject runs but AI components frequently fail or produce incorrect results. Significant functionality issues present.\nProject is functional with AI components working in most cases. Some bugs or limitations affect user experience.\nProject is fully working with AI components functioning reliably as intended. Minor issues may exist but don’t impact core functionality.\nProject is fully functional with flawless AI integration. All components work seamlessly together with robust error handling.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#innovation-and-creativity-10",
    "href": "src/00/final-rubric-394.html#innovation-and-creativity-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "Innovation and Creativity (10%)",
    "text": "Innovation and Creativity (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nGeneric AI usage with no creative application. Design is basic with poor aesthetics and no meaningful AI enhancement.\nAI used in conventional ways with limited creativity. Design shows some effort but AI doesn’t meaningfully enhance the experience.\nCreative AI application with interesting use cases. Design is competent with AI providing noticeable visual or interactive improvements.\nInnovative AI usage providing unique and compelling experiences. Strong aesthetic and functional design that effectively leverages AI capabilities.\nExceptional creativity with groundbreaking AI applications. Outstanding design that seamlessly integrates AI to create truly unique and compelling experiences.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#ethical-analysis-10",
    "href": "src/00/final-rubric-394.html#ethical-analysis-10",
    "title": "Rubric (Final Project CS-394)",
    "section": "Ethical Analysis (10%)",
    "text": "Ethical Analysis (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nNo ethical analysis provided or only superficial acknowledgment of issues. No consideration of biases or societal impacts.\nLimited ethical analysis with minimal identification of potential issues. Brief mention of biases but no mitigation strategies proposed.\nAdequate evaluation of biases and ethical implications with some discussion of societal impacts. Basic mitigation proposals included.\nThorough evaluation of potential biases, ethical implications, and societal impacts. Clear and practical proposals for mitigating identified risks.\nComprehensive and insightful ethical analysis demonstrating deep understanding of AI implications. Sophisticated mitigation strategies with consideration of broader societal impacts.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-394.html#presentation-20",
    "href": "src/00/final-rubric-394.html#presentation-20",
    "title": "Rubric (Final Project CS-394)",
    "section": "Presentation (20%)",
    "text": "Presentation (20%)\n\n\n\n\n\n\n\n\n\n\n4%\n8%\n12%\n16%\n20%\n\n\n\n\nPresentation is unclear and disorganized with minimal explanation of AI features. If in a team, roles were undefined and collaboration issues evident.\nPresentation covers basic points but lacks engagement or clear explanation of AI integration process. If in a team, some team member contributions unclear.\nClear presentation highlighting main AI features and challenges faced. If in a team, collaboration is adequate with most roles and contributions identifiable.\nClear and engaging presentation effectively showcasing AI features and their purpose. Comprehensive explanation of integration process including model selection and solutions, with well-defined team roles (if applicable).\nOutstanding presentation that captivates audience while thoroughly explaining AI implementation. Exceptional teamwork (if applicable) with seamless collaboration and clearly articulated individual contributions.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-394)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS-394/594: How Generative AI Works",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CS-394/594 course.",
    "crumbs": [
      "CS-394/594"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "CS-394/594: How Generative AI Works",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CS-394/594 course.",
    "crumbs": [
      "CS-394/594"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html",
    "href": "src/00/final-rubric-594.html",
    "title": "Rubric (Final Project CS-594)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. No fine-tuning achieved. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited fine-tuning demonstrated. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Fine-tuning demonstrated, but not optimal. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Strong fine-tuning techniques demonstrated. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Excellent approach and results from fine-tuning. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#fine-tuning-and-integration-of-ai-models-10",
    "href": "src/00/final-rubric-594.html#fine-tuning-and-integration-of-ai-models-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "",
    "text": "2%\n4%\n6%\n8%\n10%\n\n\n\n\nOnly one AI technique integrated with minimal effort or inappropriate model choice. No fine-tuning achieved. Integration is superficial with no optimization.\nTwo AI techniques used but poorly integrated or not well-suited to their tasks. Limited fine-tuning demonstrated. Limited optimization effort shown.\nTwo appropriate AI techniques integrated with basic functionality. Fine-tuning demonstrated, but not optimal. Some optimization attempted but significant improvements possible.\nTwo well-chosen AI techniques effectively integrated and optimized for their respective tasks. Strong fine-tuning techniques demonstrated. Clear rationale for model selection.\nExcellent integration of multiple AI techniques with sophisticated optimization. Excellent approach and results from fine-tuning. Models are perfectly suited to tasks with exceptional implementation.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#functionality-10",
    "href": "src/00/final-rubric-594.html#functionality-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "Functionality (10%)",
    "text": "Functionality (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nProject is largely non-functional with AI components not working. Major bugs and errors prevent basic usage.\nProject runs but AI components frequently fail or produce incorrect results. Significant functionality issues present.\nProject is functional with AI components working in most cases. Some bugs or limitations affect user experience.\nProject is fully working with AI components functioning reliably as intended. Minor issues may exist but don’t impact core functionality.\nProject is fully functional with flawless AI integration. All components work seamlessly together with robust error handling.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#innovation-and-creativity-10",
    "href": "src/00/final-rubric-594.html#innovation-and-creativity-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "Innovation and Creativity (10%)",
    "text": "Innovation and Creativity (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nGeneric AI usage with no creative application. Design is basic with poor aesthetics and no meaningful AI enhancement.\nAI used in conventional ways with limited creativity. Design shows some effort but AI doesn’t meaningfully enhance the experience.\nCreative AI application with interesting use cases. Design is competent with AI providing noticeable visual or interactive improvements.\nInnovative AI usage providing unique and compelling experiences. Strong aesthetic and functional design that effectively leverages AI capabilities.\nExceptional creativity with groundbreaking AI applications. Outstanding design that seamlessly integrates AI to create truly unique and compelling experiences.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#ethical-analysis-10",
    "href": "src/00/final-rubric-594.html#ethical-analysis-10",
    "title": "Rubric (Final Project CS-594)",
    "section": "Ethical Analysis (10%)",
    "text": "Ethical Analysis (10%)\n\n\n\n\n\n\n\n\n\n\n2%\n4%\n6%\n8%\n10%\n\n\n\n\nNo ethical analysis provided or only superficial acknowledgment of issues. No consideration of biases or societal impacts.\nLimited ethical analysis with minimal identification of potential issues. Brief mention of biases but no mitigation strategies proposed.\nAdequate evaluation of biases and ethical implications with some discussion of societal impacts. Basic mitigation proposals included.\nThorough evaluation of potential biases, ethical implications, and societal impacts. Clear and practical proposals for mitigating identified risks.\nComprehensive and insightful ethical analysis demonstrating deep understanding of AI implications. Sophisticated mitigation strategies with consideration of broader societal impacts.",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/final-rubric-594.html#presentation-and-research-20",
    "href": "src/00/final-rubric-594.html#presentation-and-research-20",
    "title": "Rubric (Final Project CS-594)",
    "section": "Presentation and Research (20%)",
    "text": "Presentation and Research (20%)\n\n\n\n\n\n\n\n\n\n\n4%\n8%\n12%\n16%\n20%\n\n\n\n\nPresentation is unclear and disorganized with minimal explanation of AI features. If in a team, roles were undefined and collaboration issues evident. No research evident and/or cited-papers.\nPresentation covers basic points but lacks engagement or clear explanation of AI integration process. If in a team, some team member contributions unclear. Limited research evident.\nClear presentation highlighting main AI features and challenges faced. If in a team, collaboration is adequate with most roles and contributions identifiable. Research evident, but with gaps or issues.\nClear and engaging presentation effectively showcasing AI features and their purpose. Comprehensive explanation of integration process including model selection and solutions, with well-defined team roles (if applicable). Strong, well-cited research evident.\nOutstanding presentation that captivates audience while thoroughly explaining AI implementation. Exceptional teamwork (if applicable) with seamless collaboration and clearly articulated individual contributions. Excellent and well-cited approach to research",
    "crumbs": [
      "**Welcome**",
      "Rubric (Final Project for CS-594)"
    ]
  },
  {
    "objectID": "src/00/weekly-rubric.html",
    "href": "src/00/weekly-rubric.html",
    "title": "Rubric (Weekly Assignments)",
    "section": "",
    "text": "1%\n2%\n3%\n4%\n5%\n\n\n\n\nThe submission fails to showcase any working features. The solution is practically unusable due to issues.\nOnly a few features are functional. Significant missing work or issues with the submission.\nMajor issues impact key features, but some functionality is evident. Noticeable gaps or confusing submission.\nMinor issues or bugs are present but do not significantly impact the functionality. Very few gaps, and a near-complete submission.\nAll features are fully functional. A complete submission that meets all requirements of the assignment.\n\n\n\n(0% for unsubmitted work)",
    "crumbs": [
      "**Welcome**",
      "Rubric (Weekly Assignments)"
    ]
  },
  {
    "objectID": "src/01/resources.html",
    "href": "src/01/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Visualize embeddings in 3D space, powered by EmbeddingGemma and Transformers.js\nOriginal Word2Vec Papers - Google Code archive with original papers\nWord2Vec Tutorial - The Skip-Gram Model - Chris McCormick’s detailed tutorial\nGensim Word2Vec Tutorial - Popular Python library for word embeddings\nA Visual Introduction to Vector Embeddings, Pamela Fox\nA Visual Exploration of Vectors, Pamela Fox",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#word2vec-and-word-embeddings",
    "href": "src/01/resources.html#word2vec-and-word-embeddings",
    "title": "Resources",
    "section": "",
    "text": "Visualize embeddings in 3D space, powered by EmbeddingGemma and Transformers.js\nOriginal Word2Vec Papers - Google Code archive with original papers\nWord2Vec Tutorial - The Skip-Gram Model - Chris McCormick’s detailed tutorial\nGensim Word2Vec Tutorial - Popular Python library for word embeddings\nA Visual Introduction to Vector Embeddings, Pamela Fox\nA Visual Exploration of Vectors, Pamela Fox",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#tokenization-and-byte-pair-encoding",
    "href": "src/01/resources.html#tokenization-and-byte-pair-encoding",
    "title": "Resources",
    "section": "Tokenization and Byte Pair Encoding",
    "text": "Tokenization and Byte Pair Encoding\n\nBPE Original Paper (1994) - Philip Gage’s original compression algorithm\nNeural Machine Translation with BPE - 2016 paper adapting BPE for NLP\nHugging Face Tokenizers - Understanding different tokenization strategies\nOpenAI Tokenizer Tool - Interactive tool to see how text is tokenized",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#transformers",
    "href": "src/01/resources.html#transformers",
    "title": "Resources",
    "section": "Transformers",
    "text": "Transformers\n\nThe Illustrated Transformer - Jay Alammar’s visual guide\n“Attention is All You Need” Paper - The original transformer paper (2017)\nThe Annotated Transformer - Harvard NLP’s line-by-line implementation guide\nTransformer Math 101 - Understanding transformer computation\nAttention Mechanism Explained - Visual explanation of attention",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#gpt-and-language-models",
    "href": "src/01/resources.html#gpt-and-language-models",
    "title": "Resources",
    "section": "GPT and Language Models",
    "text": "GPT and Language Models\n\nGPT-1 Paper: “Improving Language Understanding” - Original GPT paper (2018)\nGPT-2 Paper: “Language Models are Unsupervised Multitask Learners” - GPT-2 paper (2019)\nGPT-2 Release Blog Post - OpenAI’s staged release announcement\nUnderstanding Decoder-Only Models - Sebastian Raschka’s explanation\nAndrej Karpathy’s “Let’s build GPT” - Building GPT from scratch",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#python-and-programming-basics",
    "href": "src/01/resources.html#python-and-programming-basics",
    "title": "Resources",
    "section": "Python and Programming Basics",
    "text": "Python and Programming Basics\n\nLearn Python with Jupyter, Serena Bonaretti\nPython Official Documentation - Official Python 3 documentation\nPython for Beginners - Python.org’s getting started guide\nAutomate the Boring Stuff with Python - Free online book for Python beginners",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#python-package-management",
    "href": "src/01/resources.html#python-package-management",
    "title": "Resources",
    "section": "Python Package Management",
    "text": "Python Package Management\n\npip Documentation - Python’s standard package installer\nuv Documentation - Modern, fast Python package manager\nPyPI - Python Package Index - Repository of 500K+ Python packages",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#notebooks-and-development-environment",
    "href": "src/01/resources.html#notebooks-and-development-environment",
    "title": "Resources",
    "section": "Notebooks and Development Environment",
    "text": "Notebooks and Development Environment\n\nGoogle Colab Sign-up Page\nGoogle Colab Tips and Tricks - Making the most of Colab\nProject Jupyter Page\nJupyter Notebook Beginner Guide - Getting started with Jupyter\nVS Code Jupyter Extension - Run notebooks in VS Code",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#citations",
    "href": "src/01/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Resources"
    ]
  },
  {
    "objectID": "src/02/assignment.html",
    "href": "src/02/assignment.html",
    "title": "Module 2 Assignment: Gradio Travel Planner",
    "section": "",
    "text": "Objective: Build a chat interface (in a separate Colab notebook) that helps a user complete tasks.\nRequirements:\n\nGradio chat interface with streaming responses\nSystem prompt that defines the AI as a travel planning expert (or pick your own scenario, if you have a better idea!)\nImplements a well-thought out system prompt (with feedback on the rationale behind it).\nDemonstrates at least 2 different models via OpenRouter as a dropdown in Gradio. (Not necessarily at the same time.)\n\nTry to pick a large (foundational model) and a much smaller open source instruct model to compare the differences.\n\nUse structured outputs when returning results to the user (e.g., a downloadable itinerary that the user can download)\n\nExample schema: Trip with fields like destination, duration_days, activities (list), budget_level, daily_schedule (list of day objects)\n\n\nDeliverable: A Colab/Jupyter notebook with:\n\nCode cells with your implementation\nUses OPENROUTER_API_KEY for the API token. (Please do not include your API key in your notebook!)\nMarkdown cells explaining what the notebook does and any observations",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Assignment"
    ]
  },
  {
    "objectID": "src/02/slides.html#recap",
    "href": "src/02/slides.html#recap",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Recap",
    "text": "Recap\n\nExplored the history of vector embeddings and tokenization\nUnderstood the transformer architecture at a high level\nUsed our first transformer to translate language\nCovered a brief history of early generative transformers\nSetup and used Colab, and became familiar with the basics of notebooks and Python",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#lesson-objectives",
    "href": "src/02/slides.html#lesson-objectives",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand the evolution and licensing of models from GPT-2 through to modern day\nUnderstand instruction-tuned models, how they work, and how to configure\nSetup and use OpenRouter for accessing hosted models\nUnderstand the OpenAI API specification, the request/response payload, parameters, streaming, and structured output\nCreate and share a chatbot using a Gradio-based UI",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#from-gpt-2-to-gpt-3.5-1",
    "href": "src/02/slides.html#from-gpt-2-to-gpt-3.5-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "From GPT-2 to GPT-3.5",
    "text": "From GPT-2 to GPT-3.5\n\n\n\n\n\ntimeline\n    Feb 2019 : OpenAI releases GPT-2\n             : 1.5B parameters\n             : Initially withheld full model due to concerns about misuse\n             : Demonstrates impressive text generation capabilities with minimal fine-tuning\n\n    May 2020 : OpenAI releases GPT-3\n             : 175B parameters\n             : Demonstrates strong few-shot learning capabilities\n             : Marks a significant leap in model capabilities and scale\n\n    June 2020 : GPT-3 available through OpenAI API\n              : Still a completion model, not instruction-tuned\n\n    2021 : InstructGPT Development\n          : Built on GPT-3 with RLHF fine-tuning\n          : Trained to follow instructions and understand user intent\n          : Key innovation enabling ChatGPT\n    \n    Jan 2021 : Anthropic Founded\n             : Founded by Dario & Daniela Amodei with ~7 senior OpenAI employees\n             : Dario led GPT-2/3 development and co-invented RLHF\n\n    Nov 2022 : ChatGPT Launch\n              : Built on GPT-3.5 using RLHF\n              : 1M+ users in 5 days\n              : Sparked widespread interest in generative AI",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs. Instruction-Tuned",
    "text": "Completion vs. Instruction-Tuned\n\nCompletion Model just predicts the next token\n\nInput prompt: Mary had a little\nMax total tokens: 50\nTemperature: 0 - 1.0\ntop_k: consider only the top k tokens in the response\ntop_p: Nucleus sampling (probability cut off - 0 and 1.0)\n\nOutput\n\nMary had a little lamb, its fleece was white as snow... (up to max tokens)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#completion-vs.-instruction-tuned-1",
    "href": "src/02/slides.html#completion-vs.-instruction-tuned-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Completion vs. Instruction-Tuned",
    "text": "Completion vs. Instruction-Tuned\n\nYou can’t really converse with it\nWhat should I do on my upcoming trip to Paris? (max tokens = 75)\nWhat should I do on my upcoming trip to Paris? Please provide a detailed plan of action to help me plan my trip to Paris. 1. Research the best time to travel to Paris:",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#instruction-tuned-models",
    "href": "src/02/slides.html#instruction-tuned-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Instruction-Tuned Models",
    "text": "Instruction-Tuned Models\n\nSupervised Fine-Tuning\n\nLarge datasets of questions/answers, tasks/completions, demonstrating helpful assistant behavior\n\nRLHF (Reinforcement Learning from Human Feedback)\n\nHuman raters rank different model responses, training a reward model\n\nChat Templates\n\nStructured format to distinguish speakers in a conversation: Typically system, user, and assistant",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#system-user-assistant",
    "href": "src/02/slides.html#system-user-assistant",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "System, User, Assistant",
    "text": "System, User, Assistant\n\nSystem prompt sets the intention for the model, guiding the output\n\n“You are a helpful assistant”\n“You help students with their math homework”\n“You help travelers make plans for their trips”\nHas to come first in the conversation\nOnly one system prompt\nOptional for some models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#system-user-assistant-1",
    "href": "src/02/slides.html#system-user-assistant-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "System, User, Assistant",
    "text": "System, User, Assistant\n\nSystem Prompt best practices\n\nBe specific: “You are a Python programming tutor who explains concepts using simple analogies and provides code examples.”\nDefine output: “List no more than 3 suggestions. Always show your work step by step.”\nSet boundaries: “If you are asked questions outside coding, politely redirect the student back to the task.”",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#system-user-assistant-2",
    "href": "src/02/slides.html#system-user-assistant-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "System, User, Assistant",
    "text": "System, User, Assistant\n\nUser prompt is the message (request) from the user\n\n“How many ’r’s in Strawberry?”\n“What is linear algebra?”\n“What should I do on my upcoming trip to Paris?”\n\nAssistant prompt is the message (reply) from the model\n\n“There are three r’s in Strawberry”\n“Linear algebra is the branch of mathematics that studies vectors, etc.”\n“Here are some suggestions for your upcoming trip to Paris: 1. Explore the Louvre Museum: etc.”",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#whats-a-chat-template",
    "href": "src/02/slides.html#whats-a-chat-template",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What’s a Chat Template?",
    "text": "What’s a Chat Template?\n\nThe format used to train instructional models on conversations involving system, user, and assistant prompts.\nEach model family uses a different format (there is no universal standard)\nWrong format will likely generate nonsense/garbage",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chatml-gpt-3.5-and-other-models",
    "href": "src/02/slides.html#chatml-gpt-3.5-and-other-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "ChatML (GPT-3.5 and other models)",
    "text": "ChatML (GPT-3.5 and other models)\n&lt;|im_start|&gt;system\nYou help travelers make plans for their trips.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHello&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nHi there! How can I help you?&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat should I do on my upcoming trip to Paris?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-templates-in-practice",
    "href": "src/02/slides.html#chat-templates-in-practice",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat Templates in Practice",
    "text": "Chat Templates in Practice\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\n\ninstruct_tokenizer.apply_chat_template(\n    messages, \n    tokenize=False,\n    add_generation_prompt=True  # Adds the assistant prompt\n)\n\n'&lt;|im_start|&gt;system\\nYou help travelers make plans for their trips&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nHello&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\nHi there!&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nWhat should I do on my upcoming trip to Paris?&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n'",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#putting-this-together",
    "href": "src/02/slides.html#putting-this-together",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Putting This Together",
    "text": "Putting This Together\n\n\nbase_inputs = base_tokenizer(\"What should I do on my upcoming trip to Paris?\", return_tensors=\"pt\")\nbase_outputs = base_model.generate(\n    **base_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=base_tokenizer.eos_token_id\n)\nbase_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\nprint(base_response)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#putting-this-together-1",
    "href": "src/02/slides.html#putting-this-together-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Putting This Together",
    "text": "Putting This Together\n\n\n\nWhat should I do on my upcoming trip to Paris? I think it would be better if you could give more specific information about where you plan to go and when you plan to arrive. Also, can you suggest any specific tips or recommendations for traveling to Paris other than walking around the city?\n\nI'm sorry, but as an AI language model, I don't have any specific information about your upcoming trip to Paris. However, I can suggest some general tips and recommendations for traveling to Paris other than walking around the city:\n\n1. Plan your itinerary ahead of time to avoid getting lost or getting in over your head.\n2. Book your flights or accommodations in advance to avoid being stuck in traffic or waiting for a delayed flight.\n3. Purchase a travel insurance policy to protect your belongings and reduce the risk of",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#putting-this-together-2",
    "href": "src/02/slides.html#putting-this-together-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Putting This Together",
    "text": "Putting This Together\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\ninstruct_text = instruct_tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\ninstruct_inputs = instruct_tokenizer(instruct_text, return_tensors=\"pt\")\ninstruct_outputs = instruct_model.generate(\n    **instruct_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=instruct_tokenizer.eos_token_id,\n)\ninstruct_response = instruct_tokenizer.decode(\n    instruct_outputs[0], skip_special_tokens=True\n)\nprint(instruct_response)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#putting-this-together-3",
    "href": "src/02/slides.html#putting-this-together-3",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Putting This Together",
    "text": "Putting This Together\n\n\n\nsystem\nYou help travelers make plans for their trips.\nuser\nHello\nassistant\nHi there!\nuser\nWhat should I do on my upcoming trip to Paris?\nassistant\nGreat question! On your next trip to Paris, you can start by visiting the iconic Eiffel Tower and the Louvre Museum. Don't miss exploring the Notre-Dame Cathedral and its stunning stained glass windows. For a bit of a break, consider visiting Montmartre for some beautiful art and architecture. If you're looking for something more adventurous, you could take a stroll through the charming streets of Montmartre or explore the vibrant nightlife of Le Marais. Have fun planning your trip to Paris!",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#model-evolution-gpt-3.5-onwards",
    "href": "src/02/slides.html#model-evolution-gpt-3.5-onwards",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Model Evolution (GPT 3.5 onwards)",
    "text": "Model Evolution (GPT 3.5 onwards)\n\n\n\n\n\ntimeline\n    Nov 2022 : ChatGPT Launch\n                  : Built on GPT-3.5 using RLHF\n                  : 1M+ users in 5 days\n                  : Sparked widespread interest in generative AI\n\n    Feb 2023 : Llama 1 Released\n                  : Meta's LLaMA (7B, 13B, 33B, 65B parameters)\n                  : 13B model exceeded GPT-3 (175B) on most benchmarks\n                  : Text completion only (Alpaca fine-tune added instructions)\n\n    Jul 2023 : Llama 2 Released\n              : Available in 7B, 13B, 70B sizes\n              : Trained on 40% more data than Llama 1\n              : First open-weights Llama for commercial use",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#closed-vs.-open-models",
    "href": "src/02/slides.html#closed-vs.-open-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Closed vs. Open Models",
    "text": "Closed vs. Open Models\n\nClosed Source:\n\nHosted models\nNo ability to inspect the weights of the models\nNo ability to download the models\nOpenAI GPT-5, Claude Sonnet 4.5, Google’s Gemini\nVery large models; often referred to as foundational models or frontier models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#closed-vs.-open-models-1",
    "href": "src/02/slides.html#closed-vs.-open-models-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Closed vs. Open Models",
    "text": "Closed vs. Open Models\n\nOpen Weight:\n\nDownloadable model files\nYou can download the model files with pretrained weights, but no training data\nNo training data == No ability to recreate the model from scratch\nMeta’s Llama, Google’s Gemma, Alibaba’s Qwen, OpenAI gpt-oss-120b\nRange from small to medium in size (1Gb - 500Gb+)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#closed-vs.-open-models-2",
    "href": "src/02/slides.html#closed-vs.-open-models-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Closed vs. Open Models",
    "text": "Closed vs. Open Models\n\nOpen Source:\n\nModels with access to the training data set\nYou can download the model files with pretrained weights and the training data used to train it\ni.e., you could create the model from scratch\nExamples: AI2’s OLMo, NVIDIA Nemotron",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#discovering-open-models",
    "href": "src/02/slides.html#discovering-open-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Discovering Open Models",
    "text": "Discovering Open Models\n\nSource: https://huggingface.co",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#what-is-hugging-face",
    "href": "src/02/slides.html#what-is-hugging-face",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What is Hugging Face?",
    "text": "What is Hugging Face?\n\nIt is to AI models what GitHub is to source code\n\nExplore, download models to run on local hardware\nUpload and share your own trained/fine-tuned models and datasets\nCreate “Spaces” - web-based apps for accessing models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#hugging-face-transformers",
    "href": "src/02/slides.html#hugging-face-transformers",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Hugging Face Transformers",
    "text": "Hugging Face Transformers\n\nSource: https://huggingface.co/docs/transformers",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#hugging-face-transformers-1",
    "href": "src/02/slides.html#hugging-face-transformers-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Hugging Face Transformers",
    "text": "Hugging Face Transformers\n\nWhat is the Hugging Face Transformers Library?\nOpen-source Python library to provide easy access to using various types of pre-trained transformer models\nBrings together all of the different formats under one interface\n\nDifferent models, vendors, types, chat templates\nDifferent implementations: PyTorch, TensorFlow, JAX\n\nA few lines of code to download and run the model",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#accessing-closed-models-1",
    "href": "src/02/slides.html#accessing-closed-models-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Accessing Closed Models",
    "text": "Accessing Closed Models\n\nConsumer Website / App\n\ne.g., ChatGPT website or AppStore App\nLimited free tier; monthly subscription for more usage\n\nAPI Access\n\nOpenAI’s API Platform; Create a developer account\nCredit card required\nCharged for tokens sent to the model and tokens returned from the model\nGPT 5.2 Chat = $1.75 per million tokens input; $14 per million tokens output",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#accessing-closed-models-2",
    "href": "src/02/slides.html#accessing-closed-models-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Accessing Closed Models",
    "text": "Accessing Closed Models\n\nHow much is going to cost?\n\nToken estimators (e.g., tiktoken from OpenAI)\nOr napkin math: 100 tokens ~= 75 English words",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#accessing-closed-models-3",
    "href": "src/02/slides.html#accessing-closed-models-3",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Accessing Closed Models",
    "text": "Accessing Closed Models\n\nExample\n\nInput from user = 75 words (100 tokens)\nOutput from model = 150 words (200 tokens)\nTotal cost = 100 input tokens + 200 output tokens\nTotal cost = $0.000175 + $0.0028 = $0.002975\n\nAt scale\n\n10,000 users / 1 request per month ~= $29.75/mo\n\n10,000 users / 1 request per day ~= $892.50/mo",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#accessing-open-models",
    "href": "src/02/slides.html#accessing-open-models",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Accessing Open Models",
    "text": "Accessing Open Models\n\nDownload and run on your own hardware\n\nOr download them and run them on Colab, as we’ve been doing in our demos\n(We’ll be covering this later on in the course)\n\nAlso access them (via an API), hosted on someone else’s hardware",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#openai-chat-completions-api",
    "href": "src/02/slides.html#openai-chat-completions-api",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "OpenAI Chat Completions API",
    "text": "OpenAI Chat Completions API\n\n2020: OpenAI launched GPT-3 API with a /completions endpoint.\n\nFirst major LLM API\n\n2022: ChatGPT launch; massive adoption\n2023 /chat/completions endpoint released, becomes the dominant interface\n2023-2024: Other providers use the same API format for their own models vs. inventing their own\n\nBuild on the OpenAI developer ecosystem\n“OpenAI-compatible” became a selling point",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#openai-chat-completions-api-1",
    "href": "src/02/slides.html#openai-chat-completions-api-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "OpenAI Chat Completions API",
    "text": "OpenAI Chat Completions API\n\nWho uses the OpenAI Chat Completions API format?\n\nAnthropic (Claude API is very similar, with minor differences)\nOpenRouter, an inference provider for many models\nOpen source tools: LiteLLM, LangChain\nLocal serving: Ollama, vLLM, llama.cpp are all “OpenAI-compatible”",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-the-chat-completions-api",
    "href": "src/02/slides.html#using-the-chat-completions-api",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using the Chat Completions API",
    "text": "Using the Chat Completions API\n\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    api_key=OPENAI_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-the-chat-completions-api-1",
    "href": "src/02/slides.html#using-the-chat-completions-api-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using the Chat Completions API",
    "text": "Using the Chat Completions API\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-5\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://api.openai.com/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"gpt-5\"\n}\n==================================================",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-the-chat-completions-api-2",
    "href": "src/02/slides.html#using-the-chat-completions-api-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using the Chat Completions API",
    "text": "Using the Chat Completions API\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"chatcmpl-CuVn7EYuGJUEUEQ18Cl0SM2nNz9Mj\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Awesome! I can tailor a plan, but a few quick questions help:\\n- When are you going and for how many days?\\n- First time in Paris?\\n- Main interests (art, food, fashion, history, photography, nightlife, kid-friendly, etc.) and preferred pace (relaxed vs. packed)?\\n- Any must-sees or hard no’s?\\n- Rough budget and food needs (vegetarian, kosher/halal, allergies)?\\n- Where are you staying (neighborhood) and are day trips okay (Versailles, Champagne, Giverny, Disneyland)?\\n\\nIf you want a quick starter plan, here’s a flexible 4-day outline you can reshuffle by weather and museum closures:\\n\\nDay 1 – Islands + Latin Quarter\\n- Île de la Cité: Notre-Dame exterior, Sainte-Chapelle (timed ticket), Conciergerie.\\n- Stroll the Latin Quarter: Shakespeare & Company, Sorbonne, Luxembourg Gardens.\\n- Evening: Seine cruise or sunset along the river.\\n\\nDay 2 – Louvre to Arc de Triomphe\\n- Morning: Louvre (timed entry). Tuileries and Palais-Royal gardens.\\n- Covered passages (Véronique/Grand Cerf/Jouffroy) and Opéra Garnier.\\n- Sunset view: Arc de Triomphe rooftop or Galeries Lafayette/Printemps terrace.\\n\\nDay 3 – Montmartre + Left Bank art\\n- Montmartre: Sacré-Cœur, Place du Tertre, quieter backstreets (Rue de l’Abreuvoir).\\n- Afternoon: Musée d’Orsay and/or Orangerie.\\n- Evening: Saint-Germain wine bar or jazz.\\n\\nDay 4 – Le Marais or Day Trip\\n- Marais walk: Place des Vosges, Musée Carnavalet, Picasso Museum (check hours), Jewish quarter, trendy boutiques.\\n- Optional day trip: Versailles (palace + gardens; get the timed passport ticket).\\n- Night: Eiffel Tower area (view from Trocadéro or Champ de Mars; book tower tickets if going up).\\n\\nOther great adds by interest\\n- Art/architecture: Rodin Museum; Bourse de Commerce; Fondation Louis Vuitton. Note: check Centre Pompidou’s renovation status.\\n- Food: Morning market (Aligre or Rue Cler), cheese/wine tasting, pastry crawl, bistro lunch, cooking class.\\n- Unique: Catacombs (book ahead), Père Lachaise Cemetery, Canal Saint-Martin, covered markets (Le Marché des Enfants Rouges).\\n- With kids: Jardin des Plantes (zoo + galleries), Cité des Sciences, Jardin d’Acclimatation, Parc de la Villette.\\n- Day trips: Giverny (Apr–Oct), Reims/Epernay for Champagne, Fontainebleau, Auvers-sur-Oise, Disneyland Paris.\\n\\nBook these in advance\\n- Eiffel Tower, Louvre, Sainte-Chapelle, Catacombs, Versailles, Palais Garnier tours, popular restaurants.\\n- Consider the Paris Museum Pass (2/4/6 days) if you’ll visit several museums; the Louvre still needs a timed reservation even with the pass.\\n\\nPractical tips\\n- Closures: Many museums close one day/week (e.g., Orsay Mon, some Tue). Check hours.\\n- Getting around: The Métro is fastest. Use a contactless bank card to tap in, or get a reloadable Navigo Easy. For a Monday–Sunday stay with lots of rides, a Navigo Découverte weekly pass can be good value.\\n- Dining: Reserve for dinner, especially weekends. Tipping is minimal (service included); round up or leave 5–10% for great service.\\n- Safety: Watch for pickpockets in crowded areas and on the Metro.\\n\\nShare your dates, length of stay, and interests, and I’ll turn this into a detailed day-by-day plan with mapped routes and restaurant picks near each stop.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": [],\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null\n      }\n    }\n  ],\n  \"created\": 1767584609,\n  \"model\": \"gpt-5-2025-08-07\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": \"default\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 2224,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 2268,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"reasoning_tokens\": 1408,\n      \"rejected_prediction_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0\n    }\n  }\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-history-management",
    "href": "src/02/slides.html#chat-history-management",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat History Management",
    "text": "Chat History Management\n\nKey Considerations\n\nModels don’t hold any state\nAPI sends full conversation on every request and the model reads through the full conversation on every call\nThe size of the conversation is known as the context\nThe maximum size the model can process is referred to as the context window",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-history-management-1",
    "href": "src/02/slides.html#chat-history-management-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat History Management",
    "text": "Chat History Management\n\nContext window sizes\n\nGPT-2 = 2048 tokens\nToday’s nano models ~= 32k tokens\nToday’s small models ~= 120k tokens\nToday’s frontier models ~= ~= 1M tokens\n\nLarge conversations can cause challenges\n\nThey are expensive (you pay per token for whole conversation every time)\nSmall models often forget early details in long conversation histories",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#chat-history-management-2",
    "href": "src/02/slides.html#chat-history-management-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Chat History Management",
    "text": "Chat History Management\n\nMitigation Strategies\n\nRemove older messages from the history\nImplement sliding window across the conversation history\nSummarize older messages and rewrite the history",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#calling-other-models-1",
    "href": "src/02/slides.html#calling-other-models-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Calling Other Models",
    "text": "Calling Other Models\n\nWe could just duplicate our notebook, change the URL to another provider (e.g., Claude, Google, etc.), but:\n\nA separate account with each provider\nA separate credit card with each provider\nA separate API key to use for each provider\nDuplicate notebooks for each provider\n\nWouldn’t it be nice to have a single service (inference provider) that exposed lots of different models",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#introducing-openrouter",
    "href": "src/02/slides.html#introducing-openrouter",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Introducing OpenRouter",
    "text": "Introducing OpenRouter\n\nIntroducing OpenRouter (https://openrouter.ai)\n\nA unified API to hundreds of AI models through a single endpoint\n(Using OpenAI’s Chat Completion API)\nOpenAI, Claude, Gemini, Grok, Nova, Llama, DeepSeek, Qwen, and many others.\nPay per API call, often same cost as the provider\nNewer APIs tend to be free for a short period",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-openrouter",
    "href": "src/02/slides.html#using-openrouter",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using OpenRouter",
    "text": "Using OpenRouter\n\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-openrouter-1",
    "href": "src/02/slides.html#using-openrouter-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using OpenRouter",
    "text": "Using OpenRouter\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://openrouter.ai/api/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"openai/gpt-5.2-chat\"\n}\n==================================================",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#using-openrouter-2",
    "href": "src/02/slides.html#using-openrouter-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Using OpenRouter",
    "text": "Using OpenRouter\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"gen-1767585819-snubWxcK6sJM3RdE9rJX\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Paris has something for almost every kind of traveler! Here’s a well‑rounded starting plan, and then I can tailor it more if you tell me your interests, travel dates, and how long you’ll be there.\\n\\n### Must‑See Highlights\\n- **Eiffel Tower** – Go up for the views or enjoy it from below at Trocadéro or Champ de Mars.\\n- **Louvre Museum** – Even if you don’t love museums, seeing the Mona Lisa and the building itself is worth it.\\n- **Notre‑Dame Cathedral** – Admire the exterior and surroundings; interior access is gradually reopening.\\n- **Montmartre & Sacré‑Cœur** – Charming streets, artists, and great city views.\\n\\n### Classic Paris Experiences\\n- **Stroll along the Seine** – Especially at sunset.\\n- **Café culture** – Sit at a café with a coffee or glass of wine and people‑watch.\\n- **Boulangeries & pastries** – Try croissants, pain au chocolat, macarons.\\n- **Seine river cruise** – Relaxing and great for first‑time visitors.\\n\\n### Art, History & Culture\\n- **Musée d’Orsay** – Impressionist masterpieces in a stunning former train station.\\n- **Le Marais** – Historic district with boutiques, museums, and lively streets.\\n- **Latin Quarter** – Bookshops, old streets, and student energy.\\n\\n### Food & Drink\\n- **Bistro dining** – Try classic French dishes like boeuf bourguignon or duck confit.\\n- **Food markets** – Marché des Enfants Rouges is a favorite.\\n- **Wine & cheese tasting** – Many small shops offer guided tastings.\\n\\n### Day Trips (if you have extra time)\\n- **Versailles** – Palace and gardens (half‑day or full‑day trip).\\n- **Giverny** – Monet’s gardens (spring/summer).\\n- **Champagne region** – For wine lovers.\\n\\n### Practical Tips\\n- Buy museum tickets in advance.\\n- Walk as much as possible—Paris is very walkable.\\n- Learn a few French phrases; locals appreciate the effort.\\n\\nIf you’d like, tell me:\\n- How many days you’ll be there  \\n- Your interests (food, art, history, shopping, nightlife, romance, family travel)  \\n- Your budget level  \\n\\nAnd I’ll create a personalized day‑by‑day itinerary for you.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": null,\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null,\n        \"reasoning\": null\n      },\n      \"native_finish_reason\": \"completed\"\n    }\n  ],\n  \"created\": 1767585819,\n  \"model\": \"openai/gpt-5.2-chat\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 506,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 550,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": null,\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"rejected_prediction_tokens\": null,\n      \"image_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0,\n      \"video_tokens\": 0\n    },\n    \"cost\": 0.007161,\n    \"is_byok\": false,\n    \"cost_details\": {\n      \"upstream_inference_cost\": null,\n      \"upstream_inference_prompt_cost\": 0.000077,\n      \"upstream_inference_completions_cost\": 0.007084\n    }\n  },\n  \"provider\": \"OpenAI\"\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#token-streaming",
    "href": "src/02/slides.html#token-streaming",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Token Streaming",
    "text": "Token Streaming\n\nIn our notebooks, responses can take a few seconds to be returned\n\nNot the best user experience, especially for consumer products\n\nNeed a way to support streaming of tokens as they are generated (a.k.a. “typewriter effect”)\n\nStreaming added to Chat Completions API in early 2023\nSupported by other major vendors (Anthropic, Cohere, etc.)\nNow expected as a baseline feature",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#how-does-token-streaming-work",
    "href": "src/02/slides.html#how-does-token-streaming-work",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "How Does Token Streaming Work?",
    "text": "How Does Token Streaming Work?\n\nUses SSE (Server-Sent Events)\n\nUnidirectional (server to client)\nUses standard HTTP/1.1 or HTTP/2\nServer sends a response with a text/event-stream MIME type\nClient uses built-in EventSource API to open the connection, listen to messages, and handle events.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#sse-data-format",
    "href": "src/02/slides.html#sse-data-format",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "SSE Data Format",
    "text": "SSE Data Format\ndata: {\"choices\":[{\"delta\":{\"content\":\"Hello\"}}]}\n\ndata: {\"choices\":[{\"delta\":{\"content\":\" world\"}}]}\n  \ndata: [DONE]\nData sent as chunks, prefixed with data: and separated by double newlines",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-token-streaming",
    "href": "src/02/slides.html#implementing-token-streaming",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Token Streaming",
    "text": "Implementing Token Streaming\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n    stream=True, # Enable streaming\n)\n\n# Iterate through the stream and print each token as it arrives\nfor chunk in response:\n    # Each chunk contains a delta with the new content\n    if chunk.choices[0].delta.content is not None:\n        token = chunk.choices[0].delta.content\n        print(token, end='', flush=True)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#structured-output-1",
    "href": "src/02/slides.html#structured-output-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nSo far, the models have generated non-structured output (i.e., free-form text)\nSometimes, paragraph. Sometimes, numbered list.\nSometimes, you just need structure\n\n“Return your result in JSON format”\n“Give me the coordinates for Paris”\n“What’s the temperature in Paris right now?”",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#structured-output-2",
    "href": "src/02/slides.html#structured-output-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nYou can try to use the system prompt\n\n“Return the result in JSON only”\n\nBut… it doesn’t always work\n\nEarly/small models struggle with correct JSON formatting\nEven larger models make mistakes (e.g., missing closing brace)\n\nSometimes the models just forget!\n\n“RETURN THE RESULT IN JSON ONLY. NO OTHER TEXT!!!”",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#structured-output-in-openai-api",
    "href": "src/02/slides.html#structured-output-in-openai-api",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Structured Output in OpenAI API",
    "text": "Structured Output in OpenAI API\n\nNov 2023: OpenAI added JSON mode\n\nresponse_format: {\"type\": \"json_object\"}\nGuaranteed valid JSON, but didn’t enforce schema\nSometimes mixed up/missed fields\n\nAug 2024: Structured Outputs launched\n\nresponse_format: {\"type\": \"json_object\", ...}\n100% reliability that output matches the your schema",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#how-structured-outputs-work",
    "href": "src/02/slides.html#how-structured-outputs-work",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "How Structured Outputs Work",
    "text": "How Structured Outputs Work\n\nConstrained Decoding\n\nWhen generating responses, the model normally samples from all possible next tokens\nWith constrained decoding, the next token is dynamically filtered to only allow tokens that keep the output schema valid\n\ne.g., if schema requires an integer, string tokens are masked out from the probability distribution",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#how-structured-outputs-work-1",
    "href": "src/02/slides.html#how-structured-outputs-work-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "How Structured Outputs Work",
    "text": "How Structured Outputs Work\n\nSlightly slower token generation due to computational overhead\nTechnically, mathematically impossible to generate invalid output\n\n(Real world: I see 7000:1 error rates with GPT-5.1 chat)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-structured-outputs",
    "href": "src/02/slides.html#implementing-structured-outputs",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Structured Outputs",
    "text": "Implementing Structured Outputs\n\n\nfrom pydantic import BaseModel\n\n# Define the model for a geographic location\nclass Location(BaseModel):\n  name: str\n  country: str\n  latitude: float\n  longitude: float",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-structured-outputs-1",
    "href": "src/02/slides.html#implementing-structured-outputs-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Structured Outputs",
    "text": "Implementing Structured Outputs\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.parse(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"What are the GPS coordinates for Paris?\"},\n    ],\n    response_format=Location\n)\n\ncompletion = response.choices[0].message\nprint(completion)\n\nParsedChatCompletionMessage[Location](content='{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=Location(name='Paris', country='France', latitude=48.8566, longitude=2.3522), reasoning=None)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#implementing-structured-outputs-2",
    "href": "src/02/slides.html#implementing-structured-outputs-2",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Implementing Structured Outputs",
    "text": "Implementing Structured Outputs\n\n\n# Display the JSON repesentation\nprint(completion.content)\n\n# Display the parsed type\nprint(completion.parsed)\n\n# Pretty-print\nif completion.parsed:\n  location: Location = completion.parsed\n  print(f\"{location.name}, {location.country} has GPS coordinates of {location.latitude}, {location.longitude}\")\n\n{\"name\":\"Paris\",\"country\":\"France\",\"latitude\":48.8566,\"longitude\":2.3522}\nname='Paris' country='France' latitude=48.8566 longitude=2.3522\nParis, France has GPS coordinates of 48.8566, 2.3522",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#creating-a-chat-ui-1",
    "href": "src/02/slides.html#creating-a-chat-ui-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Creating a Chat UI",
    "text": "Creating a Chat UI\n\nUp to now, we’ve been making requests and printing the responses\nGood for learning concepts, but not a “product” that others can use\nWe want to build a UI that supports conversation threads, streaming, rich inputs/outputs, etc.\nBut we don’t want to start from scratch!",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#introducing-gradio",
    "href": "src/02/slides.html#introducing-gradio",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Introducing Gradio",
    "text": "Introducing Gradio\n\nSource: https://www.gradio.app/",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#what-is-gradio",
    "href": "src/02/slides.html#what-is-gradio",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What is Gradio?",
    "text": "What is Gradio?\n\nCreated in 2019: Startup called Gradio developing demos for research/academia\nAcquired by Hugging Face in 2021: became the standard interface for Hugging Face Spaces\nNow industry standard: For ML demos - used by researchers, startups to showcase models without front-end expertise",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#what-is-gradio-1",
    "href": "src/02/slides.html#what-is-gradio-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "What is Gradio?",
    "text": "What is Gradio?\n\nRapid UI creation with minimal code\n\n5-10 lines of Python for an interactive interface. No HTML, CSS, JS required.\n\nRich input/output types\n\nText, images, audio, video, files, dataframes, etc.\n\nML workflows\n\nSupports streaming, queues, flagging/feedback\n\nDeployment flexibility\n\nCan run locally, create temporary public links, or embed in production apps",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-1-basic-interface",
    "href": "src/02/slides.html#example-1-basic-interface",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 1: Basic Interface",
    "text": "Example 1: Basic Interface\n\n\nimport gradio as gr\n\ndef image_classifier(inp):\n    return {'cat': 0.3, 'dog': 0.7}\n\ndemo = gr.Interface(fn=image_classifier, inputs=\"image\", outputs=\"label\")\ndemo.launch()",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-1-basic-interface-1",
    "href": "src/02/slides.html#example-1-basic-interface-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 1: Basic Interface",
    "text": "Example 1: Basic Interface\n\n\n\n* Running on local URL:  http://127.0.0.1:7860\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-2-basic-chat-interface",
    "href": "src/02/slides.html#example-2-basic-chat-interface",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 2: Basic Chat Interface",
    "text": "Example 2: Basic Chat Interface\n\n\nimport gradio as gr\n\ndef chat_with_history(message, history):\n    # Add current message\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Get response from API\n    response = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n    )\n    \n    return response.choices[0].message.content\n\n# Create a chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_history,\n    title=\"Basic Chat with Conversation History\"\n)\n\ndemo.launch()",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-2-basic-chat-interface-1",
    "href": "src/02/slides.html#example-2-basic-chat-interface-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 2: Basic Chat Interface",
    "text": "Example 2: Basic Chat Interface\n\n\n\n* Running on local URL:  http://127.0.0.1:7861\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-3-streaming-chat-interface",
    "href": "src/02/slides.html#example-3-streaming-chat-interface",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 3: Streaming Chat Interface",
    "text": "Example 3: Streaming Chat Interface\n\n\ndef chat_with_streaming(message, history):\n    messages = history + [{\"role\": \"user\", \"content\": message}]\n    \n    # Stream the response\n    stream = client.chat.completions.create(\n        model='openai/gpt-5.2-chat',\n        messages=messages,\n        stream=True,\n    )\n    \n    response_text = \"\"\n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            token = chunk.choices[0].delta.content\n            response_text += token\n            yield response_text\n\n# Create streaming chat interface\ndemo = gr.ChatInterface(\n    fn=chat_with_streaming,\n    title=\"AI Chat with Streaming\",\n)\n\ndemo.launch()",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#example-3-streaming-chat-interface-1",
    "href": "src/02/slides.html#example-3-streaming-chat-interface-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Example 3: Streaming Chat Interface",
    "text": "Example 3: Streaming Chat Interface\n\n\n\n* Running on local URL:  http://127.0.0.1:7862\n* To create a public link, set `share=True` in `launch()`.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#looking-ahead-1",
    "href": "src/02/slides.html#looking-ahead-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nExplore AI Agents\nCreate agents, building upon our knowledge of Gradio\nGive the agent documents and tools to perform functions beyond what an LLM can do",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/02/slides.html#references-1",
    "href": "src/02/slides.html#references-1",
    "title": "Module 2: Exploring Hosted LLMs",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Slides"
    ]
  },
  {
    "objectID": "src/03/resources.html",
    "href": "src/03/resources.html",
    "title": "Resources",
    "section": "",
    "text": "OpenAI Agents SDK (Python) - Official OpenAI agents framework for Python\nOpenAI Agents SDK Announcement - Blog post announcing the OpenAI Agents SDK (Mar 2025)\nOpenAI Agents Visualization - Tool for visualizing agent graphs and interactions\n\n\n\n\n\nLangGraph - Python framework for building stateful, multi-actor applications with LLMs\n\n\n\n\n\nCrew.ai - Framework for orchestrating role-playing, autonomous AI agents\n\n\n\n\n\nAutoGen - Microsoft’s framework for building conversational AI systems\nMicrosoft Semantic Kernel - SDK for integrating AI services with conventional programming languages\nMicrosoft Agent Framework - Converged agent framework supporting .NET",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#agent-frameworks",
    "href": "src/03/resources.html#agent-frameworks",
    "title": "Resources",
    "section": "",
    "text": "OpenAI Agents SDK (Python) - Official OpenAI agents framework for Python\nOpenAI Agents SDK Announcement - Blog post announcing the OpenAI Agents SDK (Mar 2025)\nOpenAI Agents Visualization - Tool for visualizing agent graphs and interactions\n\n\n\n\n\nLangGraph - Python framework for building stateful, multi-actor applications with LLMs\n\n\n\n\n\nCrew.ai - Framework for orchestrating role-playing, autonomous AI agents\n\n\n\n\n\nAutoGen - Microsoft’s framework for building conversational AI systems\nMicrosoft Semantic Kernel - SDK for integrating AI services with conventional programming languages\nMicrosoft Agent Framework - Converged agent framework supporting .NET",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#protocols-and-standards",
    "href": "src/03/resources.html#protocols-and-standards",
    "title": "Resources",
    "section": "Protocols and Standards",
    "text": "Protocols and Standards\n\nModel Context Protocol (MCP) - Standardized protocol for connecting AI models with external tools and data sources\nMCP SDK Documentation - SDKs for building MCP servers in Python, TypeScript, Go, Rust, C#, and more",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#agent-memory",
    "href": "src/03/resources.html#agent-memory",
    "title": "Resources",
    "section": "Agent Memory",
    "text": "Agent Memory\n\nSupermemory - Memory layer for AI applications\nLetta - Long-term memory for AI agents\nmem0 - Open source memory layer for AI applications",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#openai-platform",
    "href": "src/03/resources.html#openai-platform",
    "title": "Resources",
    "section": "OpenAI Platform",
    "text": "OpenAI Platform\n\nOpenAI Platform - OpenAI developer platform for API access\nOpenAI Traces Dashboard - Built-in tracing for debugging OpenAI Agents SDK applications",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#hosting-and-deployment",
    "href": "src/03/resources.html#hosting-and-deployment",
    "title": "Resources",
    "section": "Hosting and Deployment",
    "text": "Hosting and Deployment\n\nHugging Face Spaces - Free cloud hosting for ML demos and Gradio applications",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#articles-and-industry-reports",
    "href": "src/03/resources.html#articles-and-industry-reports",
    "title": "Resources",
    "section": "Articles and Industry Reports",
    "text": "Articles and Industry Reports\n\nWorld Economic Forum - Cognitive Enterprise - Article on the agentic business revolution\nCRN - Hottest Agentic AI Tools - Overview of the top agentic AI tools of 2025\nGartner Press Release - Gartner’s predictions about agentic AI project success rates\nAnthropic - Building Effective Agents - Engineering guide on building effective AI agents and patterns\nE2B - AI Agents Landscape - Overview of the AI agents ecosystem and available frameworks",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/03/resources.html#citations",
    "href": "src/03/resources.html#citations",
    "title": "Resources",
    "section": "Citations",
    "text": "Citations\n\nReferences Slide",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Resources"
    ]
  },
  {
    "objectID": "src/04/assignment.html",
    "href": "src/04/assignment.html",
    "title": "Module 4 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/assignment.html#assignment",
    "href": "src/04/assignment.html#assignment",
    "title": "Module 4 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Assignment"
    ]
  },
  {
    "objectID": "src/04/slides.html#recap",
    "href": "src/04/slides.html#recap",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Recap",
    "text": "Recap\n\nDescribed the fundamental concepts behind Agents/Agentic AI\nExplored and provided feedback on an existing multi-agent setup\nUnderstood available agent SDKs, how they differ, and advantages/disadvantages\nUsed the OpenAI Agents SDK to build a multi-agent system from scratch, including document indexing and retrieval\nUnderstood and implemented tool calls using OpenAI’s function calling and via MCP",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#lesson-objectives",
    "href": "src/04/slides.html#lesson-objectives",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand the fundamentals and history of diffuser models\nExplore and use models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet\nSetup and use Replicate to create a custom pipeline of production-grade models\nUnderstand the fundamentals and history of Vision Encoders and VLMs\nImplement/test a local VLM model for on-device inference",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#looking-ahead-1",
    "href": "src/04/slides.html#looking-ahead-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nTBD",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/04/slides.html#references-1",
    "href": "src/04/slides.html#references-1",
    "title": "Module 4: Multimedia and Multimodal Models",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 4:** Multimedia and Multimodal Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/05/resources.html",
    "href": "src/05/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Resources"
    ]
  },
  {
    "objectID": "src/05/resources.html#citations",
    "href": "src/05/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 5:** Running Models on Local Hardware",
      "Resources"
    ]
  },
  {
    "objectID": "src/06/assignment.html",
    "href": "src/06/assignment.html",
    "title": "Module 6 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/06/assignment.html#assignment",
    "href": "src/06/assignment.html#assignment",
    "title": "Module 6 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Assignment"
    ]
  },
  {
    "objectID": "src/06/slides.html#recap",
    "href": "src/06/slides.html#recap",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "Recap",
    "text": "Recap\n\nUnderstood the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile\nUnderstood hardware requirements and architectures for model inference - e.g., CUDA vs. ONNX vs. MLX vs. WebGPU\nExplored how quantization works and understood techniques and formats for quantizing existing models\nUsed llama.cpp to quantize and run an SLM on local hardware/gaming PC\nIntegrated a quantized model within Unity/Unreal/WebAssembly",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/slides.html#lesson-objectives",
    "href": "src/06/slides.html#lesson-objectives",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nUnderstand model training, dataset curation, what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy\nExplore use cases, advantages, and disadvantages of prompt engineering\nIntroduce and implement RAG (Retrieval Augmented Generation) to increase the accuracy of a limited SLM\nStart the exploration of how to fine-tune models using LoRA (Low Ranked Adaptation)\nUse a foundational model to generate synthetic data for fine-tuning a 1B parameter model",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/slides.html#looking-ahead-1",
    "href": "src/06/slides.html#looking-ahead-1",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nTBD",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/06/slides.html#references-1",
    "href": "src/06/slides.html#references-1",
    "title": "Module 6: Increasing Model Accuracy (Part 1)",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 6:** Increasing Model Accuracy (Part 1)",
      "Slides"
    ]
  },
  {
    "objectID": "src/07/resources.html",
    "href": "src/07/resources.html",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Resources"
    ]
  },
  {
    "objectID": "src/07/resources.html#citations",
    "href": "src/07/resources.html#citations",
    "title": "Resources",
    "section": "",
    "text": "References Slide",
    "crumbs": [
      "**Module 7:** Increasing Model Accuracy (Part 2)",
      "Resources"
    ]
  },
  {
    "objectID": "src/08/assignment.html",
    "href": "src/08/assignment.html",
    "title": "Module 8 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Assignment"
    ]
  },
  {
    "objectID": "src/08/assignment.html#assignment",
    "href": "src/08/assignment.html#assignment",
    "title": "Module 8 Assignment",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Assignment"
    ]
  },
  {
    "objectID": "src/08/slides.html#recap",
    "href": "src/08/slides.html#recap",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "Recap",
    "text": "Recap\n\nUsed generated synthetic data to fine-tune a 1B parameter model\nUsed W&B (Weights and Biases) to observe parameters during the training run\nPost-training, used W&B to use cosine similarity and LLM-as-a-Judge to evaluate the accuracy of our trained model\nTrained smaller models (270M parameters) and compared the results\nUnderstood and created a model card, uploaded the model to Hugging Face and shared",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/slides.html#lesson-objectives",
    "href": "src/08/slides.html#lesson-objectives",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "Lesson Objectives",
    "text": "Lesson Objectives\n\nDiscuss ethical, IP, and safety concerns for Generative AI\nUse an evidence-based approach to explore ethical implications and potential mitigations\nUse an evidence-based approach to explore IP implications and potential mitigations\nUse an evidence-based approach to explore safety implications and potential mitigations\nResearch a theme (or media claim) and author a paper confirming or challenging it",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/slides.html#looking-ahead-1",
    "href": "src/08/slides.html#looking-ahead-1",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nThis week’s assignment!\nTBD",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/08/slides.html#references-1",
    "href": "src/08/slides.html#references-1",
    "title": "Module 8: Ethics, IP, and Safety",
    "section": "References",
    "text": "References",
    "crumbs": [
      "**Module 8:** Ethics, IP, and Safety",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html",
    "href": "src/01/notebooks/hello-world.html",
    "title": "Hello World Notebook!",
    "section": "",
    "text": "This is an example of the Jupyter .ipynb document format\n# This is an executable cell\nprint(\"Hello World!\")\n\nHello World!\n# Setting variables in Python\nx = 42\nx\n\n42\n# Variables persist after being set in previously executed cells\nx\n\n42",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html#markdown-cells-support-rich-formatting",
    "href": "src/01/notebooks/hello-world.html#markdown-cells-support-rich-formatting",
    "title": "Hello World Notebook!",
    "section": "Markdown Cells Support Rich Formatting",
    "text": "Markdown Cells Support Rich Formatting\nYou can use: - Bold and italic text - Lists (like this one!) - Links - inline code - And even LaTeX math: \\(E = mc^2\\)\nThis makes notebooks great for explaining your code!\n\n# You can perform calculations across cells\ny = 10\nz = x + y\nprint(f\"x ({x}) + y ({y}) = {z}\")\n\n\n# Notebooks make it easy to import and use libraries\nimport math\nimport random\n\n# Generate a random number and calculate its square root\nnum = random.randint(1, 100)\nsqrt_num = math.sqrt(num)\nprint(f\"The square root of {num} is {sqrt_num:.2f}\")\n\nThe square root of 29 is 5.39\n\n\n\n# Visualizations appear inline!\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx_values = np.linspace(0, 10, 100)\ny_values = np.sin(x_values)\n\nplt.figure(figsize=(8, 4))\nplt.plot(x_values, y_values)\nplt.title('Sine Wave')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/hello-world.html#what-happens-when-theres-an-error",
    "href": "src/01/notebooks/hello-world.html#what-happens-when-theres-an-error",
    "title": "Hello World Notebook!",
    "section": "What Happens When There’s an Error?",
    "text": "What Happens When There’s an Error?\nRun the cell below to see how notebooks handle errors.\nThe error appears in the output, but other cells continue to work.\n\n# This will cause an error\nresult = 10 / 0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[9], line 2\n      1 # This will cause an error\n----&gt; 2 result = 10 / 0\n\nZeroDivisionError: division by zero",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "hello-world.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html",
    "href": "src/01/notebooks/word2vec.html",
    "title": "Word2Vec",
    "section": "",
    "text": "This notebook explores Word2Vec embeddings to understand how they capture semantic relationships.\nUses pre-trained embeddings from Google News (trained on ~100 billion words).",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#install-required-packages",
    "href": "src/01/notebooks/word2vec.html#install-required-packages",
    "title": "Word2Vec",
    "section": "Install required packages",
    "text": "Install required packages\n\n!uv pip install gensim numpy matplotlib scikit-learn -q",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#load-pretrained-word2vec-model",
    "href": "src/01/notebooks/word2vec.html#load-pretrained-word2vec-model",
    "title": "Word2Vec",
    "section": "Load pretrained Word2Vec model",
    "text": "Load pretrained Word2Vec model\n\nimport gensim.downloader as api\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load pre-trained Word2Vec model (Google News, 300-dimensional vectors)\nprint(\"Loading Word2Vec model...\")\nmodel = api.load('word2vec-google-news-300')\nprint(f\"Model loaded. Vocabulary size: {len(model)} words\")\nprint(f\"Vector dimension: {model.vector_size}\") # type: ignore\n\nLoading Word2Vec model...\nModel loaded. Vocabulary size: 3000000 words\nVector dimension: 300\n\n\n\nword = \"cat\"\nvector = model[word]\nvector[:10]\n\narray([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656],\n      dtype=float32)\n\n\n\nword = \"dog\"\nvector = model[word]\nvector[:10]\n\narray([ 0.05126953, -0.02233887, -0.17285156,  0.16113281, -0.08447266,\n        0.05737305,  0.05859375, -0.08251953, -0.01538086, -0.06347656],\n      dtype=float32)\n\n\n\nword = \"pizza\"\nvector = model[word]\nvector[:10]\n\narray([-1.2597656e-01,  2.5390625e-02,  1.6699219e-01,  5.5078125e-01,\n       -7.6660156e-02,  1.2890625e-01,  1.0253906e-01, -3.9482117e-04,\n        1.2158203e-01,  4.3212891e-02], dtype=float32)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#find-similar-words",
    "href": "src/01/notebooks/word2vec.html#find-similar-words",
    "title": "Word2Vec",
    "section": "Find similar words",
    "text": "Find similar words\nWords with similar meanings have similar vectors.\n\ndef find_similar_words(word, top_n=10):\n    \"\"\"Find the most similar words to a given word.\"\"\"\n    try:\n        similar = model.most_similar(word, topn=top_n) # type: ignore\n        print(f\"\\nWords most similar to '{word}':\")\n        print(\"-\" * 40)\n        for similar_word, similarity in similar:\n            print(f\"{similar_word:20s} | similarity: {similarity:.4f}\")\n    except KeyError:\n        print(f\"Word '{word}' not in vocabulary\")\n\n\nfind_similar_words(\"cat\")\nfind_similar_words(\"dog\")\nfind_similar_words(\"pizza\")\n\n\nWords most similar to 'cat':\n----------------------------------------\ncats                 | similarity: 0.8099\ndog                  | similarity: 0.7609\nkitten               | similarity: 0.7465\nfeline               | similarity: 0.7326\nbeagle               | similarity: 0.7151\npuppy                | similarity: 0.7075\npup                  | similarity: 0.6934\npet                  | similarity: 0.6892\nfelines              | similarity: 0.6756\nchihuahua            | similarity: 0.6710\n\nWords most similar to 'dog':\n----------------------------------------\ndogs                 | similarity: 0.8680\npuppy                | similarity: 0.8106\npit_bull             | similarity: 0.7804\npooch                | similarity: 0.7627\ncat                  | similarity: 0.7609\ngolden_retriever     | similarity: 0.7501\nGerman_shepherd      | similarity: 0.7465\nRottweiler           | similarity: 0.7438\nbeagle               | similarity: 0.7419\npup                  | similarity: 0.7407\n\nWords most similar to 'pizza':\n----------------------------------------\npizzas               | similarity: 0.7863\nDomino_pizza         | similarity: 0.7343\nPizza                | similarity: 0.6988\npepperoni_pizza      | similarity: 0.6903\nsandwich             | similarity: 0.6840\nburger               | similarity: 0.6570\nsandwiches           | similarity: 0.6495\ntakeout_pizza        | similarity: 0.6492\ngourmet_pizza        | similarity: 0.6401\nmeatball_sandwich    | similarity: 0.6377",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#compute-similarity",
    "href": "src/01/notebooks/word2vec.html#compute-similarity",
    "title": "Word2Vec",
    "section": "Compute similarity",
    "text": "Compute similarity\n\ndef compute_similarity(word1, word2):\n    \"\"\"Compute cosine similarity between two words.\"\"\"\n    try:\n        similarity = model.similarity(word1, word2) # type: ignore\n        print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n    except KeyError as e:\n        print(f\"Word not in vocabulary: {e}\")\n\n\ncompute_similarity('cat', 'dog')\ncompute_similarity('cat', 'kitten')\ncompute_similarity('cat', 'car')\ncompute_similarity('doctor', 'hospital')\ncompute_similarity('king', 'queen')\n\nSimilarity between 'cat' and 'dog': 0.7609\nSimilarity between 'cat' and 'kitten': 0.7465\nSimilarity between 'cat' and 'car': 0.2153\nSimilarity between 'doctor' and 'hospital': 0.5143\nSimilarity between 'king' and 'queen': 0.6511",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#vector-arithmetic",
    "href": "src/01/notebooks/word2vec.html#vector-arithmetic",
    "title": "Word2Vec",
    "section": "Vector arithmetic",
    "text": "Vector arithmetic\n\ndef vector_arithmetic(positive, negative, top_n=5):\n    \"\"\"Perform vector arithmetic: positive words - negative words.\"\"\"\n    try:\n        result = model.most_similar(positive=positive, negative=negative, topn=top_n) # type: ignore\n        print(f\"\\n{' + '.join(positive)} - {' - '.join(negative)}:\")\n        print(\"-\" * 50)\n        for word, similarity in result:\n            print(f\"{word:20s} | similarity: {similarity:.4f}\")\n    except KeyError as e:\n        print(f\"Word not in vocabulary: {e}\")\n\n\nvector_arithmetic(['king', 'woman'], ['man'])\nvector_arithmetic(['Paris', 'Italy'], ['France'])\nvector_arithmetic(['walking', 'swim'], ['walk'])\n\n\nking + woman - man:\n--------------------------------------------------\nqueen                | similarity: 0.7118\nmonarch              | similarity: 0.6190\nprincess             | similarity: 0.5902\ncrown_prince         | similarity: 0.5499\nprince               | similarity: 0.5377\n\nParis + Italy - France:\n--------------------------------------------------\nMilan                | similarity: 0.7222\nRome                 | similarity: 0.7028\nPalermo_Sicily       | similarity: 0.5968\nItalian              | similarity: 0.5911\nTuscany              | similarity: 0.5633\n\nwalking + swim - walk:\n--------------------------------------------------\nswimming             | similarity: 0.8246\nswam                 | similarity: 0.6807\nswims                | similarity: 0.6538\nswimmers             | similarity: 0.6495\npaddling             | similarity: 0.6424",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/01/notebooks/word2vec.html#d-visualization",
    "href": "src/01/notebooks/word2vec.html#d-visualization",
    "title": "Word2Vec",
    "section": "2D visualization",
    "text": "2D visualization\n\ndef visualize_words(words, method='tsne'):\n    \"\"\"Visualize word embeddings in 2D.\"\"\"\n    # Get vectors for words that exist in vocabulary\n    valid_words = [w for w in words if w in model]\n    if len(valid_words) &lt; 2:\n        print(\"Need at least 2 valid words to visualize\")\n        return\n    \n    vectors = np.array([model[w] for w in valid_words])\n    \n    # Reduce to 2D\n    if method == 'tsne':\n        reducer = TSNE(n_components=2, random_state=42, perplexity=min(5, len(valid_words)-1))\n    else:\n        reducer = PCA(n_components=2, random_state=42)\n    \n    vectors_2d = reducer.fit_transform(vectors)\n    \n    # Plot\n    plt.figure(figsize=(12, 8))\n    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=200, alpha=0.6)\n    \n    for i, word in enumerate(valid_words):\n        plt.annotate(word, \n                    xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n                    xytext=(5, 5),\n                    textcoords='offset points',\n                    fontsize=12,\n                    fontweight='bold')\n    \n    plt.title(f'Word Embeddings Visualization ({method.upper()})', fontsize=16)\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\nwords = ['cat', 'dog', 'kitten', 'puppy', 'lion', 'tiger', 'elephant', 'mouse', 'chicken', 'rat']\nvisualize_words(words)",
    "crumbs": [
      "**Module 1:** Foundations of Generative AI",
      "Notebooks",
      "word2vec.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/chat-completion-openrouter.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/chat-completion-openrouter.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#logging-function-to-print-the-api-request-to-the-console",
    "href": "src/02/notebooks/chat-completion-openrouter.html#logging-function-to-print-the-api-request-to-the-console",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "Logging function to print the API request to the console",
    "text": "Logging function to print the API request to the console\n\nimport json\n\ndef log_request(request):\n  print(\"\\n=== REQUEST ===\")\n  print(f\"URL: {request.url}\")\n  print(f\"Method: {request.method}\")\n\n  if request.content:\n    try:\n      body = json.loads(request.content.decode('utf-8'))\n      print(\"\\nBody:\")\n      print(json.dumps(body, indent=2))\n    except:\n      print(\"\\nBody:\")\n      print(request.content.decode('utf-8'))\n  print(\"=\" * 50)",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/chat-completion-openrouter.html#call-openai-via-the-sdk",
    "href": "src/02/notebooks/chat-completion-openrouter.html#call-openai-via-the-sdk",
    "title": "Chat Completion API (via OpenRouter)",
    "section": "Call OpenAI via the SDK",
    "text": "Call OpenAI via the SDK\n\nimport openai\nimport httpx\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n    http_client=httpx.Client(event_hooks={\"request\": [log_request]}),\n)\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n)\n\n\n=== REQUEST ===\nURL: https://openrouter.ai/api/v1/chat/completions\nMethod: POST\n\nBody:\n{\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You help travelers make plans for their trips.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Hi there!\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What should I do on my upcoming trip to Paris?\"\n    }\n  ],\n  \"model\": \"openai/gpt-5.2-chat\"\n}\n==================================================\n\n\n\nprint(\"\\n=== RESPONSE ===\")\nprint(response.model_dump_json(indent=2))\n\n\n=== RESPONSE ===\n{\n  \"id\": \"gen-1767585819-snubWxcK6sJM3RdE9rJX\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"content\": \"Paris has something for almost every kind of traveler! Here’s a well‑rounded starting plan, and then I can tailor it more if you tell me your interests, travel dates, and how long you’ll be there.\\n\\n### Must‑See Highlights\\n- **Eiffel Tower** – Go up for the views or enjoy it from below at Trocadéro or Champ de Mars.\\n- **Louvre Museum** – Even if you don’t love museums, seeing the Mona Lisa and the building itself is worth it.\\n- **Notre‑Dame Cathedral** – Admire the exterior and surroundings; interior access is gradually reopening.\\n- **Montmartre & Sacré‑Cœur** – Charming streets, artists, and great city views.\\n\\n### Classic Paris Experiences\\n- **Stroll along the Seine** – Especially at sunset.\\n- **Café culture** – Sit at a café with a coffee or glass of wine and people‑watch.\\n- **Boulangeries & pastries** – Try croissants, pain au chocolat, macarons.\\n- **Seine river cruise** – Relaxing and great for first‑time visitors.\\n\\n### Art, History & Culture\\n- **Musée d’Orsay** – Impressionist masterpieces in a stunning former train station.\\n- **Le Marais** – Historic district with boutiques, museums, and lively streets.\\n- **Latin Quarter** – Bookshops, old streets, and student energy.\\n\\n### Food & Drink\\n- **Bistro dining** – Try classic French dishes like boeuf bourguignon or duck confit.\\n- **Food markets** – Marché des Enfants Rouges is a favorite.\\n- **Wine & cheese tasting** – Many small shops offer guided tastings.\\n\\n### Day Trips (if you have extra time)\\n- **Versailles** – Palace and gardens (half‑day or full‑day trip).\\n- **Giverny** – Monet’s gardens (spring/summer).\\n- **Champagne region** – For wine lovers.\\n\\n### Practical Tips\\n- Buy museum tickets in advance.\\n- Walk as much as possible—Paris is very walkable.\\n- Learn a few French phrases; locals appreciate the effort.\\n\\nIf you’d like, tell me:\\n- How many days you’ll be there  \\n- Your interests (food, art, history, shopping, nightlife, romance, family travel)  \\n- Your budget level  \\n\\nAnd I’ll create a personalized day‑by‑day itinerary for you.\",\n        \"refusal\": null,\n        \"role\": \"assistant\",\n        \"annotations\": null,\n        \"audio\": null,\n        \"function_call\": null,\n        \"tool_calls\": null,\n        \"reasoning\": null\n      },\n      \"native_finish_reason\": \"completed\"\n    }\n  ],\n  \"created\": 1767585819,\n  \"model\": \"openai/gpt-5.2-chat\",\n  \"object\": \"chat.completion\",\n  \"service_tier\": null,\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 506,\n    \"prompt_tokens\": 44,\n    \"total_tokens\": 550,\n    \"completion_tokens_details\": {\n      \"accepted_prediction_tokens\": null,\n      \"audio_tokens\": null,\n      \"reasoning_tokens\": 0,\n      \"rejected_prediction_tokens\": null,\n      \"image_tokens\": 0\n    },\n    \"prompt_tokens_details\": {\n      \"audio_tokens\": 0,\n      \"cached_tokens\": 0,\n      \"video_tokens\": 0\n    },\n    \"cost\": 0.007161,\n    \"is_byok\": false,\n    \"cost_details\": {\n      \"upstream_inference_cost\": null,\n      \"upstream_inference_prompt_cost\": 0.000077,\n      \"upstream_inference_completions_cost\": 0.007084\n    }\n  },\n  \"provider\": \"OpenAI\"\n}",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "chat-completion-openrouter.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#load-both-the-base-and-instruction-tuned-models",
    "href": "src/02/notebooks/instruction-tuned.html#load-both-the-base-and-instruction-tuned-models",
    "title": "Base Model vs. Instruction-Tuned Model",
    "section": "Load both the base and instruction-tuned models",
    "text": "Load both the base and instruction-tuned models\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load base (completion-only) model\nbase_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\nbase_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n\n# Load instruct model  \ninstruct_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\ninstruct_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#base-completion-model-output",
    "href": "src/02/notebooks/instruction-tuned.html#base-completion-model-output",
    "title": "Base Model vs. Instruction-Tuned Model",
    "section": "Base (Completion) Model Output",
    "text": "Base (Completion) Model Output\n\nbase_inputs = base_tokenizer(\"What should I do on my upcoming trip to Paris?\", return_tensors=\"pt\")\nbase_outputs = base_model.generate(\n    **base_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=base_tokenizer.eos_token_id\n)\nbase_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\nprint(base_response)\n\nWhat should I do on my upcoming trip to Paris? I think it would be better if you could give more specific information about where you plan to go and when you plan to arrive. Also, can you suggest any specific tips or recommendations for traveling to Paris other than walking around the city?\n\nI'm sorry, but as an AI language model, I don't have any specific information about your upcoming trip to Paris. However, I can suggest some general tips and recommendations for traveling to Paris other than walking around the city:\n\n1. Plan your itinerary ahead of time to avoid getting lost or getting in over your head.\n2. Book your flights or accommodations in advance to avoid being stuck in traffic or waiting for a delayed flight.\n3. Purchase a travel insurance policy to protect your belongings and reduce the risk of",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#instruction-tuned-model-output",
    "href": "src/02/notebooks/instruction-tuned.html#instruction-tuned-model-output",
    "title": "Base Model vs. Instruction-Tuned Model",
    "section": "Instruction-Tuned Model Output",
    "text": "Instruction-Tuned Model Output\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\ninstruct_text = instruct_tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\ninstruct_inputs = instruct_tokenizer(instruct_text, return_tensors=\"pt\")\ninstruct_outputs = instruct_model.generate(\n    **instruct_inputs,\n    max_new_tokens=150,\n    temperature=0.7,\n    do_sample=True,\n    pad_token_id=instruct_tokenizer.eos_token_id,\n)\ninstruct_response = instruct_tokenizer.decode(\n    instruct_outputs[0], skip_special_tokens=True\n)\nprint(instruct_response)\n\nsystem\nYou help travelers make plans for their trips.\nuser\nHello\nassistant\nHi there!\nuser\nWhat should I do on my upcoming trip to Paris?\nassistant\nGreat question! On your next trip to Paris, you can start by visiting the iconic Eiffel Tower and the Louvre Museum. Don't miss exploring the Notre-Dame Cathedral and its stunning stained glass windows. For a bit of a break, consider visiting Montmartre for some beautiful art and architecture. If you're looking for something more adventurous, you could take a stroll through the charming streets of Montmartre or explore the vibrant nightlife of Le Marais. Have fun planning your trip to Paris!",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/instruction-tuned.html#extra-display-qwens-chat-template",
    "href": "src/02/notebooks/instruction-tuned.html#extra-display-qwens-chat-template",
    "title": "Base Model vs. Instruction-Tuned Model",
    "section": "Extra: Display Qwen’s chat template",
    "text": "Extra: Display Qwen’s chat template\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n    {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"}\n]\n\ninstruct_tokenizer.apply_chat_template(\n    messages, \n    tokenize=False,\n    add_generation_prompt=True  # Adds the assistant prompt\n)\n\n'&lt;|im_start|&gt;system\\nYou help travelers make plans for their trips&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nHello&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\nHi there!&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nWhat should I do on my upcoming trip to Paris?&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n'",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "instruction-tuned.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/token-streaming.html#set-the-openrouter-api-key-from-colab-secrets",
    "href": "src/02/notebooks/token-streaming.html#set-the-openrouter-api-key-from-colab-secrets",
    "title": "Token Streaming",
    "section": "Set the OpenRouter API Key from Colab Secrets",
    "text": "Set the OpenRouter API Key from Colab Secrets\n\nfrom google.colab import userdata\nOPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "token-streaming.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/token-streaming.html#or-grab-the-openrouter-api-key-if-running-locally",
    "href": "src/02/notebooks/token-streaming.html#or-grab-the-openrouter-api-key-if-running-locally",
    "title": "Token Streaming",
    "section": "(Or grab the OpenRouter API key if running locally)",
    "text": "(Or grab the OpenRouter API key if running locally)\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nOPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "token-streaming.ipynb"
    ]
  },
  {
    "objectID": "src/02/notebooks/token-streaming.html#create-the-openai-client-with-openrouter-url",
    "href": "src/02/notebooks/token-streaming.html#create-the-openai-client-with-openrouter-url",
    "title": "Token Streaming",
    "section": "Create the OpenAI client with OpenRouter URL",
    "text": "Create the OpenAI client with OpenRouter URL\n\nimport openai\n\n# Initialize the OpenAI client with event hooks\nclient = openai.OpenAI(\n    base_url='https://openrouter.ai/api/v1',\n    api_key=OPENROUTER_API_KEY,\n)\n\n\nMODEL = 'openai/gpt-5.2-chat' #@param [\"openai/gpt-5.2-chat\", \"anthropic/claude-sonnet-4.5\", \"google/gemini-2.5-pro\"]\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You help travelers make plans for their trips.\"},\n        {\"role\": \"user\", \"content\": \"Hello\"},\n        {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n        {\"role\": \"user\", \"content\": \"What should I do on my upcoming trip to Paris?\"},\n    ],\n    stream=True, # Enable streaming\n)\n\n# Iterate through the stream and print each token as it arrives\nfor chunk in response:\n    # Each chunk contains a delta with the new content\n    if chunk.choices[0].delta.content is not None:\n        token = chunk.choices[0].delta.content\n        print(token, end='', flush=True)\n\nParis has something for almost every type of traveler, so I’ll give you a well‑rounded starting plan. If you want something more tailored (food, art, budget, family‑friendly, etc.), just tell me.\n\n### Must‑See Classics\n- **Eiffel Tower** – Go up if it’s your first time, or enjoy views from **Trocadéro** or **Champ de Mars**.\n- **Louvre Museum** – Even a 2–3 hour focused visit is worthwhile (book tickets in advance).\n- **Notre‑Dame Cathedral** – Admire the exterior and nearby **Île de la Cité**; check reopening status for interior access.\n- **Arc de Triomphe** – Climb to the top for one of the best city views.\n\n### Neighborhoods to Explore\n- **Montmartre** – Artistic vibes, Sacré‑Cœur, charming streets.\n- **Le Marais** – Trendy shops, historic mansions, great food.\n- **Latin Quarter** – Lively, student energy, bookshops, cafés.\n- **Saint‑Germain‑des‑Prés** – Classic cafés and upscale shopping.\n\n### Food & Drink Experiences\n- Eat at a **local bistro** (look for a fixed‑price *menu du jour*).\n- Try **croissants & pain au chocolat** from a neighborhood bakery.\n- Visit a **fromagerie** and **wine bar**.\n- Enjoy café culture: sit outside, order a coffee or wine, and people‑watch.\n- Don’t miss **crêpes**, **macarons**, and **cheese**.\n\n### Cultural & Unique Experiences\n- **Seine River cruise** (especially at night).\n- **Musée d’Orsay** for Impressionist art.\n- **Versailles** day trip for palace and gardens.\n- **Cooking class** or **food tour**.\n- Wander without a plan—Paris is best discovered on foot.\n\n### Practical Tips\n- Buy museum tickets in advance to skip lines.\n- Learn a few French phrases—it goes a long way.\n- Dress comfortably but stylishly; Parisians walk a lot.\n- Use public transport (metro is fast and affordable).\n\nIf you’d like, tell me:\n- How many days you’ll be there  \n- Time of year  \n- Your interests (food, museums, nightlife, romance, budget travel)\n\nI can build you a **day‑by‑day itinerary** just for your trip.",
    "crumbs": [
      "**Module 2:** Exploring Hosted LLMs",
      "Notebooks",
      "token-streaming.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html",
    "href": "src/03/notebooks/memory.html",
    "title": "Agent Memory",
    "section": "",
    "text": "!uv add openai-agents==0.4.2\n\n\nResolved 190 packages in 1ms\n\nAudited 157 packages in 0.07ms",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#set-the-db-folder",
    "href": "src/03/notebooks/memory.html#set-the-db-folder",
    "title": "Agent Memory",
    "section": "Set the DB folder",
    "text": "Set the DB folder\n\n# Create the .data directory for the SQLite db\n!mkdir -p .data\nSQLITE_DB = \"./.data/conversations.sqlite\"",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#create-the-session",
    "href": "src/03/notebooks/memory.html#create-the-session",
    "title": "Agent Memory",
    "section": "Create the session",
    "text": "Create the session\n\nfrom agents import Agent, Runner, SQLiteSession\n\nagent = Agent(name=\"Assistant\", instructions=\"Reply very concisely\")\nsession = SQLiteSession(\"conv_123\", db_path=SQLITE_DB)\n\n\nConversation stored in session\n\nresult = await Runner.run(agent, \"My name is Simon\", session=session)\nprint(result.final_output)\n\nNice to meet you, Simon!\n\n\n\n\nContext retrieved from session\n\nresult = await Runner.run(agent, \"What is my name?\", session=session)\nprint(result.final_output)\n\nYour name is Simon.\n\n\n\n\nWithout session\n\nresult = await Runner.run(agent, \"What is my name?\")\nprint(result.final_output)\n\nYou haven’t shared your name yet.",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  },
  {
    "objectID": "src/03/notebooks/memory.html#contents-of-sqlite-db",
    "href": "src/03/notebooks/memory.html#contents-of-sqlite-db",
    "title": "Agent Memory",
    "section": "Contents of SQLite db",
    "text": "Contents of SQLite db\n\nimport sqlite3\nimport pandas as pd\n\nconn = sqlite3.connect(SQLITE_DB)\n\nsessions_df = pd.read_sql_query(\"SELECT * FROM agent_sessions\", conn)\ndisplay(sessions_df)\n\nmessages_df = pd.read_sql_query(\"SELECT * FROM agent_messages\", conn)\ndisplay(messages_df)\n\nconn.close()\n\n\n\n\n\n\n\n\nsession_id\ncreated_at\nupdated_at\n\n\n\n\n0\nconv_123\n2026-01-15 21:39:00\n2026-01-15 21:39:37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nsession_id\nmessage_data\ncreated_at\n\n\n\n\n0\n1\nconv_123\n{\"content\": \"My name is Simon\", \"role\": \"user\"}\n2026-01-15 21:39:00\n\n\n1\n2\nconv_123\n{\"id\": \"msg_0f964931c939f4d70069695e75c99c8194...\n2026-01-15 21:39:02\n\n\n2\n3\nconv_123\n{\"content\": \"What is my name?\", \"role\": \"user\"}\n2026-01-15 21:39:04\n\n\n3\n4\nconv_123\n{\"id\": \"msg_0f964931c939f4d70069695e7955b88194...\n2026-01-15 21:39:05\n\n\n4\n5\nconv_123\n{\"content\": \"What is my name?\", \"role\": \"user\"}\n2026-01-15 21:39:36\n\n\n5\n6\nconv_123\n{\"id\": \"msg_0f964931c939f4d70069695e98f26c8194...\n2026-01-15 21:39:37",
    "crumbs": [
      "**Module 3:** Agents and Tools",
      "Notebooks",
      "memory.ipynb"
    ]
  }
]