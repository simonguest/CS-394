---
title: "Module 7: Increasing Model Accuracy (Part 2)"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Understood what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy
- Explored prompt engineering and thinking models
- Introduced and implemented Text-to-SQL and RAG (Retrieval-Augmented Generation) to increase the accuracy of a limited SLM
- Started exploring model fine-tuning
- Generated synthetic data for fine-tuning a small language model

## Lesson Objectives

- Use your generated synthetic data to fine-tune a SLM model using QLoRA
- Use W&B (Weights and Biases) to observe metrics during the training run
- Post-training, test and evaluate the accuracy of your fine-tuned model
- Merge, quantize, and upload your model to Hugging Face to share with others
- Understand and create a model card for your newly fine-tuned model

# Model Training Stages

## Model Training Stages

- There are three main stages of training a model:
  - **Pretraining:** The initial, large-scale training run that creates a base model with pretrained weights
    - Training data: Trillions of tokens from the internet, books, code, etc.
  - **Supervised Fine-Tuning (SFT):** Shapes the base model to follow instructions and respond in a desired style or fornat
    - Training data: Hundreds to thousands of input/output pairs
  - **Alignment:** Further refines the model toward preferred behaviors and values
    - Training data: Human-curated preferences and/or reward-based feedback

## Model Training Stages

TBD: Diagram

## Model Training Stages

- Many models (e.g., google/gemma-3-it) have already gone through these three stages
  - We will be adding a fourth stage. An SFT of an existing "polished" model
- Why?
  - The model already knows how to follow instructions, answer questions, etc.
  - They need far less training data to get good results
  - Training tends to be faster and more stable since they are starting from a good baseline

## Model Training Stages

TBD: Updated diagram

# Supervised Fine-Tuning (SFT)

## Supervised Fine-Tuning (SFT)

- How does SFT work?
  - Build a dataset (we did this already!)
  - Feed the prompts into the model and compare the desired output tokens with the model's predictions
  - The difference between the two is known as the "training loss"
  - Backpropagate through the model, nudging the weights to guide toward the desired output
  - This "nudging" is done using an optimizer (typically AdamW)
  - Repeat for typically 1-3 epochs (complete passes through the dataset)
  - Monitor validation loss to detect overfitting or diminishing returns

## Supervised Fine-Tuning (SFT)

TBD: Diagram 

## Supervised Fine-Tuning (SFT) with LoRA

- Adjusting all parameters in a model can be compute and memory intensive
- We will use LoRA (Low-Rank Adaptation), a Parameter Efficient Fine-Tuning (PEFT) strategy
- How LoRA works:
  - The model's base weights (W) are frozen (never updated) during training
  - Two small matrices (A and B) are introduced, whose product represents the desired change to W
  - Only A and B are updated during training — a tiny fraction of the total parameters
  - After training, these matrices are known as an "adapter"
  - The adapter can be kept separate (swappable) or merged back into W to create a new, standalone model

## Supervised Fine-Tuning (SFT) with LoRA

TBD: Diagram

## Supervised Fine-Tuning (SFT) with QLoRA

- To further increase memory efficiency, we will use QLoRA (Quantized LoRA)
  - The base model weights (W) are quantized to 4-bit (NF4 format) for storage
  - During the forward pass, W is dequantized to bf16 on the fly for computation, then discarded
  - Adapter matrices A and B are kept at full precision (bf16) for training accuracy
  - Since only A and B are trained, optimizer states (which can be 3-4x the size of W at full precision) only exist for the tiny adapter matrices — dramatically reducing memory usage

## Supervised Fine-Tuning (SFT) with QLoRA

TBD: Diagram

# Fine-Tuning Plan

## Fine-Tuning Plan

- Upload our .jsonl files to Hugging Face to create a dataset
- In our training notebook...
  - Download the dataset
  - Format our training data to match the chat template
  - Start our training run!
- Use W&B (Weights & Biases) to monitor how well our model is learning
  - Training loss
  - Validation loss

## Fine-Tuning Plan 

- After training has completed...
  - Test (both manually and automatic)
- Assuming things went well...
  - Merge and upload our model to Hugging Face
  - Quantize our model (create a GGUF) and upload to Hugging Face
  - Publish a model card
  
# Upload Training Files

## Upload Training Files

- Upload our train.jsonl, validation.jsonl, and test.jsonl files to a Hugging Face Dataset
- Why?
  - Single repository
  - Easy to hook into training runs
  - Shareable with others
  - Better UI and query semantics vs. raw .jsonl files

## Upload Training Files

{{< embed notebooks/create-dataset.ipynb#create-dataset echo=true outputs=false >}}

## Upload Training Files

{{< embed notebooks/create-dataset.ipynb#upload echo=true outputs=false >}}

## Upload Training Files

- TBD: Will take a while before you can browse your data
- TBD: Conversion to Parquet format

# Demo

Uploading the training data files to Hugging Face Dataset

# Hands-on

Upload your training data files to Hugging Face Dataset

# Let's Train!

## Let's Train!

- TBD

## Sidebar: Weights and Biases

- TBD. Explain what W&B is and how we'll be using it

## Training Notebook

- TBD: Walkthrough of the training notebook.
- TBD: LORA Hyperparameters

# Demo

Starting the training run!

# W&B Metrics

## W&B Metrics

- TBD: screenshot of system metrics

## W&B Metrics

- System Metrics
  - Metrics on the health of the underlying hardware used to train the model
  - Providing automatically via the W&B library
  - Useful for hardware capacity monitoring, especially if the training run crashes
- What to look for:
  - Stability - i.e., not running out of VRAM, disk space, etc.

## W&B Metrics

- TBD: screenshot of training metrics

## W&B Metrics

- Training Metrics
  - Metrics on the performance of the training process (how accurate are the output tokens matching the training data)
  - Number of tokens, token accuracy, training loss, grad_norm, etc.
- What to look for:
  - Token accuracy should increase quickly and plateau
  - Training loss should decrease quickly and plateau
  - Grad norm should not have drastic spikes (e.g., to 100)

## W&B Metrics

- TBD: screenshot of evaluation metrics

## W&B Metrics

- Eval Metrics
  - Metrics on the validation runs during training (how well is the model able to match the output from the validation set)
  - Number of tokens, token accuracy, eval loss (a.k.a. validation loss)
- What the look for:
  - Token accuracy should increase quickly and plateau
  - Validation loss should decrease quickly and plateau
  - Validation loss should be very close to training loss (maybe slightly behind)
  - Validation loss should not start to increase (strongest signal that the model is overfitting)

# Hands-on

Create your WandB account, download API key

Configure the training notebook for your own dataset and base model

Start the training run!

# Post-Training

## Post-Training

- TBD: Test the model with a random one from our test dataset

## Post-Training

- TBD: Create a model card, merge the models and upload everything to Hugging Face

## Post-Training

- TBD: Quantize the model and upload to Hugging Face

## Post-Training

- TBD: Download the model in LM Studio and test!!!

# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/07/assignment.html){.external target="_blank"}
- TBD

# References

## References
