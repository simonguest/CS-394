---
title: "Week 1: Foundations of Generative AI"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
logo: ../../theme/logos/DigiPen_RGB_Red.png
---

## Lesson Objectives

- Understand how generative AI fits with other AI techniques
- How to create vector embeddings and test for similarity
- Understand the transformer architecture and how it works at a high level
- Setup and use Colab Pro for experimenting with vector embeddings and downloading/testing a GPT-2 model.
- Start becoming familiar with the basics of notebooks and Python (if you havenâ€™t used it already)

# A recap of ML/Neural Network architectures

## A recap of ML/Neural Network architectures

- TBD

# Vector Embeddings

## What are Vector Embeddings?

- Vector Embeddings are **meaningful numerical representations** of words
  - Representations where strings of words (i.e., sentences) are encoded into multi-dimensional space
  - Large number of dimensions (we'll use 384 in our examples)
  - Similar sentences have similar numbers

## Example

{{< embed embeddings.ipynb#sentence-1 echo=true >}}

## Example

{{< embed embeddings.ipynb#sentence-2 echo=true >}}

## Example

{{< embed embeddings.ipynb#sentence-3 echo=true >}}

## Are They Similar?

- We can test with **cosine similarity**
- Measures the angle between two vectors: 
  - Same direction = very similar (similarity close to 1)
  - Opposite direction = very different (similarity of -1)
- Cosine similarity focuses on the angle of the vector vs. length
  - Useful for comparing texts of different sizes

## Are They Similar?

{{< embed embeddings.ipynb#similarity echo=true >}}

## Overview of Embedding Models

- TBD

## What Embeddings Are Used For

- TBD

# Let's Run Some Code!

