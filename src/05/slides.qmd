---
title: "Module 5: Running Models on Local Hardware"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Understood the fundamentals and history of diffuser models
- Explored and used models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet
- Setup and used Replicate to create a custom pipeline of production-grade models
- Understood the fundamentals and history of Vision Encoders and VLMs
- Implemented/tested a local VLM model for on-device inference

## Lesson Objectives

- Understand the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile
- Understand hardware requirements and architectures for model inference - e.g., CUDA vs. ONNX vs. MLX vs. WebGPU
- Explore how quantization works and understand techniques and formats for quantizing existing models
- Use llama.cpp to quantize and run an SLM on local hardware/gaming PC
- Integrate a quantized model within Unity/Unreal/WebAssembly

# Why Local Models?

## Why Local Models?

- **Privacy**
  - Every call you make to OpenAI/Claude/OpenRouter may (or may not) get logged and/or be used for training purposes
  - Many organizations don't want their customer/financial data logged with an AI vendor
  - There may also be legal regulations/restrictions controlling this

## Why Local Models?

- **Offline**
  - Every call you make to OpenAI/Claude/OpenRouter needs an Internet connection
  - That's not always guaranteed!
  - Education is a good example - remote school in India and/or rural districts here in the US

## Why Local Models?

- **Latency**
  - Even with a network connection, calls can suffer from increased latency
  - Can be a challenge if your application needs frequent, quick responses
  - e.g., using a VLM to determine the contents of a video stream for a user with vision impairments

## Why Local Models?

- **Cost**
  - While per-API costs are fractions of a cent, these can grow out of control with exponential growth
  - More pronounced for long conversation threads (think call center)
  - Or agents with verbose tool call JSON requests/responses

# What's Your Hardware?

## What's Your Hardware?

- NVIDIA (CUDA)
- AMD (ROCm)
- Apple Silicon
- Various NPU (Neural Processing Unit) vendors

## NVIDIA CUDA

- CUDA (Compute Unified Device Architecture)
  - Launched in 2006 to introduce programming on GPUs (GPGPUs or General Purpose GPUs)
  - A C-like programming interface
  - Perfectly timed for the deep learning revolution of the 2010s
  - Additional libraries (e.g., cuBLAS, cuDNN) make CUDA the defacto standard today

## How CUDA Works

- Massive parallelism: CUDA exploits thousands of GPU cores simultaneously, making it ideal for matrix operations.
- Memory hierarchy: A tiered memory system with global, shared, and registers.
- Kernel execution: Programs can launch kernels - same function that can operate on different data.

## NVIDIA CUDA - Hardware Support

- Consumer: RTX 40- and 50- cards with various VRAM options (8Gb - 24Gb) for local inference and small fine-tuning tasks. RTX 30- series still popular for education.
- Laptop: RTX 40- and 50- series also available on laptops (although less performant than discrete cards)
- Workstation: DGX Spark launch in 2025, with GB10 and 128Gb of unified memory for medium fine-tuning tasks
- Datacenter GPUs: A/H series and GB-series for datacenters. NVLink for multi-GPU interconnectivity.

## Sidebar: TOPS

- TOPS (Terra Operations Per Second)
  - How many trillion operations a processor can perform per second
  - Often qualified with the data type
  - 64 INT8 TOPS == 64 trillion 8-bit operations per second
- TFLOPS (Terra Floating-Point Operations Per Second)
  - 1 TFLOPS == 1 FP32 (32-bit floating point) TOPS

## Sidebar: TOPS

- Rough Throughput Calculations
  - You have an NVIDIA 3090 (advertized at 35 TFLOPS)
  - Assume a 7B param model with FP32 weights
  - Each token generation requires ~2 FLOPs per parameter
  - Each token generation ~= 14B FLOPs (7B params × 2)
  - Theoretical max = 35T FLOPs/sec ÷ 14B FLOPs/token ≈ 2,500 tokens/sec
  - Reality: 10-100 tokens/sec typical due to memory bandwidth bottlenecks

## AMD ROCm

- ROCm (Radeon Open Compute)
  - Launched in 2016 as an open-source alternative to CUDA
  - Embraced open standards (e.g., OpenCL), positioning as avoiding vendor lock-in, although this fragmentation initially hurt adoption
  - Has evolved significantly since (e.g., rocBLAS) although ecosystem gaps compared to CUDA persist

## AMD ROCm - Hardware Support

- Consumer: RX7000 series offer sustantial VRAM (up to 24Gb) at competitive prices compared to NVIDIA RTX
- Laptop: Some laptop options for AMD-based machines
- Workstation: Strix Halo, competitor to DGX Spark, with RX8060S and 128Gb unified memory
- Software support: Linux only with no Windows support (some via WSL)

## Apple Silicon

- Apple Silicon
  - Metal, a low-level graphics and compute API, launched in 2014 and later expanded for general GPU compute tasks
  - MPS (Metal Performance Shaders) introduced in 2017 and optimized primitives for neural networks. PyTorch added MPS device support in 2022.
  - MLX released in 2023, providing NumPy-like API for Apple Silicon hardware

## Apple Silicon - Hardware Support

- Available on all M-series hardware
- Unified memory by default, upto 128Gb on laptops and 512Gb for the Mac Studio with M3 Ultra
- Non portable models. (MLX uses safetensors/npz format and MLX-specific code for Metal.)

## Sidebar: Unified Memory

- GPUs have historically had separate memory (VRAM)
- Unified memory is a process to share memory between CPU and GPU
  - For Apple/MLX, it's a true SoC (System on a Chip); NVIDIA DGX Spark, two physical components connected via NVLink-C2C
- Higher memory availability, but lower memory bandwidth
  - ~275Gb/s for Spark/MLX; ~1TB/s for 5080; ~1.7TB/s for 5090; ~3TBs for H100

## NPUs

- NPUs (Neural Processing Units) are specialized AI accelerators, designed for lower power consumption
- Optimized specifically for NN operations (e.g., matmul, convolutions, activations)
- Commonly found in edge devices (smartphones, IoT, embedded systems)
- 15-80 TOPS common for NPUs (~10x less that desktop PCI-based GPUs)

## NPU Vendors

- Intel: Acquired Movidius in 2016; released Myriad X in 2017 and a neural compute stick. Superceded by NPUs in Core Ultra processors.
- Qualcomm: Snapdragon 865 range in 2019; now Snapdragon X and 8 ranges. Used in Windows/ARM devices. Popular with Android, although Google recently moved to their own TPUs.
- AMD: XDNA formerly Xilinx; Windows Copilot PC range competing with SnapDragon and Intel Core Ultra.
- Apple: ANE (Apple Neural Engine) to support CoreML workloads on iPhone, iPad devices

# Compute and Memory Challenges

## Compute and Memory Challenges

- Compute Challenges
  - Local GPUs significantly slower than datacenter-class GPUs
  - 4060 Ti (~350 TOPS) vs. H100 (~3500+ TOPS)
  - One local GPU vs. interconnected datacenter-class GPUs (using NVLink)
- Less TOPS = slower inference (tokens/second)

## Compute and Memory Challenges

- Memory Challenges
  - Consumer-grade GPUs and NPUs have significanly less memory than datacenter-class GPUs
  - 8Gb/16Gb VRAM on 4060 Ti vs. 80/94Gb for H100
  - (Also memory bandwidth is typically 10x-20x)
  - (Combined memory architecture using NVLink)
- Less memory = only smaller models can run
  - Rougly speaking, size of model must be smaller than available VRAM

# Overcoming Compute Challenges

## Overcoming Compute Challenges

- MoE (Mixture of Experts)
  - Original concept dates back to 1991. Jacobs et al. publish "Adaptive Mixture of Local Experts" [@jacobs1991adaptive] showing subnetworks and a gating mechanism
  - In 2017, Shazeer et al. (Google) publish "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer" [@shazeer2017outrageously] demonstrating activating only a small subset of experts per input
  - 2022:  Google releases Switch Transformers [@fedus2022switch], simplifying MoE by routing each token to just a single expert
  - 2023: Mixtral 8x7B (Mistral AI) brings high-quality open-source MoE to the mainstream, becoming a standard architecture for efficient large-scale models

## Popular MoE Models

- Mixtral (Mistral)
- Qwen MoE (Alibaba)
- DeepSeek MoE
- Phi MoE (Microsoft's SLMs)
- Nemotron (NVIDIA)

## Overcoming Compute Challenges

- Why MoE?
  - Faster Inference (token/second): Because you are only using a subset of experts for each token.
  - Allows you to use larger models, where you would be otherwise compute-bound
    - Dense 13B model ~= 13Gb VRAM
    - Mixtral 8x7B ~= 47GB VRAM (8×7B experts + shared layers), but faster inference than dense 47B (only ~2 experts active per token)

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#load-model echo=true outputs=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#register-hooks echo=true outputs=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-0 echo=true outputs=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-0 echo=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-8 echo=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-16 echo=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-24 echo=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-31 echo=false >}}

## How do MoE Models Work

- What the Heatmaps Show
  - Different tokens activate different experts (token-level routing)
  - Routing changes across layers (layer 0 vs. layer 31)
    - Early layers: syntax/grammar
    - Late layers: semantics/reasoning
  - Not all experts used equally (specialization emerges)
  - Router makes soft decisions (distributes probability across multiple experts)

## How do MoEs Get Trained?

- From scratch
  - Train the entire model (experts and router)
  - Router networks learn which expert to select
  - Experts learn their specializations
  - Example: Mixtral 8x7B
- Start from a trained dense model (called upcycling)
  - Replicate each layer into multiple experts; add router networks
  - Typically faster and more stable vs. starting from scratch
  - Example: Qwen1.5-MoE

## How do MoEs Get Trained?

- Combine multiple models (FrankenMoE / MoErge)
  - Newer, community-driven approach
  - Take specialized models (e.g., math, coding, chat) and use their FFN layers as separate experts; add router network
  - Train only the router (experts frozen initially)
  - Cheaper than training from scratch, but often lower quality
  - Example: Beyonder-4x7B

## How do MoEs Get Trained?

- Key challenge: Load balancing / Expert collapse
- Healthy MoE:
  - Expert 0: 8% of tokens
  - Expert 1: 7% of tokens
  - Expert 2: 9% of tokens
  - (all experts get reasonable usage)
- Collapsed MoE:
  - Expert 0: 45% of tokens
  - Expert 1: 40% of tokens
  - Expert 2-7: <15% combined (essentially dead) 

# Hands-on

Run the MoE heatmap; investigate other layers

Potentially swap out Phi model with other MoE from HF

## Overcoming Compute Challenges

- MoE gives us speed with quality
  - But we still can't fit it on consumer hardware
  - Popular MoE models often exceed the amount of available VRAM
  - We need a way to reduce the memory footprint of the model...

# Overcoming Memory Challenges

## Introducing Quantization

- Process of reducing the precision of a model's weights and activations
  - For example, converting 16-bit numbers to 4-bit
- Less precision significantly reduces memory needs
- For accuracy, parameter count matters more than precision
  - A 70B parameter model at 4-bit often beats a 13B model at bf16
  - The models knowledge remains largely intact
  - Often the extra precision doesn't meaningfully improve outputs

## Quantization Formats

- GPTQ (GPT Quantization):
  - Introduced in the paper "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers" [@frantar2023gptq]
  - First widely adopted method (in 2022) for agressive (4-bit) quantization
  - CUDA only; distributed via HF Transformers
- GGML (Georgi Gerganov Machine Learning):
  - A C/C++ library ("llama.cpp" released in Mar 2023) designed for CPU inference of models
  - Optimized for CPUs (using SIMD) with a custom binary format
  - Democratized access to LLMs overnight (ability to run Llama 7B on CPU)

## Quantization Formats

- GGUF (GPT-Generated Unified Format):
  - Replaced GGML in late 2023, adding extensibility
  - Better metadata support (model architecture, tokenizer info, quantization details)
  - Single file architecture
  - Can offload layers to GPU/NPU, if available, increasing performance
  - Multiple quantization schemes (Q4_K_M, Q5_K_S, Q6_K, etc.)

## Sidebar: Quantization Schemes

- What does Q4_K_M mean?
  - Number (Q4, Q5, Q6, etc.) is the average bits per weight
  - Letter suffix (K, 0, 1) is the quantization strategy
  - Size suffix (S, M, L) for variants within that method

## Sidebar: Quantization Schemes

- Q2_K = Agressive quantization, smallest files, noticable loss
- Q4_K_M = Best quality/size trade off; most popular
- Q6_K = Very close to full precision, but much larger files (almost double Q4_K_M)
- Q8_0 = Essentially lossless compared to FP16

## Sidebar: Quantization Schemes

- K-Quant strategy
  - A mixed quantization strategy; quantize different parts of the model at different bit depths depending on sensitivity
  - e.g., critical weights (e.g., attention layers) get higher precision; less sensitive weights get more aggressive quantization
- 0 and 1 strategy
  - Uniform quantization across all layers (1 strategy adds a zero point)
  - Less optimal than using K strategy (and rarely used)

## Running llama.cpp

- [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp){.external target="_blank"}
- C/C++ library; download (brew, nix, winget) or compile from source
- Initially just a CLI, but now ships with Web UI and OpenAI API compatible server
- Can reference a locally downloaded .gguf file or pull one from HF

# Demo

llama-cli -m my_model.gguf

llama-cli -hf ggml-org/gemma-3-1b-it-GGUF

llama-server -hf ggml-org/gemma-3-1b-it-GGUF

## llama.cpp Wrappers

- Ollama (https://ollama.com)
  - Simple CLI wrapper around llama.cpp
  - Uses a container-like Modelfile to customize system prompts, parameters, etc.
  - Curated collection of models on ollama.com/library
- LM Studio
  - Desktop GUI wrapper around llama.cpp
  - Supports browsing/downloading of models from HF - i.e., "iTunes for LLMs"
  - In-built chat interface and API server

# Demo

Browsing, downloading, and running a quantized model on LM Studio

# Hands-on

Install LM Studio, pick, and download your own model

(Alterrnatively use llama.cpp or Ollama from the CLI)

# Quantization in Other Frameworks

## MLX (Apple's ML Framework)

- ML framework created by Apple (released Dec 2023)
- Designed specifically for Apple Silicon (M-series processors)
- Python API similar to NumPy/PyTorch, but optimized for Metal (Apple's GPU API)
- Can run on non-Apple hardware - CPU and CUDA PyPi packages

## MLX (Apple's ML Framework)

- Supports various levels of quantization (4bit and 8bit)
- .npz or .safetensor format (with MLX metadata)
- Better performance on Mac (compared to GGUF)
- mlx-community on HF hosts MLX-quantized versions of popular models (separate repos)
  - [https://huggingface.co/models?library=mlx](https://huggingface.co/models?library=mlx){.external target="_blank"}

## ONNX (Open Neural Network eXchange)

- Model interchange format, created by Microsoft and Facebook in 2017 to enable model portability
- Models can be converted: PyTorch -> ONNX; TensorFlow -> ONNX; etc.
- Common export format, used in many CNNs and RNNs
- Popular for models running on mobile devices and in-browser (e.g., Transformers.js)

## ONNX (Open Neural Network eXchange)

- Supports dynamic quantization (typically int8)
- .onnx file format (uses protobuf format) includes graph structure, weights, and metadata
- Broad portability, but at the expense of performance
- ONNX models tend to live in separate folders on HF
  - [https://huggingface.co/models?library=onnx](https://huggingface.co/models?library=onnx){.external target="_blank"}

# How to Quantize a Model

## How to Quantize a Model

- Popular models have already been quantized
  - Many available on HF, contributed by [Unsloth AI](https://huggingface.co/unsloth){.external target="_blank"}
- If the model isn't already quantized (or you've fine tuned your own), it's easy to do

## How to Quantize a Model

{{< embed notebooks/quantization.ipynb#build-llama echo=false >}}

## How to Quantize a Model

{{< embed notebooks/quantization.ipynb#convert echo=false >}}

# Calling llama.cpp from Apps

## Calling llama.cpp from Apps

- Three ways to interact with a local GGUF model:
- 1. Use llama.cpp to serve the model via a local web server
  - Easier to get running, but the overhead of a local web server
- 2. If your app is C++, call llama.cpp library directly
- 3. If your app is not C++, use llama.cpp bindings
- (2 and 3 are more challenging to get setup, but no web server and better performance)

# Calling llama.cpp from Unreal Engine

## Calling llama.cpp from Unreal Engine

- C++, so can call llama.cpp directly
  - Build the llama.cpp libraries for your platform
  - Create and compile an Unreal plug-in
  - Create a Blueprint that calls the plug-in
  - Integrate within scene

# Demo

Unreal Engine Plug-in for llama.cpp

# llama.cpp Bindings

## llama.cpp Bindings

## What Are llama.cpp Bindings?

- Wrappers around the llama.cpp C/C++ libraries
- Enable you to use models via the language/platform of your choice
- What languages/platforms are supported?
  - [https://github.com/ggml-org/llama.cpp#description](https://github.com/ggml-org/llama.cpp#description){.external target="_blank"}
  - Click on the "Bindings" drop-down

## Python Binding

- llama-cpp-python
  - [https://github.com/abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python){.external target="_blank"}
  - OpenAI-like API
  - Supports chat completions, function/tool calling, structured outputs, and multimodal models.

## Python Binding

{{< embed notebooks/python-binding.ipynb#load-model echo=true outputs=false >}}

## Python Binding

{{< embed notebooks/python-binding.ipynb#chat echo=true outputs=false >}}

## Python Binding

{{< embed notebooks/python-binding.ipynb#chat echo=false >}}

## C# Binding

- LLamaSharp
  - [https://github.com/SciSharp/LLamaSharp](https://github.com/SciSharp/LLamaSharp){.external target="_blank"}
- Installed via NuGet
- Supports multiple backends (Cpu, CUDA, and Vulkan)

## Using the C# Binding in Unity

- Install Unity (2022.3 LTS)
- Install NuGet for Unity package manager
- Using NuGet, install the LlamaSharp package and correct backend
- (Possibly) copy the dylibs into Assets/Plugins/[platform] and load
- Create a LlamaTest.cs script

# Demo

Unity/LLamaSharp Demo

# Local Models in the Browser

## Local Models in the Browser

- Small models can also run in the browser
- Inference runs on WebAssembly (WASM)
- CPU or WebGPU options
- Browser limits most models to <2Gb, possibly smaller depending on VRAM

## Local Models in the Browser

- Wllama ([https://github.com/ngxson/wllama](https://github.com/ngxson/wllama)){.external target="_blank"}
  - CPU only (using SIMD for better performance)
  - JavaScript-based API
  - Download models directly from HF
  - Cached for future use

## Local Models in the Browser

```
await wllama.loadModelFromHF(
    'ggml-org/models',
    'tinyllamas/stories260K.gguf',
    {
      progressCallback,
    }
  );
const outputText = await wllama.createCompletion(prompt, {
  nPredict: 50,
  sampling: {
    temp: 0.5,
    top_k: 40,
    top_p: 0.9,
  },
});
```

## Local Models in the Browser

- WebLLM ([https://webllm.mlc.ai/](https://webllm.mlc.ai/)){.external target="_blank"}
  - Uses WebGPU for better performance
  - JavaScript-based API and playground
  - Download models directly from HF, but need to be compiled to MLC format
  - Popular pre-compiled models available

## Local Models in the Browser

```
const appConfig: webllm.AppConfig = {
  model_list: [
    {
      model: `https://huggingface.co/simonguest/Qwen3-0.6B-it-code-hint-3`,
      overrides: {
        temperature: 0.5,
      },
    },
  ],
};
engine = await webllm.CreateMLCEngine("Qwen3-0.6B-it-code-hint-3", {
  appConfig,
  initProgressCallback,
});

const chunks = await engine.chat.completions.create({
  messages: [
    { role: 'system', content: SYSTEM_PROMPT},
    { role: 'user', content: PROMPT }],
  temperature: 0.7,
  stream: true,
});
```

# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/05/assignment.html){.external target="_blank"}
- Increasing model accuracy
- Better prompt engineering
- RAG (Retreival Augmented Generation)
- Introduce fine-tuning and synthetic data generation

# References

## References
