---
title: "Module 7: Increasing Model Accuracy (Part 2)"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Understood what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy
- Explored prompt engineering and thinking models
- Introduced and implemented Text-to-SQL and RAG (Retrieval-Augmented Generation) to increase the accuracy of a limited SLM
- Started exploring model fine-tuning
- Generated synthetic data for fine-tuning a small language model

## Lesson Objectives

- Use your generated synthetic data to fine-tune an SLM using QLoRA
- Use W&B (Weights & Biases) to observe metrics during the training run
- Post-training, test and evaluate the accuracy of your fine-tuned model
- Merge, quantize, and upload your model to Hugging Face to share with others
- Understand and create a model card for your newly fine-tuned model

# Model Training Stages

## Model Training Stages

- There are three main stages of training a model:
  - **Pretraining:** The initial, large-scale training run that creates a base model with pretrained weights
    - Training data: Trillions of tokens from the internet, books, code, etc.
  - **Supervised Fine-Tuning (SFT):** Shapes the base model to follow instructions and respond in a desired style or fornat
    - Training data: Hundreds to thousands of input/output pairs
  - **Alignment:** Further refines the model toward preferred behaviors and values
    - Training data: Human-curated preferences and/or reward-based feedback

## Model Training Stages

![](diagrams/stages.svg){.lightbox fig-align="center" width="600px"}

## Model Training Stages

- Many models (e.g., google/gemma-3-it) have already gone through these three stages
  - We will be adding a fourth stage. An SFT of an existing "polished" model
- Why?
  - The model already knows how to follow instructions, answer questions, etc.
  - They need far less training data to get good results
  - Training tends to be faster and more stable since they are starting from a good baseline

## Model Training Stages

![](diagrams/stages-loop.svg){.lightbox fig-align="center" width="600px"}

# Supervised Fine-Tuning (SFT)

## Supervised Fine-Tuning (SFT)

- How does SFT work?
  - Build a dataset (we did this already!)
  - Feed the prompts into the model and compare the desired output tokens with the model's predictions
  - The difference between the two is known as the "training loss"
  - Backpropagate through the model, nudging the weights to guide toward the desired output
  - This "nudging" is done using an optimizer (typically AdamW)
  - Repeat for typically 1-3 epochs (complete passes through the dataset)
  - Monitor validation loss to detect overfitting or diminishing returns

## Supervised Fine-Tuning (SFT)

![](diagrams/sft-training.svg){.lightbox fig-align="center" width="600px"}

## Supervised Fine-Tuning (SFT)

![](diagrams/sft-validation.svg){.lightbox fig-align="center" width="600px"}

## Supervised Fine-Tuning (SFT) with LoRA

- Adjusting all parameters in a model can be compute and memory intensive
- We will use LoRA (Low-Rank Adaptation), a Parameter Efficient Fine-Tuning (PEFT) strategy

## Supervised Fine-Tuning (SFT) with LoRA

- How LoRA works:
  - The model's base weights (W) are frozen (never updated) during training
  - Two small matrices (A and B) are introduced, whose product represents the desired change to W
  - Only A and B are updated during training — a tiny fraction of the total parameters
  - After training, these matrices are known as an "adapter"
  - The adapter can be kept separate (swappable) or merged back into W to create a new, standalone model

## Supervised Fine-Tuning (SFT) with LoRA

![](diagrams/sft-lora.svg){.lightbox fig-align="center" width="600px"}

## Supervised Fine-Tuning (SFT) with QLoRA

- To further increase memory efficiency, we will use QLoRA (Quantized LoRA)
  - The base model weights (W) are quantized to 4-bit (NF4 format) for storage
  - During the forward pass, W is dequantized to bf16 on the fly for computation, then discarded
  - Adapter matrices A and B are kept at full precision (bf16) for training accuracy
  - Since only A and B are trained, optimizer states (which can be 3-4x the size of W at full precision) only exist for the tiny adapter matrices — dramatically reducing memory usage

## Supervised Fine-Tuning (SFT) with QLoRA

![](diagrams/sft-qlora.svg){.lightbox fig-align="center" width="600px"}

# Today's Fine-Tuning Plan

## Today's Fine-Tuning Plan (1/2)

- Upload our .jsonl files to Hugging Face to create a dataset
- In our training notebook...
  - Download the dataset
  - Format our training data to match the chat template
  - Start our training run!
- Use W&B (Weights & Biases) to monitor how well our model is learning
  - Training loss
  - Validation loss

## Today's Fine-Tuning Plan (2/2)

- After training has completed...
  - Test (both manually and automatic)
- Assuming things went well...
  - Merge and upload our model to Hugging Face
  - Quantize our model (create a GGUF) and upload to Hugging Face
  - Download and test our model in LM Studio
- Phew!
  
# Upload Training Files

## Upload Training Files

- Upload our train.jsonl, validation.jsonl, and test.jsonl files to a Hugging Face Dataset
- Why?
  - Single repository
  - Easy to hook into training runs
  - Shareable with others
  - Better UI and query semantics vs. raw .jsonl files

## Upload Training Files

{{< embed notebooks/create-dataset.ipynb#create-dataset echo=true outputs=false >}}

## Upload Training Files

{{< embed notebooks/create-dataset.ipynb#upload echo=true outputs=false >}}

# Demo

Uploading the training data files to Hugging Face Dataset

## Upload Training Files

- After you've uploaded
  - It will take a few minutes before you can browse your data
  - About 24-48 hours, you will likely receive an email about conversion to parquet format

# Hands-on

Upload your training data files to Hugging Face Dataset

(CPU instance on Colab is fine for this)

# The Training Notebook

## The Training Notebook

- `train-cuda.ipynb` contains the steps to fine-tune a small model
- Main sections
  - Set training parameters
  - Download dataset from Hugging Face
  - Create formatted dataset that uses the chat template
  - Load base model and configure for QLoRA
  - Create our trainer
  - Train!

# Demo

Walkthrough of the training notebook

Set parameters and start training!

## Sidebar: How long will training take?

- For Qwen3-1.7B using an A100 on Colab
  - About 45 mins
  - (20 mins if we are lucky and get upgraded to an RTX6000)
  - We can monitor using `tqdm` (a popular Python progress bar)

## LoRA Hyperparameters

`BATCH_SIZE = 4`

- Number of training examples processed together in a single forward/backward pass
- Larger batches = more stable gradient estimates, but consume more memory
- 2-8 recommended

## LoRA Hyperparameters

`GRADIENT_ACCUMULATION_STEPS = 4`

- Instead of updating weights after every batch, accumulate over this many batches before taking an optimizer step
- Memory-efficient way to increase the batch size
- 2-16 recommended

## LoRA Hyperparameters

`LEARNING_RATE = 2e-4`

- How large a step the optimizer takes when updating weights
- Too high = training becomes unstable
- Too low = training is too slow or gets stuck
- 1e-4 or 2e-4 are common starting points

## LoRA Hyperparameters

`NUM_EPOCHS = 3`

- Number of times the model sees the entire dataset
- More epochs can improve training, but with the risk of overfitting
- 2-5 epochs for a few thousand examples recommended

## LoRA Hyperparameters

`MAX_SEQ_LENGTH = 512`

- The maximum number of tokens in a single training example
- (Length of one of the lines in your jsonl file)
- Should cover your longest examples without too much padding

## LoRA Hyperparameters

`LORA_R = 16`

- The rank of the adapter matrics - i.e., how many parameters are added during fine-tuning
- Higher rank = more capacity, but also more memory and risk of overfitting
- 4, 8, 16, 32 common values; 8-16 for small models

## LoRA Hyperparameters

`LORA_ALPHA = 32`

- A scaling factor for the LoRA updates
- Usually set to 2x the rank (LORA_R)

## LoRA Hyperparameters

`LORA_DROPOUT = 0.05`

- Randomly zero out a small fraction of the adapter activations during training
- Regulation technique to prevent overfitting
- 0.0 to 0.1 common range; use lower values for very small datasets

## LoRA Hyperparameters

`USE_4BIT = True`

- Enables QLoRA by loading the base model weights (W) in 4-bit precision
- Optional for training 1B models in 8Gb VRAM
- Definitely needed for 7B and 13B fine-tuning runs

# Demo

Let's check on our training!

# Weights & Biases

## Weights & Biases

- What is Weights & Biases (W&B)?
  - Founded in 2018 after founders noticed that researchers were keeping hyperparameters and training run details in spreadsheets / on paper, etc.
  - Stream any metric (accuracy, loss, GPU utilization) to a single dashboard; compare training runs side-by-side to improve model accuracy
  - Used either standalone via `wandb.log()` calls and/or integrated into many training libraries
  - Generous free tier and education/researcher plans

## Weights & Biases

![](images/wandb-system.png){.lightbox fig-align="center" width="600px"}

## Weights & Biases

- System Metrics
  - Metrics on the health of the underlying hardware used to train the model
  - Providing automatically via the W&B library
  - Useful for hardware capacity monitoring, especially if the training run crashes
- What to look for:
  - Stability - i.e., not running out of VRAM, disk space, etc.

## Weights & Biases

![](images/wandb-train.png){.lightbox fig-align="center" width="600px"}

## Weights & Biases

- Training Metrics
  - Metrics on the performance of the training process (how accurate are the output tokens matching the training data)
  - Number of tokens, token accuracy, training loss, grad_norm, etc.
- What to look for:
  - Token accuracy should increase quickly and plateau
  - Training loss should decrease quickly and plateau
  - Grad norm should not have drastic spikes (e.g., to 100)

## Weights & Biases

![](images/wandb-eval.png){.lightbox fig-align="center" width="600px"}

## Weights & Biases

- Eval Metrics
  - Metrics on the validation runs during training (how well is the model able to match the output from the validation set)
  - Number of tokens, token accuracy, eval loss (a.k.a. validation loss)
- What the look for:
  - Token accuracy should increase quickly and plateau
  - Validation loss should decrease quickly and plateau
  - Validation loss should be very close to training loss (maybe slightly behind)
  - Validation loss should not start to increase (strongest signal that the model is overfitting)

# Demo

W&B output during current training run

# Hands-on

Create your W&B account (https://wandb.ai), download API key, set in Colab

Configure the training notebook for your own dataset, base model, and WANDB API key

Start your training run!

# Post-Training

## Post-Training

- After training has completed:
  - Test the model using a random input from our test dataset
  - Create a model card, merge the models, and upload to Hugging Face
  - Quantize the model and upgrade the GGUF to Hugging Face
  - Download the GGUF in LM Studio!

## Post-Training: Test

{{< embed notebooks/train-cuda.ipynb#test-model echo=true outputs=false >}}

## Post-Training: Create Model Card

- What's a model card?
  - `README.md` for your model on Hugging Face
  - Common sections
    - An overview of the model
    - How the model was trained
    - Benchmarks
    - How to use the model
    - Limitations, best practices, risks, safety, author details

## Post-Training: Create Model Card

- A default `README.md` will be generated by `save_pretrained()` method
- You can overwrite this with your own
- Or create one programmatically

## Post-Training: Create Model Card

{{< embed notebooks/train-cuda.ipynb#model-card echo=true outputs=false >}}

## Post-Training: Merge and Upload

- Merging "fuses" the LoRA adapter and base model, creating a new model
- Doing this will enable others to use your model
- Upload your merged model (and optionally your adapter in a sub-folder) to Hugging Face

## Post-Training: Merge and Upload

{{< embed notebooks/train-cuda.ipynb#merge-model echo=true outputs=false >}}

## Post-Training: Merge and Upload

{{< embed notebooks/train-cuda.ipynb#upload-model echo=true outputs=false >}}

# Demo

Create model card, merge, and upload to Hugging Face

## Post-Training: Quantize

- Even though we used QLoRA, your merged model is still full precision
- To enable others to run locally, we want to create quantized versions (GGUF)
- Use llama-quantize (part of llama.cpp) to quantize
- Then upload to Hugging Face (in the root of the model repo so it's discoverable)

## Post-Training: Quantize

{{< embed notebooks/quantize.ipynb#build-llama echo=true outputs=false >}}

## Post-Training: Quantize

{{< embed notebooks/quantize.ipynb#convert echo=true outputs=false >}}

## Post-Training: Quantize

{{< embed notebooks/quantize.ipynb#upload echo=true outputs=false >}}

# Demo

Quantize and upload to Hugging Face

## Post-Training: Use in LM Studio!

- Use LM Studio, search for your Hugging Face username
- Download and test the GGUF of your newly fine-tuned model
- (Don't forget to turn off your A100 Colab instance, if you haven't already!)

## Post-Training: Use in LM Studio!

![](images/lmstudio-search){.lightbox fig-align="center" width="600px"}

## Post-Training: Use in LM Studio!

![](images/lmstudio-chat){.lightbox fig-align="center" width="600px"}

# Demo

Search and chat in LM Studio!

# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/07/assignment.html){.external target="_blank"}
- Our last lecture before spring break and final project!
- Discuss ethical, IP, and safety concerns for Generative AI
- Use an evidence-based approach for each of the areas

# References

## References
