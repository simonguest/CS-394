---
title: "Module 4: Multimedia and Multimodal Models"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Described the fundamental concepts behind Agents/Agentic AI
- Explored and provided feedback on an existing multi-agent setup
- Understood available agent SDKs, how they differ, and advantages/disadvantages
- Used the OpenAI Agents SDK to build a multi-agent system from scratch, including document indexing and retrieval
- Understood and implemented tool calls using OpenAI's function calling and via MCP

## Lesson Objectives

- Understand the fundamentals and history of diffuser models
- Explore and use models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet
- Setup and use Replicate to create a custom pipeline of production-grade models
- Understand the fundamentals and history of Vision Encoders and VLMs
- Implement/test a local VLM model for on-device inference

# Multimedia vs. Multimodal

## Multimedia vs. Multimodal

- **Multimedia** models
  - Single input/output models for images, video, audio, etc.
  - Also known as computer vision, audio models
- Examples
  - **Text-to-Image** (generate an image from a text prompt)
  - **Image-to-Image** (generate an image from an existing image)
  - **Image-to-3D** (generate a 3D object from an image)

## Multimedia vs. Multimodal

- **Multimodal** models
  - Process multiple datatypes such as text, images, and audio
  - Also known as VLMs (Vision-Language Models) or ALMs (Audio-Language Models)
- Examples
  - **Image-Text-to-Text** (ask a question about this image)
  - **Image-Text-to-Image** (decompose this image into multiple layers)
  - **Audio-Text-to-Text** (what is this sound?)

## Text-to-Image

:::: {.columns}

::: {.column width="50%"}
"A photograph of an astronaut riding a horse."

- Based on a concept called a diffusion transformer
- Commonly known as a **diffuser**
- Two stage process, inspired by thermodynamics
:::

::: {.column width="50%"}
![](./images/astronaut.png)
:::
::::

## Introducing the Diffuser

- Training
  - During training, random noise is added to images in steps
  - Model learns to predict what noise was added (forward diffusion process)
- Inference (process runs in reverse)
  - Start with pure random noise
  - Model estimates what noise should be removed to create a realistic image
  - Using the text prompt, the model steers the process towards images that match the description

## Image Diffusion Models in 2022

```{mermaid}
timeline
  August 2022 : Stable Diffusion v1.4
              : First open-source high-quality model
  September 2022 : Stable Diffusion v1.5
                  : Refined version
  October 2022 : eDiff-I (NVIDIA)
                : Ensemble approach
  November 2022 : Stable Diffusion v2.0/2.1
                : Higher resolution (768x768)
```

## Image Diffusion Models in 2023

```{mermaid}
timeline
  March 2023 : Midjourney v5
              : Exceptional artistic quality
  April 2023 : ControlNet
        : Precise spatial control
        : AnimateDiff - Video generation
  July 2023 : SDXL (Stable Diffusion XL)
            : 1024x1024 native resolution
  August 2023 : SDXL Turbo
              : Real-time capable generation
```

## Image Diffusion Models in 2024

```{mermaid}
timeline
  February 2024 : Stable Diffusion 3
                : Improved text understanding
  June 2024 : Stable Diffusion 3.5
            : Multiple model sizes
  2024 : FLUX.1 (Black Forest Labs)
        : State-of-the-art open model
        : Imagen 3 (Google DeepMind)
        : Photorealistic quality
```

## Image Diffusion Models in 2025

```{mermaid}
timeline
  May 2025 : Imagen 4 (Google DeepMind)
           : Improved text rendering, 2K resolution
  August 2025 : Nano Banana (Google)
              : Autoregressive model in Gemini 2.5 Flash
  November 2025 : FLUX.2 (Black Forest Labs)
                : 32B parameters, multi-image references
```

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#load-model echo=true outputs=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#steps echo=true outputs=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#steps echo=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#final echo=false >}}

## Sidebar: What are these pipelines?

- Our use of HF Transformers use so far
  - Convert text to input tokens, pass to model, decode output tokens to text
- HF Pipelines provides a layer of abstraction
  - (Setup the pipeline, then call `pipe` method)
  - While still giving access to underlying components
- Pipelines also standardize other areas
  - e.g., `pipe(prompt).images[0]` works for all model types
  - `.to("cuda")` moves all components of the model to the GPU

## Sidebar: Seeds

- What is a seed?
  - (Optional) Integer value used to initialize the image generation
  - Used to generate the initial random noise
  - Using the same seed will generate the same image
- Why use a seed?
  - Controlling the seed allows you to then experiment with different prompts or parameters
  - Gives you more control/predictability vs. starting from random seed every time

# Hands-on

Open Text-to-Image notebook (text-to-image-sd-1.5.ipynb)

Test different prompts, seed values, and steps 

# Beyond SD 1.5

## Beyond SD 1.5

- Stable Diffusion 1.5
  - Great for learning about the diffusion process
  - But the image quality isn't great!
- Image models get large quickly
  - Higher resolutions demand more GPU/VRAM

## Introducing Replicate

![Source: https://replicate.com](./images/replicate.png)

## Introducing Replicate

- Similar to OpenRouter
  - But with a focus on image and video models
  - Extensive access to larger models (e.g., FLUX 2, Nano Banana, ImageGen)
  - Pay-per-call pricing (expect 2c per image for higher quality models)
  - API access (with Python and NodeJS library)
  - Fine-tune and share your own models

# Demo

Browsing models on Replicate

## Using the Replicate API

{{< embed notebooks/text-to-image-replicate.ipynb#run echo=true >}}

## Using the Replicate API

- **steps**: Number to steps to run through
- **seed**: Random seed value
- **prompt**: Prompt to use to guide the model
- **output_format**: Output format to return
- **safety_tolerance**: Safety tolerance (1 is most strict; 6 is most permissive)
- **prompt_upsampling**: Run the prompt through an LLM to be more descriptive/creative

## Using the Replicate API

:::: {.columns}
::: {.column width="50%"}
{{< embed notebooks/text-to-image-replicate.ipynb#result echo=false >}}
:::
::::

## Image-to-Image

Image-to-Image: "Make this image different"

- Originally solved by GAN approaches, but evolved into extension of the diffuser concept
  - Add noise to the original image (partial denoising)
  - Regenerate it with modifications based on the prompt
  - The original image heavily influences the output structure

## Image-to-Image

TBD: Notebook that shows the step-by-step process

## Image-to-Image

- Can also be used for...
  - Super resolution
  - Style transfer
  - Colorization (grayscale to color)
  - Depth maps

## Depth Maps

- Images (often greyscale) where each pixel's value represents the distance
- Historically required custom hardware
  - Depth Camera (e.g., RealSense) - ~$300-500
  - Module/processing for realtime (60fps) sensing

## Depth Maps

- Image-to-Image depth estimation models
  - Depth Anything, MiDaS, ZoeDepth
  - Low latency (MiDaS 3.1 @ 20fps on embedded GPU)
- Used for
  - 3D effects/estimation
  - Simple/low-cost robotics
  - Control input for other images

## Depth Maps

{{< embed notebooks/generate-depth-map.ipynb#input-image echo=false >}}

## Depth Maps

{{< embed notebooks/generate-depth-map.ipynb#run echo=true >}}

## Depth Maps

{{< embed notebooks/generate-depth-map.ipynb#result echo=false >}}

## Depth Maps

- Why do this?
  - Depth map can be used as control image for new image

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#control-image echo=false >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#run echo=true >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#result echo=false >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#run-2 echo=true >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#result-2 echo=false >}}

# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/04/assignment.html){.external target="_blank"}
- TBD

# References

## References
