{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3a97e9",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning on CUDA\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/simonguest/CS-394/blob/main/src/07/notebooks/train-cuda.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/simonguest/CS-394/raw/refs/heads/main/src/07/notebooks/train-cuda.ipynb\">\n",
    "  <img src=\"https://img.shields.io/badge/Download_.ipynb-blue\" alt=\"Download .ipynb\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14cd517",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e119297",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc10de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATASET_REPO = \"simonguest/test-dataset\" # Change this to your uploaded dataset location on HF\n",
    "\n",
    "BASE_MODEL_VENDOR = \"Qwen\" # Change this to your desired base model\n",
    "BASE_MODEL_NAME = \"Qwen3-1.7B\" # Change this to your desired base model\n",
    "BASE_MODEL = f\"{BASE_MODEL_VENDOR}/{BASE_MODEL_NAME}\"\n",
    "\n",
    "PROJECT_NAME = \"code-explainer\" # Name of your training project\n",
    "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME # WandB will use the same project name\n",
    "\n",
    "MODEL_NAME = f\"{BASE_MODEL_NAME}-{PROJECT_NAME}\" # Final model name\n",
    "HF_USERNAME = \"simonguest\" # Hugging Face username - used to upload your final models\n",
    "\n",
    "MODEL_FOLDER = f\"./models/{MODEL_NAME}\" # Local folder for storing the model files during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac00334",
   "metadata": {},
   "source": [
    "## Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "# LoRA-specific Parameters\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Use 4-bit quantization for efficiency (QLoRA)\n",
    "USE_4BIT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ef94b",
   "metadata": {},
   "source": [
    "## API keys and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import userdata # type:ignore\n",
    "  os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
    "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "  print(\"HF and WANDB API Tokens set for Colab\")\n",
    "else:\n",
    "  load_dotenv()\n",
    "  print(\"Loaded env vars from .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0da88",
   "metadata": {},
   "source": [
    "## Load dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05403d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(DATASET_REPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eaa8b2",
   "metadata": {},
   "source": [
    "## Format dataset for correct chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def format_chat_template(example):\n",
    "    messages = example[\"messages\"]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "formatted_dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    remove_columns=dataset['train'].column_names # type: ignore\n",
    ")\n",
    "\n",
    "print(\"\\nFormatted example:\")\n",
    "print(formatted_dataset['train'][0]['text'][:500])  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5c266",
   "metadata": {},
   "source": [
    "## Load base model with QLoRA configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca4006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "if USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "if USE_4BIT:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2f20d",
   "metadata": {},
   "source": [
    "## Create training configuration and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "report_to = \"none\"\n",
    "if os.environ.get(\"WANDB_API_KEY\") != None:\n",
    "  report_to = \"wandb\"\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_FOLDER,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=report_to,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Save the tokenizer model as this won't change during training\n",
    "tokenizer.save_pretrained(f\"{MODEL_FOLDER}/lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d061b",
   "metadata": {},
   "source": [
    "## Train and save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(f\"{MODEL_FOLDER}/lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a6413",
   "metadata": {},
   "source": [
    "## Load the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=f\"{MODEL_FOLDER}/lora\",\n",
    "    tokenizer=f\"{MODEL_FOLDER}/lora\",\n",
    "    temperature = 0.7,\n",
    "    max_new_tokens = 256,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc4c30",
   "metadata": {},
   "source": [
    "## Test the final model using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "idx = random.randint(0, len(formatted_dataset['test']) -1)\n",
    "test_data = formatted_dataset['test'][idx][\"text\"]\n",
    "\n",
    "output = pipe(test_data)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26ac30",
   "metadata": {},
   "source": [
    "## Create a model card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-card",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard\n",
    "\n",
    "card_content = f\"\"\"---\n",
    "base_model: {BASE_MODEL}\n",
    "tags:\n",
    "- peft\n",
    "- lora\n",
    "- text-generation\n",
    "---\n",
    "\n",
    "# {MODEL_NAME}\n",
    "\n",
    "## Model Description\n",
    "Fine-tuned from `{BASE_MODEL}` using QLoRA (4-bit) with supervised fine-tuning.\n",
    "\n",
    "## Training Details\n",
    "- Dataset: `{DATASET_REPO}`\n",
    "- LoRA rank: {LORA_R}, alpha: {LORA_ALPHA}\n",
    "- Epochs: {NUM_EPOCHS}, Learning rate: {LEARNING_RATE}\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "This model is a test model used for the CS-394/594 class at DigiPen.\n",
    "\n",
    "The model is designed to provide a summary explanation of a snippet of Python code, to be used in an IDE. This model takes a snippet of code (passed as the user prompt) and returns a two paragraph explanation of what the code does, including an analogy that helps students better understand how the code functions.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "This model is a single-turn model and has not been trained on support long, multi-turn conversations.\n",
    "\"\"\"\n",
    "\n",
    "card = ModelCard(card_content)\n",
    "card.save(f\"{MODEL_FOLDER}/lora/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e603dd",
   "metadata": {},
   "source": [
    "## Merge the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the configuration and model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "adapter_model = PeftModel.from_pretrained(base_model, f\"{MODEL_FOLDER}/lora\")\n",
    "\n",
    "# Merge and save the model\n",
    "merged_model = adapter_model.merge_and_unload() # type: ignore\n",
    "merged_model.save_pretrained(f\"{MODEL_FOLDER}/merged\")\n",
    "# Save the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.save_pretrained(f\"{MODEL_FOLDER}/merged\")\n",
    "\n",
    "# Copy the model card to the merged folder\n",
    "!cp {MODEL_FOLDER}/lora/README.md {MODEL_FOLDER}/merged/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cfcfb",
   "metadata": {},
   "source": [
    "## Upload the merged model and adapter to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# Set the MODEL_REPO\n",
    "MODEL_REPO = f\"{HF_USERNAME}/{MODEL_NAME}\"\n",
    "\n",
    "# Initialize the API\n",
    "api = HfApi()\n",
    "\n",
    "# Create the repository (if it doesn't exist)\n",
    "try:\n",
    "    create_repo(MODEL_REPO, repo_type=\"model\", exist_ok=True, private=False)\n",
    "    print(f\"Repository {MODEL_REPO} created or already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating repo: {e}\")\n",
    "\n",
    "# Upload merged model files to root (this is the main model)\n",
    "print(\"\\nUploading merged model to root...\")\n",
    "api.upload_folder(\n",
    "    folder_path=f\"{MODEL_FOLDER}/merged\",\n",
    "    repo_id=MODEL_REPO,\n",
    "    repo_type=\"model\",\n",
    "    path_in_repo=\"\",  # Empty string uploads to root\n",
    "    commit_message=\"Upload merged model\"\n",
    ")\n",
    "\n",
    "# Upload LoRA adapter to subfolder (optional but useful for reference)\n",
    "print(\"\\nUploading LoRA adapter...\")\n",
    "api.upload_folder(\n",
    "    folder_path=f\"{MODEL_FOLDER}/lora\",\n",
    "    repo_id=MODEL_REPO,\n",
    "    repo_type=\"model\",\n",
    "    path_in_repo=\"lora_adapter\",  # Keep adapter in subfolder\n",
    "    commit_message=\"Upload LoRA adapter\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ All files uploaded successfully to https://huggingface.co/{MODEL_REPO}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
