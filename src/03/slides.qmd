---
title: "Module 3: Agents and Tools"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Understood the evolution and licensing of models from GPT-2 through to modern day
- Understood instruction-tuned models, how they work, and how to configure
- Setup and used OpenRouter for accessing hosted models
- Understood the OpenAI API specification, the request/response payload, parameters, streaming, and structured output
- Created and shared a chatbot using a Gradio-based UI

## Lesson Objectives

- Describe the fundamental concepts behind Agents/Agentic AI
- Explore and provide feedback on an existing multi-agent setup
- Understand available agent SDKs, how they differ, and advantages/disadvantages
- Use the OpenAI Agents SDK to build an multi-agent system from scratch, including document indexing and retrieval
- Understand and implement tool calls and implement using OpenAIâ€™s function calling and via MCP

# Why Agents?

## Why Agents?

- Limitations of our prior chatbots
  - Needs constant human input every turn; No ability to plan beyond a single interaction
  - Single model with single context (conversation)
  - No ability to interact with external systems

# Introducing Agents

## What is an Agent?

![Source: https://www.youtube.com/watch?v=bwXaJXgezf4](./images/quote-1.png)

## What is an Agent?

![Source: https://www.weforum.org/stories/2025/06/cognitive-enterprise-agentic-business-revolution/](./images/quote-2.png)

## What is an Agent?

![Source: https://www.crn.com/news/ai/2025/10-hottest-agentic-ai-tools-and-agents-of-2025-so-far](./images/quote-3.png)

## What is an Agent?

![Source: https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027](./images/quote-4.png)

## What is an Agent?

Imagine a __DigiPen Campus Assistant__: An AI agent that can help you navigate anything and everything at DigiPen!

- "Where can I find the 'Hopper' room?"
- "Can you tell me more about FLM201?"
- "Oh, and what's today's vegetarian option at the Bytes Cafe?"

## Five Characteristics of Agents

1. Agents are **Planners**

- Agents are driven by goals
- And they can put together a plan for the steps to complete that goal.
  - "First, I will discover where course information is located"
  - "Then I will search for any courses that reference FLM201"
  - "Then I summarize all of the key points for the student"

## Five Characteristics of Agents

2. Agents are **Autonomous**

- Agents can then go off and execute the plan, independent of human input
- The concept of "human in the loop" still applies for confirmation
  - e.g. "Do you really want to place this order at the Bytes Cafe?"

## Five Characteristics of Agents

3. Agents are **Reactive**

- Agents can change mid-course depending on what they find and/or the environment.
  - e.g. "I couldn't find any course information on FLM201. I'm going to check if there are other 200-level FLM courses before responding to the student."

## Five Characteristics of Agents

4. Agents have **Persistence**

- Agents often have memory systems beyond the current conversation
- Broadly classified as short and long-term memory 
  - Short-term memory could be your order request at the Bytes cafe
  - Long-term memory could be your food preferences

## Five Characteristics of Agents

5. Agents can **Interact** with external systems

- Agents can **delegate** to other agents for complex tasks
  - (Or for tasks where other agents are better suited for.)
  - e.g., Campus Agent -> delegating to a Course Agent
- Agents can also be given access to external **tools**
  - e.g., File search, Web search, access to the Bytes Cafe API

# Hands-On

Try the DigiPen Campus Agent!

[**https://simonguest-campus-agent.hf.space/**](https://simonguest-campus-agent.hf.space/){.external target="_blank"}

Q: What worked? What surprised you?

Q: What didn't work? Where did the agent fail?

# OpenAI Agents SDK

## OpenAI Agents SDK

- [Announced](https://openai.com/index/new-tools-for-building-agents/){.external target="_blank"} in Mar 2025
  - Together with web search, file search, and computer use
  - And a new Responses API (formerly Assistants API)

## OpenAI Agents SDK

- Created to address the gap between chat completions (what we were using last week) and multi-step systems
  - vs. building your own, which a lot of developers were doing at the time
- Integrates function calling, handoffs, and statement management in the same package
- Supports Python and TypeScript; MIT licensed

## Not the only Agent SDK in town!

![Source: https://e2b.dev](./images/ai-agents-landscape.png)

## LangGraph 
- https://langchain-ai.github.io/langgraph/
- Python only
- MIT License
- One of the first agent frameworks, building on LangChain
  - IMO, too abstract/complex/bloated

## Crew.ai

- https://github.com/crewaiinc/crewai
- Python only
- One of the more popular commercial offerings
  - (Although they do have an MIT License/freemium model)

## Microsoft 

- AutoGen
  - https://microsoft.github.io/autogen/stable/
  - Python (.NET coming soon)
  - MIT License
- Microsoft Semantic Kernel
  - https://github.com/microsoft/semantic-kernel
  - Python, .NET, Java
  - MIT License

## Microsoft

- Now converging into the [Microsoft Agent Framework](https://github.com/microsoft/agent-framework){.external target="_blank"}
- (One of the few agent SDKs to support .NET)

# How Does the Campus Agent Work?

## Agent Structure

![Agents and tools for the DigiPen Campus Agent](./images/agent_graph.png)

## Campus Agent

{{< embed notebooks/campus-agent.ipynb#campus-agent echo=true outputs=false >}}

## Building Agent

{{< embed notebooks/campus-agent.ipynb#building-agent echo=true outputs=false >}}

## Course Agent

{{< embed notebooks/campus-agent.ipynb#course-agent echo=true outputs=false >}}

## Handbook Agent

{{< embed notebooks/campus-agent.ipynb#handbook-agent echo=true outputs=false >}}

## What's a Vector Store?

- A way to provide domain-specific knowledge beyond training data
  - e.g., current semester course information that is newer than GPT-5.2's cutoff date
- We could just insert these into the context window
  - But doesn't scale to more than a few documents

## What's a Vector Store?

- Instead, we use a vector store
  - Converts documents, paragraphs, or sentences into vector embeddings
  - (Remember these from module 1? :)
- Similar concepts are close to each other in vector space
  - Which makes it efficient to query
- Queries return the document (or pages within a document) that match
  - e.g., "Michelangelo" returns "Page 1 of the floor map"

## What's a Vector Store?

- Foundation for RAG (Retrieval Augmented Generation)
  - (Which we will cover in module 6)
- Many different types of vectors stores/databases
- For now, we will be using the OpenAI built in one

## What's a Vector Store

![](./images/openai-vector-store.png){.lightbox}

## Agent Structure

![Agents and tools for the DigiPen Campus Agent](./images/agent_graph.png)

## Why Do Agents Need Tools?

- The scope of the agents ability is contained within the model
- Tools enable the agent to reach out to systems beyond the model
- Examples
  - Read a file from disk or search the web (built in)
  - Calculator (because LLMs aren't great at math)
  - Code interpreter (running code on the fly)

## OpenAI Tool Calling

- Introduced by OpenAI in June 2023
- Originally called Function Calling
- Models are fine-tuned to return a structured function_call JSON object, specifying which function to call and with what arguments.
- Tools are provided as functions
- Option for the LLM to decide when to call the tool (always, never, auto)

## Cafe Agent

{{< embed notebooks/campus-agent.ipynb#cafe-agent echo=true outputs=false >}}

## Cafe Agent Tool

{{< embed notebooks/campus-agent.ipynb#cafe-agent-tool echo=true outputs=false >}}

## Sidebar: Fine-tuning models for Tools

- How do models know when they should call a tool?
  - Models are fine-tuned on conversations with tool call examples
  - The model learns patterns like "when the user asks about the weather, call the get_weather tool"
  - The request to call the tool is returned as a JSON payload

## Sidebar: Fine-tuning models for Tools

```json
{
  "role": "assistant",
  "content": null,
  "tool_calls": [
    {
      "id": "call_abc123",
      "type": "function",
      "function": {
        "name": "get_weather",
        "arguments": "{\"location\": \"San Francisco\", \"unit\": \"celsius\"}"
      }
    }
  ]
}
```

## Sidebar: Fine-tuning models for Tools

- The client then calls the tool with the required parameters
- And returns the result back to the model as a "tool" role message

## Sidebar: Fine-tuning models for Tools

```json
{
  "role": "tool",
  "tool_call_id": "call_abc123",
  "content": "{\"temperature\": 18, \"condition\": \"partly cloudy\", \"humidity\": 65, \"wind_speed\": 12}"
}
```

## Sidebar: Fine-tuning models for Tools

- RLHF is used for improve the accuracy for tool selection
  - Rewards are given for correctly choosing the right tool for a task
  - Or penalized for hallucinating tools and methods that don't exist

# Let's Run the Campus Agent

## Let's Run the Campus Agent

- Does the OpenAI Agents SDK work with OpenRouter?
  - Yes and No :)
- Yes to core functionality
  - Creating an agent, handoffs, calling custom tools
- No to calling built-in OpenAI tools
  - File search, Web search, Code interpreter 

## Let's Run the Campus Agent

- We'll need to create an OpenAI developer account
- Potentially add some credits to it

# Hands-On

Create a new developer account at [https://platform.openai.com](https://platform.openai.com){.external target="_blank"}

Create a new API key at [https://platform.openai.com/settings/organization/api-keys](https://platform.openai.com/settings/organization/api-keys){.external target="_blank"}

# Hands-On

Get the Campus Agent Notebook up and running (campus-agent.ipynb)

# Hands-On

Pick your own mini-scenario and create your own agents from scratch.

# Multiple Agents

## Why Multiple Agents?

- Context window limitations
- Each agent can have a different system prompt (instructions)
- Each agent can have a different underlying model
  - Specialized models (e.g., a vision encoder)
  - Or to blend cost
- Makes tool separation cleaner and more accurate

## Example of Multiple Agents

- Code generation
  - Agents for 'architect', code writer, tester, debugger, etc.
- Content generation
  - Agent to create content, other agents to generate images, translate content, etc.
- Travel booking
  - Agent to book flights, hotels, cars, etc. for packages

## Patterns for Agents

- As you get deeper into building agents, patterns start to emerge
  - Router (which is what we used in our demo) - hand off of tasks
  - Orchestrator (using other agents as tools)
  - Parallel agents (calling other agents in parallel and aggregating results)

## Patterns for Agents

![Source: https://www.anthropic.com/engineering/building-effective-agents](./images/agent-patterns.png)

# Agent Memory

## The Need for Memory

- Just like API calls, agents need the conversation/context every call
- This can be challenging with agents working on long-running tasks
  - And/or agents working on multiple threads with other agents
- Short-term and long-term memory

## Short-term Memory

- Used to store/retreive the current conversation thread
- Built-in to most SDKs
- In OpenAI Agents SDK called a `session`

## Short-term Memory

{{< embed notebooks/memory.ipynb#create-session echo=true >}}

## Short-term Memory

{{< embed notebooks/memory.ipynb#conversation-stored echo=true >}}

## Short-term Memory

{{< embed notebooks/memory.ipynb#conversation-retrieved echo=true >}}

## Short-term Memory

{{< embed notebooks/memory.ipynb#without-session echo=true >}}

# Demo

Short-Term memory in OpenAI Agents SDK (memory.ipynb)

## Long-term Memory

- More challenging
- You don't want to store/retrieve the entire conversations
- Long-term memory types
  - **Factual**: General facts (e.g., name, address, seating preferences)
  - **Episodic**: Past conversations (e.g., user booked a trip to Paris)
  - **Procedural**: Learnings (e.g., the best hotel site to book accomodation)

## Implementing Long-term Memory

- Lots of startup options!
  - [Supermemory](https://supermemory.ai){.external target="_blank"}
  - [Letta](https://www.letta.com){.external target="_blank"}
  - [mem0](https://github.com/mem0ai/mem0){.external target="_blank"}
  - ...and lots more

## Implementing Long-term Memory (mem0)

```python
from mem0 import Memory
memory = Memory()

# Create new memories from the conversation
messages.append({"role": "assistant", "content": assistant_response})
memory.add(messages, user_id=user_id)

# Retrieve relevant memories
relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
# (append these to the system prompt)
```

## Implementing Long-term Memory

- Or "roll your own"
- Long-term memory types
  - **Factual**: General facts (e.g., name, address, seating preferences)
  - **Episodic**: Past conversations (e.g., user booked a trip to Paris)
  - **Procedural**: Learnings (e.g., the best hotel site to book accomodation)
- Create tools for **factual** storage
  - e.g., a profile tool with set/get options
- Use LLM to summarize short-term session conversations to store **episodic** and **procedural** learnings.

# Beyond Tool Calling

## Beyond Tool Calling

- Tool calling is super useful, but...
  - You need to write the function(s) yourself
  - And then expose them to OpenAI using the `@function_tool` method
- What if there was a way to standardize this?

## MCP (Model Context Protocol)

- Released by Anthropic in March 2024
- Provides a standard interface for tools - akin to a USB standard for peripherals
- Implementations are known as "MCP servers"
  - A server exposes one or more tools (functions)
  - Uses JSON-RPC 2.0 as underlying RPC protocol
  - Servers can run remotely over HTTP (supports SSE)
  - Or can be hosted locally and accessed via stdio
  - Many servers hosted using Node.js

## MCP (Model Context Protocol)

![](./images/awesome-mcp.png){.lightbox}

## MCP (Model Context Protocol)

![](./images/open-meteo.png){.lightbox}

## MCP (Model Context Protocol)

![](./images/open-meteo-mcp-server.png){.lightbox}

# Demo

MCP Model Inspector (`npx @modelcontextprotocol/inspector`)

Command: `npx -y open-meteo-mcp-server`

# OpenAI Agents SDK and MCP

## OpenAI Agents SDK and MCP

- MCP supported in OpenAI Agents SDK (as of Sep 2025)
- Exposes `MCPServerStdio` and `MCPServerSse` to connect to local and remote servers

## OpenAI Agents SDK and MCP

{{< embed notebooks/open-meteo-mcp.ipynb#list-available-tools echo=true >}}

## OpenAI Agents SDK and MCP

{{< embed notebooks/open-meteo-mcp.ipynb#simple-query echo=true >}}

## Creating Your Own MCP Server

- Multiple SDKs on [https://modelcontextprotocol.io/docs/sdk](https://modelcontextprotocol.io/docs/sdk){.external target="_blank"}
  - Python, TypeScript, Go, Rust, C#, and more
- Very similar to tool calling
  - Define your MCP server
  - Annotate your functions with `@mcp.tool()`
  - Add descriptions to the tool methods to help the LLM select which tool to call

## Example: micro:bit MCP Server

![https://simonguest.com/p/microbit-mcp/](./images/microbit_mcp_sm.mp4){.lightbox}

# Hugging Face Spaces

## Hugging Face Spaces

- We've been using Gradio, but hosting via notebooks isn't ideal
  - Even with `share=True` you have to keep the notebook running
- Wouldn't it be nice if we could easily host our Gradio app?

## Hugging Face Spaces

![](./images/hf-spaces.png){.lightbox}

## Hugging Face Spaces

- Free cloud hosting for ML demos and applications
  - Supports Gradio, Streamlit, and static HTML/JS
- For Gradio, either upload a `main.py` or a Docker configuration file
- Hugging Face handles resource allocation
  - Sleeps the space if it's inactive
  - Integrates with the queuing mechanism of Gradio to batch requests
  - Supports multiple GPU types (if signed up for Pro account)

# Demo

Hosting the DigiPen campus agent on Hugging Face Spaces

# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/03/assignment.html){.external target="_blank"}
- Leave text behind and explore image-based models!
- Introduce the diffuser
- And go other way with vision encoders

# References

## References
