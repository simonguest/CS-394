---
title: "Module 7: Increasing Model Accuracy (Part 2)"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Understood model training, dataset curation, what leads to hallucinations in models, how models are evaluated, and an overview of techniques to increase accuracy
- Explored use cases, advantages, and disadvantages of prompt engineering
- Introduced and implemented RAG (Retrieval Augmented Generation) to increase the accuracy of a limited SLM
- Started the exploration of how to fine-tune models
- Used a foundational model to generate synthetic data for fine-tuning a small language model

## Lesson Objectives

- Use generated synthetic data to fine-tune a 1B parameter model
- Use W&B (Weights and Biases) to observe parameters during the training run
- Post-training, use W&B to use cosine similarity and LLM-as-a-Judge to evaluate the accuracy of our trained model
- Train smaller models (270M parameters) and compare the results
- Understand and create a model card, upload the model to Hugging Face and share

# Model Training Stages

## Model Training Stages

- There are typically three main stages of training a model:
  - **Pretraining:** The initial, large-scale training run that creates a base model with pretrained weights
    - Training data: Trillions of tokens from the internet, books, code, etc.
  - **Supervised Fine-Tuning (SFT):** Shapes the base model to follow instructions and respond in a desired style or fornat
    - Training data: Hundreds to thousands of input/output pairs
  - **Alignment:** Further refines the model toward preferred behaviors and values
    - Training data: Human-curated preferences and/or reward-based feedback

## Model Training Stages

TBD: Diagram

## Model Training Stages

- Many models (e.g., Gemma3-it) have already gone through these three stages
  - We will be adding a fourth stage. An SFT of an existing "polished" model
- Why?
  - The model already knows how to follow instructions, answer questions, etc.
  - They need far less training data to get good results
  - Training tends to be faster and more stable since they are starting from a good baseline

## Model Training Stages

TBD: Updated diagram

# Supervised Fine-Tuning (SFT)

## Supervised Fine-Tuning (SFT)

- How does SFT work?
  - Build a dataset (we did this already!)
  - Feed the prompts into the model and compare the desired output tokens with the model's predictions
  - The difference between the two is known as the "training loss"
  - Backpropagate through the model, nudging the weights to guide toward the desired output
  - This "nudging" is done using an optimizer (typically AdamW)
  - Repeat for typically 1-3 epochs (complete passes through the dataset)
  - Monitor validation loss to detect overfitting or diminishing returns

## Supervised Fine-Tuning (SFT)

TBD: Diagram 

## Supervised Fine-Tuning (SFT) with LoRA

- Adjusting all parameters in a model can be compute and memory intensive
- We will use LoRA (Low-Rank Adaptation), a Parameter Efficient Fine-Tuning (PEFT) strategy
- How LoRA works:
  - The model's base weights (W) are frozen (never updated) during training
  - Two small matrices (A and B) are introduced, whose product represents the desired change to W
  - Only A and B are updated during training — a tiny fraction of the total parameters
  - After training, these matrices are known as an "adapter"
  - The adapter can be kept separate (swappable) or merged back into W to create a new, standalone model

## Supervised Fine-Tuning (SFT) with LoRA

TBD: Diagram

## Supervised Fine-Tuning (SFT) with QLoRA

- To further increase memory efficiency, we will use QLoRA (Quantized LoRA)
  - The base model weights (W) are quantized to 4-bit (NF4 format) for storage
  - During the forward pass, W is dequantized to bf16 on the fly for computation, then discarded
  - Adapter matrices A and B are kept at full precision (bf16) for training accuracy
  - Since only A and B are trained, optimizer states (which can be 3-4x the size of W at full precision) only exist for the tiny adapter matrices — dramatically reducing memory usage

## Supervised Fine-Tuning (SFT) with QLoRA

TBD: Diagram

# Fine-Tuning Steps

## Fine-Tuning Steps

- Upload our .jsonl files to Hugging Face to create a dataset
- In our training notebook...
  - Download the dataset
  - Format our training data to match the chat template
  - Start our training run!
- Use W&B (Weights & Biases) to monitor how well our model is learning
  - Training loss
  - Validation loss

## Fine-Tuning Steps 

- After training has completed...
  - Test (both manually and automatic)
- Assuming things went well...
  - Merge and upload our model to Hugging Face
  - Quantize our model (create a GGUF) and upload to Hugging Face
  - Publish a model card
  
# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/07/assignment.html){.external target="_blank"}
- TBD

# References

## References
