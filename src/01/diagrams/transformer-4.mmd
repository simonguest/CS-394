graph LR
    Input["Input: 'Bonjour, comment allez-vous?'"]
    
    subgraph Transformer
        direction TB
        
        subgraph "Encoder Layer"
            direction TB
            E_SelfAttn[Multi-Head<br/>Self-Attention]
            E_AddNorm1[Add & Norm]
            E_FFN[Feed-Forward<br/>Network]
            E_AddNorm2[Add & Norm]
            
            E_SelfAttn --> E_AddNorm1 --> E_FFN --> E_AddNorm2
        end
        
        subgraph "Decoder Layer"
            direction TB
            D_SelfAttn[Masked Multi-Head<br/>Self-Attention]
            D_AddNorm1[Add & Norm]
            D_CrossAttn[Multi-Head<br/>Cross-Attention]
            D_AddNorm2[Add & Norm]
            D_FFN[Feed-Forward<br/>Network]
            D_AddNorm3[Add & Norm]
            
            D_SelfAttn --> D_AddNorm1 --> D_CrossAttn --> D_AddNorm2 --> D_FFN --> D_AddNorm3
        end
        
        E_AddNorm2 -.->|Encoder<br/>Output| D_CrossAttn
    end
    
    Output["Output: 'Hello, how are you?'"]
    
    Input --> E_SelfAttn
    D_AddNorm3 --> Output