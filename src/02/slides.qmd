---
title: "Week 2: Exploring Hosted LLMs"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
execute:
  enabled: false
---

## Recap of Last Week's Lecture

- Explored the history of vector embeddings and tokenization
- Understood the transformer architecture at a high level
- Used our first transformer to translate language
- Covered a brief history of early generative transformers
- Setup and used Colab, and became familiar with the basics of notebooks and Python

## Lesson Objectives

- Understand the evolution and licensing of models from GPT-2 through to modern day
- Understand instruction-tuned models, how they work, and how to configure
- Setup and use OpenRouter for accessing hosted models
- Understand the OpenAI API specification, the request/response payload, parameters, streaming, and structured output
- Create and share a chatbot using a Gradio-based UI

# From GPT-2 to GPT-3.5

## From GPT-2 to GPT-3.5

```{mermaid}
timeline
    Feb 2019 : OpenAI releases GPT-2
             : 1.5B parameters
             : Initially withheld full model due to concerns about misuse
             : Demonstrates impressive text generation capabilities with minimal fine-tuning

    May 2020 : OpenAI releases GPT-3
             : 175B parameters
             : Demonstrates strong few-shot learning capabilities
             : Marks a significant leap in model capabilities and scale

    June 2020 : GPT-3 available through OpenAI API
              : Still a completion model, not instruction-tuned

    2021 : InstructGPT Development
          : Built on GPT-3 with RLHF fine-tuning
          : Trained to follow instructions and understand user intent
          : Key innovation enabling ChatGPT
    
    Jan 2021 : Anthropic Founded
             : Founded by Dario & Daniela Amodei with ~7 senior OpenAI employees
             : Dario led GPT-2/3 development and co-invented RLHF

    Nov 2022 : ChatGPT Launch
              : Built on GPT-3.5 using RLHF
              : 1M+ users in 5 days
              : Sparked widespread interest in generative AI
```

## Completion vs. Instruction-Tuned

- Completion Model just predicts the next token
  - Input prompt: `Mary had a little`
  - Max total tokens: `50`
  - Temperature: `0 - 1.0`
  - top_k: consider only the top k tokens in the response
  - top_p: Nucleus sampling (probability cut off - 0 and 1.0)
- Output
  - `Mary had a little lamb, its fleece was white as snow...`  (up to max tokens)

## Completion vs. Instruction-Tuned

- You can't really converse with it
- `What should I do on my upcoming trip to Paris?` (max tokens = 75)
- `What should I do on my upcoming trip to Paris? Please provide a detailed plan of action to help me plan my trip to Paris. 1. Research the best time to travel to Paris:`

## Instruction-Tuned Models

- Supervised Fine-Tuning
  - Large datasets of questions/answers, tasks/completions, demonstrating helpful assistant behavior
- RLHF (Reinforcement Learning from Human Feedback)
  - Human raters rank different model responses, training a reward model
- Chat Templates
  - Structured format to distinguish speakers in a conversation: Typically **system**, **user**, and **assistant**

## System, User, Assistant

- **System** prompt sets the intention for the model, guiding the output
  - "You are a helpful assistant"
  - "You help students with their math homework"
  - "You help travelers make plans for their trips"
  - Has to come first in the conversation
  - Only one system prompt
  - Optional for some models

## System, User, Assistant

- System Prompt best practices
  - **Be specific**: "You are a Python programming tutor who explains concepts using simple analogies and provides code examples."
  - **Define output**: "List no more than 3 suggestions. Always show your work step by step."
  - **Set boundaries**: "If you are asked questions outside coding, politely redirect the student back to the task."

## System, User, Assistant

- **User** prompt is the message (request) from the user
  - "How many 'r's in Strawberry?"
  - "What is linear algebra?"
  - "What should I do on my upcoming trip to Paris?"
- **Assistant** prompt is the message (reply) from the model
  - "There are three r's in Strawberry"
  - "Linear algebra is the branch of mathematics that studies vectors, etc."
  - "Here are some suggestions for your upcoming trip to Paris: 1. Explore the Louvre Museum: etc."

## What's a Chat Template?

- The format used to train instructional models on conversations involving system, user, and assistant prompts.
- Each model family uses a different format (there is no universal standard)
- Wrong format will likely generate nonsense/garbage

## ChatML (GPT-3.5 and other models)

```
<|im_start|>system
You help travelers make plans for their trips.<|im_end|>
<|im_start|>user
Hello<|im_end|>
<|im_start|>assistant
Hi there! How can I help you?<|im_end|>
<|im_start|>user
What should I do on my upcoming trip to Paris?<|im_end|>
<|im_start|>assistant

```
## Chat Templates in Practice

{{< embed notebooks/instruction-tuned.ipynb#chat-template echo=true >}}

## Putting This Together

{{< embed notebooks/instruction-tuned.ipynb#base-model echo=true >}}

## Putting This Together

{{< embed notebooks/instruction-tuned.ipynb#it-model echo=true >}}

# Demo

Base vs. Instruction-Tuned Model notebook in Colab

# Hands-On

Experiment with your own phrases in the instruction-tuned.ipynb notebook

# Model Evolution

## Model Evolution (GPT 3.5 onwards)

```{mermaid}
timeline
    Nov 2022 : ChatGPT Launch
                  : Built on GPT-3.5 using RLHF
                  : 1M+ users in 5 days
                  : Sparked widespread interest in generative AI

    Feb 2023 : Llama 1 Released
                  : Meta's LLaMA (7B, 13B, 33B, 65B parameters)
                  : 13B model exceeded GPT-3 (175B) on most benchmarks
                  : Limited researcher access
                  : Text completion only (Alpaca fine-tune added instructions)

    Jul 2023 : Llama 2 Released
              : Available in 7B, 13B, 70B sizes
              : Trained on 40% more data than Llama 1
              : First open-weights Llama for commercial use
```

## Closed vs. Open Models

- **Closed Source:** 
  - Hosted models
  - No ability to inspect the weights of the models
  - No ability to download the models
  - OpenAI GPT-5, Claude Sonnet 4.5, Google's Gemini
  - Very large models; often referred to as **foundational** models or **frontier** models

## Closed vs. Open Models

- **Open Weight:** 
  - Downloadable model files
  - You can download the model files with pretrained weights, but no training data
  - No training data == No ability to recreate the model from scratch
  - Meta's Llama, Google's Gemma, Alibaba's Qwen, OpenAI gpt-oss-120b 
  - Range from small to medium in size (1Gb - 500Gb+)

## Closed vs. Open Models

- **Open Source:** 
  - Models with access to the training data set
  - You can download the model files with pretrained weights **and** the training data used to train it
  - i.e., you could create the model from scratch
  - Examples: AI2's OLMo

## Accessing Closed Models

- Consumer Website / App
  - e.g., ChatGPT website or AppStore App
  - Limited free tier; monthly subscription for more usage
- API Access
  - OpenAI's API Platform; Create a developer account
  - Credit card required
  - Charged for tokens sent to the model and tokens returned from the model
  - GPT 5.2 Chat = $1.75 per million tokens input; $14 per million tokens output

## Accessing Closed Models

- How much is going to cost?
  - Token estimators (e.g., tiktoken from OpenAI)
  - Or napkin math: 100 tokens ~= 75 English words

## Accessing Closed Models

- Example
  - Input from user = 75 words (100 tokens)
  - Output from model = 150 words (200 tokens)
  - Total cost = 100 input tokens + 200 output tokens
  - Total cost = $0.000175 + $0.0028 = $0.002975
- At scale
  - 10,000 users / 1 request per month ~= $29.75/mo   
  - 10,000 users / 1 request per day ~= $892.50/mo

## Accessing Open Models

- Download and run on your own hardware
  - Or download them and run them on Colab, as we've been doing in our demos
  - (We'll be covering this later on in the course)
- Also access them (via an API), hosted on someone else's hardware

# Calling Models via APIs

## OpenAI Chat Completions API

- 2020: OpenAI launched GPT-3 API with a `/completions` endpoint.
  - First major LLM API
- 2022: ChatGPT launch; massive adoption
- 2023 `/chat/completions` endpoint released, becomes the dominant interface
- 2023-2024: Other providers use the same API format for their own models vs. inventing their own
  - Build on the OpenAI developer ecosystem
  - "OpenAI-compatible" became a selling point

## OpenAI Chat Completions API

- Who uses the OpenAI Chat Completions API format?
  - Anthropic (Claude API is very similar, with minor differences)
  - OpenRouter, an inference provider for many models
  - Open source tools: LiteLLM, LangChain
  - Local serving: Ollama, vLLM, llama.cpp are all "OpenAI-compatible"

## Using the Chat Completions API

{{< embed notebooks/chat-completion-openai.ipynb#init echo=true >}}

## Using the Chat Completions API

{{< embed notebooks/chat-completion-openai.ipynb#request echo=true >}}

## Using the Chat Completions API

{{< embed notebooks/chat-completion-openai.ipynb#response echo=true >}}

## Chat History Management

- Key Considerations
  - Models don't hold any state
  - API sends **full** conversation on every request and the model reads through the full conversation on every call
  - The size of the conversation is known as the context
  - The maximum size the model can process is referred to as the **context window**

## Chat History Management

- Context window sizes
  - Nano models ~= 32k tokens
  - Small models ~= 120k tokens
  - Frontier models ~= ~= 1M tokens
- Large conversations can cause challenges
  - They are expensive (you pay per token for whole conversation every time)
  - Small models often forget early details in long conversation histories

## Chat History Management

- Mitigation Strategies
  - Remove older messages from the history
  - Implement sliding window across the conversation history
  - Summarize older messages and rewrite the history
  
# Calling Other Models

## Calling Other Models

- We could just duplicate our notebook, change the URL to another provider (e.g., Claude, Google, etc.), but:
  - A separate account with each provider
  - A separate credit card with each provider
  - A separate API key to use for each provider
  - Duplicate notebooks for each provider
- Wouldn't it be nice to have a single service (inference provider) that exposed lots of different models

## Introducing OpenRouter

- Introducing OpenRouter (https://openrouter.ai)
  - A unified API to hundreds of AI models through a single endpoint
  - (Using OpenAI's Chat Completion API)
  - OpenAI, Claude, Gemini, Grok, Nova, Llama, DeepSeek, Qwen, and many others.
  - Pay per API call, often same cost as the provider
  - Newer APIs tend to be free for a short period

## Using OpenRouter

{{< embed notebooks/chat-completion-openrouter.ipynb#init echo=true >}}

## Using OpenRouter

{{< embed notebooks/chat-completion-openrouter.ipynb#request echo=true >}}

## Using OpenRouter

{{< embed notebooks/chat-completion-openrouter.ipynb#response echo=true >}}

# Demo

OpenRouter overview, OpenRouter Notebook, and setting secrets in Colab

# Hands-On

Register for an OpenRouter account, create an API key, import into the OpenRouter Notebook (chat-completion-openrouter.ipynb) and experiment with different models

# Token Streaming and Structured Output

## Token Streaming

- In our notebooks, responses can take a few seconds to be returned
  - Not the best user experience, especially for consumer products
- Need a way to support streaming of tokens as they are generated (a.k.a. "typewriter effect")
  - Streaming added to Chat Completions API in early 2023
  - Supported by other major vendors (Anthropic, Cohere, etc.)
  - Now expected as a baseline feature

## How Does Token Streaming Work?

- Uses SSE (Server-Sent Events)
  - Unidirectional (server to client)
  - Uses standard HTTP/1.1 or HTTP/2
  - Server sends a response with a `text/event-stream` MIME type
  - Client uses built-in `EventSource` API to open the connection, listen to messages, and handle events.

## SSE Data Format

```
data: {"choices":[{"delta":{"content":"Hello"}}]}

data: {"choices":[{"delta":{"content":" world"}}]}
  
data: [DONE]
```

Data sent as chunks, prefixed with `data:` and separated by double newlines

## Implementing Token Streaming

{{< embed notebooks/token-streaming.ipynb#request echo=true >}}

# Structured Output

## Structured Output

- So far, the models have generated non-structured output (i.e., free-form text)
- Sometimes, paragraph. Sometimes, numbered list.
- Sometimes, you just need structure
  - "Return your result in JSON format"
  - "Give me the coordinates for Paris"
  - "What's the temperature in Paris right now?"

## Structured Output

- You can try to use the system prompt
  - "Return the result in JSON only"
- But... it doesn't always work
  - Early/small models struggle with correct JSON formatting
  - Even larger models make mistakes (e.g., missing closing brace)
- Sometimes the models just forget!
  - "RETURN THE RESULT IN JSON ONLY. NO OTHER TEXT!!!"

## Structured Output in OpenAI API

- Nov 2023: OpenAI added JSON mode
  - `response_format: {"type": "json_object"}`
  - Guaranteed valid JSON, but didn't enforce schema
  - Sometimes mixed up/missed fields
- Aug 2024: **Structured Outputs** launched
  - `response_format: {"type": "json_object", ...}`
  - 100% reliability that output matches the your schema

## How Structured Outputs Work

- Constrained Decoding
  - When generating responses, the model normally samples from all possible next tokens
  - With **constrained decoding**, the next token is **dynamically filtered** to only allow tokens that keep the output schema valid
    - e.g., if schema requires an integer, string tokens are masked out from the probability distribution

## How Structured Outputs Work

- Slightly slower token generation due to computational overhead
- Technically, mathematically impossible to generate invalid output
  - (Real world: I see 7000:1 error rates with GPT-5.1 chat)

## Implementing Structured Outputs

{{< embed notebooks/structured-outputs.ipynb#model echo=true >}}

## Implementing Structured Outputs

{{< embed notebooks/structured-outputs.ipynb#request echo=true >}}

## Implementing Structured Outputs

{{< embed notebooks/structured-outputs.ipynb#display echo=true >}}

# Demo

Token Streaming and Structured Outputs

# Hands-On

Investigate token-streaming.ipynb and structured-outputs.ipynb. Try adding an additional field to the Location model.

# Creating a Chat UI

## Creating a Chat UI

- Up to now, we've been making requests and printing the responses
- Good for learning concepts, but not a "product" that others can use
- We want to build a UI that supports conversation threads, streaming, rich inputs/outputs, etc.
- But we don't want to start from scratch!

## Introducing Gradio

![Source: https://www.gradio.app/](images/gradio-website.png)

## What is Gradio?

- **Created in 2019:** Startup called Gradio developing demos for research/academia
- **Acquired by Hugging Face in 2021:** became the standard interface for Hugging Face Spaces
- **Now industry standard:** For ML demos - used by researchers, startups to showcase models without front-end expertise

## What is Gradio?

- Rapid UI creation with minimal code
  - 5-10 lines of Python for an interactive interface. No HTML, CSS, JS required.
- Rich input/output types
  - Text, images, audio, video, files, dataframes, etc.
- ML workflows
  - Supports streaming, queue management (for server-side deployment), flagging/feedback collection
- Deployment flexibility
  - Can run locally, create temporary public links, or embed in production apps

## Example 1: Basic Interface

{{< embed notebooks/gradio.ipynb#example-1 echo=true outputs=false >}}

## Example 1: Basic Interface

{{< embed notebooks/gradio.ipynb#example-1 echo=false >}}

## Example 2: Basic Chat Interface

{{< embed notebooks/gradio.ipynb#example-2 echo=true outputs=false >}}

## Example 2: Basic Chat Interface

{{< embed notebooks/gradio.ipynb#example-2 echo=false >}}

## Example 3: Streaming Chat Interface

{{< embed notebooks/gradio.ipynb#example-3 echo=true outputs=false >}}

## Example 3: Streaming Chat Interface

{{< embed notebooks/gradio.ipynb#example-3 echo=false >}}

# Demo

Gradio Notebook

# Hands-On

Explore the three examples in gradio.ipynb. 

# Looking Ahead to Next Week

## Looking Ahead to Next Week

- [This week's assignment!](https://simonguest.github.io/CS-394/src/02/assignment.html){.external target="_blank"}
- Explore the world of AI Agents
- Create agents, building upon our knowledge of Gradio
- Give the agent tools to perform functions beyond what an LLM can do

# References

## References
