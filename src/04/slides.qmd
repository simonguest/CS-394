---
title: "Module 4: Multimedia and Multimodal Models"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Described the fundamental concepts behind Agents/Agentic AI
- Explored and provided feedback on an existing multi-agent setup
- Understood available agent SDKs, how they differ, and advantages/disadvantages
- Used the OpenAI Agents SDK to build a multi-agent system from scratch, including document indexing and retrieval
- Understood and implemented tool calls using OpenAI's function calling and via MCP

## Lesson Objectives

- Understand the fundamentals and history of diffuser models
- Explore and use models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet
- Setup and use Replicate to create a custom pipeline of production-grade models
- Understand the fundamentals and history of Vision Encoders and VLMs
- Implement/test a local VLM model for on-device inference

# Multimedia vs. Multimodal

## Multimedia vs. Multimodal

- **Multimedia** models
  - Single input/output models for images, video, audio, etc.
  - Also known as computer vision, audio models
- Examples
  - **Text-to-Image** (generate an image from a text prompt)
  - **Image-to-Image** (generate an image from an existing image)
  - **Image-to-3D** (generate a 3D object from an image)

## Multimedia vs. Multimodal

- **Multimodal** models
  - Process multiple datatypes such as text, images, and audio
  - Also known as VLMs (Vision-Language Models) or ALMs (Audio-Language Models)
- Examples
  - **Image-Text-to-Text** (ask a question about this image)
  - **Image-Text-to-Image** (decompose this image into multiple layers)
  - **Audio-Text-to-Text** (what is this sound?)

## Text-to-Image

:::: {.columns}

::: {.column width="50%"}
"A photograph of an astronaut riding a horse."

- Based on a concept called a diffusion transformer
- Commonly known as a **diffuser**
- Two stage process, inspired by thermodynamics
:::

::: {.column width="50%"}
![](./images/astronaut.png)
:::
::::

## Introducing the Diffuser

- Training
  - During training, random noise is added to images in steps
  - Model learns to predict what noise was added (forward diffusion process)
- Inference (process runs in reverse)
  - Start with pure random noise
  - Model estimates what noise should be removed to create a realistic image
  - Using the text prompt, the model steers the process towards images that match the description

## Image Diffusion Models in 2022

```{mermaid}
timeline
  August 2022 : Stable Diffusion v1.4
              : First open-source high-quality model
  September 2022 : Stable Diffusion v1.5
                  : Refined version
  October 2022 : eDiff-I (NVIDIA)
                : Ensemble approach
  November 2022 : Stable Diffusion v2.0/2.1
                : Higher resolution (768x768)
```

## Image Diffusion Models in 2023

```{mermaid}
timeline
  March 2023 : Midjourney v5
              : Exceptional artistic quality
  April 2023 : ControlNet
        : Precise spatial control
        : AnimateDiff - Video generation
  July 2023 : SDXL (Stable Diffusion XL)
            : 1024x1024 native resolution
  August 2023 : SDXL Turbo
              : Real-time capable generation
```

## Image Diffusion Models in 2024

```{mermaid}
timeline
  February 2024 : Stable Diffusion 3
                : Improved text understanding
  June 2024 : Stable Diffusion 3.5
            : Multiple model sizes
  2024 : FLUX.1 (Black Forest Labs)
        : State-of-the-art open model
        : Imagen 3 (Google DeepMind)
        : Photorealistic quality
```

## Image Diffusion Models in 2025

```{mermaid}
timeline
  May 2025 : Imagen 4 (Google DeepMind)
           : Improved text rendering, 2K resolution
  August 2025 : Nano Banana (Google)
              : Autoregressive model in Gemini 2.5 Flash
  November 2025 : FLUX.2 (Black Forest Labs)
                : 32B parameters, multi-image references
```

# Text-to-Image

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#load-model echo=true outputs=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#steps echo=true outputs=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#steps echo=false >}}

## Text-to-Image with SD 1.5

{{< embed notebooks/text-to-image-sd-1.5.ipynb#final echo=false >}}

## Sidebar: What are these pipelines?

- Our use of HF Transformers use so far
  - Convert text to input tokens, pass to model, decode output tokens to text
- HF Pipelines provides a layer of abstraction
  - (Setup the pipeline, then call `pipe` method)
  - While still giving access to underlying components
- Pipelines also standardize other areas
  - e.g., `pipe(prompt).images[0]` works for all model types
  - `.to("cuda")` moves all components of the model to the GPU

## Sidebar: Seeds

- What is a seed?
  - (Optional) Integer value used to initialize the image generation
  - Used to generate the initial random noise
  - Using the same seed will generate the same image
- Why use a seed?
  - Controlling the seed allows you to then experiment with different prompts or parameters
  - Gives you more control/predictability vs. starting from random seed every time

# Image-to-Image

## Image-to-Image

- Image-to-Image: "Make this image different"
- Originally solved by GAN approaches, but evolved into extension of the diffuser concept
  - Add noise to the original image (partial denoising)
  - Regenerate it with modifications based on the prompt
  - Strength parameter (0.0 - 1.0) to indicate the weight to the new image vs. original
  - The original image heavily influences the output structure

## Image-to-Image (Generate)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#generate echo=true >}}

## Image-to-Image (Original)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#original echo=false >}}

## Image-to-Image (Original)

Prompt: "a goldendoodle wearing sunglasses, high quality, detailed"

## Image-to-Image (Original)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#original echo=false >}}

## Image-to-Image (0.3)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#0.3 echo=false >}}

## Image-to-Image (0.5)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#0.5 echo=false >}}

## Image-to-Image (0.7)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#0.7 echo=false >}}

## Image-to-Image (0.9)

{{< embed notebooks/image-to-image-sd-1.5.ipynb#0.9 echo=false >}}

# Hands-on

Explore Text-to-Image and Image-to-Image notebooks (text-to-image-sd-1.5.ipynb and image-to-image-sd-1.5.ipynb)

Test different prompts, images, seed values, and steps.

# Beyond SD 1.5

## Beyond SD 1.5

- Stable Diffusion 1.5
  - Great for learning about the diffusion process
  - But the image quality isn't great!
- Image models get large quickly
  - Higher resolutions demand more GPU/VRAM

## Introducing Replicate

![Source: https://replicate.com](./images/replicate.png)

## Introducing Replicate

- Similar to OpenRouter
  - But with a focus on image and video models
  - Extensive access to larger models (e.g., FLUX 2, Nano Banana, ImageGen)
  - Pay-per-call pricing (expect 2c per image for higher quality models)
  - API access (with Python and NodeJS library)
  - Fine-tune and share your own models

# Demo

Browsing models on Replicate

# Using the Replicate API

## Using the Replicate API

{{< embed notebooks/text-to-image-replicate.ipynb#run echo=true >}}

## Using the Replicate API

- **steps**: Number to steps to run through
- **seed**: Random seed value
- **prompt**: Prompt to use to guide the model
- **output_format**: Output format to return
- **safety_tolerance**: Safety tolerance (1 is most strict; 6 is most permissive)
- **prompt_upsampling**: Run the prompt through an LLM to be more descriptive/creative

## Using the Replicate API

:::: {.columns}
::: {.column width="50%"}
{{< embed notebooks/text-to-image-replicate.ipynb#result echo=false >}}
:::
::::

## Sidebar: Safety Tolerance

- How does safety tolerance work?
  - Safety/guardrails are not typically embedded into the model
  - e.g., SD 1.5 will generate NSFW images easily
- Instead, separate classifiers run alongside the model
  - Input filtering: Analyzes the prompt prior to generation
  - Output filtering: Image classifier examines the model before showing it to the user
  - Thresholds used to control the classifiers

## Image-to-Image

- Can also be used for...
  - Super resolution (increase the resolution of this image)
  - Style transfer (recreate this image in the style of...)
  - Colorization (grayscale to color)
  - Depth maps

## Depth Maps

:::: {.columns}
::: {.column width="50%"}

- Images (often greyscale) where each pixel's value represents the distance from the viewer
  - i.e., objects in the foreground are lighter, background are darker

:::
::: {.column width="50%"}
![](./images/example-depth.png)
:::
::::

## Depth Maps

:::: {.columns}
::: {.column width="50%"}

![](./images/real-sense.jpg)

:::
::: {.column width="50%"}
- Historically, required custom hardware
  - Depth Camera (e.g., RealSense) - ~$300-500
  - Module/processing for realtime (60fps) sensing
:::
::::

## Depth Maps

- Image-to-Image depth estimation models
  - Depth Anything, MiDaS, ZoeDepth
  - Low latency (MiDaS 3.1 @ 20fps on embedded GPU)
- Used for
  - 3D effects/estimation
  - Simple/low-cost robotics
  - Control input for other images

## Depth Maps

{{< embed notebooks/generate-depth-map.ipynb#input-image echo=false >}}

## Depth Maps

{{< embed notebooks/generate-depth-map.ipynb#run echo=true >}}

## Depth Maps

{{< embed notebooks/generate-depth-map.ipynb#result echo=false >}}

## Depth Maps

- Why do this?
  - Depth map can be used as control image for new image

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#control-image echo=false >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#run echo=true >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#result echo=false >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#run-2 echo=true >}}

## Depth Map as Control

{{< embed notebooks/depth-map-as-context.ipynb#result-2 echo=false >}}

## Inpainting

- Filling in missing or masked regions of an image in a realistic way
- Model is given an image with certain areas masked out
- Generates plausible content to fill those areas based on the surrounding context (and steered by a text prompt)

## Inpainting

- Can be challenging
  - Model needs to understand context around the area
  - Generate content that matches the style, lighting, and perspective
  - Follow a text prompt that was likely different from the original

## Inpainting

{{< embed notebooks/inpainting.ipynb#gradio echo=true outputs=false >}}

# Demo

Inpainting using Gradio and Replicate: inpainting.ipynb

## Outpainting

- The opposite of inpainting, kind of :)
- How does it work?
  - Supply a prompt: "2x zoom out this image"
  - Treat the new empty regions around the image as masked areas
  - Use impainting technique to fill in the regions

## Outpainting

- More challenging
  - Less context at the edges of the image vs. center/surrounded
  - Need to maintain the style, lighting, and perspective
  - Has to be creative. Can't just be a repetitive pattern.

## Outpainting

{{< embed notebooks/outpainting.ipynb#input echo=false >}}

## Outpainting

{{< embed notebooks/outpainting.ipynb#run echo=true outputs=false >}}

## Outpainting

{{< embed notebooks/outpainting.ipynb#input echo=false >}}

## Outpainting

{{< embed notebooks/outpainting.ipynb#result echo=false >}}

## Inpainting and Outpainting

- Popular models
  - Stable Diffusion (Many inpainting variants)
  - Flux Fill from Black Forest Labs
  - LaMa: Large Mask inpainting
  - Ideogram

# Hands on

Register for a Replicate account; create an API key

Text to Image using text-to-image-replicate.ipynb
Inpainting and Outpainting using inpainting/outpainting.ipynb

https://replicate.com/collections/try-for-free

# I Want More Control!

## I Want More Control!

- Prompt engineering for text-to-image and image-to-image is important
  - Bad prompt: `a cat`
  - Good prompt: `A fluffy orange tabby cat sitting on a wooden windowsill, golden hour lighting, soft focus background of a garden, photorealistic, highly detailed fur texture, warm color palette, shot with 85mm lens, shallow depth of field`

## I Want More Control!

- Key components to include in prompts:
  - **Subject**: What you want
  - **Style/Medium**: photorealistic, oil painting, digital art
  - **Lighting**: studio lighting, dramatic shadows, soft diffused
  - **Composition**: close-up, wide-angle, rule of thirds
  - **Quality**: highly detailed, 4K, sharp focus
  - **Technical specs**: 85mm lens, f/1.8, bokeh

## I Want More Control!

- Negative prompts (popular in some models)
  - Tell the model what to avoid - particularly usedful with Stable Diffusion
  - `blurry, low quality, distorted, deformed, ugly, bad anatomy, extra limbs, watermark, text, signature, overexposed, underexposed, cartoon`

## I Want Even More Control!

- Extensive prompts (both positive and negative) can help, but only so far
  - Ultimately, "hoping the model guesses what I mean"
  - Fine-tuning possible, but it's expensive and risks degrading quality (and potential overfitting)
  - Need a different approach...

# Introducing ControlNet

## Introducing ControlNet

- Developed by Lvmin Zhang and Maneesh Agrawala at Stanford University
- Published in February 2023 [@zhang2023controlnet]
- ControlNet represented a paradigm shift from "describe what you want" to "show the structure you want".

## How ControlNet Works

- Stable Diffusion's U-Net has an encoder and decoder
- Create a trainable copy of the encoder blocks
- Train the copy of the encoder alongside the frozen SD model
  - During training: use paired data (e.g., pose skeleton â†’ original image)
  - During inference: both encoders run together
  - Features from both are combined via zero convolutions
- Key: The weights in the original SD model don't change
- ControlNet is analogous to a "Plug in" model

## How ControlNet Works

Examples of conditioning types:

- Depth maps: 3D structure information
- Human pose: skeleton/keypoint detection
- Canny edges: line drawings and edge detection
- Scribbles: rough user drawings
- QR codes: blended into images

## How ControlNet Works

{{< embed notebooks/controlnet-openpose-sd-1.5.ipynb#original-image echo=false >}}

## How ControlNet Works

{{< embed notebooks/controlnet-openpose-sd-1.5.ipynb#openpose echo=true outputs=false >}}

## How ControlNet Works

{{< embed notebooks/controlnet-openpose-sd-1.5.ipynb#openpose echo=false >}}

## How ControlNet Works

{{< embed notebooks/controlnet-openpose-sd-1.5.ipynb#load-controlnet echo=true outputs=false >}}

## How ControlNet Works

{{< embed notebooks/controlnet-openpose-sd-1.5.ipynb#load-sd echo=true outputs=false >}}

## How ControlNet Works

{{< embed notebooks/controlnet-openpose-sd-1.5.ipynb#new-image echo=true outputs=false >}}

## How ControlNet Works

{{< embed notebooks/controlnet-openpose-sd-1.5.ipynb#new-image echo=false >}}

# Hands on

ControlNet (OpenPose) using controlnet-openpose-sd-1.5.ipynb

# Demo: Putting this all together

Using text-to-image, ControlNet, inpainting, and image-to-image (depth map) to create PBR materials

pbr-creator.ipynb

# Using Transformers for Computer Vision

## CNNs to Vision Transformer

Historically, computer vision has used classification models called CNNs (Convolutional Neural Networks)

- Enter the Vision Transformer (ViT)
  - "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" [@dosovitskiy2021vit]
- Tipping Point
  - Initially, ViTs didn't outperform CNNs
  - But exceeded SOTA CNNs on larger datasets (such as Google's JFT-300M)

## Popular Vision Transformers

- OpenAI's CLIP
  - Trained on 400M image-text pairs
  - Foundation of most VLMs
- Meta's DINO/DINO-2
  - (Self DIstillation with NO Labels)
  - Self-supervised on 142M images
- Microsoft's Swin
  - Use "shifted windows" approach
  - Excels at dense prediction tasks

## Vision Language Models (VLMs)

- ViTs by themselves are only so useful
- Introducing VLMs (Vision Language Models)
  - A vision encoder
  - Adapter/projector layer
  - Language model (LLaMa or GPT)
- Also known as "Multimodal"
  - Image-Text-to-Text

## Demo

Using Gemma 3 (4B) to describe an image

Notebook: vlm-gemma-3-4b.ipynb

## Sidebar: Image URL Dereferencing

{{< embed notebooks/vlm-gemma-3-4b.ipynb#image echo=true outputs=false >}}

## Sidebar: Image URL Dereferencing

- Handled within the pipeline library
  - Detects the image URL in the message structure
  - Downloads the image
  - Loads as a PIL Image object
  - Preprocessing (resizing, normalizing pixel values)
  - Converts to tensors

## Sidebar: Image URL Dereferencing

- Text gets tokenized into token IDs
- Image gets processed into pixel tensors
- Both feed into their respective encoders (text encoder, vision encoder)
- Features are aligned via shared embedding space
  - e.g., visual concepts (furry, four legs, whiskers, etc.) are close to the word "cat" in shared embedding space

## Use Cases for VLMs

- Visual Understanding: "What's in this image?"
- Accessibility: Assisting users with visual impairments
- Content Moderation and Safety: Identifying harmful content
- Retail: Finding products with photos
- Education: Helping students understand charts, diagrams, equations
- Robotics: Providing Robots with information to navigate their environment

## Popular VLMs

- Closed Source
  - GPT4-V, Claude, Gemini Flash
- Open Source
  - LLaVa: Research collaboration between University of Wisconsin-Maddison and MSR
  - Gemma: Google's Gemma-3
  - FastVLM: Apple's Fast Vision Language Model

## FastVLM

- Release release from Apple (presented at CVPR 2025)
  - Paper: FastVLM: Efficient Vision Encoding for Vision Language Models [@vasu2025fastvlm]
  - [https://huggingface.co/apple/FastVLM-0.5B](https://huggingface.co/apple/FastVLM-0.5B){.external target="_blank"}
  - Small VLM, optimized for on-device, real-time performance
  - Custom vision transformer: FastViTHD. Combines transformers and convolutional layers

# Demo

Apple's FastVLM

[Apple FastVLM running in WebGPU](https://huggingface.co/spaces/apple/fastvlm-webgpu){.external target="blank"}

# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/04/assignment.html){.external target="_blank"}
- Deep dive on model architectures
- Running models on local hardware
- Quantization

# References

## References
