---
title: "Week 1: Foundations of Generative AI"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid-format: png
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Lesson Objectives

- How to create vector embeddings and test for similarity
- Understand the transformer architecture and how it works at a high level
- Cover a brief history of early generative models
- Setup and use Colab Pro for experimenting with vector embeddings and downloading/testing a GPT-2 model.
- Start becoming familiar with the basics of notebooks and Python (if you haven’t used it already)

# Introducing Vector Embeddings

## What are Vector Embeddings?

- Vector Embeddings are **meaningful numerical representations** of words
  - Representations where strings of words (i.e., sentences) are encoded into multi-dimensional space
  - Large number of dimensions (we'll use 384 in our examples)
  - Similar sentences have similar numbers

## Example

{{< embed embeddings.ipynb#sentence-1 echo=true >}}

## Example

{{< embed embeddings.ipynb#sentence-2 echo=true >}}

## Example

{{< embed embeddings.ipynb#sentence-3 echo=true >}}

## Are They Similar?

- We can test with **cosine similarity**
- Measures the angle between two vectors: 
  - Same direction = very similar (similarity close to 1)
  - Opposite direction = very different (similarity of -1)
- Cosine similarity focuses on the angle of the vector vs. length
  - Useful for comparing texts of different sizes

## Are They Similar?

{{< embed embeddings.ipynb#similarity echo=true >}}

## Overview of Embedding Models

- TBD

## What Embeddings Are Used For

- TBD

# Introducing the Transformer

## Introducing the Transformer

- Let's rewind to June 2017
- Google researchers publish "Attention is all you need" [@vaswani2017attention]
  - Introduced a novel neural network architecture, eliminating the need for RNNs for sequence-to-sequence models
  - Attention mechanism allows the model to weigh the importance of words in a sequence
  - Achieved SOTA (State Of The Art) performance on language translation, while also being faster to train

## Introducing the Transformer

```{mermaid .lightbox}
%%| fig-height: 100%
%%| label: transformer-1
%%| file: diagrams/transformer-1.mmd
```

## Introducing the Transformer

```{mermaid}
%%| fig-height: 100%
%%| label: transformer-2
%%| file: diagrams/transformer-2.mmd
```

## Introducing the Transformer

```{mermaid}
%%| fig-height: 100%
%%| label: transformer-3
%%| file: diagrams/transformer-3.mmd
```

## Introducing the Transformer

```{mermaid}
%%| fig-height: 100%
%%| label: transformer-4
%%| file: diagrams/transformer-4.mmd
```

## Lead into GPT

- TBD

## Then show the history

- TBD

# Let's Get Into Some Code!

# What is a Notebook?

## What is a Notebook?

- An **interactive document** that combines:
  - Live code that can be executed
  - Rich text explanations (markdown)
  - Visualizations and outputs
- Think of it as a **computational narrative**
  - Tell a story with code, data, and explanations
- Originally designed for data science and research
- Also used for learning, experimenting, and sharing results

## A Brief History of Notebooks

- **2011**: IPython Notebook project begins
  - Interactive Python shell → web-based notebook
- **2014**: Renamed to **Jupyter** (Julia, Python, R)
  - Now supports 40+ programming languages
- **2017**: Google launches **Colab**
  - Free cloud-based Jupyter notebooks
  - Free access to GPUs and TPUs
- **Today**: Industry standard for ML/AI development

## Anatomy of a Notebook

- **Format**: Extension is .ipynb
  - JSON format, using Jupyter Document Schema
- **Cells**: Building blocks of notebooks
  - **Code cells**: Executable Python code
  - **Markdown cells**: Text, headings, images, equations
- **Kernel**: The computational engine running your code
  - Maintains state between cell executions
- **Outputs**: Results appear directly below code cells
  - Text, tables, plots, interactive widgets

## How to Run Notebooks

- **Jupyter Notebook Server** (Classic approach)
   - Web interface on localhost
- **VS Code** (Local development)
   - Jupyter extension for VS Code
   - Run on your own machine
- **Google Colab** (Recommended)
   - Browser-based, no installation needed
   - Free GPU access

## Advantages of Google Colab

- Access to GPUs and TPUs for AI-based tasks
  - e.g., A100 and H100 with 40Gb/80Gb VRAM
- Model downloaded between cloud vendors
  - vs. downloading large models via the DigiPen network
- Many libraries pre-installed
- Easy to share notebooks with others
- Generous (free) GPU limits for students!

# Demo

Hello World notebook in **Colab**, **VS Code**, and **Local Jupyter server**

## References
