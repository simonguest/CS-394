<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.26">

  <title>CS-394 – Module 5: Running Models on Local Hardware</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #24292e;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #24292e; } /* Normal */
    code span.al { color: #ff5555; font-weight: bold; } /* Alert */
    code span.an { color: #6a737d; } /* Annotation */
    code span.at { color: #d73a49; } /* Attribute */
    code span.bn { color: #005cc5; } /* BaseN */
    code span.bu { color: #d73a49; } /* BuiltIn */
    code span.cf { color: #d73a49; } /* ControlFlow */
    code span.ch { color: #032f62; } /* Char */
    code span.cn { color: #005cc5; } /* Constant */
    code span.co { color: #6a737d; } /* Comment */
    code span.cv { color: #6a737d; } /* CommentVar */
    code span.do { color: #6a737d; } /* Documentation */
    code span.dt { color: #d73a49; } /* DataType */
    code span.dv { color: #005cc5; } /* DecVal */
    code span.er { color: #ff5555; text-decoration: underline; } /* Error */
    code span.ex { color: #d73a49; font-weight: bold; } /* Extension */
    code span.fl { color: #005cc5; } /* Float */
    code span.fu { color: #6f42c1; } /* Function */
    code span.im { color: #032f62; } /* Import */
    code span.in { color: #6a737d; } /* Information */
    code span.kw { color: #d73a49; } /* Keyword */
    code span.op { color: #24292e; } /* Operator */
    code span.ot { color: #6f42c1; } /* Other */
    code span.pp { color: #d73a49; } /* Preprocessor */
    code span.re { color: #6a737d; } /* RegionMarker */
    code span.sc { color: #005cc5; } /* SpecialChar */
    code span.ss { color: #032f62; } /* SpecialString */
    code span.st { color: #032f62; } /* String */
    code span.va { color: #e36209; } /* Variable */
    code span.vs { color: #032f62; } /* VerbatimString */
    code span.wa { color: #ff5555; } /* Warning */
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-eeafb82a00776dbd0312b01cd21cfa25.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Module 5: Running Models on Local Hardware</h1>

<div class="quarto-title-authors">
</div>

</section>
<section id="recap" class="slide level2">
<h2>Recap</h2>
<ul>
<li class="fragment">Understood the fundamentals and history of diffuser models</li>
<li class="fragment">Explored and used models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet</li>
<li class="fragment">Setup and used Replicate to create a custom pipeline of production-grade models</li>
<li class="fragment">Understood the fundamentals and history of Vision Encoders and VLMs</li>
<li class="fragment">Implemented/tested a local VLM model for on-device inference</li>
</ul>
</section>
<section id="lesson-objectives" class="slide level2">
<h2>Lesson Objectives</h2>
<ul>
<li class="fragment">Understand the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile</li>
<li class="fragment">Understand hardware requirements and architectures for model inference - e.g., CUDA vs.&nbsp;ONNX vs.&nbsp;MLX vs.&nbsp;WebGPU</li>
<li class="fragment">Explore how quantization works and understand techniques and formats for quantizing existing models</li>
<li class="fragment">Use llama.cpp to quantize and run an SLM on local hardware/gaming PC</li>
<li class="fragment">Integrate a quantized model within Unity/Unreal/WebAssembly</li>
</ul>
</section>
<section>
<section id="why-local-models" class="title-slide slide level1 center">
<h1>Why Local Models?</h1>

</section>
<section id="why-local-models-1" class="slide level2">
<h2>Why Local Models?</h2>
<ul>
<li class="fragment"><strong>Privacy</strong>
<ul>
<li class="fragment">Every call you make to OpenAI/Claude/OpenRouter may (or may not) get logged and/or be used for training purposes</li>
<li class="fragment">Many organizations don’t want their customer/financial data logged with an AI vendor</li>
<li class="fragment">There may also be legal regulations/restrictions controlling this</li>
</ul></li>
</ul>
</section>
<section id="why-local-models-2" class="slide level2">
<h2>Why Local Models?</h2>
<ul>
<li class="fragment"><strong>Offline</strong>
<ul>
<li class="fragment">Every call you make to OpenAI/Claude/OpenRouter needs an Internet connection</li>
<li class="fragment">That’s not always guaranteed!</li>
<li class="fragment">Education is a good example - remote school in India and/or rural districts here in the US</li>
</ul></li>
</ul>
</section>
<section id="why-local-models-3" class="slide level2">
<h2>Why Local Models?</h2>
<ul>
<li class="fragment"><strong>Latency</strong>
<ul>
<li class="fragment">Even with a network connection, calls can suffer from increased latency</li>
<li class="fragment">Can be a challenge if your application needs frequent, quick responses</li>
<li class="fragment">e.g., using a VLM to determine the contents of a video stream for a user with vision impairments</li>
</ul></li>
</ul>
</section>
<section id="why-local-models-4" class="slide level2">
<h2>Why Local Models?</h2>
<ul>
<li class="fragment"><strong>Cost</strong>
<ul>
<li class="fragment">While per-API costs are fractions of a cent, these can grow out of control with exponential growth</li>
<li class="fragment">More pronounced for long conversation threads (think call center)</li>
<li class="fragment">Or agents with verbose tool call JSON requests/responses</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="whats-your-hardware" class="title-slide slide level1 center">
<h1>What’s Your Hardware?</h1>

</section>
<section id="whats-your-hardware-1" class="slide level2">
<h2>What’s Your Hardware?</h2>
<ul>
<li class="fragment">NVIDIA (CUDA)</li>
<li class="fragment">AMD (ROCm)</li>
<li class="fragment">Apple Silicon</li>
<li class="fragment">Various NPU (Neural Processing Unit) vendors</li>
</ul>
</section>
<section id="nvidia-cuda" class="slide level2">
<h2>NVIDIA CUDA</h2>
<ul>
<li class="fragment">CUDA (Compute Unified Device Architecture)
<ul>
<li class="fragment">Launched in 2006 to introduce programming on GPUs (GPGPUs or General Purpose GPUs)</li>
<li class="fragment">A C-like programming interface</li>
<li class="fragment">Perfectly timed for the deep learning revolution of the 2010s</li>
<li class="fragment">Additional libraries (e.g., cuBLAS, cuDNN) make CUDA the defacto standard today</li>
</ul></li>
</ul>
</section>
<section id="how-cuda-works" class="slide level2">
<h2>How CUDA Works</h2>
<ul>
<li class="fragment">Massive parallelism: CUDA exploits thousands of GPU cores simultaneously, making it ideal for matrix operations.</li>
<li class="fragment">Memory hierarchy: A tiered memory system with global, shared, and registers.</li>
<li class="fragment">Kernel execution: Programs can launch kernels - same function that can operate on different data.</li>
</ul>
</section>
<section id="nvidia-cuda---hardware-support" class="slide level2">
<h2>NVIDIA CUDA - Hardware Support</h2>
<ul>
<li class="fragment">Consumer: RTX 40- and 50- cards with various VRAM options (8Gb - 24Gb) for local inference and small fine-tuning tasks. RTX 30- series still popular for education.</li>
<li class="fragment">Laptop: RTX 40- and 50- series also available on laptops (although less performant than discrete cards)</li>
<li class="fragment">Workstation: DGX Spark launch in 2025, with GB10 and 128Gb of unified memory for medium fine-tuning tasks</li>
<li class="fragment">Datacenter GPUs: A/H series and GB-series for datacenters. NVLink for multi-GPU interconnectivity.</li>
</ul>
</section>
<section id="sidebar-tops" class="slide level2">
<h2>Sidebar: TOPS</h2>
<ul>
<li class="fragment">TOPS (Terra Operations Per Second)
<ul>
<li class="fragment">How many trillion operations a processor can perform per second</li>
<li class="fragment">Often qualified with the data type</li>
<li class="fragment">64 INT8 TOPS == 64 trillion 8-bit operations per second</li>
</ul></li>
<li class="fragment">TFLOPS (Terra Floating-Point Operations Per Second)
<ul>
<li class="fragment">1 TFLOPS == 1 FP32 (32-bit floating point) TOPS</li>
</ul></li>
</ul>
</section>
<section id="sidebar-tops-1" class="slide level2">
<h2>Sidebar: TOPS</h2>
<ul>
<li class="fragment">Rough Throughput Calculations
<ul>
<li class="fragment">You have an NVIDIA 3090 (advertized at 35 TFLOPS)</li>
<li class="fragment">Assume a 7B param model with FP32 weights</li>
<li class="fragment">Each token generation requires ~2 FLOPs per parameter</li>
<li class="fragment">Each token generation ~= 14B FLOPs (7B params × 2)</li>
<li class="fragment">Theoretical max = 35T FLOPs/sec ÷ 14B FLOPs/token ≈ 2,500 tokens/sec</li>
<li class="fragment">Reality: 10-100 tokens/sec typical due to memory bandwidth bottlenecks</li>
</ul></li>
</ul>
</section>
<section id="amd-rocm" class="slide level2">
<h2>AMD ROCm</h2>
<ul>
<li class="fragment">ROCm (Radeon Open Compute)
<ul>
<li class="fragment">Launched in 2016 as an open-source alternative to CUDA</li>
<li class="fragment">Embraced open standards (e.g., OpenCL), positioning as avoiding vendor lock-in, although this fragmentation initially hurt adoption</li>
<li class="fragment">Has evolved significantly since (e.g., rocBLAS) although ecosystem gaps compared to CUDA persist</li>
</ul></li>
</ul>
</section>
<section id="amd-rocm---hardware-support" class="slide level2">
<h2>AMD ROCm - Hardware Support</h2>
<ul>
<li class="fragment">Consumer: RX7000 series offer sustantial VRAM (up to 24Gb) at competitive prices compared to NVIDIA RTX</li>
<li class="fragment">Laptop: Some laptop options for AMD-based machines</li>
<li class="fragment">Workstation: Strix Halo, competitor to DGX Spark, with RX8060S and 128Gb unified memory</li>
<li class="fragment">Software support: Linux only with no Windows support (some via WSL)</li>
</ul>
</section>
<section id="apple-silicon" class="slide level2">
<h2>Apple Silicon</h2>
<ul>
<li class="fragment">Apple Silicon
<ul>
<li class="fragment">Metal, a low-level graphics and compute API, launched in 2014 and later expanded for general GPU compute tasks</li>
<li class="fragment">MPS (Metal Performance Shaders) introduced in 2017 and optimized primitives for neural networks. PyTorch added MPS device support in 2022.</li>
<li class="fragment">MLX released in 2023, providing NumPy-like API for Apple Silicon hardware</li>
</ul></li>
</ul>
</section>
<section id="apple-silicon---hardware-support" class="slide level2">
<h2>Apple Silicon - Hardware Support</h2>
<ul>
<li class="fragment">Available on all M-series hardware</li>
<li class="fragment">Unified memory by default, upto 128Gb on laptops and 512Gb for the Mac Studio with M3 Ultra</li>
<li class="fragment">Non portable models. (MLX uses safetensors/npz format and MLX-specific code for Metal.)</li>
</ul>
</section>
<section id="sidebar-unified-memory" class="slide level2">
<h2>Sidebar: Unified Memory</h2>
<ul>
<li class="fragment">GPUs have historically had separate memory (VRAM)</li>
<li class="fragment">Unified memory is a process to share memory between CPU and GPU
<ul>
<li class="fragment">For Apple/MLX, it’s a true SoC (System on a Chip); NVIDIA DGX Spark, two physical components connected via NVLink-C2C</li>
</ul></li>
<li class="fragment">Higher memory availability, but lower memory bandwidth
<ul>
<li class="fragment">~275Gb/s for Spark/MLX; ~1TB/s for 5080; ~1.7TB/s for 5090; ~3TBs for H100</li>
</ul></li>
</ul>
</section>
<section id="npus" class="slide level2">
<h2>NPUs</h2>
<ul>
<li class="fragment">NPUs (Neural Processing Units) are specialized AI accelerators, designed for lower power consumption</li>
<li class="fragment">Optimized specifically for NN operations (e.g., matmul, convolutions, activations)</li>
<li class="fragment">Commonly found in edge devices (smartphones, IoT, embedded systems)</li>
<li class="fragment">15-80 TOPS common for NPUs (~10x less that desktop PCI-based GPUs)</li>
</ul>
</section>
<section id="npu-vendors" class="slide level2">
<h2>NPU Vendors</h2>
<ul>
<li class="fragment">Intel: Acquired Movidius in 2016; released Myriad X in 2017 and a neural compute stick. Superceded by NPUs in Core Ultra processors.</li>
<li class="fragment">Qualcomm: Snapdragon 865 range in 2019; now Snapdragon X and 8 ranges. Used in Windows/ARM devices. Popular with Android, although Google recently moved to their own TPUs.</li>
<li class="fragment">AMD: XDNA formerly Xilinx; Windows Copilot PC range competing with SnapDragon and Intel Core Ultra.</li>
<li class="fragment">Apple: ANE (Apple Neural Engine) to support CoreML workloads on iPhone, iPad devices</li>
</ul>
</section></section>
<section>
<section id="compute-and-memory-challenges" class="title-slide slide level1 center">
<h1>Compute and Memory Challenges</h1>

</section>
<section id="compute-and-memory-challenges-1" class="slide level2">
<h2>Compute and Memory Challenges</h2>
<ul>
<li class="fragment">Compute Challenges
<ul>
<li class="fragment">Local GPUs significantly slower than datacenter-class GPUs</li>
<li class="fragment">4060 Ti (~350 TOPS) vs.&nbsp;H100 (~3500+ TOPS)</li>
<li class="fragment">One local GPU vs.&nbsp;interconnected datacenter-class GPUs (using NVLink)</li>
</ul></li>
<li class="fragment">Less TOPS = slower inference (tokens/second)</li>
</ul>
</section>
<section id="compute-and-memory-challenges-2" class="slide level2">
<h2>Compute and Memory Challenges</h2>
<ul>
<li class="fragment">Memory Challenges
<ul>
<li class="fragment">Consumer-grade GPUs and NPUs have significanly less memory than datacenter-class GPUs</li>
<li class="fragment">8Gb/16Gb VRAM on 4060 Ti vs.&nbsp;80/94Gb for H100</li>
<li class="fragment">(Also memory bandwidth is typically 10x-20x)</li>
<li class="fragment">(Combined memory architecture using NVLink)</li>
</ul></li>
<li class="fragment">Less memory = only smaller models can run
<ul>
<li class="fragment">Rougly speaking, size of model must be smaller than available VRAM</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="overcoming-compute-challenges" class="title-slide slide level1 center">
<h1>Overcoming Compute Challenges</h1>

</section>
<section id="overcoming-compute-challenges-1" class="slide level2">
<h2>Overcoming Compute Challenges</h2>
<ul>
<li class="fragment">MoE (Mixture of Experts)
<ul>
<li class="fragment">Original concept dates back to 1991. Jacobs et al.&nbsp;publish “Adaptive Mixture of Local Experts” showing subnetworks and a gating mechanism</li>
<li class="fragment">In 2017, Shazeer et al.&nbsp;(Google) publish “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer” demonstrating activating only a small subset of experts per input</li>
<li class="fragment">2022: Google releases Switch Transformers (Fedus et al.), simplifying MoE by routing each token to just a single expert</li>
<li class="fragment">2023: Mixtral 8x7B (Mistral AI) brings high-quality open-source MoE to the mainstream, becoming a standard architecture for efficient large-scale models</li>
</ul></li>
</ul>
</section>
<section id="popular-moe-models" class="slide level2">
<h2>Popular MoE Models</h2>
<ul>
<li class="fragment">Mixtral (Mistral)</li>
<li class="fragment">Qwen MoE (Alibaba)</li>
<li class="fragment">DeepSeek MoE</li>
<li class="fragment">Phi MoE (Microsoft’s SLMs)</li>
<li class="fragment">Nemotron (NVIDIA)</li>
</ul>
</section>
<section id="overcoming-compute-challenges-2" class="slide level2">
<h2>Overcoming Compute Challenges</h2>
<ul>
<li class="fragment">Why MoE?
<ul>
<li class="fragment">Faster Inference (token/second): Because you are only using a subset of experts for each token.</li>
<li class="fragment">Allows you to use larger models, where you would be otherwise compute-bound
<ul>
<li class="fragment">Dense 13B model ~= 13Gb VRAM</li>
<li class="fragment">Mixtral 8x7B ~= 47GB VRAM (8×7B experts + shared layers), but faster inference than dense 47B (only ~2 experts active per token)</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="how-do-moe-models-work" class="slide level2">
<h2>How do MoE Models Work?</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/moe-heatmap.ipynb" data-notebook-title="Routing Heatmap for Small MoE Model" data-notebook-cellid="load-model">
<div id="load-model" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:540,&quot;referenced_widgets&quot;:[&quot;52581879eb7b4382b8c14efdbe5b2ec0&quot;,&quot;d679de2b167d464bac398623ebe638b1&quot;,&quot;c76da39949224260bee780ad76cdd329&quot;,&quot;8074228464dc43c98a5b8a40ed31d3d9&quot;,&quot;6e7558f19cf4483597453b08ec5db385&quot;,&quot;e87deac2da7b4ef8ab661abd62c2b1e1&quot;,&quot;dff8560843f34c9abefa435d03384f01&quot;,&quot;fa5e94d54c7d4fa9a360e8dfd7cff621&quot;,&quot;e8029cdf39154820934671f62b15069e&quot;,&quot;707a1d5a8df544abaee79edcef843e2e&quot;,&quot;bbae47b658ae49b3a85cf5513b69c7fa&quot;,&quot;0ec9c12663b44fdb8a8e67989befe062&quot;,&quot;ad595d9de09d4542acf41506e47c7b83&quot;,&quot;aae62ea7e70f4bc6b3002176a3f4e289&quot;,&quot;a53149e53b67476b931c937f4261d9c1&quot;,&quot;3a2be454021740aa89230c358fc71f47&quot;,&quot;424cf682c33b4618859f6d1d21355f93&quot;,&quot;88aee749b0414055b10c5e135a08414d&quot;,&quot;0a82f43f5b344dada8f3a5a768544b42&quot;,&quot;b196e8aec950451dbc0f46776fa22bf8&quot;,&quot;1928779088a54bee84100ec187684865&quot;,&quot;026a4317590d4dd1aacd6c30add19f78&quot;,&quot;087a8dc7e0e04cf48da262cd257a12c2&quot;,&quot;40f2c9e54eda4ad083bb35c69d08c26c&quot;,&quot;b4a3bf7bfea04e54b57bafdbdea7b1a7&quot;,&quot;96bb1218c4ce438da92558d38e8b4c15&quot;,&quot;8315beea701c49188be024b035eb5b6f&quot;,&quot;6751fb43a31e426d835dbc6ef4be6861&quot;,&quot;b01e2b04e65e411882c45117e85ff235&quot;,&quot;a4d0ff9cad8846c5a13a5b2e50b978f6&quot;,&quot;12d359896acc41028540a244538a3773&quot;,&quot;6b8cdd45f5904e9587300978a774ee2e&quot;,&quot;9465ec32acb642bdba8d5efc47114463&quot;,&quot;2848f63b2ac1440ab84dde43f1b75583&quot;,&quot;e5e1864611f34917b18a286712aa58a0&quot;,&quot;996ad9a47a364e66b030ab1b4b8b2555&quot;,&quot;dbbd033a0ccb4b8ab3cbe7ab21b708da&quot;,&quot;343be4ec82f24e2aa4bc55a7bdb5a503&quot;,&quot;f8eafb7b833e4926821ffd52b863d51c&quot;,&quot;fd47a4dd3a7e498bb53b7389bd0bb75b&quot;,&quot;38626ed766aa48a1bdb3bbcd4766a3ce&quot;,&quot;33d73591890f435d9fadf644d4fadf3e&quot;,&quot;fb648ce3f655477382700f2b07a18f67&quot;,&quot;2a0f3c728b0c4c5eb359034069404bc7&quot;,&quot;285cd10a80bf43bdb70497131ccc4919&quot;,&quot;36be9990b9374475acc2cddeceacc76b&quot;,&quot;5db023acb6534e1fae292f1333b8caba&quot;,&quot;e8ac7ec77b25472281b7b5446530c481&quot;,&quot;270b75d266674399b42644c54f879b98&quot;,&quot;38b25e3ec7de40f9b9029e3b865647e8&quot;,&quot;2b3f5a9b350d49ee9e21c3b391aafd4d&quot;,&quot;f48be9097f31447bba0d46135de9ffc5&quot;,&quot;7d2829bcfd2f4d4fb148969b4b0f42b9&quot;,&quot;1edb92d21a5f48a1958068d9975d1af2&quot;,&quot;921380dca30a4ae9a82c76955866d164&quot;,&quot;2d9ecd2ffc484dae8a8fdaa323ecc2fc&quot;,&quot;866fa7582e4740afbf80074938ab74ec&quot;,&quot;5190ea15c6734f519025ad8b63071ea0&quot;,&quot;c0a854c699354eacb6b2ae25b07698d5&quot;,&quot;9ba166652e5d474aad0431855f2a5f14&quot;,&quot;8ddc47ae164d414aa6b10ef2552723c1&quot;,&quot;9ee5a96dec084d7e9b660a236b7c93d4&quot;,&quot;b382de9c91f04ce494f3109c1ae50e0f&quot;,&quot;09ada04e114842998643106bbf536b4a&quot;,&quot;90aa9cbabdd9478ea3c05cb7d83e30fc&quot;,&quot;d1ff58255c9b4d37b47407599336d5f0&quot;,&quot;3c032d8a3a694ca6a810b4b6d1be984f&quot;,&quot;ea7cbd3a1ee343c399ba7865a252d262&quot;,&quot;c7f8384643e44293bfba76499f7b2928&quot;,&quot;5b47058afdc84caa943f7643c0248539&quot;,&quot;3d7fad42830141b9a5548c93e301cf9f&quot;,&quot;54ec6b1f034546f7b813f2595338678d&quot;,&quot;e51e816ced254d67b7e73f427a81bf0a&quot;,&quot;5ab3785ec9f741c0b907b4691c78b318&quot;,&quot;43f577ded6a34f55b922ec695b803120&quot;,&quot;7478bf3ff768493eb2f8c709c4972ae0&quot;,&quot;4f336f337a274b51b9198179826bcdc8&quot;,&quot;f25f6c1f8d794ad08186e5711063d634&quot;,&quot;7518ac82080e4aaf8b87a19ad4951939&quot;,&quot;7aa88f7a36c743c69789f6ea3a8f7623&quot;,&quot;a39f64f9fc9e492b88ca5e8b9c410c1f&quot;,&quot;4db62cf2648140489364664bbc59d4c2&quot;,&quot;8cc66db1c079460e895e728b5810c351&quot;,&quot;06808f07cc8a40f6b4d3dc23ce9f1599&quot;,&quot;67260cf4d35c42ac943916e17e687d79&quot;,&quot;c1518fb1868846b49256d06e362a1856&quot;,&quot;f92a300231aa4830b1b731081f87fbb6&quot;,&quot;98ef9252ca9743c39341e0813875e474&quot;,&quot;665d5f8ba76c43abaadfb3c0f748a80c&quot;,&quot;244893c23d87427fbf731f76a365213c&quot;,&quot;fcb3f332704f4cc898e37f8de8324f46&quot;,&quot;f20b7a48965a40c3a87508f675c6890a&quot;,&quot;4bdb3f175df6428c9f4eb42371c3d5c0&quot;,&quot;6a6d4ed72ada48a9a0bfd8b6db8216ce&quot;,&quot;66082b3a6e454ecfa4616d80fc8b5e66&quot;,&quot;5d062c22cebf4eec86eae303e2f83767&quot;,&quot;0ca3942d618646f7b830ccf0f79f7779&quot;,&quot;98e0daae4f8240ffba25eddce352adfb&quot;,&quot;7eaacd783bd941c8b75599626be30fc9&quot;,&quot;39e0544fc39b4d7fa9d48ce9be14aebb&quot;,&quot;92bece1efa944186a016bab06cd283e0&quot;,&quot;48d2290cd6fa4044a67cbbacd380d052&quot;,&quot;fe125a77270048e5ba2182143f828044&quot;,&quot;5d681e224164482da70f104e556c021e&quot;,&quot;fa10bdbab2ec4cf0a95e392eb3d77f60&quot;,&quot;1d32cfb0a8af43f98d2174e023280269&quot;,&quot;256c87fe002d4fb58dea05089c70fb57&quot;,&quot;f8893c54cd6a43bc8fdb754e60d36071&quot;,&quot;81455ed1f07245e4b46e3a10fc201533&quot;,&quot;8e0491e2f4234a9abbace036e6169254&quot;,&quot;b8562328710d4173acf57b27562fdeff&quot;,&quot;6dbea3ac541447919eaa511603ed368e&quot;,&quot;2572568afce645eb84d7fa8e8aba733c&quot;,&quot;b387d05735c44b869a01863c33b77f61&quot;,&quot;09ef91241ea7473688a568e7a141ca47&quot;,&quot;1af2ae948e0946d2b1440b67ab26e79c&quot;,&quot;00f7918248e14fe3a7c6001cace0c609&quot;,&quot;22a47854213247e78c3c13da4f989e01&quot;,&quot;efc76cca05da47fba8b26df81024c2bf&quot;,&quot;79db35f755b549d7b8659d67717b8b6f&quot;,&quot;4f0a2205ce194fb8b39c84006afe0a3e&quot;,&quot;e8499bead7fc45d8b4019d0b1fb48be5&quot;,&quot;4581f770419c442b9755a664bdf3bdcb&quot;,&quot;573d11a996ac4130a5d274ef26a51c5a&quot;,&quot;7faf3a6b13264c57bf392490df6f02ee&quot;,&quot;96a85974be1a4c868bda65370c536cbe&quot;,&quot;13f7778e30954988b5ed558f44ea39ca&quot;,&quot;a92c04928c35448db742bbd13cafbc10&quot;,&quot;2017bac6d85c479f8386a27db96d8f51&quot;,&quot;c5ede252294144ad82185ee7b5f2ffb9&quot;,&quot;cef98a9bb9324e50ba60cd3ae6c4d1d9&quot;,&quot;7e7bebf54dfd453b8cddf86b1f93ec71&quot;,&quot;86ac9142885d48e39d4313d2bb8cbd21&quot;,&quot;98acf19e9d8044d9bce4c44683187205&quot;,&quot;670d6ea9bc9b40448edf30f7dbe0f9a3&quot;,&quot;50cb17eab83f4b648ac017107bd28a64&quot;,&quot;c78d55a0c1014c49a43849548858a711&quot;,&quot;b9d7b3f8890f45a79524b8abd02affc1&quot;,&quot;7bca462d35774d7bac409afff08edc7e&quot;,&quot;c30d60f355884de09c5a84b810e365ee&quot;,&quot;da7cc2daf89b4335bf1e69b564e0cbdd&quot;,&quot;3071c866c768441cbfda8fe568a0f910&quot;,&quot;194abb2955e745aa82725c9729d3397e&quot;]}}" data-outputid="3cb7e88f-2117-48d3-eb40-236c682947bf" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb1-2"><a></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a></a></span>
<span id="cb1-4"><a></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb1-5"><a></a>    <span class="st">"microsoft/Phi-tiny-MoE-instruct"</span>,</span>
<span id="cb1-6"><a></a>    device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb1-7"><a></a>    dtype<span class="op">=</span>torch.float16,</span>
<span id="cb1-8"><a></a>    trust_remote_code<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-9"><a></a>    output_router_logits<span class="op">=</span><span class="va">True</span>  <span class="co"># Enable router outputs</span></span>
<span id="cb1-10"><a></a>)</span>
<span id="cb1-11"><a></a></span>
<span id="cb1-12"><a></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(</span>
<span id="cb1-13"><a></a>    <span class="st">"microsoft/Phi-tiny-MoE-instruct"</span>,</span>
<span id="cb1-14"><a></a>    trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb1-15"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="how-do-moe-models-work-1" class="slide level2">
<h2>How do MoE Models Work?</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/moe-heatmap.ipynb" data-notebook-title="Routing Heatmap for Small MoE Model" data-notebook-cellid="register-hooks">
<div id="register-hooks" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="76118d64-31d6-4bda-f4b1-36cfd61ddfc4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a>PROMPT <span class="op">=</span> <span class="st">"What is hello in Japanese?"</span></span>
<span id="cb2-2"><a></a></span>
<span id="cb2-3"><a></a>gate_layers <span class="op">=</span> []</span>
<span id="cb2-4"><a></a><span class="cf">for</span> name, module <span class="kw">in</span> model.named_modules():</span>
<span id="cb2-5"><a></a>    <span class="cf">if</span> <span class="st">'block_sparse_moe.gate'</span> <span class="kw">in</span> name:</span>
<span id="cb2-6"><a></a>        gate_layers.append((name, module))</span>
<span id="cb2-7"><a></a></span>
<span id="cb2-8"><a></a><span class="bu">print</span>(<span class="ss">f"Found </span><span class="sc">{</span><span class="bu">len</span>(gate_layers)<span class="sc">}</span><span class="ss"> gate layers</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb2-9"><a></a></span>
<span id="cb2-10"><a></a>router_outputs <span class="op">=</span> []</span>
<span id="cb2-11"><a></a></span>
<span id="cb2-12"><a></a><span class="kw">def</span> router_hook(module, <span class="bu">input</span>, output):</span>
<span id="cb2-13"><a></a>    router_outputs.append(output.detach().cpu())</span>
<span id="cb2-14"><a></a></span>
<span id="cb2-15"><a></a><span class="co"># Register hooks</span></span>
<span id="cb2-16"><a></a>hooks <span class="op">=</span> []</span>
<span id="cb2-17"><a></a><span class="cf">for</span> name, module <span class="kw">in</span> gate_layers:</span>
<span id="cb2-18"><a></a>    hooks.append(module.register_forward_hook(router_hook))</span>
<span id="cb2-19"><a></a></span>
<span id="cb2-20"><a></a>inputs <span class="op">=</span> tokenizer(PROMPT, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(model.device)</span>
<span id="cb2-21"><a></a></span>
<span id="cb2-22"><a></a><span class="co"># Run forward pass</span></span>
<span id="cb2-23"><a></a>router_outputs.clear()</span>
<span id="cb2-24"><a></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-25"><a></a>    _ <span class="op">=</span> model(<span class="op">**</span>inputs, use_cache<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-26"><a></a></span>
<span id="cb2-27"><a></a><span class="co"># Analyze first layer</span></span>
<span id="cb2-28"><a></a>router_probs <span class="op">=</span> torch.softmax(router_outputs[<span class="dv">0</span>], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb2-29"><a></a><span class="bu">print</span>(<span class="st">"First layer routing:"</span>)</span>
<span id="cb2-30"><a></a><span class="bu">print</span>(<span class="ss">f"  Number of experts: </span><span class="sc">{</span>router_probs<span class="sc">.</span>shape[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-31"><a></a><span class="bu">print</span>(<span class="ss">f"  Shape: </span><span class="sc">{</span>router_probs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-32"><a></a></span>
<span id="cb2-33"><a></a><span class="co"># Show top-k experts for first 5 tokens</span></span>
<span id="cb2-34"><a></a><span class="cf">for</span> tok_idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">min</span>(<span class="dv">5</span>, router_probs.shape[<span class="dv">0</span>])):</span>
<span id="cb2-35"><a></a>    top_k <span class="op">=</span> torch.topk(router_probs[tok_idx], k<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb2-36"><a></a>    <span class="bu">print</span>(<span class="ss">f"  Token </span><span class="sc">{</span>tok_idx<span class="sc">}</span><span class="ss">: top experts </span><span class="sc">{</span>top_k<span class="sc">.</span>indices<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> "</span></span>
<span id="cb2-37"><a></a>          <span class="ss">f"with probs </span><span class="sc">{</span>[<span class="ss">f'</span><span class="sc">{</span>p<span class="sc">:.3f}</span><span class="ss">'</span> <span class="cf">for</span> p <span class="kw">in</span> top_k.values.tolist()]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="how-do-moe-models-work-2" class="slide level2">
<h2>How do MoE Models Work?</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/moe-heatmap.ipynb" data-notebook-title="Routing Heatmap for Small MoE Model" data-notebook-cellid="routing-0">
<div id="routing-0" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:535}}" data-outputid="32aac517-6524-4d1b-d7df-e448f64f443f" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-3"><a></a></span>
<span id="cb3-4"><a></a>PROMPT <span class="op">=</span> <span class="st">"What is hello in Japanese?"</span></span>
<span id="cb3-5"><a></a></span>
<span id="cb3-6"><a></a><span class="kw">def</span> capture_routing(prompt):</span>
<span id="cb3-7"><a></a>    inputs <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(model.device)</span>
<span id="cb3-8"><a></a>    tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(inputs[<span class="st">'input_ids'</span>][<span class="dv">0</span>])</span>
<span id="cb3-9"><a></a></span>
<span id="cb3-10"><a></a>    router_outputs.clear()</span>
<span id="cb3-11"><a></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-12"><a></a>        _ <span class="op">=</span> model(<span class="op">**</span>inputs, use_cache<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-13"><a></a></span>
<span id="cb3-14"><a></a>    <span class="cf">return</span> router_outputs[:<span class="dv">32</span>], tokens</span>
<span id="cb3-15"><a></a></span>
<span id="cb3-16"><a></a><span class="kw">def</span> routing_heatmap(router_logits, tokens, layer_idx<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb3-17"><a></a>    <span class="co"># Get routing probabilities</span></span>
<span id="cb3-18"><a></a>    router_probs <span class="op">=</span> torch.softmax(router_logits[layer_idx], dim<span class="op">=-</span><span class="dv">1</span>).numpy()</span>
<span id="cb3-19"><a></a>    num_tokens, num_experts <span class="op">=</span> router_probs.shape</span>
<span id="cb3-20"><a></a></span>
<span id="cb3-21"><a></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb3-22"><a></a></span>
<span id="cb3-23"><a></a>    <span class="co"># Use seaborn to display routing probabilities</span></span>
<span id="cb3-24"><a></a>    sns.heatmap(</span>
<span id="cb3-25"><a></a>        router_probs.T,</span>
<span id="cb3-26"><a></a>        cmap<span class="op">=</span><span class="st">'YlOrRd'</span>,</span>
<span id="cb3-27"><a></a>        ax<span class="op">=</span>axes,</span>
<span id="cb3-28"><a></a>        cbar_kws<span class="op">=</span>{<span class="st">'label'</span>: <span class="st">'Probability'</span>},</span>
<span id="cb3-29"><a></a>        xticklabels<span class="op">=</span>tokens[:num_tokens],</span>
<span id="cb3-30"><a></a>        yticklabels<span class="op">=</span>[<span class="ss">f'E</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_experts)]</span>
<span id="cb3-31"><a></a>    )</span>
<span id="cb3-32"><a></a>    axes.set_title(<span class="ss">f'Layer </span><span class="sc">{</span>layer_idx<span class="sc">}</span><span class="ss">: Routing Probabilities per Token'</span>)</span>
<span id="cb3-33"><a></a>    axes.set_xlabel(<span class="st">'Token'</span>)</span>
<span id="cb3-34"><a></a>    axes.set_ylabel(<span class="st">'Expert'</span>)</span>
<span id="cb3-35"><a></a>    plt.setp(axes.get_xticklabels(), rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span>
<span id="cb3-36"><a></a></span>
<span id="cb3-37"><a></a>outputs, tokens <span class="op">=</span> capture_routing(PROMPT)</span>
<span id="cb3-38"><a></a></span>
<span id="cb3-39"><a></a><span class="co"># Display heatmap for layer 0</span></span>
<span id="cb3-40"><a></a>routing_heatmap(outputs, tokens, layer_idx<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="how-do-moe-models-work-3" class="slide level2">
<h2>How do MoE Models Work?</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/moe-heatmap.ipynb" data-notebook-title="Routing Heatmap for Small MoE Model" data-notebook-cellid="routing-0">
<div id="routing-0" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:535}}" data-outputid="32aac517-6524-4d1b-d7df-e448f64f443f" data-execution_count="34">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-moe-heatmap-cell-5-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="how-do-moe-models-work-4" class="slide level2">
<h2>How do MoE Models Work?</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/moe-heatmap.ipynb" data-notebook-title="Routing Heatmap for Small MoE Model" data-notebook-cellid="routing-8">
<div id="routing-8" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:535}}" data-outputid="eda6bd7e-c248-4f1e-85e6-5c4c8b8185ed" data-execution_count="35">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-moe-heatmap-cell-6-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="how-do-moe-models-work-5" class="slide level2">
<h2>How do MoE Models Work?</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/moe-heatmap.ipynb" data-notebook-title="Routing Heatmap for Small MoE Model" data-notebook-cellid="routing-16">
<div id="routing-16" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:535}}" data-outputid="22f56c92-ec53-4999-8c0f-5950b49e9371" data-execution_count="36">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-moe-heatmap-cell-7-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="how-do-moe-models-work-6" class="slide level2">
<h2>How do MoE Models Work?</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/moe-heatmap.ipynb" data-notebook-title="Routing Heatmap for Small MoE Model" data-notebook-cellid="routing-24">
<div id="routing-24" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:535}}" data-outputid="3d29edf9-abac-4f92-d818-a8442c05b819" data-execution_count="37">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-moe-heatmap-cell-8-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="how-do-moe-models-work-7" class="slide level2">
<h2>How do MoE Models Work?</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/moe-heatmap.ipynb" data-notebook-title="Routing Heatmap for Small MoE Model" data-notebook-cellid="routing-31">
<div id="routing-31" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:535}}" data-outputid="681c6d3e-3d6e-4924-ff5e-ab689383e4ac" data-execution_count="38">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="slides_files/figure-revealjs/notebooks-moe-heatmap-cell-9-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="how-do-moe-models-work-8" class="slide level2">
<h2>How do MoE Models Work</h2>
<ul>
<li class="fragment">What the Heatmaps Show
<ul>
<li class="fragment">Different tokens activate different experts (token-level routing)</li>
<li class="fragment">Routing changes across layers (layer 0 vs.&nbsp;layer 31)
<ul>
<li class="fragment">Early layers: syntax/grammar</li>
<li class="fragment">Late layers: semantics/reasoning</li>
</ul></li>
<li class="fragment">Not all experts used equally (specialization emerges)</li>
<li class="fragment">Router makes soft decisions (distributes probability across multiple experts)</li>
</ul></li>
</ul>
</section>
<section id="how-do-moes-get-trained" class="slide level2">
<h2>How do MoEs Get Trained?</h2>
<ul>
<li class="fragment">From scratch
<ul>
<li class="fragment">Train the entire model (experts and router)</li>
<li class="fragment">Router networks learn which expert to select</li>
<li class="fragment">Experts learn their specializations</li>
<li class="fragment">Example: Mixtral 8x7B</li>
</ul></li>
<li class="fragment">Start from a trained dense model (called upcycling)
<ul>
<li class="fragment">Replicate each layer into multiple experts; add router networks</li>
<li class="fragment">Typically faster and more stable vs.&nbsp;starting from scratch</li>
<li class="fragment">Example: Qwen1.5-MoE</li>
</ul></li>
</ul>
</section>
<section id="how-do-moes-get-trained-1" class="slide level2">
<h2>How do MoEs Get Trained?</h2>
<ul>
<li class="fragment">Combine multiple models (FrankenMoE / MoErge)
<ul>
<li class="fragment">Newer, community-driven approach</li>
<li class="fragment">Take specialized models (e.g., math, coding, chat) and use their FFN layers as separate experts; add router network</li>
<li class="fragment">Train only the router (experts frozen initially)</li>
<li class="fragment">Cheaper than training from scratch, but often lower quality</li>
<li class="fragment">Example: Beyonder-4x7B</li>
</ul></li>
</ul>
</section>
<section id="how-do-moes-get-trained-2" class="slide level2">
<h2>How do MoEs Get Trained?</h2>
<ul>
<li class="fragment">Key challenge: Load balancing / Expert collapse</li>
<li class="fragment">Healthy MoE:
<ul>
<li class="fragment">Expert 0: 8% of tokens</li>
<li class="fragment">Expert 1: 7% of tokens</li>
<li class="fragment">Expert 2: 9% of tokens</li>
<li class="fragment">(all experts get reasonable usage)</li>
</ul></li>
<li class="fragment">Collapsed MoE:
<ul>
<li class="fragment">Expert 0: 45% of tokens</li>
<li class="fragment">Expert 1: 40% of tokens</li>
<li class="fragment">Expert 2-7: &lt;15% combined (essentially dead)</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="hands-on" class="title-slide slide level1 center">
<h1>Hands-on</h1>
<p>Run the MoE heatmap; investigate other layers</p>
<p>Potentially swap out Phi model with other MoE from HF</p>
</section>
<section id="overcoming-compute-challenges-3" class="slide level2">
<h2>Overcoming Compute Challenges</h2>
<ul>
<li class="fragment">MoE gives us speed with quality
<ul>
<li class="fragment">But we still can’t fit it on consumer hardware</li>
<li class="fragment">Popular MoE models often exceed the amount of available VRAM</li>
<li class="fragment">We need a way to reduce the memory footprint of the model…</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="overcoming-memory-challenges" class="title-slide slide level1 center">
<h1>Overcoming Memory Challenges</h1>

</section>
<section id="introducing-quantization" class="slide level2">
<h2>Introducing Quantization</h2>
<ul>
<li class="fragment">Process of reducing the precision of a model’s weights and activations
<ul>
<li class="fragment">For example, converting 16-bit numbers to 4-bit</li>
</ul></li>
<li class="fragment">Less precision significantly reduces memory needs</li>
<li class="fragment">For accuracy, parameter count matters more than precision
<ul>
<li class="fragment">A 70B parameter model at 4-bit often beats a 13B model at bf16</li>
<li class="fragment">The models knowledge remains largely intact</li>
<li class="fragment">Often the extra precision doesn’t meaningfully improve outputs</li>
</ul></li>
</ul>
</section>
<section id="quantization-formats" class="slide level2">
<h2>Quantization Formats</h2>
<ul>
<li class="fragment">GPTQ (GPT Quantization):
<ul>
<li class="fragment">Introduced in the paper “GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers”</li>
<li class="fragment">First widely adopted method (in 2022) for agressive (4-bit) quantization</li>
<li class="fragment">CUDA only; distributed via HF Transformers</li>
</ul></li>
<li class="fragment">GGML (Georgi Gerganov Machine Learning):
<ul>
<li class="fragment">A C/C++ library (“llama.cpp” released in Mar 2023) designed for CPU inference of models</li>
<li class="fragment">Optimized for CPUs (using SIMD) with a custom binary format</li>
<li class="fragment">Democratized access to LLMs overnight (ability to run Llama 7B on CPU)</li>
</ul></li>
</ul>
</section>
<section id="quantization-formats-1" class="slide level2">
<h2>Quantization Formats</h2>
<ul>
<li class="fragment">GGUF (GPT-Generated Unified Format):
<ul>
<li class="fragment">Replaced GGML in late 2023, adding extensibility</li>
<li class="fragment">Better metadata support (model architecture, tokenizer info, quantization details)</li>
<li class="fragment">Single file architecture</li>
<li class="fragment">Can offload layers to GPU/NPU, if available, increasing performance</li>
<li class="fragment">Multiple quantization schemes (Q4_K_M, Q5_K_S, Q6_K, etc.)</li>
</ul></li>
</ul>
</section>
<section id="sidebar-quantization-schemes" class="slide level2">
<h2>Sidebar: Quantization Schemes</h2>
<ul>
<li class="fragment">What does Q4_K_M mean?
<ul>
<li class="fragment">Number (Q4, Q5, Q6, etc.) is the average bits per weight</li>
<li class="fragment">Letter suffix (K, 0, 1) is the quantization strategy</li>
<li class="fragment">Size suffix (S, M, L) for variants within that method</li>
</ul></li>
</ul>
</section>
<section id="sidebar-quantization-schemes-1" class="slide level2">
<h2>Sidebar: Quantization Schemes</h2>
<ul>
<li class="fragment">Q2_K = Agressive quantization, smallest files, noticable loss</li>
<li class="fragment">Q4_K_M = Best quality/size trade off; most popular</li>
<li class="fragment">Q6_K = Very close to full precision, but much larger files (almost double Q4_K_M)</li>
<li class="fragment">Q8_0 = Essentially lossless compared to FP16</li>
</ul>
</section>
<section id="sidebar-quantization-schemes-2" class="slide level2">
<h2>Sidebar: Quantization Schemes</h2>
<ul>
<li class="fragment">K-Quant strategy
<ul>
<li class="fragment">A mixed quantization strategy; quantize different parts of the model at different bit depths depending on sensitivity</li>
<li class="fragment">e.g., critical weights (e.g., attention layers) get higher precision; less sensitive weights get more aggressive quantization</li>
</ul></li>
<li class="fragment">0 and 1 strategy
<ul>
<li class="fragment">Uniform quantization across all layers (1 strategy adds a zero point)</li>
<li class="fragment">Less optimal than using K strategy (and rarely used)</li>
</ul></li>
</ul>
</section>
<section id="running-llama.cpp" class="slide level2">
<h2>Running llama.cpp</h2>
<ul>
<li class="fragment"><a href="https://github.com/ggml-org/llama.cpp" class="external" target="_blank">https://github.com/ggml-org/llama.cpp</a></li>
<li class="fragment">C/C++ library; download (brew, nix, winget) or compile from source</li>
<li class="fragment">Initially just a CLI, but now ships with Web UI and OpenAI API compatible server</li>
<li class="fragment">Can reference a locally downloaded .gguf file or pull one from HF</li>
</ul>
</section></section>
<section>
<section id="demo" class="title-slide slide level1 center">
<h1>Demo</h1>
<p>llama-cli -m my_model.gguf</p>
<p>llama-cli -hf ggml-org/gemma-3-1b-it-GGUF</p>
<p>llama-server -hf ggml-org/gemma-3-1b-it-GGUF</p>
</section>
<section id="llama.cpp-wrappers" class="slide level2">
<h2>llama.cpp Wrappers</h2>
<ul>
<li class="fragment">Ollama (https://ollama.com)
<ul>
<li class="fragment">Simple CLI wrapper around llama.cpp</li>
<li class="fragment">Uses a container-like Modelfile to customize system prompts, parameters, etc.</li>
<li class="fragment">Curated collection of models on ollama.com/library</li>
</ul></li>
<li class="fragment">LM Studio
<ul>
<li class="fragment">Desktop GUI wrapper around llama.cpp</li>
<li class="fragment">Supports browsing/downloading of models from HF - i.e., “iTunes for LLMs”</li>
<li class="fragment">In-built chat interface and API server</li>
</ul></li>
</ul>
</section></section>
<section id="demo-1" class="title-slide slide level1 center">
<h1>Demo</h1>
<p>Browsing, downloading, and running a quantized model on LM Studio</p>
</section>

<section id="hands-on-1" class="title-slide slide level1 center">
<h1>Hands-on</h1>
<p>Install LM Studio, pick, and download your own model</p>
<p>(Alterrnatively use llama.cpp or Ollama from the CLI)</p>
</section>

<section>
<section id="quantization-in-other-frameworks" class="title-slide slide level1 center">
<h1>Quantization in Other Frameworks</h1>

</section>
<section id="mlx-apples-ml-framework" class="slide level2">
<h2>MLX (Apple’s ML Framework)</h2>
<ul>
<li class="fragment">ML framework created by Apple (released Dec 2023)</li>
<li class="fragment">Designed specifically for Apple Silicon (M-series processors)</li>
<li class="fragment">Python API similar to NumPy/PyTorch, but optimized for Metal (Apple’s GPU API)</li>
<li class="fragment">Can run on non-Apple hardware - CPU and CUDA PyPi packages</li>
</ul>
</section>
<section id="mlx-apples-ml-framework-1" class="slide level2">
<h2>MLX (Apple’s ML Framework)</h2>
<ul>
<li class="fragment">Supports various levels of quantization (4bit and 8bit)</li>
<li class="fragment">.npz or .safetensor format (with MLX metadata)</li>
<li class="fragment">Better performance on Mac (compared to GGUF)</li>
<li class="fragment">mlx-community on HF hosts MLX-quantized versions of popular models (separate repos)
<ul>
<li class="fragment"><a href="https://huggingface.co/models?library=mlx" class="external" target="_blank">https://huggingface.co/models?library=mlx</a></li>
</ul></li>
</ul>
</section>
<section id="onnx-open-neural-network-exchange" class="slide level2">
<h2>ONNX (Open Neural Network eXchange)</h2>
<ul>
<li class="fragment">Model interchange format, created by Microsoft and Facebook in 2017 to enable model portability</li>
<li class="fragment">Models can be converted: PyTorch -&gt; ONNX; TensorFlow -&gt; ONNX; etc.</li>
<li class="fragment">Common export format, used in many CNNs and RNNs</li>
<li class="fragment">Popular for models running on mobile devices and in-browser (e.g., Transformers.js)</li>
</ul>
</section>
<section id="onnx-open-neural-network-exchange-1" class="slide level2">
<h2>ONNX (Open Neural Network eXchange)</h2>
<ul>
<li class="fragment">Supports dynamic quantization (typically int8)</li>
<li class="fragment">.onnx file format (uses protobuf format) includes graph structure, weights, and metadata</li>
<li class="fragment">Broad portability, but at the expense of performance</li>
<li class="fragment">ONNX models tend to live in separate folders on HF
<ul>
<li class="fragment"><a href="https://huggingface.co/models?library=onnx" class="external" target="_blank">https://huggingface.co/models?library=onnx</a></li>
</ul></li>
</ul>
</section></section>
<section>
<section id="how-to-quantize-a-model" class="title-slide slide level1 center">
<h1>How to Quantize a Model</h1>

</section>
<section id="how-to-quantize-a-model-1" class="slide level2">
<h2>How to Quantize a Model</h2>
<ul>
<li class="fragment">Popular models have already been quantized
<ul>
<li class="fragment">Many available on HF, contributed by <a href="https://huggingface.co/unsloth" class="external" target="_blank">Unsloth AI</a></li>
</ul></li>
<li class="fragment">If the model isn’t already quantized (or you’ve fine tuned your own), it’s easy to do</li>
</ul>
</section>
<section id="how-to-quantize-a-model-2" class="slide level2">
<h2>How to Quantize a Model</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/quantization.ipynb" data-notebook-title="Quantization of Hugging Face model using llama.cpp" data-notebook-cellid="build-llama">
<div id="build-llama" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="14eb357d-4ef0-49c3-bf2f-bfbb4a3371a6" data-execution_count="1">
<div class="cell-output cell-output-stdout">
<div class="ansi-escaped-output">
<pre>Cloning into 'llama.cpp'...

remote: Enumerating objects: 77965, done.

remote: Counting objects: 100% (228/228), done.

remote: Compressing objects: 100% (156/156), done.

remote: Total 77965 (delta 141), reused 76 (delta 72), pack-reused 77737 (from 4)

Receiving objects: 100% (77965/77965), 286.82 MiB | 15.73 MiB/s, done.

Resolving deltas: 100% (56335/56335), done.

-- The C compiler identification is GNU 11.4.0

-- The CXX compiler identification is GNU 11.4.0

-- Detecting C compiler ABI info

-- Detecting C compiler ABI info - done

-- Check for working C compiler: /usr/bin/cc - skipped

-- Detecting C compile features

-- Detecting C compile features - done

-- Detecting CXX compiler ABI info

-- Detecting CXX compiler ABI info - done

-- Check for working CXX compiler: /usr/bin/c++ - skipped

-- Detecting CXX compile features

-- Detecting CXX compile features - done

CMAKE_BUILD_TYPE=Release

-- Found Git: /usr/bin/git (found version "2.34.1")

-- The ASM compiler identification is GNU

-- Found assembler: /usr/bin/cc

-- Performing Test CMAKE_HAVE_LIBC_PTHREAD

-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success

-- Found Threads: TRUE

-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF

-- CMAKE_SYSTEM_PROCESSOR: x86_64

-- GGML_SYSTEM_ARCH: x86

-- Including CPU backend

-- Found OpenMP_C: -fopenmp (found version "4.5")

-- Found OpenMP_CXX: -fopenmp (found version "4.5")

-- Found OpenMP: TRUE (found version "4.5")

-- x86 detected

-- Adding CPU backend variant ggml-cpu: -march=native 

-- ggml version: 0.9.5

-- ggml commit:  0dfcd3b60

-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version "3.0.2")

-- Performing Test OPENSSL_VERSION_SUPPORTED

-- Performing Test OPENSSL_VERSION_SUPPORTED - Success

-- OpenSSL found: 3.0.2

-- Generating embedded license file for target: common

-- Configuring done (1.6s)

-- Generating done (0.3s)

-- Build files have been written to: /content/tmp/llama.cpp/build

[  0%] <span class="ansi-green-fg">Building CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o</span>

[  0%] <span class="ansi-green-fg ansi-bold">Linking CXX static library libcpp-httplib.a</span>

[  0%] Built target cpp-httplib

[  0%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o</span>

[  2%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o</span>

[  2%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o</span>

[  2%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o</span>

[  2%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o</span>

[  2%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o</span>

[  4%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o</span>

[  4%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o</span>

[  4%] <span class="ansi-green-fg ansi-bold">Linking CXX shared library ../../bin/libggml-base.so</span>

[  4%] Built target ggml-base

[  4%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o</span>

[  6%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o</span>

[  6%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o</span>

[  6%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o</span>

[  6%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o</span>

[  8%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o</span>

[  8%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o</span>

[  8%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o</span>

[  8%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o</span>

[ 11%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o</span>

[ 11%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o</span>

[ 11%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o</span>

[ 11%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o</span>

[ 13%] <span class="ansi-green-fg">Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o</span>

[ 13%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o</span>

[ 13%] <span class="ansi-green-fg ansi-bold">Linking CXX shared library ../../bin/libggml-cpu.so</span>

[ 13%] Built target ggml-cpu

[ 15%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o</span>

[ 15%] <span class="ansi-green-fg">Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o</span>

[ 15%] <span class="ansi-green-fg ansi-bold">Linking CXX shared library ../../bin/libggml.so</span>

[ 15%] Built target ggml

[ 15%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o</span>

[ 15%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o</span>

[ 17%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o</span>

[ 17%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o</span>

[ 17%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o</span>

[ 17%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o</span>

[ 20%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o</span>

[ 20%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o</span>

[ 20%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o</span>

[ 20%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o</span>

[ 22%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o</span>

[ 22%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o</span>

[ 22%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o</span>

[ 22%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o</span>

[ 24%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o</span>

[ 24%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o</span>

[ 24%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o</span>

[ 24%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o</span>

[ 26%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o</span>

[ 26%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o</span>

[ 26%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o</span>

[ 26%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o</span>

[ 26%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o</span>

[ 28%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o</span>

[ 28%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o</span>

[ 28%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o</span>

[ 28%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o</span>

[ 31%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o</span>

[ 31%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o</span>

[ 31%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o</span>

[ 31%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o</span>

[ 33%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o</span>

[ 33%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o</span>

[ 33%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o</span>

[ 33%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o</span>

[ 35%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o</span>

[ 35%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o</span>

[ 35%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o</span>

[ 35%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o</span>

[ 35%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o</span>

[ 37%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o</span>

[ 37%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o</span>

[ 37%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o</span>

[ 37%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o</span>

[ 40%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o</span>

[ 40%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o</span>

[ 40%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o</span>

[ 40%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o</span>

[ 42%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o</span>

[ 42%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o</span>

[ 42%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o</span>

[ 42%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o</span>

[ 44%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o</span>

[ 44%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o</span>

[ 44%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o</span>

[ 44%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o</span>

[ 46%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o</span>

[ 46%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o</span>

[ 46%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o</span>

[ 46%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o</span>

[ 46%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o</span>

[ 48%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o</span>

[ 48%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o</span>

[ 48%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o</span>

[ 48%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o</span>

[ 51%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o</span>

[ 51%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o</span>

[ 51%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o</span>

[ 51%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o</span>

[ 53%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o</span>

[ 53%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o</span>

[ 53%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o</span>

[ 53%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o</span>

[ 55%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o</span>

[ 55%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o</span>

[ 55%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o</span>

[ 55%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o</span>

[ 57%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o</span>

[ 57%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o</span>

[ 57%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o</span>

[ 57%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o</span>

[ 57%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o</span>

[ 60%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o</span>

[ 60%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o</span>

[ 60%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o</span>

[ 60%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o</span>

[ 62%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o</span>

[ 62%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o</span>

[ 62%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o</span>

[ 62%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o</span>

[ 64%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o</span>

[ 64%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o</span>

[ 64%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o</span>

[ 64%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o</span>

[ 66%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o</span>

[ 66%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o</span>

[ 66%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o</span>

[ 66%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o</span>

[ 68%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o</span>

[ 68%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o</span>

[ 68%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o</span>

[ 68%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o</span>

[ 68%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o</span>

[ 71%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o</span>

[ 71%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o</span>

[ 71%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o</span>

[ 71%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o</span>

[ 73%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o</span>

[ 73%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o</span>

[ 73%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o</span>

[ 73%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o</span>

[ 75%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o</span>

[ 75%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o</span>

[ 75%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o</span>

[ 75%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o</span>

[ 77%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o</span>

[ 77%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o</span>

[ 77%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o</span>

[ 77%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o</span>

[ 77%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o</span>

[ 80%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o</span>

[ 80%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o</span>

[ 80%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o</span>

[ 80%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o</span>

[ 82%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o</span>

[ 82%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o</span>

[ 82%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o</span>

[ 82%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o</span>

[ 84%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o</span>

[ 84%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o</span>

[ 84%] <span class="ansi-green-fg">Building CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o</span>

[ 84%] <span class="ansi-green-fg ansi-bold">Linking CXX shared library ../bin/libllama.so</span>

[ 84%] Built target llama

[ 84%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o</span>

[ 84%] Built target build_info

[ 84%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/arg.cpp.o</span>

[ 84%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o</span>

[ 84%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o</span>

[ 86%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o</span>

[ 86%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/chat.cpp.o</span>

[ 86%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/common.cpp.o</span>

[ 86%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/console.cpp.o</span>

[ 88%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/debug.cpp.o</span>

[ 88%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/download.cpp.o</span>

[ 88%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/json-partial.cpp.o</span>

[ 88%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o</span>

[ 91%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o</span>

[ 91%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/log.cpp.o</span>

[ 91%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o</span>

[ 91%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/ngram-map.cpp.o</span>

[ 93%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/ngram-mod.cpp.o</span>

[ 93%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o</span>

[ 93%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/preset.cpp.o</span>

[ 93%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o</span>

[ 93%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o</span>

[ 95%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o</span>

[ 95%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/unicode.cpp.o</span>

[ 95%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/jinja/lexer.cpp.o</span>

[ 95%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/jinja/parser.cpp.o</span>

[ 97%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/jinja/runtime.cpp.o</span>

[ 97%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/jinja/value.cpp.o</span>

[ 97%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/jinja/string.cpp.o</span>

[ 97%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/jinja/caps.cpp.o</span>

[100%] <span class="ansi-green-fg">Building CXX object common/CMakeFiles/common.dir/__/license.cpp.o</span>

[100%] <span class="ansi-green-fg ansi-bold">Linking CXX static library libcommon.a</span>

[100%] Built target common

[100%] <span class="ansi-green-fg">Building CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o</span>

[100%] <span class="ansi-green-fg ansi-bold">Linking CXX executable ../../bin/llama-quantize</span>

[100%] Built target llama-quantize
</pre>
</div>
</div>
</div>
</div>
</section>
<section id="how-to-quantize-a-model-3" class="slide level2">
<h2>How to Quantize a Model</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/quantization.ipynb" data-notebook-title="Quantization of Hugging Face model using llama.cpp" data-notebook-cellid="convert">
<div id="convert" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000,&quot;referenced_widgets&quot;:[&quot;ae44dee311524895926f9e8abe85e25a&quot;,&quot;d1da0dbc1b8d453fb6c740d42284191e&quot;,&quot;ebe59d82062f48acb14d90a7589fdb92&quot;,&quot;19faff8ac6bd4c1f963eaff0cd36575b&quot;,&quot;06caea8b08674c928090c7672ae471e4&quot;,&quot;7df4f34f4593486faaead96cf6876032&quot;,&quot;fcc82250e51546f0bb8f5449b9241c60&quot;,&quot;429e052d7c234d5c989f718978a194e9&quot;,&quot;9e39c89dcb2a4595993d578108d85fbb&quot;,&quot;09efed9120334bf986783fff5fd076da&quot;,&quot;7cc3c04122044d8ab102f7a196324e62&quot;,&quot;a1e304a7afc7480fa3bf4cca5acbf413&quot;,&quot;6846af1e73874a2f9892b72daa8e8918&quot;,&quot;29a7646006b145c5bf9ebc26019a0921&quot;,&quot;87b1d00a16954c4e97e6fa1c49b5cc27&quot;,&quot;b02b701b76fa4262b437298a11088a9d&quot;,&quot;49db6beebb0140f5930cf7d532d2454d&quot;,&quot;a607fd74f4b14ce7949db2a12a867a1a&quot;,&quot;a2fb68ecde6b4e4f90e6fe0d528a1f75&quot;,&quot;4f67de272467463fbc5f7234513cd0da&quot;,&quot;06aa74172e7940508c67303fb454ae16&quot;,&quot;d83d1c14aa23406d869503afbae312fa&quot;]}}" data-outputid="5c4c9d8f-15af-4979-c93a-88f227f1e4f5">
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ae44dee311524895926f9e8abe85e25a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a1e304a7afc7480fa3bf4cca5acbf413","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>INFO:hf-to-gguf:Loading model: Qwen3-0.6B
INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM
INFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:output.weight,             torch.bfloat16 --&gt; F16, shape = {1024, 151936}
INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --&gt; F16, shape = {1024, 151936}
INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --&gt; F16, shape = {3072, 1024}
INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 3072}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --&gt; F16, shape = {2048, 1024}
INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.bfloat16 --&gt; F32, shape = {128}
INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 2048}
INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --&gt; F16, shape = {1024, 1024}
INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --&gt; F32, shape = {1024}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 40960
INFO:hf-to-gguf:gguf: embedding length = 1024
INFO:hf-to-gguf:gguf: feed forward length = 3072
INFO:hf-to-gguf:gguf: head count = 16
INFO:hf-to-gguf:gguf: key-value head count = 8
WARNING:hf-to-gguf:Unknown RoPE type: default
INFO:hf-to-gguf:gguf: rope scaling type = NONE
INFO:hf-to-gguf:gguf: rope theta = 1000000
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model quantization version
INFO:hf-to-gguf:Set model tokenizer
INFO:numexpr.utils:NumExpr defaulting to 12 threads.
INFO:gguf.vocab:Adding 151387 merge(s).
INFO:gguf.vocab:Setting special token type eos to 151645
INFO:gguf.vocab:Setting special token type pad to 151643
INFO:gguf.vocab:Setting special token type bos to 151643
INFO:gguf.vocab:Setting add_bos_token to False
INFO:gguf.vocab:Setting chat_template to {%- if tools %}
    {{- '&lt;|im_start|&gt;system\n' }}
    {%- if messages[0].role == 'system' %}
        {{- messages[0].content + '\n\n' }}
    {%- endif %}
    {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\n&lt;tools&gt;" }}
    {%- for tool in tools %}
        {{- "\n" }}
        {{- tool | tojson }}
    {%- endfor %}
    {{- "\n&lt;/tools&gt;\n\nFor each function call, return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags:\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;, \"arguments\": &lt;args-json-object&gt;}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n" }}
{%- else %}
    {%- if messages[0].role == 'system' %}
        {{- '&lt;|im_start|&gt;system\n' + messages[0].content + '&lt;|im_end|&gt;\n' }}
    {%- endif %}
{%- endif %}
{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
{%- for message in messages[::-1] %}
    {%- set index = (messages|length - 1) - loop.index0 %}
    {%- if ns.multi_step_tool and message.role == "user" and message.content is string and not(message.content.startswith('&lt;tool_response&gt;') and message.content.endswith('&lt;/tool_response&gt;')) %}
        {%- set ns.multi_step_tool = false %}
        {%- set ns.last_query_index = index %}
    {%- endif %}
{%- endfor %}
{%- for message in messages %}
    {%- if message.content is string %}
        {%- set content = message.content %}
    {%- else %}
        {%- set content = '' %}
    {%- endif %}
    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
        {{- '&lt;|im_start|&gt;' + message.role + '\n' + content + '&lt;|im_end|&gt;' + '\n' }}
    {%- elif message.role == "assistant" %}
        {%- set reasoning_content = '' %}
        {%- if message.reasoning_content is string %}
            {%- set reasoning_content = message.reasoning_content %}
        {%- else %}
            {%- if '&lt;/think&gt;' in content %}
                {%- set reasoning_content = content.split('&lt;/think&gt;')[0].rstrip('\n').split('&lt;think&gt;')[-1].lstrip('\n') %}
                {%- set content = content.split('&lt;/think&gt;')[-1].lstrip('\n') %}
            {%- endif %}
        {%- endif %}
        {%- if loop.index0 &gt; ns.last_query_index %}
            {%- if loop.last or (not loop.last and reasoning_content) %}
                {{- '&lt;|im_start|&gt;' + message.role + '\n&lt;think&gt;\n' + reasoning_content.strip('\n') + '\n&lt;/think&gt;\n\n' + content.lstrip('\n') }}
            {%- else %}
                {{- '&lt;|im_start|&gt;' + message.role + '\n' + content }}
            {%- endif %}
        {%- else %}
            {{- '&lt;|im_start|&gt;' + message.role + '\n' + content }}
        {%- endif %}
        {%- if message.tool_calls %}
            {%- for tool_call in message.tool_calls %}
                {%- if (loop.first and content) or (not loop.first) %}
                    {{- '\n' }}
                {%- endif %}
                {%- if tool_call.function %}
                    {%- set tool_call = tool_call.function %}
                {%- endif %}
                {{- '&lt;tool_call&gt;\n{"name": "' }}
                {{- tool_call.name }}
                {{- '", "arguments": ' }}
                {%- if tool_call.arguments is string %}
                    {{- tool_call.arguments }}
                {%- else %}
                    {{- tool_call.arguments | tojson }}
                {%- endif %}
                {{- '}\n&lt;/tool_call&gt;' }}
            {%- endfor %}
        {%- endif %}
        {{- '&lt;|im_end|&gt;\n' }}
    {%- elif message.role == "tool" %}
        {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
            {{- '&lt;|im_start|&gt;user' }}
        {%- endif %}
        {{- '\n&lt;tool_response&gt;\n' }}
        {{- content }}
        {{- '\n&lt;/tool_response&gt;' }}
        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
            {{- '&lt;|im_end|&gt;\n' }}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {{- '&lt;|im_start|&gt;assistant\n' }}
    {%- if enable_thinking is defined and enable_thinking is false %}
        {{- '&lt;think&gt;\n\n&lt;/think&gt;\n\n' }}
    {%- endif %}
{%- endif %}
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:/content/ggufs/Qwen3-0.6B-F16.gguf: n_tensors = 311, total_size = 1.5G
Writing: 100% 1.50G/1.50G [00:05&lt;00:00, 287Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to /content/ggufs/Qwen3-0.6B-F16.gguf
main: build = 7916 (0dfcd3b60)
main: built with GNU 11.4.0 for Linux x86_64
main: quantizing '/content/ggufs/Qwen3-0.6B-F16.gguf' to '/content/ggufs/Qwen3-0.6B-Q4_K_M.gguf' as Q4_K_M
llama_model_loader: loaded meta data with 37 key-value pairs and 311 tensors from /content/ggufs/Qwen3-0.6B-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                     general.sampling.top_k i32              = 20
llama_model_loader: - kv   3:                     general.sampling.top_p f32              = 0.950000
llama_model_loader: - kv   4:                      general.sampling.temp f32              = 0.600000
llama_model_loader: - kv   5:                               general.name str              = Qwen3 0.6B
llama_model_loader: - kv   6:                           general.basename str              = Qwen3
llama_model_loader: - kv   7:                         general.size_label str              = 0.6B
llama_model_loader: - kv   8:                            general.license str              = apache-2.0
llama_model_loader: - kv   9:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-0.6...
llama_model_loader: - kv  10:                   general.base_model.count u32              = 1
llama_model_loader: - kv  11:                  general.base_model.0.name str              = Qwen3 0.6B Base
llama_model_loader: - kv  12:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  13:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-0.6...
llama_model_loader: - kv  14:                               general.tags arr[str,1]       = ["text-generation"]
llama_model_loader: - kv  15:                          qwen3.block_count u32              = 28
llama_model_loader: - kv  16:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv  17:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  18:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  19:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  20:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  21:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  22:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  23:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  24:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  25:                          general.file_type u32              = 1
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&amp;", "'", ...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  34:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '&lt;|im_start|&gt;...
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type  f16:  198 tensors
[   1/ 311]                        output.weight - [ 1024, 151936,     1,     1], type =    f16, converting to q6_K .. size =   296.75 MiB -&gt;   121.71 MiB
[   2/ 311]                   output_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[   3/ 311]                    token_embd.weight - [ 1024, 151936,     1,     1], type =    f16, converting to q4_K .. size =   296.75 MiB -&gt;    83.46 MiB
[   4/ 311]                  blk.0.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[   5/ 311]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[   6/ 311]               blk.0.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[   7/ 311]             blk.0.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[   8/ 311]                  blk.0.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[   9/ 311]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  10/ 311]                  blk.0.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[  11/ 311]                blk.0.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[  12/ 311]                blk.0.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  13/ 311]                blk.0.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  14/ 311]                  blk.0.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  15/ 311]                  blk.1.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  16/ 311]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  17/ 311]               blk.1.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  18/ 311]             blk.1.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  19/ 311]                  blk.1.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  20/ 311]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  21/ 311]                  blk.1.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[  22/ 311]                blk.1.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[  23/ 311]                blk.1.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  24/ 311]                blk.1.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  25/ 311]                  blk.1.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  26/ 311]                  blk.2.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  27/ 311]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  28/ 311]               blk.2.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  29/ 311]             blk.2.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  30/ 311]                  blk.2.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  31/ 311]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  32/ 311]                  blk.2.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[  33/ 311]                blk.2.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[  34/ 311]                blk.2.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  35/ 311]                blk.2.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  36/ 311]                  blk.2.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  37/ 311]                  blk.3.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  38/ 311]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  39/ 311]               blk.3.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  40/ 311]             blk.3.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  41/ 311]                  blk.3.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  42/ 311]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  43/ 311]                  blk.3.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  44/ 311]                blk.3.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  45/ 311]                blk.3.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  46/ 311]                blk.3.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  47/ 311]                  blk.3.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  48/ 311]                  blk.4.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  49/ 311]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  50/ 311]               blk.4.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  51/ 311]             blk.4.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  52/ 311]                  blk.4.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  53/ 311]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  54/ 311]                  blk.4.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  55/ 311]                blk.4.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  56/ 311]                blk.4.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  57/ 311]                blk.4.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  58/ 311]                  blk.4.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  59/ 311]                  blk.5.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  60/ 311]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  61/ 311]               blk.5.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  62/ 311]             blk.5.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  63/ 311]                  blk.5.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  64/ 311]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  65/ 311]                  blk.5.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[  66/ 311]                blk.5.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[  67/ 311]                blk.5.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  68/ 311]                blk.5.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  69/ 311]                  blk.5.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  70/ 311]                  blk.6.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  71/ 311]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  72/ 311]               blk.6.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  73/ 311]             blk.6.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  74/ 311]                  blk.6.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  75/ 311]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  76/ 311]                  blk.6.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  77/ 311]                blk.6.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  78/ 311]                blk.6.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  79/ 311]                blk.6.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  80/ 311]                  blk.6.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  81/ 311]                  blk.7.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  82/ 311]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  83/ 311]               blk.7.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  84/ 311]             blk.7.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  85/ 311]                  blk.7.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  86/ 311]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  87/ 311]                  blk.7.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  88/ 311]                blk.7.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  89/ 311]                blk.7.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  90/ 311]                blk.7.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  91/ 311]                  blk.7.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[  92/ 311]                  blk.8.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[  93/ 311]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  94/ 311]               blk.8.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[  95/ 311]             blk.8.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  96/ 311]                  blk.8.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[  97/ 311]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  98/ 311]                  blk.8.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[  99/ 311]                blk.8.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 100/ 311]                blk.8.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 101/ 311]                blk.8.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 102/ 311]                  blk.8.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 103/ 311]                  blk.9.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 104/ 311]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 105/ 311]               blk.9.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 106/ 311]             blk.9.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 107/ 311]                  blk.9.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 108/ 311]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 109/ 311]                  blk.9.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 110/ 311]                blk.9.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 111/ 311]                blk.9.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 112/ 311]                blk.9.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 113/ 311]                  blk.9.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 114/ 311]                 blk.10.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 115/ 311]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 116/ 311]              blk.10.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 117/ 311]            blk.10.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 118/ 311]                 blk.10.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 119/ 311]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 120/ 311]                 blk.10.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 121/ 311]               blk.10.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 122/ 311]               blk.10.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 123/ 311]               blk.10.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 124/ 311]                 blk.10.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 125/ 311]                 blk.11.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 126/ 311]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 127/ 311]              blk.11.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 128/ 311]            blk.11.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 129/ 311]                 blk.11.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 130/ 311]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 131/ 311]                 blk.11.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[ 132/ 311]               blk.11.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 133/ 311]               blk.11.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 134/ 311]               blk.11.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 135/ 311]                 blk.11.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 136/ 311]                 blk.12.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 137/ 311]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 138/ 311]              blk.12.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 139/ 311]            blk.12.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 140/ 311]                 blk.12.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 141/ 311]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 142/ 311]                 blk.12.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 143/ 311]               blk.12.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 144/ 311]               blk.12.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 145/ 311]               blk.12.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 146/ 311]                 blk.12.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 147/ 311]                 blk.13.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 148/ 311]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 149/ 311]              blk.13.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 150/ 311]            blk.13.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 151/ 311]                 blk.13.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 152/ 311]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 153/ 311]                 blk.13.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 154/ 311]               blk.13.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 155/ 311]               blk.13.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 156/ 311]               blk.13.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 157/ 311]                 blk.13.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 158/ 311]                 blk.14.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 159/ 311]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 160/ 311]              blk.14.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 161/ 311]            blk.14.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 162/ 311]                 blk.14.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 163/ 311]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 164/ 311]                 blk.14.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[ 165/ 311]               blk.14.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 166/ 311]               blk.14.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 167/ 311]               blk.14.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 168/ 311]                 blk.14.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 169/ 311]                 blk.15.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 170/ 311]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 171/ 311]              blk.15.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 172/ 311]            blk.15.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 173/ 311]                 blk.15.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 174/ 311]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 175/ 311]                 blk.15.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 176/ 311]               blk.15.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 177/ 311]               blk.15.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 178/ 311]               blk.15.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 179/ 311]                 blk.15.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 180/ 311]                 blk.16.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 181/ 311]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 182/ 311]              blk.16.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 183/ 311]            blk.16.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 184/ 311]                 blk.16.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 185/ 311]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 186/ 311]                 blk.16.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 187/ 311]               blk.16.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 188/ 311]               blk.16.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 189/ 311]               blk.16.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 190/ 311]                 blk.16.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 191/ 311]                 blk.17.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 192/ 311]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 193/ 311]              blk.17.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 194/ 311]            blk.17.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 195/ 311]                 blk.17.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 196/ 311]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 197/ 311]                 blk.17.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[ 198/ 311]               blk.17.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 199/ 311]               blk.17.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 200/ 311]               blk.17.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 201/ 311]                 blk.17.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 202/ 311]                 blk.18.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 203/ 311]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 204/ 311]              blk.18.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 205/ 311]            blk.18.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 206/ 311]                 blk.18.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 207/ 311]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 208/ 311]                 blk.18.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 209/ 311]               blk.18.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 210/ 311]               blk.18.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 211/ 311]               blk.18.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 212/ 311]                 blk.18.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 213/ 311]                 blk.19.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 214/ 311]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 215/ 311]              blk.19.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 216/ 311]            blk.19.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 217/ 311]                 blk.19.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 218/ 311]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 219/ 311]                 blk.19.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 220/ 311]               blk.19.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 221/ 311]               blk.19.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 222/ 311]               blk.19.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 223/ 311]                 blk.19.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 224/ 311]                 blk.20.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 225/ 311]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 226/ 311]              blk.20.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 227/ 311]            blk.20.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 228/ 311]                 blk.20.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 229/ 311]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 230/ 311]                 blk.20.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[ 231/ 311]               blk.20.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 232/ 311]               blk.20.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 233/ 311]               blk.20.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 234/ 311]                 blk.20.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 235/ 311]                 blk.21.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 236/ 311]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 237/ 311]              blk.21.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 238/ 311]            blk.21.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 239/ 311]                 blk.21.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 240/ 311]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 241/ 311]                 blk.21.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 242/ 311]               blk.21.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 243/ 311]               blk.21.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 244/ 311]               blk.21.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 245/ 311]                 blk.21.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 246/ 311]                 blk.22.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 247/ 311]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 248/ 311]              blk.22.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 249/ 311]            blk.22.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 250/ 311]                 blk.22.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 251/ 311]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 252/ 311]                 blk.22.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 253/ 311]               blk.22.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 254/ 311]               blk.22.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 255/ 311]               blk.22.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 256/ 311]                 blk.22.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 257/ 311]                 blk.23.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 258/ 311]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 259/ 311]              blk.23.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 260/ 311]            blk.23.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 261/ 311]                 blk.23.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 262/ 311]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 263/ 311]                 blk.23.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[ 264/ 311]               blk.23.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 265/ 311]               blk.23.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 266/ 311]               blk.23.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 267/ 311]                 blk.23.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 268/ 311]                 blk.24.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 269/ 311]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 270/ 311]              blk.24.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 271/ 311]            blk.24.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 272/ 311]                 blk.24.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 273/ 311]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 274/ 311]                 blk.24.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[ 275/ 311]               blk.24.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 276/ 311]               blk.24.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 277/ 311]               blk.24.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 278/ 311]                 blk.24.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 279/ 311]                 blk.25.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 280/ 311]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 281/ 311]              blk.25.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 282/ 311]            blk.25.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 283/ 311]                 blk.25.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 284/ 311]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 285/ 311]                 blk.25.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[ 286/ 311]               blk.25.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 287/ 311]               blk.25.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 288/ 311]               blk.25.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 289/ 311]                 blk.25.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 290/ 311]                 blk.26.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 291/ 311]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 292/ 311]              blk.26.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 293/ 311]            blk.26.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 294/ 311]                 blk.26.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 295/ 311]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 296/ 311]                 blk.26.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[ 297/ 311]               blk.26.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 298/ 311]               blk.26.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 299/ 311]               blk.26.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 300/ 311]                 blk.26.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 301/ 311]                 blk.27.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB -&gt;     0.56 MiB
[ 302/ 311]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 303/ 311]              blk.27.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 304/ 311]            blk.27.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 305/ 311]                 blk.27.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB -&gt;     1.12 MiB
[ 306/ 311]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 307/ 311]                 blk.27.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB -&gt;     0.82 MiB
[ 308/ 311]               blk.27.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB -&gt;     2.46 MiB
[ 309/ 311]               blk.27.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
[ 310/ 311]               blk.27.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB
[ 311/ 311]                 blk.27.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB -&gt;     1.69 MiB
llama_model_quantize_impl: model size  =  1433.75 MiB
llama_model_quantize_impl: quant size  =   456.11 MiB

main: quantize time = 17247.52 ms
main:    total time = 17247.52 ms</code></pre>
</div>
</div>
</div>
</section></section>
<section>
<section id="calling-llama.cpp-from-apps" class="title-slide slide level1 center">
<h1>Calling llama.cpp from Apps</h1>

</section>
<section id="calling-llama.cpp-from-apps-1" class="slide level2">
<h2>Calling llama.cpp from Apps</h2>
<ul>
<li class="fragment">Three ways to interact with a local GGUF model:</li>
<li class="fragment"><ol type="1">
<li class="fragment">Use llama.cpp to serve the model via a local web server</li>
</ol>
<ul>
<li class="fragment">Easier to get running, but the overhead of a local web server</li>
</ul></li>
<li class="fragment"><ol start="2" type="1">
<li class="fragment">If your app is C++, call llama.cpp library directly</li>
</ol></li>
<li class="fragment"><ol start="3" type="1">
<li class="fragment">If your app is not C++, use llama.cpp bindings</li>
</ol></li>
<li class="fragment">(2 and 3 are more challenging to get setup, but no web server and better performance)</li>
</ul>
</section></section>
<section>
<section id="calling-llama.cpp-from-unreal-engine" class="title-slide slide level1 center">
<h1>Calling llama.cpp from Unreal Engine</h1>

</section>
<section id="calling-llama.cpp-from-unreal-engine-1" class="slide level2">
<h2>Calling llama.cpp from Unreal Engine</h2>
<ul>
<li class="fragment">C++, so can call llama.cpp directly
<ul>
<li class="fragment">Build the llama.cpp libraries for your platform</li>
<li class="fragment">Create and compile an Unreal plug-in</li>
<li class="fragment">Create a Blueprint that calls the plug-in</li>
<li class="fragment">Integrate within scene</li>
</ul></li>
</ul>
</section></section>
<section id="demo-2" class="title-slide slide level1 center">
<h1>Demo</h1>
<p>Unreal Engine Plug-in for llama.cpp</p>
</section>

<section>
<section id="llama.cpp-bindings" class="title-slide slide level1 center">
<h1>llama.cpp Bindings</h1>

</section>
<section id="llama.cpp-bindings-1" class="slide level2">
<h2>llama.cpp Bindings</h2>
</section>
<section id="what-are-llama.cpp-bindings" class="slide level2">
<h2>What Are llama.cpp Bindings?</h2>
<ul>
<li class="fragment">Wrappers around the llama.cpp C/C++ libraries</li>
<li class="fragment">Enable you to use models via the language/platform of your choice</li>
<li class="fragment">What languages/platforms are supported?
<ul>
<li class="fragment"><a href="https://github.com/ggml-org/llama.cpp#description" class="external" target="_blank">https://github.com/ggml-org/llama.cpp#description</a></li>
<li class="fragment">Click on the “Bindings” drop-down</li>
</ul></li>
</ul>
</section>
<section id="python-binding" class="slide level2">
<h2>Python Binding</h2>
<ul>
<li class="fragment">llama-cpp-python
<ul>
<li class="fragment"><a href="https://github.com/abetlen/llama-cpp-python" class="external" target="_blank">https://github.com/abetlen/llama-cpp-python</a></li>
<li class="fragment">OpenAI-like API</li>
<li class="fragment">Supports chat completions, function/tool calling, structured outputs, and multimodal models.</li>
</ul></li>
</ul>
</section>
<section id="python-binding-1" class="slide level2">
<h2>Python Binding</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/python-binding.ipynb" data-notebook-title="Using llama.cpp Binding for Python" data-notebook-cellid="load-model">
<div id="load-model" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="im">from</span> llama_cpp <span class="im">import</span> Llama</span>
<span id="cb5-2"><a></a></span>
<span id="cb5-3"><a></a>GGUF_MODEL <span class="op">=</span> <span class="ss">f"../code/gguf/gemma-3-1b-it-Q4_K_M.gguf"</span></span>
<span id="cb5-4"><a></a></span>
<span id="cb5-5"><a></a>llm <span class="op">=</span> Llama(</span>
<span id="cb5-6"><a></a>      model_path<span class="op">=</span>GGUF_MODEL,</span>
<span id="cb5-7"><a></a>      chat_format<span class="op">=</span><span class="st">"gemma"</span></span>
<span id="cb5-8"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="python-binding-2" class="slide level2">
<h2>Python Binding</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/python-binding.ipynb" data-notebook-title="Using llama.cpp Binding for Python" data-notebook-cellid="chat">
<div id="chat" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>llm.create_chat_completion(</span>
<span id="cb6-2"><a></a>      messages <span class="op">=</span> [</span>
<span id="cb6-3"><a></a>          {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"You are a helpful assistant."</span>},</span>
<span id="cb6-4"><a></a>          {</span>
<span id="cb6-5"><a></a>              <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb6-6"><a></a>              <span class="st">"content"</span>: <span class="st">"Hello"</span></span>
<span id="cb6-7"><a></a>          }</span>
<span id="cb6-8"><a></a>      ]</span>
<span id="cb6-9"><a></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</section>
<section id="python-binding-3" class="slide level2">
<h2>Python Binding</h2>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/CS-394/CS-394/src/05/notebooks/python-binding.ipynb" data-notebook-title="Using llama.cpp Binding for Python" data-notebook-cellid="chat">
<div id="chat" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>{'id': 'chatcmpl-2887bb2b-b1a5-428d-a42b-46c83a97fb6b',
 'object': 'chat.completion',
 'created': 1770147671,
 'model': '../code/gguf/gemma-3-1b-it-Q4_K_M.gguf',
 'choices': [{'index': 0,
   'message': {'role': 'assistant',
    'content': "Hello there! How's your day going so far? 😊 \n\nIs there anything you'd like to chat about, or need any help with?"},
   'logprobs': None,
   'finish_reason': 'stop'}],
 'usage': {'prompt_tokens': 10, 'completion_tokens': 32, 'total_tokens': 42}}</code></pre>
</div>
</div>
</div>
</section>
<section id="c-binding" class="slide level2">
<h2>C# Binding</h2>
<ul>
<li class="fragment">LLamaSharp
<ul>
<li class="fragment"><a href="https://github.com/SciSharp/LLamaSharp" class="external" target="_blank">https://github.com/SciSharp/LLamaSharp</a></li>
</ul></li>
<li class="fragment">Installed via NuGet</li>
<li class="fragment">Supports multiple backends (Cpu, CUDA, and Vulkan)</li>
</ul>
</section>
<section id="using-the-c-binding-in-unity" class="slide level2">
<h2>Using the C# Binding in Unity</h2>
<ul>
<li class="fragment">Install Unity (2022.3 LTS)</li>
<li class="fragment">Install NuGet for Unity package manager</li>
<li class="fragment">Using NuGet, install the LlamaSharp package and correct backend</li>
<li class="fragment">(Possibly) copy the dylibs into Assets/Plugins/[platform] and load</li>
<li class="fragment">Create a LlamaTest.cs script</li>
</ul>
</section></section>
<section id="demo-3" class="title-slide slide level1 center">
<h1>Demo</h1>
<p>Unity/LLamaSharp Demo</p>
</section>

<section>
<section id="local-models-in-the-browser" class="title-slide slide level1 center">
<h1>Local Models in the Browser</h1>

</section>
<section id="local-models-in-the-browser-1" class="slide level2">
<h2>Local Models in the Browser</h2>
<ul>
<li class="fragment">Small models can also run in the browser</li>
<li class="fragment">Inference runs on WebAssembly (WASM)</li>
<li class="fragment">CPU or WebGPU options</li>
<li class="fragment">Browser limits most models to &lt;2Gb, possibly smaller depending on VRAM</li>
</ul>
</section>
<section id="local-models-in-the-browser-2" class="slide level2">
<h2>Local Models in the Browser</h2>
<ul>
<li class="fragment">Wllama (<a href="https://github.com/ngxson/wllama">https://github.com/ngxson/wllama</a>){.external target=“_blank”}
<ul>
<li class="fragment">CPU only (using SIMD for better performance)</li>
<li class="fragment">JavaScript-based API</li>
<li class="fragment">Download models directly from HF</li>
<li class="fragment">Cached for future use</li>
</ul></li>
</ul>
</section>
<section id="local-models-in-the-browser-3" class="slide level2">
<h2>Local Models in the Browser</h2>
<pre><code>await wllama.loadModelFromHF(
    'ggml-org/models',
    'tinyllamas/stories260K.gguf',
    {
      progressCallback,
    }
  );
const outputText = await wllama.createCompletion(prompt, {
  nPredict: 50,
  sampling: {
    temp: 0.5,
    top_k: 40,
    top_p: 0.9,
  },
});</code></pre>
</section>
<section id="local-models-in-the-browser-4" class="slide level2">
<h2>Local Models in the Browser</h2>
<ul>
<li class="fragment">WebLLM (<a href="https://webllm.mlc.ai/">https://webllm.mlc.ai/</a>){.external target=“_blank”}
<ul>
<li class="fragment">Uses WebGPU for better performance</li>
<li class="fragment">JavaScript-based API and playground</li>
<li class="fragment">Download models directly from HF, but need to be compiled to MLC format</li>
<li class="fragment">Popular pre-compiled models available</li>
</ul></li>
</ul>
</section>
<section id="local-models-in-the-browser-5" class="slide level2">
<h2>Local Models in the Browser</h2>
<pre><code>const appConfig: webllm.AppConfig = {
  model_list: [
    {
      model: `https://huggingface.co/simonguest/Qwen3-0.6B-it-code-hint-3`,
      overrides: {
        temperature: 0.5,
      },
    },
  ],
};
engine = await webllm.CreateMLCEngine("Qwen3-0.6B-it-code-hint-3", {
  appConfig,
  initProgressCallback,
});

const chunks = await engine.chat.completions.create({
  messages: [
    { role: 'system', content: SYSTEM_PROMPT},
    { role: 'user', content: PROMPT }],
  temperature: 0.7,
  stream: true,
});</code></pre>
</section></section>
<section>
<section id="looking-ahead" class="title-slide slide level1 center">
<h1>Looking Ahead</h1>

</section>
<section id="looking-ahead-1" class="slide level2">
<h2>Looking Ahead</h2>
<ul>
<li class="fragment"><a href="https://simonguest.github.io/CS-394/src/05/assignment.html" class="external" target="_blank">This week’s assignment!</a></li>
<li class="fragment">Increasing model accuracy</li>
<li class="fragment">Better prompt engineering</li>
<li class="fragment">RAG (Retreival Augmented Generation)</li>
<li class="fragment">Introduce fine-tuning and synthetic data generation</li>
</ul>
</section></section>
<section>
<section id="references" class="title-slide slide level1 center">
<h1>References</h1>

</section>
<section id="references-1" class="slide level2">
<h2>References</h2>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../theme/logos/DigiPen_RGB_Red.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.15,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/simonguest\.github\.io\/CS-394\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>