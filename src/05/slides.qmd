---
title: "Module 5: Running Models on Local Hardware"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap

- Understood the fundamentals and history of diffuser models
- Explored and used models that demonstrate text-to-image, image-to-image, inpainting, outpainting, and ControlNet
- Setup and used Replicate to create a custom pipeline of production-grade models
- Understood the fundamentals and history of Vision Encoders and VLMs
- Implemented/tested a local VLM model for on-device inference

## Lesson Objectives

- Understand the use cases, advantages/disadvantages for running models on local hardware - desktop, web, mobile
- Understand hardware requirements and architectures for model inference - e.g., CUDA vs. ONNX vs. MLX vs. WebGPU
- Explore how quantization works and understand techniques and formats for quantizing existing models
- Use llama.cpp to quantize and run an SLM on local hardware/gaming PC
- Integrate a quantized model within Unity/Unreal/WebAssembly

# Why Local Models?

## Why Local Models?

- **Privacy**
  - Every call you make to OpenAI/Claude/OpenRouter may (or may not) get logged and/or be used for training purposes
  - Many organizations don't want their customer/financial data logged with an AI vendor
  - There may also be legal regulations/restrictions controlling this

## Why Local Models?

- **Offline**
  - Every call you make to OpenAI/Claude/OpenRouter needs an Internet connection
  - That's not always guaranteed!
  - Education is a good example - remote school in India and/or rural districts here in the US

## Why Local Models?

- **Latency**
  - Even with a network connection, calls can suffer from increased latency
  - Can be a challenge if your application needs frequent, quick responses
  - e.g., using a VLM to determine the contents of a video stream for a user with vision impairments

## Why Local Models?

- **Cost**
  - While per-API costs are fractions of a cent, these can grow out of control with exponential growth
  - More pronounced for long conversation threads (think call center)
  - Or agents with verbose tool call JSON requests/responses

# What's Your Hardware?

## What's Your Hardware?

- NVIDIA (CUDA)
- AMD (ROCm)
- Apple Silicon
- Various NPU (Neural Processing Unit) vendors

## NVIDIA CUDA

- CUDA (Compute Unified Device Architecture)
  - Launched in 2006 to introduce programming on GPUs (GPGPUs or General Purpose GPUs)
  - A C-like programming interface
  - Perfectly timed for the deep learning revolution of the 2010s
  - Additional libraries (e.g., cuBLAS, cuDNN) make CUDA the defacto standard today

## How CUDA Works

- Massive parallelism: CUDA exploits thousands of GPU cores simultaneously, making it ideal for matrix operations.
- Memory hierarchy: A tiered memory system with global, shared, and registers.
- Kernel execution: Programs can launch kernels - same function that can operate on different data.

## NVIDIA CUDA - Hardware Support

- Consumer: RTX 40- and 50- cards with various VRAM options (8Gb - 24Gb) for local inference and small fine-tuning tasks. RTX 30- series still popular for education.
- Laptop: RTX 40- and 50- series also available on laptops (although less performant than discrete cards)
- Workstation: DGX Spark launch in 2025, with GB10 and 128Gb of unified memory for medium fine-tuning tasks
- Datacenter GPUs: A/H series and GB-series for datacenters. NVLink for multi-GPU interconnectivity.

## Sidebar: TOPS

- TOPS (Terra Operations Per Second)
  - How many trillion operations a processor can perform per second
  - Often qualified with the data type
  - 64 INT8 TOPS == 64 trillion 8-bit operations per second
- TFLOPS (Terra Floating-Point Operations Per Second)
  - 1 TFLOPS == 1 FP32 (32-bit floating point) TOPS

## Sidebar: TOPS

- Rough Throughput Calculations
  - You have an NVIDIA 3090 (advertized at 35 TFLOPS)
  - Assume a 7B param model with FP32 weights
  - Each token generation requires ~2 FLOPs per parameter
  - Each token generation ~= 14B FLOPs (7B params × 2)
  - Theoretical max = 35T FLOPs/sec ÷ 14B FLOPs/token ≈ 2,500 tokens/sec
  - Reality: 10-100 tokens/sec typical due to memory bandwidth bottlenecks

## AMD ROCm

- ROCm (Radeon Open Compute)
  - Launched in 2016 as an open-source alternative to CUDA
  - Embraced open standards (e.g., OpenCL), positioning as avoiding vendor lock-in, although this fragmentation initially hurt adoption
  - Has evolved significantly since (e.g., rocBLAS) although ecosystem gaps compared to CUDA persist

## AMD ROCm - Hardware Support

- Consumer: RX7000 series offer sustantial VRAM (up to 24Gb) at competitive prices compared to NVIDIA RTX
- Laptop: Some laptop options for AMD-based machines
- Workstation: Strix Halo, competitor to DGX Spark, with RX8060S and 128Gb unified memory
- Software support: Linux only with no Windows support (some via WSL)

## Apple Silicon

- Apple Silicon
  - Metal, a low-level graphics and compute API, launched in 2014 and later expanded for general GPU compute tasks
  - MPS (Metal Performance Shaders) introduced in 2017 and optimized primitives for neural networks. PyTorch added MPS device support in 2022.
  - MLX released in 2023, providing NumPy-like API for Apple Silicon hardware

## Apple Silicon - Hardware Support

- Available on all M-series hardware
- Unified memory by default, upto 128Gb on laptops and 512Gb for the Mac Studio with M3 Ultra
- Non portable models. (MLX uses safetensors/npz format and MLX-specific code for Metal.)

## Sidebar: Unified Memory

- GPUs have historically had separate memory (VRAM)
- Unified memory is a process to share memory between CPU and GPU
  - For Apple/MLX, it's a true SoC (System on a Chip); NVIDIA DGX Spark, two physical components connected via NVLink-C2C
- Higher memory availability, but lower memory bandwidth
  - ~275Gb/s for Spark/MLX; ~1TB/s for 5080; ~1.7TB/s for 5090; ~3TBs for H100

## NPUs

- NPUs (Neural Processing Units) are specialized AI accelerators, designed for lower power consumption
- Optimized specifically for NN operations (e.g., matmul, convolutions, activations)
- Commonly found in edge devices (smartphones, IoT, embedded systems)
- 15-80 TOPS common for NPUs (~10x less that desktop PCI-based GPUs)

## NPU Vendors

- Intel: Acquired Movidius in 2016; released Myriad X in 2017 and a neural compute stick. Superceded by NPUs in Core Ultra processors.
- Qualcomm: Snapdragon 865 range in 2019; now Snapdragon X and 8 ranges. Used in Windows/ARM devices. Popular with Android, although Google recently moved to their own TPUs.
- AMD: XDNA formerly Xilinx; Windows Copilot PC range competing with SnapDragon and Intel Core Ultra.
- Apple: ANE (Apple Neural Engine) to support CoreML workloads on iPhone, iPad devices

# Compute and Memory Challenges

## Compute and Memory Challenges

- Compute Challenges
  - Local GPUs significantly slower than datacenter-class GPUs
  - 4060 Ti (~350 TOPS) vs. H100 (~3500+ TOPS)
  - One local GPU vs. interconnected datacenter-class GPUs (using NVLink)
- Less TOPS = slower inference (tokens/second)

## Compute and Memory Challenges

- Memory Challenges
  - Consumer-grade GPUs and NPUs have significanly less memory than datacenter-class GPUs
  - 8Gb/16Gb VRAM on 4060 Ti vs. 80/94Gb for H100
  - (Also memory bandwidth is typically 10x-20x)
  - (Combined memory architecture using NVLink)
- Less memory = only smaller models can run
  - Rougly speaking, size of model must be smaller than available VRAM

# Overcoming Compute Challenges

## Overcoming Compute Challenges

- MoE (Mixture of Experts)
  - Original concept dates back to 1991. Jacobs et al. publish "Adaptive Mixture of Local Experts" showing subnetworks and a gating mechanism
  - In 2017, Shazeer et al. (Google) publish "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer" demonstrating activating only a small subset of experts per input
  - 2022:  Google releases Switch Transformers (Fedus et al.), simplifying MoE by routing each token to just a single expert
  - 2023: Mixtral 8x7B (Mistral AI) brings high-quality open-source MoE to the mainstream, becoming a standard architecture for efficient large-scale models

## Popular MoE Models

- Mixtral (Mistral)
- Qwen MoE (Alibaba)
- DeepSeek MoE
- Phi MoE (Microsoft's SLMs)
- Nemotron (NVIDIA)

## Overcoming Compute Challenges

- Why MoE?
  - Faster Inference (token/second): Because you are only using a subset of experts for each token.
  - Allows you to use larger models, where you would be otherwise compute-bound
    - Dense 13B model ~= 13Gb VRAM
    - Mixtral 8x7B ~= 47GB VRAM (8×7B experts + shared layers), but faster inference than dense 47B (only ~2 experts active per token)

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#load-model echo=true outputs=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#register-hooks echo=true outputs=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-0 echo=true outputs=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-0 echo=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-8 echo=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-16 echo=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-24 echo=false >}}

## How do MoE Models Work?

{{< embed notebooks/moe-heatmap.ipynb#routing-31 echo=false >}}

## How do MoE Models Work

- What the Heatmaps Show
  - Different tokens activate different experts (token-level routing)
  - Routing changes across layers (layer 0 vs. layer 31)
    - Early layers: syntax/grammar
    - Late layers: semantics/reasoning
  - Not all experts used equally (specialization emerges)
  - Router makes soft decisions (distributes probability across multiple experts)

## How do MoEs Get Trained?

- From scratch
  - Train the entire model (experts and router)
  - Router networks learn which expert to select
  - Experts learn their specializations
  - Example: Mixtral 8x7B
- Start from a trained dense model (called upcycling)
  - Replicate each layer into multiple experts; add router networks
  - Typically faster and more stable vs. starting from scratch
  - Example: Qwen1.5-MoE

## How do MoEs Get Trained?

- Combine multiple models (FrankenMoE / MoErge)
  - Newer, community-driven approach
  - Take specialized models (e.g., math, coding, chat) and use their FFN layers as separate experts; add router network
  - Train only the router (experts frozen initially)
  - Cheaper than training from scratch, but often lower quality
  - Example: Beyonder-4x7B

## How do MoEs Get Trained?

- Key challenge: Load balancing / Expert collapse
- Healthy MoE:
  - Expert 0: 8% of tokens
  - Expert 1: 7% of tokens
  - Expert 2: 9% of tokens
  - (all experts get reasonable usage)
- Collapsed MoE:
  - Expert 0: 45% of tokens
  - Expert 1: 40% of tokens
  - Expert 2-7: <15% combined (essentially dead) 

# Hands-on

Run the MoE heatmap; investigate other layers

Potentially swap out Phi model with other MoE from HF

## Overcoming Compute Challenges

- MoE gives us speed with quality
  - But we still can't fit it on consumer hardware
  - Popular MoE models often exceed the amount of available VRAM
  - We need a way to reduce the memory footprint of the model...

# Overcoming Memory Challenges

## Overcoming Memory Challenges




## Quantization

Process of reducing the precision of a model's weights and activations. For example, 16-bit numbers to 4-bit.

- Parameter count matters more than precision
  - A 70B parameter model at 4-bit often beats a 13B model at b16
  - The models knowledge remains largely intact
  - Often the extra precision doesn't meaningfully improve outputs

## Quantization Formats

The llama.cpp project (implementing LLMs in pure C/C++) has driven advancements in quantization

- GGUF (GPT-Generated Unified Format)
  - Single file architecture
  - Model format supporting multiple quantization levels (2-bit through 8-bit) with CPU and GPU handoff
- MLX (Apple's ML framework and format for Apple Silicon)
  - Debuted in late 2023
  - Supports 4 and 8 bit quantization schemes

## Running Quantized Models

- Tools built upon llama.cpp
  - Ollama, LM Studio, koboldcpp

## Demo: C# Client <-> Gemma 3 27B Local {.center}

demos/01/lmstudio-client/LMStudioClient.csproj

## Hosting Models in Unity

- Download the GGUF model locally to Assets/StreamingAssets folder
- Use llama.cpp bindings for C# to host
  - LLAMASharp: https://github.com/SciSharp/LLamaSharp
- Use OpenAI SDK (or similar) as client
- Unity Demo
  - https://github.com/eublefar/LLAMASharpUnityDemo

# Looking Ahead

## Looking Ahead

- [This week's assignment!](https://simonguest.github.io/CS-394/src/05/assignment.html){.external target="_blank"}
- TBD

# References

## References
