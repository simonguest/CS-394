---
title: "Week 1: Foundations of Generative AI"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: [default, ../../theme/digipen.scss]
    highlight-style: github
    width: 1050
    height: 700
    margin: 0.15
    mermaid:
      theme: neutral
logo: ../../theme/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Lesson Objectives

- Explore the history of vector embeddings and tokenization
- Understand the transformer architecture at a high level
- Use our first transformer to translate language
- Cover a brief history of early generative transformers
- Setup and use Colab, and become familiar with the basics of notebooks and Python (if you haven’t used them already)

# Let's Rewind To 2013...

## Rewind To 2013

- NLP (Natural Language Processing) was the thing!
  - Sentiment analysis, named entity recognition, parsing, etc.
- But, you had limited options...
  - One-hot encoding
  - Hand crafted features
  - Neural language models

## 2013: Word2Vec Released

- Word2Vec introduced by Mikolov and colleagues at Google Research in two papers
  - Skip-gram and Continuous Bag-of-Words (CBOW) [@mikolov2013efficient]
  - Negative sampling and subsampling techniques [@mikolov2013distributed]
- Paradigm shift from count-based methods
  - Used Neural Networks (NNs) to predict words vs. large matrices
- Foundation for modern NLP tasks

## How does Word2Vec Work?

- Word Embeddings are **meaningful numerical representations** of words
  - Representations where words are encoded into multi-dimensional space
  - Large number of dimensions (200-500 is typical)
  - Similar words have similar numbers

## How does Word2Vec Work?

{{< embed notebooks/word2vec.ipynb#word-1 echo=true >}}

## How does Word2Vec Work?

{{< embed notebooks/word2vec.ipynb#word-2 echo=true >}}

## How does Word2Vec Work?

{{< embed notebooks/word2vec.ipynb#word-3 echo=true >}}

## Why Do This?

- Mapping words to multi-dimensional vectors enables
  - Test for similarity
  - Compute similarity
  - Perform vector arithmetic
  - Explore sets of words through visualizations

## How does Word2Vec Work?

{{< embed notebooks/word2vec.ipynb#similar-words echo=true >}}

## How does Word2Vec Work?

{{< embed notebooks/word2vec.ipynb#compute-similarity echo=true >}}

## How does Word2Vec Work?

{{< embed notebooks/word2vec.ipynb#vector-arithmetic echo=true >}}

## How does Word2Vec Work?

{{< embed notebooks/word2vec.ipynb#2d-visualization >}}

# Let's Run Some Code!

# Introducing Notebooks

## What is a Notebook?

- An **interactive document** that combines:
  - Live code that can be executed
  - Rich text explanations (markdown)
  - Visualizations and outputs
- Think of it as a **computational narrative**
  - Tell a story with code, data, and explanations
- Originally designed for data science and research
- Also used for learning, experimenting, and sharing results

## A Brief History of Notebooks

- **2011**: IPython Notebook project begins
  - Interactive Python shell → web-based notebook
- **2014**: Renamed to **Jupyter** (Julia, Python, R)
  - Now supports 40+ programming languages
  - Python is most popular by far
- **2017**: Google launches **Colab**
  - Free cloud-based Jupyter notebooks
  - Free access to GPUs and TPUs
- **Today**: Industry standard for ML/AI development

## Anatomy of a Python Notebook

- **Format**: Extension is .ipynb
  - JSON format, using Jupyter Document Schema
- **Cells**: Building blocks of notebooks
  - **Code cells**: Executable Python code
  - **Markdown cells**: Text, headings, images, equations
- **Kernel**: The computational engine running your code
  - Maintains state between cell executions
- **Outputs**: Results appear directly below code cells
  - Text, tables, plots, interactive widgets

## How to Run Notebooks

- **Jupyter Notebook Server** (Classic approach)
   - Web interface on localhost
- **VS Code** (Local development)
   - Jupyter extension for VS Code
   - Run on your own machine
- **Google Colab** (Recommended)
   - Browser-based, no installation needed
   - Free(-ish) GPU access
   - Can also access local GPU

## Advantages of Google Colab

- Access to GPUs and TPUs for AI-based tasks
  - e.g., A100 and H100 with 40Gb/80Gb VRAM
- Model downloaded between cloud vendors
  - vs. downloading large models via the DigiPen network
- Many libraries pre-installed
- Easy to share notebooks with others
- Generous (free) GPU limits for students!

# Demo

Hello World and Word2Vec notebooks in **Colab**, **VS Code**, and **Local Jupyter server**

# Hands-On

Setup Colab, get the two notebooks up and running (hello-world, word2vec)

## Challenges with Word Embeddings

- Large vocabularies
  - 100K+ words
  - And not particularly friendly to non-English vocabularies
- Little representation between certain words
  - "Run" and "Running" should be related
- Lack of context
  - Embedding for the word "bank" is the same, regardless of context
  - **River bank** != **Savings bank**

## Challenges with Word Embeddings

- Some researchers tried character-level models
  - Small vocabulary (26 letters + puntuation for English)
  - But very long sequences
  - And hard to extra meaning

## 2016: Byte Pair Encoding (BPE)

- Originally developed in 1994 as a simple compression algorithm [@gage1994new]
  - Frequent pairs of adjacent bytes represented as a single byte
- In 2016, adapted to neural machine translation [@sennrich-etal-2016-neural]
  - Applied BPE to break words into subword units for better handling of rare words

## 2016: Byte Pair Encoding (BPE)

- Breaks words into frequent subword units (a.k.a. **tokens**)
  - "unbelievable" → **["un", "believ", "able"]**
- Balance between word level (large vocab) and character level (long sequences)
  - Supports related words: **["Run"]** and **["Run", "ning"]**
  - Supports unknown words
  - 30-50K tokens vs. 100K
  - Also works well for non-English languages

## Search for Context

- BPE provided efficiency and representation between words
- But still didn't solve context
  - e.g., the **River bank** != **Savings bank** problem
- Researchers working on "attention tasks" using Recurrent Neural Networks (RNNs)
  - Bahdanau et al. introduce attention for translation [@bahdanau2015neural]
  - Showed that focusing on relevant parts of input improved translation quality

## 2017: "Attention is all you need"

- Google researchers publish "Attention is all you need" [@vaswani2017attention]
  - Introduced the **Transformer** a novel Neural Network (NN) architecture, eliminating the need for RNNs for sequence-to-sequence models
  - Used BPE tokenization, and creates contextual embeddings during training process
  - Attention mechanism allows the model to weigh the importance of words in a sequence
  - Achieved State Of The Art (SOTA) performance on language translation, while also being faster to train

# Introducing the Transformer

## Introducing the Transformer

```{mermaid}
%%| fig-height: 100%
%%| label: transformer-1
%%| file: diagrams/transformer-1.mmd
```

## Example

{{< embed notebooks/translation-transformer.ipynb#load-model echo=true >}}

## Example

{{< embed notebooks/translation-transformer.ipynb#tokenize echo=true >}}

## Example

{{< embed notebooks/translation-transformer.ipynb#transformer echo=true >}}

## Example

{{< embed notebooks/translation-transformer.ipynb#decode echo=true >}}

## Introducing the Transformer

```{mermaid}
%%| fig-height: 100%
%%| label: transformer-1
%%| file: diagrams/transformer-1.mmd
```

## Introducing the Transformer

```{mermaid}
%%| fig-height: 100%
%%| label: transformer-2
%%| file: diagrams/transformer-2.mmd
```

## Introducing the Transformer

```{mermaid}
%%| fig-height: 100%
%%| label: transformer-3
%%| file: diagrams/transformer-3.mmd
```

## How Does the Encoder/Decoder Work?

`output_ids = model.generate(input_ids)`

- Takes input ids, runs through encoder
  - Generates contextual vectors using self attention across input tokens
- Runs the decoder iteratively to generate **one token at a time**
  - Uses self attention on previously generated tokens
  - Uses cross-attention to attend to encoder output
- Continues until it generates an end-of-sequence token or hits max length

## Introducing the Transformer

```{mermaid}
%%| fig-height: 100%
%%| label: transformer-4
%%| file: diagrams/transformer-4.mmd
```

# Demo

Translation Transformer in Colab

# Hands-On

Experiment with your own phrases in the translation-transformer.ipynb notebook

# 2018: Origin of "GPT"

## 2018: Origin of "GPT"

- **G**enerative **P**re-trained **T**ransformer
- Name coined by OpenAI researchers in "Improving Language Understanding by Generative Pre-Training" [@radford2018improving]

## What is a GPT?

- "Decoder-only" architecture
  - Self attention is causal/masked - tokens can only attend to previous tokens, not future ones
- Pre-training objective: Next token prediction
  - Trained on a massive text corpora
  - Learns grammar, facts, reasoning patterns just from this objective

## What is a GPT?

- Autoregressive generation
  - Generates one token at a time, feeding back each output as input
  - Temperature and sampling strategies
  - Same prompt can produce different outputs
- Context window
  - Fixed maximum length (2048 for GPT-2)
  - Everything must fit within this window during generation
  - Introduced the concept of "context" vs. "knowledge" (prompt vs. training)

## GPT-2

- Released in 2019 by OpenAI
  - Initially, only 117M param model released in Feb 2019 due to safety concerns
  - Staged releases throughout the year, 1.5B in Nov 2019
- Trained on WebText, 8 million web pages/40GB of text
- Zero-shot task performance
  - Did well on translation, summarization, and question answering without task-specific training

## Example

{{< embed notebooks/GPT-2.ipynb#load-model echo=true >}}

## Example

{{< embed notebooks/GPT-2.ipynb#autocomplete echo=true >}}

## Example

{{< embed notebooks/GPT-2.ipynb#prompts echo=true >}}

# Demo

GPT-2 notebook in Colab

# Hands-On

Experiment with your own phrases in the GPT-2.ipynb notebook

# References

## References
