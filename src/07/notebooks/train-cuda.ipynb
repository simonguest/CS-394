{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3a97e9",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning on CUDA\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/simonguest/CS-394/blob/main/src/07/notebooks/train-cuda.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/simonguest/CS-394/raw/refs/heads/main/src/07/notebooks/train-cuda.ipynb\">\n",
    "  <img src=\"https://img.shields.io/badge/Download_.ipynb-blue\" alt=\"Download .ipynb\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14cd517",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e119297",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc10de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATASET_REPO = \"simonguest/test-dataset\" # Change this to your own dataset location on HF\n",
    "BASE_MODEL = \"google/gemma-3-1b-it\" # Base model that will be fine-tuned\n",
    "MODEL_NAME = \"code-explainer\" # Name of the model you want to create\n",
    "os.environ[\"WANDB_PROJECT\"] = MODEL_NAME # WandB project name\n",
    "\n",
    "MODEL_FOLDER = f\"./models/{MODEL_NAME}\"\n",
    "\n",
    "# Training run parameters\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "# LoRA parameters\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Use 4-bit quantization for efficiency (QLoRA)\n",
    "USE_4BIT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ef94b",
   "metadata": {},
   "source": [
    "## API keys and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import userdata # type:ignore\n",
    "  os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
    "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "  print(\"HF and WANDB API Tokens set for Colab\")\n",
    "else:\n",
    "  load_dotenv()\n",
    "  print(\"Loaded env vars from .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0da88",
   "metadata": {},
   "source": [
    "## Load dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05403d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(DATASET_REPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eaa8b2",
   "metadata": {},
   "source": [
    "## Format dataset for correct chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def format_chat_template(example):\n",
    "    messages = example[\"messages\"]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "formatted_dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    remove_columns=dataset['train'].column_names # type: ignore\n",
    ")\n",
    "\n",
    "print(\"\\nFormatted example:\")\n",
    "print(formatted_dataset['train'][0]['text'][:500])  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5c266",
   "metadata": {},
   "source": [
    "## Load base model with QLoRA configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca4006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "if USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "if USE_4BIT:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2f20d",
   "metadata": {},
   "source": [
    "## Create training configuration and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "report_to = \"none\"\n",
    "if os.environ.get(\"WANDB_API_KEY\") != None:\n",
    "  report_to = \"wandb\"\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_FOLDER,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=report_to,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Save the tokenizer model as this won't change during training\n",
    "tokenizer.save_pretrained(f\"{MODEL_FOLDER}/lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d061b",
   "metadata": {},
   "source": [
    "# Train and save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(f\"{MODEL_FOLDER}/lora\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
